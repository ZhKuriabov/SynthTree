{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a008799",
   "metadata": {},
   "outputs": [],
   "source": [
    "from synth_tree_original.DECISION_TREE_CLASSIFIER import DistanceDecisionTree as dtc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "161eea90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://test.pypi.org/simple/\n",
      "Collecting synth-tree-package==0.0.2\n",
      "  Obtaining dependency information for synth-tree-package==0.0.2 from https://test-files.pythonhosted.org/packages/e6/5e/4281ec4a1d90ef85eb80befaa94dea9ca7c49d61eb98cb601dc17c03a6ae/synth_tree_package-0.0.2-py3-none-any.whl.metadata\n",
      "  Downloading https://test-files.pythonhosted.org/packages/e6/5e/4281ec4a1d90ef85eb80befaa94dea9ca7c49d61eb98cb601dc17c03a6ae/synth_tree_package-0.0.2-py3-none-any.whl.metadata (506 bytes)\n",
      "Downloading https://test-files.pythonhosted.org/packages/e6/5e/4281ec4a1d90ef85eb80befaa94dea9ca7c49d61eb98cb601dc17c03a6ae/synth_tree_package-0.0.2-py3-none-any.whl (8.0 kB)\n",
      "Installing collected packages: synth-tree-package\n",
      "  Attempting uninstall: synth-tree-package\n",
      "    Found existing installation: synth_tree_package 0.0.1\n",
      "    Uninstalling synth_tree_package-0.0.1:\n",
      "      Successfully uninstalled synth_tree_package-0.0.1\n",
      "Successfully installed synth-tree-package-0.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -i https://test.pypi.org/simple/ synth-tree-package==0.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8357b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, roc_auc_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ccf70cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(388, 73) (388,) 73\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('AGE', 69),\n",
       " ('DAYS_TO_COLLECTION', 357),\n",
       " ('ETHNICITY', 2),\n",
       " ('FORM_COMPLETION_DATE', 104),\n",
       " ('FRACTION_GENOME_ALTERED', 372),\n",
       " ('HISTORY_NEOADJUVANT_TRTYN', 2),\n",
       " ('HISTORY_OTHER_MALIGNANCY', 2),\n",
       " ('ICD_10', 39),\n",
       " ('ICD_O_3_HISTOLOGY', 10),\n",
       " ('ICD_O_3_SITE', 35),\n",
       " ('INITIAL_PATHOLOGIC_DX_YEAR', 31),\n",
       " ('NEW_TUMOR_EVENT_AFTER_INITIAL_TREATMENT', 2),\n",
       " ('OCT_EMBEDDED', 2),\n",
       " ('OS_MONTHS', 368),\n",
       " ('PRIMARY_MELANOMA_KNOWN_DX', 2),\n",
       " ('PROSPECTIVE_COLLECTION', 2),\n",
       " ('RADIATION_TREATMENT_ADJUVANT', 2),\n",
       " ('RETROSPECTIVE_COLLECTION', 2),\n",
       " ('SAMPLE_COUNT', 2),\n",
       " ('SAMPLE_INITIAL_WEIGHT', 76),\n",
       " ('SAMPLE_TYPE', 2),\n",
       " ('SAMPLE_TYPE_ID', 2),\n",
       " ('SEX', 2),\n",
       " ('SUBMITTED_TUMOR_DX_DAYS_TO', 260),\n",
       " ('VIAL_NUMBER', 2),\n",
       " ('AJCC_NODES_PATHOLOGIC_PN0', 2),\n",
       " ('AJCC_NODES_PATHOLOGIC_PN1', 2),\n",
       " ('AJCC_NODES_PATHOLOGIC_PN2', 2),\n",
       " ('AJCC_NODES_PATHOLOGIC_PN3', 2),\n",
       " ('AJCC_NODES_PATHOLOGIC_PN4', 2),\n",
       " ('AJCC_NODES_PATHOLOGIC_PN5', 2),\n",
       " ('AJCC_NODES_PATHOLOGIC_PN6', 2),\n",
       " ('AJCC_NODES_PATHOLOGIC_PN7', 2),\n",
       " ('AJCC_NODES_PATHOLOGIC_PN8', 2),\n",
       " ('AJCC_STAGING_EDITION0', 2),\n",
       " ('AJCC_STAGING_EDITION1', 2),\n",
       " ('AJCC_STAGING_EDITION2', 2),\n",
       " ('AJCC_STAGING_EDITION3', 2),\n",
       " ('AJCC_STAGING_EDITION4', 2),\n",
       " ('AJCC_STAGING_EDITION5', 2),\n",
       " ('RACE0', 2),\n",
       " ('RACE1', 2),\n",
       " ('TISSUE_SOURCE_SITE0', 2),\n",
       " ('TISSUE_SOURCE_SITE1', 2),\n",
       " ('TISSUE_SOURCE_SITE2', 2),\n",
       " ('TISSUE_SOURCE_SITE3', 2),\n",
       " ('TISSUE_SOURCE_SITE4', 2),\n",
       " ('TISSUE_SOURCE_SITE5', 2),\n",
       " ('TISSUE_SOURCE_SITE6', 2),\n",
       " ('TISSUE_SOURCE_SITE7', 2),\n",
       " ('TISSUE_SOURCE_SITE8', 2),\n",
       " ('TISSUE_SOURCE_SITE9', 2),\n",
       " ('TISSUE_SOURCE_SITE10', 2),\n",
       " ('TISSUE_SOURCE_SITE11', 2),\n",
       " ('TISSUE_SOURCE_SITE12', 2),\n",
       " ('TISSUE_SOURCE_SITE13', 2),\n",
       " ('TISSUE_SOURCE_SITE14', 2),\n",
       " ('TISSUE_SOURCE_SITE15', 2),\n",
       " ('TISSUE_SOURCE_SITE16', 2),\n",
       " ('TISSUE_SOURCE_SITE17', 2),\n",
       " ('TISSUE_SOURCE_SITE18', 2),\n",
       " ('TISSUE_SOURCE_SITE19', 2),\n",
       " ('TISSUE_SOURCE_SITE20', 2),\n",
       " ('TUMOR_SITE0', 2),\n",
       " ('TUMOR_SITE1', 2),\n",
       " ('TUMOR_SITE2', 2),\n",
       " ('TUMOR_SITE3', 2),\n",
       " ('TUMOR_SITE4', 2),\n",
       " ('TUMOR_SITE5', 2),\n",
       " ('TUMOR_SITE6', 2),\n",
       " ('TUMOR_SITE7', 2),\n",
       " ('TUMOR_SITE8', 2),\n",
       " ('TUMOR_SITE9', 2)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = './data/TCGA/'\n",
    "df = pd.read_csv(path + \"TCGA_skcm.csv\")\n",
    "\n",
    "data = df.iloc[:,1:]\n",
    "orig_headers = list(data)\n",
    "outcome_header = \"OS_STATUS\"\n",
    "\n",
    "feature_names = list(data)\n",
    "feature_names.remove(outcome_header)\n",
    "\n",
    "one_vars = np.where(np.array([len(np.unique(x)) for x in np.array(data).T])==1)[0]\n",
    "for i in [orig_headers[i] for i in one_vars]:\n",
    "    feature_names.remove(i)\n",
    "    \n",
    "Xo = data[feature_names].values\n",
    "y = data[outcome_header].values\n",
    "\n",
    "\n",
    "cate_vars = np.where((np.array([len(np.unique(x)) for x in np.array(Xo).T])<=12) &\n",
    "                    (np.array([len(np.unique(x)) for x in np.array(Xo).T])>2))[0]\n",
    "cont_vars = np.where((np.array([len(np.unique(x)) for x in np.array(Xo).T])>12) |\n",
    "                    (np.array([len(np.unique(x)) for x in np.array(Xo).T])<=2))[0]\n",
    "cate_vars = np.setdiff1d(cate_vars,[10])\n",
    "cate_vars = np.sort(np.append(cate_vars,27))\n",
    "cont_vars = np.setdiff1d(cont_vars,[27])\n",
    "cont_vars = np.sort(np.append(cont_vars,10))\n",
    "\n",
    "X_cate = Xo[:,cate_vars]\n",
    "X_cont = Xo[:,cont_vars]\n",
    "names_cont = [feature_names[i] for i in cont_vars]\n",
    "names_cate = [feature_names[i] for i in cate_vars] \n",
    "\n",
    "\n",
    "X_cont_min, X_cont_max = X_cont.min(axis=0), X_cont.max(axis=0)\n",
    "X_cont = (X_cont-X_cont_min)/(X_cont_max - X_cont_min + 1e-9)\n",
    "    \n",
    "df_cate = [pd.get_dummies(Xo[:,i], drop_first=True).values for i in cate_vars]\n",
    "X_cate = np.hstack(df_cate)\n",
    "a = sum([list(np.repeat(names_cate[i],j)) for i,j in enumerate([x.shape[1] for x in df_cate])],[])\n",
    "b = sum([list(range(i)) for i in [x.shape[1] for x in df_cate]],[])\n",
    "names_cate = ['{}{}'.format(a[i],b[i]) for i in range(len(b))]     \n",
    "    \n",
    "X = np.hstack([X_cont,X_cate])\n",
    "feature_names = sum([names_cont,names_cate],[]) \n",
    "\n",
    "\n",
    "print(X.shape, y.shape, len(feature_names))\n",
    "list(zip(feature_names,[len(np.unique(X[:,i])) for i in range(X.shape[1])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e45d4c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>AGE</th>\n",
       "      <th>AJCC_NODES_PATHOLOGIC_PN</th>\n",
       "      <th>AJCC_STAGING_EDITION</th>\n",
       "      <th>DAYS_TO_COLLECTION</th>\n",
       "      <th>DAYS_TO_INITIAL_PATHOLOGIC_DIAGNOSIS</th>\n",
       "      <th>ETHNICITY</th>\n",
       "      <th>FORM_COMPLETION_DATE</th>\n",
       "      <th>FRACTION_GENOME_ALTERED</th>\n",
       "      <th>HISTORY_NEOADJUVANT_TRTYN</th>\n",
       "      <th>HISTORY_OTHER_MALIGNANCY</th>\n",
       "      <th>ICD_10</th>\n",
       "      <th>ICD_O_3_HISTOLOGY</th>\n",
       "      <th>ICD_O_3_SITE</th>\n",
       "      <th>INFORMED_CONSENT_VERIFIED</th>\n",
       "      <th>INITIAL_PATHOLOGIC_DX_YEAR</th>\n",
       "      <th>IS_FFPE</th>\n",
       "      <th>NEW_TUMOR_EVENT_AFTER_INITIAL_TREATMENT</th>\n",
       "      <th>OCT_EMBEDDED</th>\n",
       "      <th>OS_MONTHS</th>\n",
       "      <th>OS_STATUS</th>\n",
       "      <th>PRIMARY_MELANOMA_KNOWN_DX</th>\n",
       "      <th>PROSPECTIVE_COLLECTION</th>\n",
       "      <th>RACE</th>\n",
       "      <th>RADIATION_TREATMENT_ADJUVANT</th>\n",
       "      <th>RETROSPECTIVE_COLLECTION</th>\n",
       "      <th>SAMPLE_COUNT</th>\n",
       "      <th>SAMPLE_INITIAL_WEIGHT</th>\n",
       "      <th>SAMPLE_TYPE</th>\n",
       "      <th>SAMPLE_TYPE_ID</th>\n",
       "      <th>SEX</th>\n",
       "      <th>SUBMITTED_TUMOR_DX_DAYS_TO</th>\n",
       "      <th>TISSUE_SOURCE_SITE</th>\n",
       "      <th>TUMOR_SITE</th>\n",
       "      <th>VIAL_NUMBER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>77</td>\n",
       "      <td>0.5784</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15.70</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>400</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>457</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "      <td>0.3233</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>22.93</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>254</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3453</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>0.3384</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2004</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>29.04</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>536</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>75</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>2994</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0580</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>2003</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>23.75</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>58</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>63</td>\n",
       "      <td>0.3817</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>18.17</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>474</td>\n",
       "      <td>85</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>203</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>78</td>\n",
       "      <td>0.2086</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>20.17</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>220</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>475</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>5332</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5553</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>1996</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>176.41</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>170</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4679</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>476</td>\n",
       "      <td>46</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1086</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>74</td>\n",
       "      <td>0.2394</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>26.71</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>631</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>478</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2339</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.1207</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>2004</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>115.87</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1633</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>479</td>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>93</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "      <td>0.4571</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>15.51</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1010</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>388 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  AGE  ...  TUMOR_SITE  VIAL_NUMBER\n",
       "0             1   34  ...           8            1\n",
       "1             2   56  ...           9            1\n",
       "2             3   66  ...           9            1\n",
       "3             5   75  ...          11            1\n",
       "4             6   58  ...           9            1\n",
       "..          ...  ...  ...         ...          ...\n",
       "383         474   85  ...           2            1\n",
       "384         475   45  ...           9            1\n",
       "385         476   46  ...           2            1\n",
       "386         478   46  ...           2            1\n",
       "387         479   56  ...           2            1\n",
       "\n",
       "[388 rows x 35 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d265eb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37779a71",
   "metadata": {},
   "source": [
    "# MLM Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8275b682",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-27 16:28:48.408749: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-27 16:28:48.498694: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-27 16:28:48.501802: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-27 16:28:50.493006: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from generalized_mlm_2 import MixtureLinearModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39a336a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = [clf_1, clf_2, clf_3]\n",
    "\n",
    "sorted_clf = []\n",
    "for i in clfs:\n",
    "    sorted_clf.append((i, roc_auc_score(i.predict(X_test), y_test)))\n",
    "sorted_clf = sorted(sorted_clf, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "074306bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(GradientBoostingClassifier(), 0.7479619565217391),\n",
       " (RandomForestClassifier(), 0.7514950166112956),\n",
       " (MLPClassifier(alpha=1e-07, hidden_layer_sizes=(15, 2), random_state=1,\n",
       "                solver='lbfgs'),\n",
       "  0.8134920634920635)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76972bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_variance = np.mean(np.var(X_train, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cc67238",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLM = MixtureLinearModel(sorted_clf, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95898a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 (73,)\n",
      "# of CELL:26 / min size:6 / avg size:11.9 / max size:19 / # of singleton CELL:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "MLM.compute_kmeans_CELL(X_train, K=13, random_seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e140f657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of clf: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:00<00:00, 312.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: 0 model: MLPClassifier(alpha=1e-07, hidden_layer_sizes=(15, 2), random_state=1,\n",
      "              solver='lbfgs')\n",
      "cluster: 1 model: MLPClassifier(alpha=1e-07, hidden_layer_sizes=(15, 2), random_state=1,\n",
      "              solver='lbfgs')\n",
      "cluster: 2 model: MLPClassifier(alpha=1e-07, hidden_layer_sizes=(15, 2), random_state=1,\n",
      "              solver='lbfgs')\n",
      "cluster: 3 model: MLPClassifier(alpha=1e-07, hidden_layer_sizes=(15, 2), random_state=1,\n",
      "              solver='lbfgs')\n",
      "cluster: 4 model: RandomForestClassifier()\n",
      "cluster: 5 model: RandomForestClassifier()\n",
      "cluster: 6 model: MLPClassifier(alpha=1e-07, hidden_layer_sizes=(15, 2), random_state=1,\n",
      "              solver='lbfgs')\n",
      "cluster: 7 model: MLPClassifier(alpha=1e-07, hidden_layer_sizes=(15, 2), random_state=1,\n",
      "              solver='lbfgs')\n",
      "cluster: 8 model: MLPClassifier(alpha=1e-07, hidden_layer_sizes=(15, 2), random_state=1,\n",
      "              solver='lbfgs')\n",
      "cluster: 9 model: MLPClassifier(alpha=1e-07, hidden_layer_sizes=(15, 2), random_state=1,\n",
      "              solver='lbfgs')\n",
      "cluster: 10 model: MLPClassifier(alpha=1e-07, hidden_layer_sizes=(15, 2), random_state=1,\n",
      "              solver='lbfgs')\n",
      "cluster: 11 model: MLPClassifier(alpha=1e-07, hidden_layer_sizes=(15, 2), random_state=1,\n",
      "              solver='lbfgs')\n",
      "cluster: 12 model: MLPClassifier(alpha=1e-07, hidden_layer_sizes=(15, 2), random_state=1,\n",
      "              solver='lbfgs')\n",
      "cluster: 13 model: RandomForestClassifier()\n",
      "cluster: 14 model: MLPClassifier(alpha=1e-07, hidden_layer_sizes=(15, 2), random_state=1,\n",
      "              solver='lbfgs')\n",
      "cluster: 15 model: MLPClassifier(alpha=1e-07, hidden_layer_sizes=(15, 2), random_state=1,\n",
      "              solver='lbfgs')\n",
      "cluster: 16 model: MLPClassifier(alpha=1e-07, hidden_layer_sizes=(15, 2), random_state=1,\n",
      "              solver='lbfgs')\n",
      "cluster: 17 model: MLPClassifier(alpha=1e-07, hidden_layer_sizes=(15, 2), random_state=1,\n",
      "              solver='lbfgs')\n",
      "cluster: 18 model: RandomForestClassifier()\n",
      "cluster: 19 model: MLPClassifier(alpha=1e-07, hidden_layer_sizes=(15, 2), random_state=1,\n",
      "              solver='lbfgs')\n",
      "cluster: 20 model: MLPClassifier(alpha=1e-07, hidden_layer_sizes=(15, 2), random_state=1,\n",
      "              solver='lbfgs')\n",
      "cluster: 21 model: MLPClassifier(alpha=1e-07, hidden_layer_sizes=(15, 2), random_state=1,\n",
      "              solver='lbfgs')\n",
      "cluster: 22 model: MLPClassifier(alpha=1e-07, hidden_layer_sizes=(15, 2), random_state=1,\n",
      "              solver='lbfgs')\n",
      "cluster: 23 model: MLPClassifier(alpha=1e-07, hidden_layer_sizes=(15, 2), random_state=1,\n",
      "              solver='lbfgs')\n",
      "cluster: 24 model: MLPClassifier(alpha=1e-07, hidden_layer_sizes=(15, 2), random_state=1,\n",
      "              solver='lbfgs')\n",
      "cluster: 25 model: MLPClassifier(alpha=1e-07, hidden_layer_sizes=(15, 2), random_state=1,\n",
      "              solver='lbfgs')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/26 [00:00<?, ?it/s] This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " This problem is unconstrained.\n",
      " 27%|██▋       | 7/26 [00:00<00:00, 65.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  2.56905D-01\n",
      "\n",
      "At iterate    1    f=  5.72221D-01    |proj g|=  6.06522D-02\n",
      "\n",
      "At iterate    2    f=  5.50401D-01    |proj g|=  5.19383D-02\n",
      "\n",
      "At iterate    3    f=  4.39989D-01    |proj g|=  1.37121D-01\n",
      "\n",
      "At iterate    4    f=  3.47991D-01    |proj g|=  1.33231D-01\n",
      "\n",
      "At iterate    5    f=  1.78039D-01    |proj g|=  5.69949D-02\n",
      "\n",
      "At iterate    6    f=  1.07655D-01    |proj g|=  2.60137D-02\n",
      "\n",
      "At iterate    7    f=  6.45759D-02    |proj g|=  1.06375D-02\n",
      "\n",
      "At iterate    8    f=  3.79004D-02    |proj g|=  4.21217D-03\n",
      "\n",
      "At iterate    9    f=  2.01670D-02    |proj g|=  1.98609D-03\n",
      "\n",
      "At iterate   10    f=  9.87092D-03    |proj g|=  8.42262D-04\n",
      "\n",
      "At iterate   11    f=  4.75410D-03    |proj g|=  1.02391D-03\n",
      "\n",
      "At iterate   12    f=  2.36284D-03    |proj g|=  6.42062D-04\n",
      "\n",
      "At iterate   13    f=  1.24187D-03    |proj g|=  2.66420D-04\n",
      "\n",
      "At iterate   14    f=  6.13362D-04    |proj g|=  2.76019D-04\n",
      "\n",
      "At iterate   15    f=  5.38249D-04    |proj g|=  1.84121D-04\n",
      "\n",
      "At iterate   16    f=  2.56289D-04    |proj g|=  2.13855D-05\n",
      "\n",
      "At iterate   17    f=  1.40159D-04    |proj g|=  1.22943D-05\n",
      "\n",
      "At iterate   18    f=  6.80760D-05    |proj g|=  6.23080D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     18     22      1     0     0   6.231D-06   6.808D-05\n",
      "  F =   6.8076000490284615E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  5.39560D-02\n",
      "\n",
      "At iterate    1    f=  6.77125D-01    |proj g|=  8.93533D-02\n",
      "\n",
      "At iterate    2    f=  5.68487D-01    |proj g|=  8.18466D-02\n",
      "\n",
      "At iterate    3    f=  2.58699D-01    |proj g|=  2.16386D-02\n",
      "\n",
      "At iterate    4    f=  1.80184D-01    |proj g|=  1.23150D-02\n",
      "\n",
      "At iterate    5    f=  1.06161D-01    |proj g|=  5.90660D-03\n",
      "\n",
      "At iterate    6    f=  5.73380D-02    |proj g|=  4.97119D-03\n",
      "\n",
      "At iterate    7    f=  2.93873D-02    |proj g|=  2.27147D-03\n",
      "\n",
      "At iterate    8    f=  1.56355D-02    |proj g|=  1.74475D-03\n",
      "\n",
      "At iterate    9    f=  9.88135D-03    |proj g|=  1.22296D-03\n",
      "\n",
      "At iterate   10    f=  4.91811D-03    |proj g|=  7.41867D-04\n",
      "\n",
      "At iterate   11    f=  2.58273D-03    |proj g|=  2.41897D-04\n",
      "\n",
      "At iterate   12    f=  2.27843D-03    |proj g|=  1.25606D-03\n",
      "\n",
      "At iterate   13    f=  1.16712D-03    |proj g|=  4.30536D-04\n",
      "\n",
      "At iterate   14    f=  6.46469D-04    |proj g|=  1.97718D-04\n",
      "\n",
      "At iterate   15    f=  3.35283D-04    |proj g|=  8.57963D-05\n",
      "\n",
      "At iterate   16    f=  1.74535D-04    |proj g|=  3.84533D-05\n",
      "\n",
      "At iterate   17    f=  8.88989D-05    |proj g|=  1.69785D-05\n",
      "\n",
      "At iterate   18    f=  4.50296D-05    |proj g|=  7.36444D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     18     22      1     0     0   7.364D-06   4.503D-05\n",
      "  F =   4.5029642743701556E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.58788D-01\n",
      "\n",
      "At iterate    1    f=  6.48698D-01    |proj g|=  6.56212D-02\n",
      "\n",
      "At iterate    2    f=  5.16509D-01    |proj g|=  5.10092D-02\n",
      "\n",
      "At iterate    3    f=  2.43180D-01    |proj g|=  2.67957D-02\n",
      "\n",
      "At iterate    4    f=  1.69071D-01    |proj g|=  1.86188D-02\n",
      "\n",
      "At iterate    5    f=  9.96842D-02    |proj g|=  1.05829D-02\n",
      "\n",
      "At iterate    6    f=  5.66319D-02    |proj g|=  7.33627D-03\n",
      "\n",
      "At iterate    7    f=  2.88716D-02    |proj g|=  2.60020D-03\n",
      "\n",
      "At iterate    8    f=  1.62058D-02    |proj g|=  1.49583D-03\n",
      "\n",
      "At iterate    9    f=  7.61677D-03    |proj g|=  7.04788D-04\n",
      "\n",
      "At iterate   10    f=  3.86995D-03    |proj g|=  4.03469D-04\n",
      "\n",
      "At iterate   11    f=  1.74866D-03    |proj g|=  1.74755D-04\n",
      "\n",
      "At iterate   12    f=  1.69347D-03    |proj g|=  4.22625D-04\n",
      "\n",
      "At iterate   13    f=  9.78955D-04    |proj g|=  1.83756D-04\n",
      "\n",
      "At iterate   14    f=  4.86391D-04    |proj g|=  9.25833D-05\n",
      "\n",
      "At iterate   15    f=  2.46418D-04    |proj g|=  4.53106D-05\n",
      "\n",
      "At iterate   16    f=  1.23144D-04    |proj g|=  2.20500D-05\n",
      "\n",
      "At iterate   17    f=  6.17757D-05    |proj g|=  1.07551D-05\n",
      "\n",
      "At iterate   18    f=  3.09151D-05    |proj g|=  5.19843D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     18     24      1     0     0   5.198D-06   3.092D-05\n",
      "  F =   3.0915094783618899E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  9.09321D-02\n",
      "\n",
      "At iterate    1    f=  6.79895D-01    |proj g|=  7.11922D-02\n",
      "\n",
      "At iterate    2    f=  5.16069D-01    |proj g|=  7.94983D-02\n",
      "\n",
      "At iterate    3    f=  2.99552D-01    |proj g|=  2.21801D-02\n",
      "\n",
      "At iterate    4    f=  2.39694D-01    |proj g|=  1.56915D-02\n",
      "\n",
      "At iterate    5    f=  1.77862D-01    |proj g|=  1.19213D-02\n",
      "\n",
      "At iterate    6    f=  9.60862D-02    |proj g|=  4.88867D-03\n",
      "\n",
      "At iterate    7    f=  5.97605D-02    |proj g|=  2.40100D-03\n",
      "\n",
      "At iterate    8    f=  3.24922D-02    |proj g|=  4.92359D-03\n",
      "\n",
      "At iterate    9    f=  1.51323D-02    |proj g|=  1.49717D-03\n",
      "\n",
      "At iterate   10    f=  9.31250D-03    |proj g|=  1.14460D-03\n",
      "\n",
      "At iterate   11    f=  4.36231D-03    |proj g|=  4.52476D-04\n",
      "\n",
      "At iterate   12    f=  3.69027D-03    |proj g|=  1.94954D-03\n",
      "\n",
      "At iterate   13    f=  2.14207D-03    |proj g|=  7.98057D-04\n",
      "\n",
      "At iterate   14    f=  1.16186D-03    |proj g|=  3.76760D-04\n",
      "\n",
      "At iterate   15    f=  6.20878D-04    |proj g|=  1.94190D-04\n",
      "\n",
      "At iterate   16    f=  3.21766D-04    |proj g|=  9.85667D-05\n",
      "\n",
      "At iterate   17    f=  1.63602D-04    |proj g|=  4.80879D-05\n",
      "\n",
      "At iterate   18    f=  8.26861D-05    |proj g|=  2.29443D-05\n",
      "\n",
      "At iterate   19    f=  4.16386D-05    |proj g|=  1.07524D-05\n",
      "\n",
      "At iterate   20    f=  2.09363D-05    |proj g|=  5.07575D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     20     25      1     0     0   5.076D-06   2.094D-05\n",
      "  F =   2.0936308467690943E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.14884D-01\n",
      "\n",
      "At iterate    1    f=  6.66203D-01    |proj g|=  6.27092D-02\n",
      "\n",
      "At iterate    2    f=  5.78238D-01    |proj g|=  5.25851D-02\n",
      "\n",
      "At iterate    3    f=  3.00716D-01    |proj g|=  2.07854D-02\n",
      "\n",
      "At iterate    4    f=  2.35170D-01    |proj g|=  1.37195D-02\n",
      "\n",
      "At iterate    5    f=  1.64522D-01    |proj g|=  4.70717D-03\n",
      "\n",
      "At iterate    6    f=  1.10923D-01    |proj g|=  3.95797D-03\n",
      "\n",
      "At iterate    7    f=  6.69001D-02    |proj g|=  5.21871D-03\n",
      "\n",
      "At iterate    8    f=  3.46515D-02    |proj g|=  3.69117D-03\n",
      "\n",
      "At iterate    9    f=  1.69531D-02    |proj g|=  2.61018D-03\n",
      "\n",
      "At iterate   10    f=  8.54122D-03    |proj g|=  7.61606D-04\n",
      "\n",
      "At iterate   11    f=  4.74547D-03    |proj g|=  3.79139D-04\n",
      "\n",
      "At iterate   12    f=  2.36566D-03    |proj g|=  2.94985D-04\n",
      "\n",
      "At iterate   13    f=  2.20935D-03    |proj g|=  1.16259D-03\n",
      "\n",
      "At iterate   14    f=  1.07763D-03    |proj g|=  3.30639D-04\n",
      "\n",
      "At iterate   15    f=  5.92743D-04    |proj g|=  1.59264D-04\n",
      "\n",
      "At iterate   16    f=  2.97953D-04    |proj g|=  7.22949D-05\n",
      "\n",
      "At iterate   17    f=  1.53559D-04    |proj g|=  3.39783D-05\n",
      "\n",
      "At iterate   18    f=  7.78501D-05    |proj g|=  1.56273D-05\n",
      "\n",
      "At iterate   19    f=  3.95471D-05    |proj g|=  7.17555D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     19     24      1     0     0   7.176D-06   3.955D-05\n",
      "  F =   3.9547087043812444E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.94636D-01\n",
      "\n",
      "At iterate    1    f=  6.25266D-01    |proj g|=  4.64196D-02\n",
      "\n",
      "At iterate    2    f=  6.06226D-01    |proj g|=  4.29377D-02\n",
      "\n",
      "At iterate    3    f=  4.53338D-01    |proj g|=  1.21158D-01\n",
      "\n",
      "At iterate    4    f=  3.92607D-01    |proj g|=  9.21834D-02\n",
      "\n",
      "At iterate    5    f=  3.35121D-01    |proj g|=  4.38159D-02\n",
      "\n",
      "At iterate    6    f=  3.09366D-01    |proj g|=  1.52752D-02\n",
      "\n",
      "At iterate    7    f=  2.94764D-01    |proj g|=  1.86093D-02\n",
      "\n",
      "At iterate    8    f=  2.82150D-01    |proj g|=  2.32679D-02\n",
      "\n",
      "At iterate    9    f=  2.60286D-01    |proj g|=  2.57110D-02\n",
      "\n",
      "At iterate   10    f=  1.92914D-01    |proj g|=  6.17210D-03\n",
      "\n",
      "At iterate   11    f=  1.61273D-01    |proj g|=  2.21824D-02\n",
      "\n",
      "At iterate   12    f=  1.46259D-01    |proj g|=  1.14027D-02\n",
      "\n",
      "At iterate   13    f=  1.23414D-01    |proj g|=  1.15065D-02\n",
      "\n",
      "At iterate   14    f=  1.15426D-01    |proj g|=  1.67969D-02\n",
      "\n",
      "At iterate   15    f=  8.89189D-02    |proj g|=  1.26484D-02\n",
      "\n",
      "At iterate   16    f=  6.09032D-02    |proj g|=  4.36030D-03\n",
      "\n",
      "At iterate   17    f=  5.01137D-02    |proj g|=  2.17269D-03\n",
      "\n",
      "At iterate   18    f=  4.12867D-02    |proj g|=  1.89983D-03\n",
      "\n",
      "At iterate   19    f=  2.89560D-02    |proj g|=  2.68456D-03\n",
      "\n",
      "At iterate   20    f=  1.39826D-02    |proj g|=  1.19946D-03\n",
      "\n",
      "At iterate   21    f=  6.03525D-03    |proj g|=  1.17132D-03\n",
      "\n",
      "At iterate   22    f=  2.80997D-03    |proj g|=  1.13048D-03\n",
      "\n",
      "At iterate   23    f=  1.69291D-03    |proj g|=  6.19639D-04\n",
      "\n",
      "At iterate   24    f=  8.03930D-04    |proj g|=  2.47852D-04\n",
      "\n",
      "At iterate   25    f=  4.24122D-04    |proj g|=  1.94517D-04\n",
      "\n",
      "At iterate   26    f=  3.68366D-04    |proj g|=  1.14461D-04\n",
      "\n",
      "At iterate   27    f=  1.75316D-04    |proj g|=  2.36794D-05\n",
      "\n",
      "At iterate   28    f=  1.00664D-04    |proj g|=  1.57045D-05\n",
      "\n",
      "At iterate   29    f=  4.76099D-05    |proj g|=  6.47840D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     29     34      1     0     0   6.478D-06   4.761D-05\n",
      "  F =   4.7609870954909712E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  5.86503D-02\n",
      "\n",
      "At iterate    1    f=  5.87425D-01    |proj g|=  1.37605D-01\n",
      "\n",
      "At iterate    2    f=  3.82301D-01    |proj g|=  8.64605D-02\n",
      "\n",
      "At iterate    3    f=  2.49889D-01    |proj g|=  1.80002D-02\n",
      "\n",
      "At iterate    4    f=  1.86358D-01    |proj g|=  1.32653D-02\n",
      "\n",
      "At iterate    5    f=  1.16825D-01    |proj g|=  1.13615D-02\n",
      "\n",
      "At iterate    6    f=  7.51009D-02    |proj g|=  4.20411D-02\n",
      "\n",
      "At iterate    7    f=  4.86871D-02    |proj g|=  2.20786D-02\n",
      "\n",
      "At iterate    8    f=  2.80130D-02    |proj g|=  8.36451D-03\n",
      "\n",
      "At iterate    9    f=  1.72745D-02    |proj g|=  2.69562D-03\n",
      "\n",
      "At iterate   10    f=  1.02118D-02    |proj g|=  1.00938D-03\n",
      "\n",
      "At iterate   11    f=  5.67553D-03    |proj g|=  1.13094D-03\n",
      "\n",
      "At iterate   12    f=  2.79191D-03    |proj g|=  4.09448D-04\n",
      "\n",
      "At iterate   13    f=  1.52121D-03    |proj g|=  1.79935D-04\n",
      "\n",
      "At iterate   14    f=  7.38415D-04    |proj g|=  1.36557D-04\n",
      "\n",
      "At iterate   15    f=  4.49860D-04    |proj g|=  1.61173D-04\n",
      "\n",
      "At iterate   16    f=  2.59025D-04    |proj g|=  7.86132D-05\n",
      "\n",
      "At iterate   17    f=  1.27488D-04    |proj g|=  3.96684D-05\n",
      "\n",
      "At iterate   18    f=  6.69392D-05    |proj g|=  6.80049D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     18     19      1     0     0   6.800D-06   6.694D-05\n",
      "  F =   6.6939235822485716E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  7.17595D-02\n",
      "\n",
      "At iterate    1    f=  6.79248D-01    |proj g|=  9.54712D-02\n",
      "\n",
      "At iterate    2    f=  5.97972D-01    |proj g|=  8.64712D-02\n",
      "\n",
      "At iterate    3    f=  3.04290D-01    |proj g|=  2.62565D-02\n",
      "\n",
      "At iterate    4    f=  2.38231D-01    |proj g|=  1.54070D-02\n",
      "\n",
      "At iterate    5    f=  1.54904D-01    |proj g|=  1.03839D-02\n",
      "\n",
      "At iterate    6    f=  9.70235D-02    |proj g|=  1.09982D-02\n",
      "\n",
      "At iterate    7    f=  6.43017D-02    |proj g|=  4.04842D-03\n",
      "\n",
      "At iterate    8    f=  4.40191D-02    |proj g|=  1.58995D-03\n",
      "\n",
      "At iterate    9    f=  2.97986D-02    |proj g|=  1.02190D-03\n",
      "\n",
      "At iterate   10    f=  1.80308D-02    |proj g|=  1.51041D-03\n",
      "\n",
      "At iterate   11    f=  8.65764D-03    |proj g|=  1.38945D-03\n",
      "\n",
      "At iterate   12    f=  8.41012D-03    |proj g|=  4.92619D-04\n",
      "\n",
      "At iterate   13    f=  5.45117D-03    |proj g|=  3.24761D-04\n",
      "\n",
      "At iterate   14    f=  2.59111D-03    |proj g|=  1.68746D-04\n",
      "\n",
      "At iterate   15    f=  1.35236D-03    |proj g|=  1.80842D-04\n",
      "\n",
      "At iterate   16    f=  5.75102D-04    |proj g|=  1.39978D-04\n",
      "\n",
      "At iterate   17    f=  2.99553D-04    |proj g|=  5.76371D-05\n",
      "\n",
      "At iterate   18    f=  1.83539D-04    |proj g|=  2.97193D-05\n",
      "\n",
      "At iterate   19    f=  8.72903D-05    |proj g|=  1.15661D-05\n",
      "\n",
      "At iterate   20    f=  4.52828D-05    |proj g|=  5.04514D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     20     24      1     0     0   5.045D-06   4.528D-05\n",
      "  F =   4.5282777008257843E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.63160D-01\n",
      "\n",
      "At iterate    1    f=  6.49849D-01    |proj g|=  6.69507D-02\n",
      "\n",
      "At iterate    2    f=  5.30760D-01    |proj g|=  5.50051D-02\n",
      "\n",
      "At iterate    3    f=  2.22412D-01    |proj g|=  3.39877D-02\n",
      "\n",
      "At iterate    4    f=  1.52423D-01    |proj g|=  2.16132D-02\n",
      "\n",
      "At iterate    5    f=  9.43129D-02    |proj g|=  1.04333D-02\n",
      "\n",
      "At iterate    6    f=  6.07585D-02    |proj g|=  4.48321D-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate    7    f=  3.86768D-02    |proj g|=  2.42944D-03\n",
      "\n",
      "At iterate    8    f=  2.42436D-02    |proj g|=  1.30897D-03\n",
      "\n",
      "At iterate    9    f=  1.59337D-02    |proj g|=  2.31117D-03\n",
      "\n",
      "At iterate   10    f=  8.10128D-03    |proj g|=  2.27144D-03\n",
      "\n",
      "At iterate   11    f=  5.64112D-03    |proj g|=  1.54190D-03\n",
      "\n",
      "At iterate   12    f=  2.86895D-03    |proj g|=  4.63307D-04\n",
      "\n",
      "At iterate   13    f=  1.63987D-03    |proj g|=  9.69413D-04\n",
      "\n",
      "At iterate   14    f=  6.49818D-04    |proj g|=  4.52154D-04\n",
      "\n",
      "At iterate   15    f=  3.71087D-04    |proj g|=  2.07651D-04\n",
      "\n",
      "At iterate   16    f=  2.14325D-04    |proj g|=  8.85796D-05\n",
      "\n",
      "At iterate   17    f=  1.23983D-04    |proj g|=  3.38875D-05\n",
      "\n",
      "At iterate   18    f=  7.08748D-05    |proj g|=  1.49986D-05\n",
      "\n",
      "At iterate   19    f=  3.74571D-05    |proj g|=  1.26397D-05\n",
      "\n",
      "At iterate   20    f=  1.90824D-05    |proj g|=  7.03967D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     20     24      1     0     0   7.040D-06   1.908D-05\n",
      "  F =   1.9082402467646874E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.21505D-01\n",
      "\n",
      "At iterate    1    f=  6.65286D-01    |proj g|=  6.03779D-02\n",
      "\n",
      "At iterate    2    f=  5.49606D-01    |proj g|=  4.97216D-02\n",
      "\n",
      "At iterate    3    f=  2.38145D-01    |proj g|=  2.14590D-02\n",
      "\n",
      "At iterate    4    f=  1.56709D-01    |proj g|=  1.65306D-02\n",
      "\n",
      "At iterate    5    f=  8.36895D-02    |proj g|=  1.24308D-02\n",
      "\n",
      "At iterate    6    f=  4.37738D-02    |proj g|=  1.05965D-02\n",
      "\n",
      "At iterate    7    f=  2.04539D-02    |proj g|=  4.06523D-03\n",
      "\n",
      "At iterate    8    f=  1.27295D-02    |proj g|=  2.72118D-03\n",
      "\n",
      "At iterate    9    f=  6.05850D-03    |proj g|=  1.43571D-03\n",
      "\n",
      "At iterate   10    f=  3.14823D-03    |proj g|=  8.05726D-04\n",
      "\n",
      "At iterate   11    f=  1.57654D-03    |proj g|=  4.33233D-04\n",
      "\n",
      "At iterate   12    f=  9.48298D-04    |proj g|=  3.94994D-04\n",
      "\n",
      "At iterate   13    f=  3.15145D-04    |proj g|=  4.02289D-05\n",
      "\n",
      "At iterate   14    f=  1.95802D-04    |proj g|=  2.49495D-05\n",
      "\n",
      "At iterate   15    f=  9.06011D-05    |proj g|=  1.06909D-05\n",
      "\n",
      "At iterate   16    f=  4.70160D-05    |proj g|=  5.91025D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     16     20      1     0     0   5.910D-06   4.702D-05\n",
      "  F =   4.7016032213108583E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.32559D-01\n",
      "\n",
      "At iterate    1    f=  6.64050D-01    |proj g|=  6.53632D-02\n",
      "\n",
      "At iterate    2    f=  5.33164D-01    |proj g|=  5.09481D-02\n",
      "\n",
      "At iterate    3    f=  2.52548D-01    |proj g|=  2.50437D-02\n",
      "\n",
      "At iterate    4    f=  1.75045D-01    |proj g|=  1.87337D-02\n",
      "\n",
      "At iterate    5    f=  9.46025D-02    |proj g|=  7.11222D-03\n",
      "\n",
      "At iterate    6    f=  5.88954D-02    |proj g|=  2.77689D-03\n",
      "\n",
      "At iterate    7    f=  3.74470D-02    |proj g|=  1.52754D-03\n",
      "\n",
      "At iterate    8    f=  2.24486D-02    |proj g|=  1.50044D-03\n",
      "\n",
      "At iterate    9    f=  1.31339D-02    |proj g|=  1.16552D-03\n",
      "\n",
      "At iterate   10    f=  8.00592D-03    |proj g|=  3.23273D-04\n",
      "\n",
      "At iterate   11    f=  4.83883D-03    |proj g|=  2.62818D-04\n",
      "\n",
      "At iterate   12    f=  4.79581D-03    |proj g|=  1.13455D-03\n",
      "\n",
      "At iterate   13    f=  2.90409D-03    |proj g|=  3.25119D-04\n",
      "\n",
      "At iterate   14    f=  1.73360D-03    |proj g|=  1.60547D-04\n",
      "\n",
      "At iterate   15    f=  9.50451D-04    |proj g|=  1.00511D-04\n",
      "\n",
      "At iterate   16    f=  4.80111D-04    |proj g|=  5.49349D-05\n",
      "\n",
      "At iterate   17    f=  2.47282D-04    |proj g|=  3.12133D-05\n",
      "\n",
      "At iterate   18    f=  1.22695D-04    |proj g|=  1.22720D-05\n",
      "\n",
      "At iterate   19    f=  6.16141D-05    |proj g|=  5.88243D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     19     24      1     0     0   5.882D-06   6.161D-05\n",
      "  F =   6.1614112061947405E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.84092D-01\n",
      "\n",
      "At iterate    1    f=  6.35228D-01    |proj g|=  6.81536D-02\n",
      "\n",
      "At iterate    2    f=  5.16806D-01    |proj g|=  5.71056D-02\n",
      "\n",
      "At iterate    3    f=  2.87081D-01    |proj g|=  4.98550D-02\n",
      "\n",
      "At iterate    4    f=  2.00310D-01    |proj g|=  3.30271D-02\n",
      "\n",
      "At iterate    5    f=  1.24165D-01    |proj g|=  1.76882D-02\n",
      "\n",
      "At iterate    6    f=  8.04928D-02    |proj g|=  1.02388D-02\n",
      "\n",
      "At iterate    7    f=  4.28220D-02    |proj g|=  5.84825D-03\n",
      "\n",
      "At iterate    8    f=  1.96164D-02    |proj g|=  2.41467D-03\n",
      "\n",
      "At iterate    9    f=  9.76108D-03    |proj g|=  1.51527D-03\n",
      "\n",
      "At iterate   10    f=  4.95821D-03    |proj g|=  6.45012D-04\n",
      "\n",
      "At iterate   11    f=  2.55769D-03    |proj g|=  2.24916D-04\n",
      "\n",
      "At iterate   12    f=  1.85697D-03    |proj g|=  1.58812D-03\n",
      "\n",
      "At iterate   13    f=  5.10646D-04    |proj g|=  2.68702D-04\n",
      "\n",
      "At iterate   14    f=  3.35204D-04    |proj g|=  1.57918D-04\n",
      "\n",
      "At iterate   15    f=  1.54319D-04    |proj g|=  5.95692D-05\n",
      "\n",
      "At iterate   16    f=  8.14053D-05    |proj g|=  2.62715D-05\n",
      "\n",
      "At iterate   17    f=  4.06413D-05    |proj g|=  1.04357D-05\n",
      "\n",
      "At iterate   18    f=  2.06858D-05    |proj g|=  4.01924D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     18     22      1     0     0   4.019D-06   2.069D-05\n",
      "  F =   2.0685846787228711E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  3.08050D-01\n",
      "\n",
      "At iterate    1    f=  5.07857D-01    |proj g|=  4.95045D-02\n",
      "\n",
      "At iterate    2    f=  4.83219D-01    |proj g|=  4.63569D-02\n",
      "\n",
      "At iterate    3    f=  3.55597D-01    |proj g|=  1.44738D-01\n",
      "\n",
      "At iterate    4    f=  2.80333D-01    |proj g|=  1.15651D-01\n",
      "\n",
      "At iterate    5    f=  1.68508D-01    |proj g|=  5.33882D-02\n",
      "\n",
      "At iterate    6    f=  1.22155D-01    |proj g|=  2.97634D-02\n",
      "\n",
      "At iterate    7    f=  8.85971D-02    |proj g|=  1.63770D-02\n",
      "\n",
      "At iterate    8    f=  6.23379D-02    |proj g|=  9.57908D-03\n",
      "\n",
      "At iterate    9    f=  3.42049D-02    |proj g|=  4.77693D-03\n",
      "\n",
      "At iterate   10    f=  1.45948D-02    |proj g|=  1.90511D-03\n",
      "\n",
      "At iterate   11    f=  7.31175D-03    |proj g|=  1.13017D-03\n",
      "\n",
      "At iterate   12    f=  3.54087D-03    |proj g|=  8.76891D-04\n",
      "\n",
      "At iterate   13    f=  1.66778D-03    |proj g|=  3.26825D-04\n",
      "\n",
      "At iterate   14    f=  8.02880D-04    |proj g|=  6.20861D-04\n",
      "\n",
      "At iterate   15    f=  4.08232D-04    |proj g|=  2.54611D-04\n",
      "\n",
      "At iterate   16    f=  2.18130D-04    |proj g|=  1.14315D-04\n",
      "\n",
      "At iterate   17    f=  1.11437D-04    |proj g|=  4.91040D-05\n",
      "\n",
      "At iterate   18    f=  5.73007D-05    |proj g|=  2.13331D-05\n",
      "\n",
      "At iterate   19    f=  2.91749D-05    |proj g|=  9.09742D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     19     22      1     0     0   9.097D-06   2.917D-05\n",
      "  F =   2.9174906571585152E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.68212D-01\n",
      "\n",
      "At iterate    1    f=  6.39606D-01    |proj g|=  5.96318D-02\n",
      "\n",
      "At iterate    2    f=  5.75592D-01    |proj g|=  6.24334D-02\n",
      "\n",
      "At iterate    3    f=  4.35866D-01    |proj g|=  1.04588D-01\n",
      "\n",
      "At iterate    4    f=  3.51845D-01    |proj g|=  6.91284D-02\n",
      "\n",
      "At iterate    5    f=  2.98320D-01    |proj g|=  3.59675D-02\n",
      "\n",
      "At iterate    6    f=  2.69151D-01    |proj g|=  1.59973D-02\n",
      "\n",
      "At iterate    7    f=  2.49225D-01    |proj g|=  1.46733D-02\n",
      "\n",
      "At iterate    8    f=  2.27158D-01    |proj g|=  1.83710D-02\n",
      "\n",
      "At iterate    9    f=  1.78617D-01    |proj g|=  1.20218D-02\n",
      "\n",
      "At iterate   10    f=  1.14312D-01    |proj g|=  5.11040D-03\n",
      "\n",
      "At iterate   11    f=  8.80477D-02    |proj g|=  5.27929D-03\n",
      "\n",
      "At iterate   12    f=  8.60483D-02    |proj g|=  7.46849D-03\n",
      "\n",
      "At iterate   13    f=  8.06090D-02    |proj g|=  3.87408D-03\n",
      "\n",
      "At iterate   14    f=  7.52354D-02    |proj g|=  2.48181D-03\n",
      "\n",
      "At iterate   15    f=  7.08186D-02    |proj g|=  1.89312D-03\n",
      "\n",
      "At iterate   16    f=  6.40312D-02    |proj g|=  2.04052D-03\n",
      "\n",
      "At iterate   17    f=  5.70918D-02    |proj g|=  3.05673D-03\n",
      "\n",
      "At iterate   18    f=  5.17699D-02    |proj g|=  3.82684D-03\n",
      "\n",
      "At iterate   19    f=  4.62603D-02    |proj g|=  2.61060D-03\n",
      "\n",
      "At iterate   20    f=  4.09947D-02    |proj g|=  2.10974D-03\n",
      "\n",
      "At iterate   21    f=  2.95066D-02    |proj g|=  1.77668D-03\n",
      "\n",
      "At iterate   22    f=  2.27542D-02    |proj g|=  3.62178D-03\n",
      "\n",
      "At iterate   23    f=  2.07795D-02    |proj g|=  9.26999D-03\n",
      "\n",
      "At iterate   24    f=  1.31649D-02    |proj g|=  1.87695D-03\n",
      "\n",
      "At iterate   25    f=  1.11745D-02    |proj g|=  2.10434D-03\n",
      "\n",
      "At iterate   26    f=  5.25833D-03    |proj g|=  1.03775D-03\n",
      "\n",
      "At iterate   27    f=  2.36937D-03    |proj g|=  4.40201D-04\n",
      "\n",
      "At iterate   28    f=  1.13852D-03    |proj g|=  2.25490D-04\n",
      "\n",
      "At iterate   29    f=  5.70225D-04    |proj g|=  1.41608D-04\n",
      "\n",
      "At iterate   30    f=  3.08818D-04    |proj g|=  8.15274D-05\n",
      "\n",
      "At iterate   31    f=  1.50361D-04    |proj g|=  4.55773D-05\n",
      "\n",
      "At iterate   32    f=  7.77066D-05    |proj g|=  3.00272D-05\n",
      "\n",
      "At iterate   33    f=  3.85745D-05    |proj g|=  1.40463D-05\n",
      "\n",
      "At iterate   34    f=  2.37182D-05    |proj g|=  1.03371D-05\n",
      "\n",
      "At iterate   35    f=  2.10695D-05    |proj g|=  8.24433D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     35     42      1     0     0   8.244D-06   2.107D-05\n",
      "  F =   2.1069542478132071E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  6.72458D-02\n",
      "\n",
      "At iterate    1    f=  6.79043D-01    |proj g|=  8.20534D-02\n",
      "\n",
      "At iterate    2    f=  5.89113D-01    |proj g|=  7.32744D-02\n",
      "\n",
      "At iterate    3    f=  2.89057D-01    |proj g|=  2.33376D-02\n",
      "\n",
      "At iterate    4    f=  2.22968D-01    |proj g|=  1.34974D-02\n",
      "\n",
      "At iterate    5    f=  1.44166D-01    |proj g|=  8.39889D-03\n",
      "\n",
      "At iterate    6    f=  8.07622D-02    |proj g|=  1.08269D-02\n",
      "\n",
      "At iterate    7    f=  5.03572D-02    |proj g|=  6.22488D-03\n",
      "\n",
      "At iterate    8    f=  2.54897D-02    |proj g|=  3.27592D-03\n",
      "\n",
      "At iterate    9    f=  1.36207D-02    |proj g|=  1.52257D-03\n",
      "\n",
      "At iterate   10    f=  7.08256D-03    |proj g|=  6.77039D-04\n",
      "\n",
      "At iterate   11    f=  3.72685D-03    |proj g|=  3.60853D-04\n",
      "\n",
      "At iterate   12    f=  1.94414D-03    |proj g|=  3.51956D-04\n",
      "\n",
      "At iterate   13    f=  1.93467D-03    |proj g|=  3.04747D-04\n",
      "\n",
      "At iterate   14    f=  1.08144D-03    |proj g|=  1.00366D-04\n",
      "\n",
      "At iterate   15    f=  5.68001D-04    |proj g|=  5.29505D-05\n",
      "\n",
      "At iterate   16    f=  2.94485D-04    |proj g|=  2.94773D-05\n",
      "\n",
      "At iterate   17    f=  1.51096D-04    |proj g|=  1.59506D-05\n",
      "\n",
      "At iterate   18    f=  7.61800D-05    |proj g|=  2.55841D-05\n",
      "\n",
      "At iterate   19    f=  4.54970D-05    |proj g|=  1.26528D-05\n",
      "\n",
      "At iterate   20    f=  2.28562D-05    |proj g|=  4.73324D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     20     24      1     0     0   4.733D-06   2.286D-05\n",
      "  F =   2.2856247646148193E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.20033D-01\n",
      "\n",
      "At iterate    1    f=  6.76867D-01    |proj g|=  5.46176D-02\n",
      "\n",
      "At iterate    2    f=  5.08534D-01    |proj g|=  6.83329D-02\n",
      "\n",
      "At iterate    3    f=  2.92618D-01    |proj g|=  1.73647D-02\n",
      "\n",
      "At iterate    4    f=  2.26561D-01    |proj g|=  1.08643D-02\n",
      "\n",
      "At iterate    5    f=  1.55360D-01    |proj g|=  6.30751D-03\n",
      "\n",
      "At iterate    6    f=  9.40914D-02    |proj g|=  5.07971D-03\n",
      "\n",
      "At iterate    7    f=  6.88601D-02    |proj g|=  3.64303D-03\n",
      "\n",
      "At iterate    8    f=  5.03950D-02    |proj g|=  2.56510D-03\n",
      "\n",
      "At iterate    9    f=  4.12952D-02    |proj g|=  5.27247D-03\n",
      "\n",
      "At iterate   10    f=  3.38011D-02    |proj g|=  1.74666D-03\n",
      "\n",
      "At iterate   11    f=  2.90301D-02    |proj g|=  1.62791D-03\n",
      "\n",
      "At iterate   12    f=  2.86770D-02    |proj g|=  2.70790D-03\n",
      "\n",
      "At iterate   13    f=  2.52166D-02    |proj g|=  8.96837D-04\n",
      "\n",
      "At iterate   14    f=  2.14129D-02    |proj g|=  1.22917D-03\n",
      "\n",
      "At iterate   15    f=  1.84712D-02    |proj g|=  9.87477D-04\n",
      "\n",
      "At iterate   16    f=  1.49721D-02    |proj g|=  4.53206D-04\n",
      "\n",
      "At iterate   17    f=  1.05682D-02    |proj g|=  1.02041D-03\n",
      "\n",
      "At iterate   18    f=  5.30178D-03    |proj g|=  6.89128D-04\n",
      "\n",
      "At iterate   19    f=  2.41318D-03    |proj g|=  4.67663D-04\n",
      "\n",
      "At iterate   20    f=  1.14030D-03    |proj g|=  1.51253D-04\n",
      "\n",
      "At iterate   21    f=  5.65880D-04    |proj g|=  6.12707D-05\n",
      "\n",
      "At iterate   22    f=  2.77568D-04    |proj g|=  3.53234D-05\n",
      "\n",
      "At iterate   23    f=  2.48909D-04    |proj g|=  9.00633D-05\n",
      "\n",
      "At iterate   24    f=  1.17155D-04    |proj g|=  2.29241D-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/discrete/discrete_model.py:1819: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " 62%|██████▏   | 16/26 [00:00<00:00, 74.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   25    f=  6.10188D-05    |proj g|=  1.05643D-05\n",
      "\n",
      "At iterate   26    f=  2.99900D-05    |proj g|=  4.77592D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     26     32      1     0     0   4.776D-06   2.999D-05\n",
      "  F =   2.9989973962371498E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.13422D-01\n",
      "\n",
      "At iterate    1    f=  6.64446D-01    |proj g|=  7.43157D-02\n",
      "\n",
      "At iterate    2    f=  4.96431D-01    |proj g|=  7.13278D-02\n",
      "\n",
      "At iterate    3    f=  2.57790D-01    |proj g|=  1.66326D-02\n",
      "\n",
      "At iterate    4    f=  1.85721D-01    |proj g|=  1.61233D-02\n",
      "\n",
      "At iterate    5    f=  1.02579D-01    |proj g|=  8.60725D-03\n",
      "\n",
      "At iterate    6    f=  5.78404D-02    |proj g|=  6.32920D-03\n",
      "\n",
      "At iterate    7    f=  3.18345D-02    |proj g|=  3.88416D-03\n",
      "\n",
      "At iterate    8    f=  1.72924D-02    |proj g|=  1.54898D-03\n",
      "\n",
      "At iterate    9    f=  9.28157D-03    |proj g|=  1.18415D-03\n",
      "\n",
      "At iterate   10    f=  4.67487D-03    |proj g|=  6.93364D-04\n",
      "\n",
      "At iterate   11    f=  2.36060D-03    |proj g|=  3.81885D-04\n",
      "\n",
      "At iterate   12    f=  1.72414D-03    |proj g|=  1.17045D-03\n",
      "\n",
      "At iterate   13    f=  4.89241D-04    |proj g|=  1.10160D-04\n",
      "\n",
      "At iterate   14    f=  3.35243D-04    |proj g|=  5.80647D-05\n",
      "\n",
      "At iterate   15    f=  1.52029D-04    |proj g|=  1.67129D-05\n",
      "\n",
      "At iterate   16    f=  8.03343D-05    |proj g|=  7.44185D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     16     20      1     0     0   7.442D-06   8.033D-05\n",
      "  F =   8.0334313990142433E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  3.06115D-01\n",
      "\n",
      "At iterate    1    f=  5.19547D-01    |proj g|=  5.08913D-02\n",
      "\n",
      "At iterate    2    f=  5.00266D-01    |proj g|=  4.74310D-02\n",
      "\n",
      "At iterate    3    f=  4.13427D-01    |proj g|=  1.31574D-01\n",
      "\n",
      "At iterate    4    f=  3.35758D-01    |proj g|=  1.33540D-01\n",
      "\n",
      "At iterate    5    f=  1.82797D-01    |proj g|=  6.59587D-02\n",
      "\n",
      "At iterate    6    f=  1.13735D-01    |proj g|=  3.26156D-02\n",
      "\n",
      "At iterate    7    f=  7.10689D-02    |proj g|=  1.47506D-02\n",
      "\n",
      "At iterate    8    f=  4.51821D-02    |proj g|=  6.13064D-03\n",
      "\n",
      "At iterate    9    f=  2.67681D-02    |proj g|=  2.78950D-03\n",
      "\n",
      "At iterate   10    f=  1.32569D-02    |proj g|=  1.42045D-03\n",
      "\n",
      "At iterate   11    f=  6.36278D-03    |proj g|=  8.08108D-04\n",
      "\n",
      "At iterate   12    f=  3.10856D-03    |proj g|=  5.91084D-04\n",
      "\n",
      "At iterate   13    f=  1.69946D-03    |proj g|=  2.35693D-04\n",
      "\n",
      "At iterate   14    f=  8.61447D-04    |proj g|=  6.27533D-04\n",
      "\n",
      "At iterate   15    f=  3.72015D-04    |proj g|=  1.91027D-04\n",
      "\n",
      "At iterate   16    f=  2.20179D-04    |proj g|=  9.01967D-05\n",
      "\n",
      "At iterate   17    f=  1.11554D-04    |proj g|=  3.35989D-05\n",
      "\n",
      "At iterate   18    f=  5.89715D-05    |proj g|=  1.28837D-05\n",
      "\n",
      "At iterate   19    f=  3.01587D-05    |proj g|=  4.38472D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     19     22      1     0     0   4.385D-06   3.016D-05\n",
      "  F =   3.0158676935269935E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  3.55484D-01\n",
      "\n",
      "At iterate    1    f=  5.87424D-01    |proj g|=  1.49857D-01\n",
      "\n",
      "At iterate    2    f=  4.78342D-01    |proj g|=  1.01516D-01\n",
      "\n",
      "At iterate    3    f=  4.30601D-01    |proj g|=  3.40788D-02\n",
      "\n",
      "At iterate    4    f=  4.23897D-01    |proj g|=  3.18603D-02\n",
      "\n",
      "At iterate    5    f=  3.52193D-01    |proj g|=  9.67573D-02\n",
      "\n",
      "At iterate    6    f=  2.90705D-01    |proj g|=  8.87208D-02\n",
      "\n",
      "At iterate    7    f=  1.86173D-01    |proj g|=  5.07818D-02\n",
      "\n",
      "At iterate    8    f=  1.29968D-01    |proj g|=  2.98810D-02\n",
      "\n",
      "At iterate    9    f=  9.64711D-02    |proj g|=  1.83655D-02\n",
      "\n",
      "At iterate   10    f=  7.17839D-02    |proj g|=  1.06054D-02\n",
      "\n",
      "At iterate   11    f=  4.98148D-02    |proj g|=  4.86907D-03\n",
      "\n",
      "At iterate   12    f=  2.91095D-02    |proj g|=  4.69030D-03\n",
      "\n",
      "At iterate   13    f=  1.52940D-02    |proj g|=  4.12338D-03\n",
      "\n",
      "At iterate   14    f=  7.01109D-03    |proj g|=  4.98438D-03\n",
      "\n",
      "At iterate   15    f=  3.38662D-03    |proj g|=  1.64908D-03\n",
      "\n",
      "At iterate   16    f=  1.94172D-03    |proj g|=  8.56725D-04\n",
      "\n",
      "At iterate   17    f=  9.84509D-04    |proj g|=  4.14271D-04\n",
      "\n",
      "At iterate   18    f=  5.06942D-04    |proj g|=  1.97933D-04\n",
      "\n",
      "At iterate   19    f=  2.55454D-04    |proj g|=  9.86451D-05\n",
      "\n",
      "At iterate   20    f=  1.28889D-04    |proj g|=  3.74073D-05\n",
      "\n",
      "At iterate   21    f=  6.29366D-05    |proj g|=  2.46424D-05\n",
      "\n",
      "At iterate   22    f=  4.94464D-05    |proj g|=  2.44858D-05\n",
      "\n",
      "At iterate   23    f=  2.43033D-05    |proj g|=  7.82561D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     23     26      1     0     0   7.826D-06   2.430D-05\n",
      "  F =   2.4303299428922982E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  7.77214D-02\n",
      "\n",
      "At iterate    1    f=  6.74172D-01    |proj g|=  1.11325D-01\n",
      "\n",
      "At iterate    2    f=  5.41419D-01    |proj g|=  8.99035D-02\n",
      "\n",
      "At iterate    3    f=  2.98570D-01    |proj g|=  2.14054D-02\n",
      "\n",
      "At iterate    4    f=  2.32095D-01    |proj g|=  1.39420D-02\n",
      "\n",
      "At iterate    5    f=  1.52135D-01    |proj g|=  8.56666D-03\n",
      "\n",
      "At iterate    6    f=  8.15767D-02    |proj g|=  7.56681D-03\n",
      "\n",
      "At iterate    7    f=  4.26635D-02    |proj g|=  3.14502D-03\n",
      "\n",
      "At iterate    8    f=  2.42994D-02    |proj g|=  3.32936D-03\n",
      "\n",
      "At iterate    9    f=  1.29462D-02    |proj g|=  2.25129D-03\n",
      "\n",
      "At iterate   10    f=  6.51057D-03    |proj g|=  8.73998D-04\n",
      "\n",
      "At iterate   11    f=  3.61076D-03    |proj g|=  2.78912D-04\n",
      "\n",
      "At iterate   12    f=  3.21253D-03    |proj g|=  1.85458D-03\n",
      "\n",
      "At iterate   13    f=  1.46979D-03    |proj g|=  5.87987D-04\n",
      "\n",
      "At iterate   14    f=  8.17942D-04    |proj g|=  2.89190D-04\n",
      "\n",
      "At iterate   15    f=  4.17294D-04    |proj g|=  1.31653D-04\n",
      "\n",
      "At iterate   16    f=  2.19263D-04    |proj g|=  6.19400D-05\n",
      "\n",
      "At iterate   17    f=  1.12461D-04    |proj g|=  2.74504D-05\n",
      "\n",
      "At iterate   18    f=  5.75093D-05    |proj g|=  1.14708D-05\n",
      "\n",
      "At iterate   19    f=  2.92516D-05    |proj g|=  4.44503D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     19     23      1     0     0   4.445D-06   2.925D-05\n",
      "  F =   2.9251588392083250E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.37428D-01\n",
      "\n",
      "At iterate    1    f=  6.58147D-01    |proj g|=  6.40322D-02\n",
      "\n",
      "At iterate    2    f=  5.12749D-01    |proj g|=  5.23505D-02\n",
      "\n",
      "At iterate    3    f=  2.29815D-01    |proj g|=  1.61316D-02\n",
      "\n",
      "At iterate    4    f=  1.58677D-01    |proj g|=  1.19758D-02\n",
      "\n",
      "At iterate    5    f=  8.35090D-02    |proj g|=  8.34489D-03\n",
      "\n",
      "At iterate    6    f=  4.49908D-02    |proj g|=  4.51955D-03\n",
      "\n",
      "At iterate    7    f=  2.34791D-02    |proj g|=  2.71647D-03\n",
      "\n",
      "At iterate    8    f=  1.20756D-02    |proj g|=  1.28581D-03\n",
      "\n",
      "At iterate    9    f=  6.16103D-03    |proj g|=  6.92454D-04\n",
      "\n",
      "At iterate   10    f=  3.06089D-03    |proj g|=  4.23783D-04\n",
      "\n",
      "At iterate   11    f=  1.42387D-03    |proj g|=  1.32764D-04\n",
      "\n",
      "At iterate   12    f=  1.31071D-03    |proj g|=  6.68536D-04\n",
      "\n",
      "At iterate   13    f=  7.92040D-04    |proj g|=  3.12343D-04\n",
      "\n",
      "At iterate   14    f=  3.83852D-04    |proj g|=  1.38450D-04\n",
      "\n",
      "At iterate   15    f=  1.98237D-04    |proj g|=  6.88379D-05\n",
      "\n",
      "At iterate   16    f=  9.92772D-05    |proj g|=  3.34577D-05\n",
      "\n",
      "At iterate   17    f=  5.00488D-05    |proj g|=  1.65023D-05\n",
      "\n",
      "At iterate   18    f=  2.50862D-05    |proj g|=  8.11905D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     18     23      1     0     0   8.119D-06   2.509D-05\n",
      "  F =   2.5086189886233178E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.54176D-01\n",
      "\n",
      "At iterate    1    f=  6.47210D-01    |proj g|=  5.91160D-02\n",
      "\n",
      "At iterate    2    f=  5.14233D-01    |proj g|=  5.10243D-02\n",
      "\n",
      "At iterate    3    f=  3.06964D-01    |proj g|=  3.87342D-02\n",
      "\n",
      "At iterate    4    f=  2.09170D-01    |proj g|=  2.74906D-02\n",
      "\n",
      "At iterate    5    f=  1.07900D-01    |proj g|=  1.78508D-02\n",
      "\n",
      "At iterate    6    f=  5.35576D-02    |proj g|=  8.48932D-03\n",
      "\n",
      "At iterate    7    f=  2.81712D-02    |proj g|=  5.27087D-03\n",
      "\n",
      "At iterate    8    f=  1.35480D-02    |proj g|=  2.70022D-03\n",
      "\n",
      "At iterate    9    f=  6.87125D-03    |proj g|=  1.53325D-03\n",
      "\n",
      "At iterate   10    f=  3.44292D-03    |proj g|=  8.06081D-04\n",
      "\n",
      "At iterate   11    f=  1.74139D-03    |proj g|=  4.29946D-04\n",
      "\n",
      "At iterate   12    f=  1.01828D-03    |proj g|=  4.34127D-04\n",
      "\n",
      "At iterate   13    f=  3.12966D-04    |proj g|=  8.96571D-05\n",
      "\n",
      "At iterate   14    f=  1.99203D-04    |proj g|=  5.13092D-05\n",
      "\n",
      "At iterate   15    f=  9.15756D-05    |proj g|=  2.20669D-05\n",
      "\n",
      "At iterate   16    f=  4.77557D-05    |proj g|=  1.13590D-05\n",
      "\n",
      "At iterate   17    f=  2.36828D-05    |proj g|=  5.54947D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     17     21      1     0     0   5.549D-06   2.368D-05\n",
      "  F =   2.3682812515685987E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.55927D-01\n",
      "\n",
      "At iterate    1    f=  6.47688D-01    |proj g|=  7.03710D-02\n",
      "\n",
      "At iterate    2    f=  5.16015D-01    |proj g|=  5.49708D-02\n",
      "\n",
      "At iterate    3    f=  2.89056D-01    |proj g|=  4.25983D-02\n",
      "\n",
      "At iterate    4    f=  2.13984D-01    |proj g|=  2.98855D-02\n",
      "\n",
      "At iterate    5    f=  1.31166D-01    |proj g|=  1.60233D-02\n",
      "\n",
      "At iterate    6    f=  8.00055D-02    |proj g|=  8.79905D-03\n",
      "\n",
      "At iterate    7    f=  4.31098D-02    |proj g|=  3.76854D-03\n",
      "\n",
      "At iterate    8    f=  2.13679D-02    |proj g|=  1.56862D-03\n",
      "\n",
      "At iterate    9    f=  1.05037D-02    |proj g|=  1.39733D-03\n",
      "\n",
      "At iterate   10    f=  7.89422D-03    |proj g|=  2.95455D-03\n",
      "\n",
      "At iterate   11    f=  2.24101D-03    |proj g|=  3.74615D-04\n",
      "\n",
      "At iterate   12    f=  1.89035D-03    |proj g|=  1.21291D-03\n",
      "\n",
      "At iterate   13    f=  1.12436D-03    |proj g|=  5.50551D-04\n",
      "\n",
      "At iterate   14    f=  5.92227D-04    |proj g|=  2.45523D-04\n",
      "\n",
      "At iterate   15    f=  3.10167D-04    |proj g|=  1.15495D-04\n",
      "\n",
      "At iterate   16    f=  1.60256D-04    |proj g|=  5.44793D-05\n",
      "\n",
      "At iterate   17    f=  8.21878D-05    |proj g|=  2.56506D-05\n",
      "\n",
      "At iterate   18    f=  4.18242D-05    |proj g|=  1.20501D-05\n",
      "\n",
      "At iterate   19    f=  2.11900D-05    |proj g|=  5.68658D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     19     23      1     0     0   5.687D-06   2.119D-05\n",
      "  F =   2.1190027553761497E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  7.40945D-02\n",
      "\n",
      "At iterate    1    f=  6.81608D-01    |proj g|=  6.70269D-02\n",
      "\n",
      "At iterate    2    f=  6.30994D-01    |proj g|=  7.40886D-02\n",
      "\n",
      "At iterate    3    f=  3.40229D-01    |proj g|=  2.68678D-02\n",
      "\n",
      "At iterate    4    f=  2.53336D-01    |proj g|=  1.96341D-02\n",
      "\n",
      "At iterate    5    f=  1.24027D-01    |proj g|=  8.30257D-03\n",
      "\n",
      "At iterate    6    f=  5.89619D-02    |proj g|=  6.33284D-03\n",
      "\n",
      "At iterate    7    f=  3.22484D-02    |proj g|=  5.39666D-03\n",
      "\n",
      "At iterate    8    f=  1.55640D-02    |proj g|=  2.05725D-03\n",
      "\n",
      "At iterate    9    f=  8.11948D-03    |proj g|=  8.24005D-04\n",
      "\n",
      "At iterate   10    f=  4.09744D-03    |proj g|=  3.83490D-04\n",
      "\n",
      "At iterate   11    f=  2.07819D-03    |proj g|=  1.83163D-04\n",
      "\n",
      "At iterate   12    f=  1.53405D-03    |proj g|=  1.15657D-03\n",
      "\n",
      "At iterate   13    f=  4.77196D-04    |proj g|=  7.77792D-05\n",
      "\n",
      "At iterate   14    f=  3.47008D-04    |proj g|=  5.86459D-05\n",
      "\n",
      "At iterate   15    f=  1.53541D-04    |proj g|=  2.45984D-05\n",
      "\n",
      "At iterate   16    f=  7.14713D-05    |proj g|=  2.75912D-05\n",
      "\n",
      "At iterate   17    f=  3.89692D-05    |proj g|=  1.18389D-05\n",
      "\n",
      "At iterate   18    f=  1.96983D-05    |proj g|=  6.29303D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     18     21      1     0     0   6.293D-06   1.970D-05\n",
      "  F =   1.9698277501192364E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.34519D-01\n",
      "\n",
      "At iterate    1    f=  6.61739D-01    |proj g|=  6.20902D-02\n",
      "\n",
      "At iterate    2    f=  5.73735D-01    |proj g|=  5.17460D-02\n",
      "\n",
      "At iterate    3    f=  3.03982D-01    |proj g|=  4.63444D-02\n",
      "\n",
      "At iterate    4    f=  2.13539D-01    |proj g|=  3.17197D-02\n",
      "\n",
      "At iterate    5    f=  1.25142D-01    |proj g|=  1.68292D-02\n",
      "\n",
      "At iterate    6    f=  7.55896D-02    |proj g|=  8.63978D-03\n",
      "\n",
      "At iterate    7    f=  4.46681D-02    |proj g|=  3.80006D-03\n",
      "\n",
      "At iterate    8    f=  2.75665D-02    |proj g|=  1.78197D-03\n",
      "\n",
      "At iterate    9    f=  1.74466D-02    |proj g|=  1.19731D-03\n",
      "\n",
      "At iterate   10    f=  1.11989D-02    |proj g|=  1.12221D-03\n",
      "\n",
      "At iterate   11    f=  8.07019D-03    |proj g|=  1.52036D-03\n",
      "\n",
      "At iterate   12    f=  7.52983D-03    |proj g|=  1.11852D-03\n",
      "\n",
      "At iterate   13    f=  5.12971D-03    |proj g|=  4.22005D-04\n",
      "\n",
      "At iterate   14    f=  3.44223D-03    |proj g|=  3.54939D-04\n",
      "\n",
      "At iterate   15    f=  1.97042D-03    |proj g|=  1.47432D-04\n",
      "\n",
      "At iterate   16    f=  1.00781D-03    |proj g|=  1.71323D-04\n",
      "\n",
      "At iterate   17    f=  4.05580D-04    |proj g|=  3.26066D-05\n",
      "\n",
      "At iterate   18    f=  2.55150D-04    |proj g|=  2.23704D-05\n",
      "\n",
      "At iterate   19    f=  1.15956D-04    |proj g|=  1.09669D-05\n",
      "\n",
      "At iterate   20    f=  5.99457D-05    |proj g|=  5.25828D-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     20     25      1     0     0   5.258D-06   5.995D-05\n",
      "  F =   5.9945709404041637E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  3.61349D-01\n",
      "\n",
      "At iterate    1    f=  5.65670D-01    |proj g|=  1.37566D-01\n",
      "\n",
      "At iterate    2    f=  4.66276D-01    |proj g|=  9.30622D-02\n",
      "\n",
      "At iterate    3    f=  4.19274D-01    |proj g|=  3.46881D-02\n",
      "\n",
      "At iterate    4    f=  4.11770D-01    |proj g|=  3.66144D-02\n",
      "\n",
      "At iterate    5    f=  3.15775D-01    |proj g|=  1.13183D-01\n",
      "\n",
      "At iterate    6    f=  2.42010D-01    |proj g|=  9.55014D-02\n",
      "\n",
      "At iterate    7    f=  1.48619D-01    |proj g|=  4.66745D-02\n",
      "\n",
      "At iterate    8    f=  9.80632D-02    |proj g|=  2.26730D-02\n",
      "\n",
      "At iterate    9    f=  6.52375D-02    |proj g|=  1.02071D-02\n",
      "\n",
      "At iterate   10    f=  4.23820D-02    |proj g|=  4.39368D-03\n",
      "\n",
      "At iterate   11    f=  2.34547D-02    |proj g|=  2.92635D-03\n",
      "\n",
      "At iterate   12    f=  1.14444D-02    |proj g|=  1.85975D-03\n",
      "\n",
      "At iterate   13    f=  4.56618D-03    |proj g|=  1.25780D-03\n",
      "\n",
      "At iterate   14    f=  2.71899D-03    |proj g|=  6.14175D-04\n",
      "\n",
      "At iterate   15    f=  1.28585D-03    |proj g|=  4.56672D-04\n",
      "\n",
      "At iterate   16    f=  1.26381D-03    |proj g|=  3.72975D-04\n",
      "\n",
      "At iterate   17    f=  6.65671D-04    |proj g|=  1.63026D-04\n",
      "\n",
      "At iterate   18    f=  3.36664D-04    |proj g|=  8.56487D-05\n",
      "\n",
      "At iterate   19    f=  1.69939D-04    |proj g|=  4.30681D-05\n",
      "\n",
      "At iterate   20    f=  8.52248D-05    |proj g|=  2.15637D-05\n",
      "\n",
      "At iterate   21    f=  4.27450D-05    |proj g|=  1.07685D-05\n",
      "\n",
      "At iterate   22    f=  2.14101D-05    |proj g|=  5.37211D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     22     25      1     0     0   5.372D-06   2.141D-05\n",
      "  F =   2.1410126894904256E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:00<00:00, 78.87it/s]\n"
     ]
    }
   ],
   "source": [
    "MLM.fit_LocalModels(X_train, y_train, \n",
    "                    eps=avg_variance, num_noise_samp=100, \n",
    "                    classification=True, alpha=0, max_iter=10000, random_seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "589594b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:00<00:00, 5520.22it/s]\n",
      "100%|██████████| 26/26 [00:00<00:00, 590.68it/s]\n",
      "100%|██████████| 26/26 [00:00<00:00, 6307.22it/s]\n",
      "100%|██████████| 26/26 [00:00<00:00, 881.96it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_lmm_train = MLM.predict(X_train, covariance_tied=False, uniform_prior=False)\n",
    "pred_lmm_test = MLM.predict(X_test, covariance_tied=False, uniform_prior=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2b46e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLM-CELL: Train RMSE:1.000 / Test RMSE:0.755\n"
     ]
    }
   ],
   "source": [
    "print('MLM-CELL: Train RMSE:{:3.3f} / Test RMSE:{:3.3f}'.format(\n",
    "            roc_auc_score(y_train,np.array(pred_lmm_train)),\n",
    "            roc_auc_score(y_test,np.array(pred_lmm_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42e4e058",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/26 [00:00<?, ?it/s]/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/discrete/discrete_model.py:1819: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "100%|██████████| 26/26 [00:00<00:00, 321.30it/s]\n",
      "/storage/work/eak5582/Research/generalized_mlm_2.py:404: ClusterWarning: scipy.cluster: The symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix\n",
      "  LocalModelsTree = linkage(self.dist_mat_avg, 'ward')\n",
      "  0%|          | 0/10 [00:00<?, ?it/s] This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " This problem is unconstrained.\n",
      " 70%|███████   | 7/10 [00:00<00:00, 68.66it/s] This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " This problem is unconstrained.\n",
      "100%|██████████| 10/10 [00:00<00:00, 51.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.68212D-01\n",
      "\n",
      "At iterate    1    f=  6.39606D-01    |proj g|=  5.96318D-02\n",
      "\n",
      "At iterate    2    f=  5.75592D-01    |proj g|=  6.24334D-02\n",
      "\n",
      "At iterate    3    f=  4.35866D-01    |proj g|=  1.04588D-01\n",
      "\n",
      "At iterate    4    f=  3.51845D-01    |proj g|=  6.91284D-02\n",
      "\n",
      "At iterate    5    f=  2.98320D-01    |proj g|=  3.59675D-02\n",
      "\n",
      "At iterate    6    f=  2.69151D-01    |proj g|=  1.59973D-02\n",
      "\n",
      "At iterate    7    f=  2.49225D-01    |proj g|=  1.46733D-02\n",
      "\n",
      "At iterate    8    f=  2.27158D-01    |proj g|=  1.83710D-02\n",
      "\n",
      "At iterate    9    f=  1.78617D-01    |proj g|=  1.20218D-02\n",
      "\n",
      "At iterate   10    f=  1.14312D-01    |proj g|=  5.11040D-03\n",
      "\n",
      "At iterate   11    f=  8.80477D-02    |proj g|=  5.27929D-03\n",
      "\n",
      "At iterate   12    f=  8.60483D-02    |proj g|=  7.46849D-03\n",
      "\n",
      "At iterate   13    f=  8.06090D-02    |proj g|=  3.87408D-03\n",
      "\n",
      "At iterate   14    f=  7.52354D-02    |proj g|=  2.48181D-03\n",
      "\n",
      "At iterate   15    f=  7.08186D-02    |proj g|=  1.89312D-03\n",
      "\n",
      "At iterate   16    f=  6.40312D-02    |proj g|=  2.04052D-03\n",
      "\n",
      "At iterate   17    f=  5.70918D-02    |proj g|=  3.05673D-03\n",
      "\n",
      "At iterate   18    f=  5.17699D-02    |proj g|=  3.82684D-03\n",
      "\n",
      "At iterate   19    f=  4.62603D-02    |proj g|=  2.61060D-03\n",
      "\n",
      "At iterate   20    f=  4.09947D-02    |proj g|=  2.10974D-03\n",
      "\n",
      "At iterate   21    f=  2.95066D-02    |proj g|=  1.77668D-03\n",
      "\n",
      "At iterate   22    f=  2.27542D-02    |proj g|=  3.62178D-03\n",
      "\n",
      "At iterate   23    f=  2.07795D-02    |proj g|=  9.26999D-03\n",
      "\n",
      "At iterate   24    f=  1.31649D-02    |proj g|=  1.87695D-03\n",
      "\n",
      "At iterate   25    f=  1.11745D-02    |proj g|=  2.10434D-03\n",
      "\n",
      "At iterate   26    f=  5.25833D-03    |proj g|=  1.03775D-03\n",
      "\n",
      "At iterate   27    f=  2.36937D-03    |proj g|=  4.40201D-04\n",
      "\n",
      "At iterate   28    f=  1.13852D-03    |proj g|=  2.25490D-04\n",
      "\n",
      "At iterate   29    f=  5.70225D-04    |proj g|=  1.41608D-04\n",
      "\n",
      "At iterate   30    f=  3.08818D-04    |proj g|=  8.15274D-05\n",
      "\n",
      "At iterate   31    f=  1.50361D-04    |proj g|=  4.55773D-05\n",
      "\n",
      "At iterate   32    f=  7.77066D-05    |proj g|=  3.00272D-05\n",
      "\n",
      "At iterate   33    f=  3.85745D-05    |proj g|=  1.40463D-05\n",
      "\n",
      "At iterate   34    f=  2.37182D-05    |proj g|=  1.03371D-05\n",
      "\n",
      "At iterate   35    f=  2.10695D-05    |proj g|=  8.24433D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     35     42      1     0     0   8.244D-06   2.107D-05\n",
      "  F =   2.1069542478132071E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  3.55484D-01\n",
      "\n",
      "At iterate    1    f=  5.87424D-01    |proj g|=  1.49857D-01\n",
      "\n",
      "At iterate    2    f=  4.78342D-01    |proj g|=  1.01516D-01\n",
      "\n",
      "At iterate    3    f=  4.30601D-01    |proj g|=  3.40788D-02\n",
      "\n",
      "At iterate    4    f=  4.23897D-01    |proj g|=  3.18603D-02\n",
      "\n",
      "At iterate    5    f=  3.52193D-01    |proj g|=  9.67573D-02\n",
      "\n",
      "At iterate    6    f=  2.90705D-01    |proj g|=  8.87208D-02\n",
      "\n",
      "At iterate    7    f=  1.86173D-01    |proj g|=  5.07818D-02\n",
      "\n",
      "At iterate    8    f=  1.29968D-01    |proj g|=  2.98810D-02\n",
      "\n",
      "At iterate    9    f=  9.64711D-02    |proj g|=  1.83655D-02\n",
      "\n",
      "At iterate   10    f=  7.17839D-02    |proj g|=  1.06054D-02\n",
      "\n",
      "At iterate   11    f=  4.98148D-02    |proj g|=  4.86907D-03\n",
      "\n",
      "At iterate   12    f=  2.91095D-02    |proj g|=  4.69030D-03\n",
      "\n",
      "At iterate   13    f=  1.52940D-02    |proj g|=  4.12338D-03\n",
      "\n",
      "At iterate   14    f=  7.01109D-03    |proj g|=  4.98438D-03\n",
      "\n",
      "At iterate   15    f=  3.38662D-03    |proj g|=  1.64908D-03\n",
      "\n",
      "At iterate   16    f=  1.94172D-03    |proj g|=  8.56725D-04\n",
      "\n",
      "At iterate   17    f=  9.84509D-04    |proj g|=  4.14271D-04\n",
      "\n",
      "At iterate   18    f=  5.06942D-04    |proj g|=  1.97933D-04\n",
      "\n",
      "At iterate   19    f=  2.55454D-04    |proj g|=  9.86451D-05\n",
      "\n",
      "At iterate   20    f=  1.28889D-04    |proj g|=  3.74073D-05\n",
      "\n",
      "At iterate   21    f=  6.29366D-05    |proj g|=  2.46424D-05\n",
      "\n",
      "At iterate   22    f=  4.94464D-05    |proj g|=  2.44858D-05\n",
      "\n",
      "At iterate   23    f=  2.43033D-05    |proj g|=  7.82561D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     23     26      1     0     0   7.826D-06   2.430D-05\n",
      "  F =   2.4303299428922982E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.94636D-01\n",
      "\n",
      "At iterate    1    f=  6.25266D-01    |proj g|=  4.64196D-02\n",
      "\n",
      "At iterate    2    f=  6.06226D-01    |proj g|=  4.29377D-02\n",
      "\n",
      "At iterate    3    f=  4.53338D-01    |proj g|=  1.21158D-01\n",
      "\n",
      "At iterate    4    f=  3.92607D-01    |proj g|=  9.21834D-02\n",
      "\n",
      "At iterate    5    f=  3.35121D-01    |proj g|=  4.38159D-02\n",
      "\n",
      "At iterate    6    f=  3.09366D-01    |proj g|=  1.52752D-02\n",
      "\n",
      "At iterate    7    f=  2.94764D-01    |proj g|=  1.86093D-02\n",
      "\n",
      "At iterate    8    f=  2.82150D-01    |proj g|=  2.32679D-02\n",
      "\n",
      "At iterate    9    f=  2.60286D-01    |proj g|=  2.57110D-02\n",
      "\n",
      "At iterate   10    f=  1.92914D-01    |proj g|=  6.17210D-03\n",
      "\n",
      "At iterate   11    f=  1.61273D-01    |proj g|=  2.21824D-02\n",
      "\n",
      "At iterate   12    f=  1.46259D-01    |proj g|=  1.14027D-02\n",
      "\n",
      "At iterate   13    f=  1.23414D-01    |proj g|=  1.15065D-02\n",
      "\n",
      "At iterate   14    f=  1.15426D-01    |proj g|=  1.67969D-02\n",
      "\n",
      "At iterate   15    f=  8.89189D-02    |proj g|=  1.26484D-02\n",
      "\n",
      "At iterate   16    f=  6.09032D-02    |proj g|=  4.36030D-03\n",
      "\n",
      "At iterate   17    f=  5.01137D-02    |proj g|=  2.17269D-03\n",
      "\n",
      "At iterate   18    f=  4.12867D-02    |proj g|=  1.89983D-03\n",
      "\n",
      "At iterate   19    f=  2.89560D-02    |proj g|=  2.68456D-03\n",
      "\n",
      "At iterate   20    f=  1.39826D-02    |proj g|=  1.19946D-03\n",
      "\n",
      "At iterate   21    f=  6.03525D-03    |proj g|=  1.17132D-03\n",
      "\n",
      "At iterate   22    f=  2.80997D-03    |proj g|=  1.13048D-03\n",
      "\n",
      "At iterate   23    f=  1.69291D-03    |proj g|=  6.19639D-04\n",
      "\n",
      "At iterate   24    f=  8.03930D-04    |proj g|=  2.47852D-04\n",
      "\n",
      "At iterate   25    f=  4.24122D-04    |proj g|=  1.94517D-04\n",
      "\n",
      "At iterate   26    f=  3.68366D-04    |proj g|=  1.14461D-04\n",
      "\n",
      "At iterate   27    f=  1.75316D-04    |proj g|=  2.36794D-05\n",
      "\n",
      "At iterate   28    f=  1.00664D-04    |proj g|=  1.57045D-05\n",
      "\n",
      "At iterate   29    f=  4.76099D-05    |proj g|=  6.47840D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     29     34      1     0     0   6.478D-06   4.761D-05\n",
      "  F =   4.7609870954909712E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.20033D-01\n",
      "\n",
      "At iterate    1    f=  6.76867D-01    |proj g|=  5.46176D-02\n",
      "\n",
      "At iterate    2    f=  5.08534D-01    |proj g|=  6.83329D-02\n",
      "\n",
      "At iterate    3    f=  2.92618D-01    |proj g|=  1.73647D-02\n",
      "\n",
      "At iterate    4    f=  2.26561D-01    |proj g|=  1.08643D-02\n",
      "\n",
      "At iterate    5    f=  1.55360D-01    |proj g|=  6.30751D-03\n",
      "\n",
      "At iterate    6    f=  9.40914D-02    |proj g|=  5.07971D-03\n",
      "\n",
      "At iterate    7    f=  6.88601D-02    |proj g|=  3.64303D-03\n",
      "\n",
      "At iterate    8    f=  5.03950D-02    |proj g|=  2.56510D-03\n",
      "\n",
      "At iterate    9    f=  4.12952D-02    |proj g|=  5.27247D-03\n",
      "\n",
      "At iterate   10    f=  3.38011D-02    |proj g|=  1.74666D-03\n",
      "\n",
      "At iterate   11    f=  2.90301D-02    |proj g|=  1.62791D-03\n",
      "\n",
      "At iterate   12    f=  2.86770D-02    |proj g|=  2.70790D-03\n",
      "\n",
      "At iterate   13    f=  2.52166D-02    |proj g|=  8.96837D-04\n",
      "\n",
      "At iterate   14    f=  2.14129D-02    |proj g|=  1.22917D-03\n",
      "\n",
      "At iterate   15    f=  1.84712D-02    |proj g|=  9.87477D-04\n",
      "\n",
      "At iterate   16    f=  1.49721D-02    |proj g|=  4.53206D-04\n",
      "\n",
      "At iterate   17    f=  1.05682D-02    |proj g|=  1.02041D-03\n",
      "\n",
      "At iterate   18    f=  5.30178D-03    |proj g|=  6.89128D-04\n",
      "\n",
      "At iterate   19    f=  2.41318D-03    |proj g|=  4.67663D-04\n",
      "\n",
      "At iterate   20    f=  1.14030D-03    |proj g|=  1.51253D-04\n",
      "\n",
      "At iterate   21    f=  5.65880D-04    |proj g|=  6.12707D-05\n",
      "\n",
      "At iterate   22    f=  2.77568D-04    |proj g|=  3.53234D-05\n",
      "\n",
      "At iterate   23    f=  2.48909D-04    |proj g|=  9.00633D-05\n",
      "\n",
      "At iterate   24    f=  1.17155D-04    |proj g|=  2.29241D-05\n",
      "\n",
      "At iterate   25    f=  6.10188D-05    |proj g|=  1.05643D-05\n",
      "\n",
      "At iterate   26    f=  2.99900D-05    |proj g|=  4.77592D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     26     32      1     0     0   4.776D-06   2.999D-05\n",
      "  F =   2.9989973962371498E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  3.34697D-01\n",
      "\n",
      "At iterate    1    f=  6.46525D-01    |proj g|=  1.66990D-01\n",
      "\n",
      "At iterate    2    f=  5.09114D-01    |proj g|=  1.10943D-01\n",
      "\n",
      "At iterate    3    f=  4.59748D-01    |proj g|=  4.11297D-02\n",
      "\n",
      "At iterate    4    f=  4.53089D-01    |proj g|=  4.12550D-02\n",
      "\n",
      "At iterate    5    f=  3.72513D-01    |proj g|=  1.07294D-01\n",
      "\n",
      "At iterate    6    f=  2.99326D-01    |proj g|=  1.04688D-01\n",
      "\n",
      "At iterate    7    f=  1.92426D-01    |proj g|=  6.87749D-02\n",
      "\n",
      "At iterate    8    f=  1.40513D-01    |proj g|=  3.96882D-02\n",
      "\n",
      "At iterate    9    f=  9.55050D-02    |proj g|=  1.75604D-02\n",
      "\n",
      "At iterate   10    f=  6.97650D-02    |proj g|=  8.17467D-03\n",
      "\n",
      "At iterate   11    f=  4.91825D-02    |proj g|=  3.68133D-03\n",
      "\n",
      "At iterate   12    f=  2.94595D-02    |proj g|=  2.42068D-03\n",
      "\n",
      "At iterate   13    f=  1.27409D-02    |proj g|=  1.56272D-03\n",
      "\n",
      "At iterate   14    f=  6.33286D-03    |proj g|=  1.79121D-03\n",
      "\n",
      "At iterate   15    f=  3.29919D-03    |proj g|=  6.09624D-04\n",
      "\n",
      "At iterate   16    f=  1.54858D-03    |proj g|=  3.94965D-04\n",
      "\n",
      "At iterate   17    f=  8.80494D-04    |proj g|=  1.86773D-04\n",
      "\n",
      "At iterate   18    f=  4.45675D-04    |proj g|=  1.09560D-04\n",
      "\n",
      "At iterate   19    f=  2.24639D-04    |proj g|=  2.92613D-05\n",
      "\n",
      "At iterate   20    f=  1.12192D-04    |proj g|=  2.10483D-05\n",
      "\n",
      "At iterate   21    f=  6.78065D-05    |proj g|=  3.21833D-05\n",
      "\n",
      "At iterate   22    f=  3.75732D-05    |proj g|=  1.44268D-05\n",
      "\n",
      "At iterate   23    f=  1.88272D-05    |proj g|=  6.71205D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     23     25      1     0     0   6.712D-06   1.883D-05\n",
      "  F =   1.8827156282269553E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  9.61327D-02\n",
      "\n",
      "At iterate    1    f=  6.76397D-01    |proj g|=  6.36428D-02\n",
      "\n",
      "At iterate    2    f=  6.15135D-01    |proj g|=  6.04437D-02\n",
      "\n",
      "At iterate    3    f=  4.47608D-01    |proj g|=  4.56965D-02\n",
      "\n",
      "At iterate    4    f=  3.14957D-01    |proj g|=  3.21490D-02\n",
      "\n",
      "At iterate    5    f=  2.29205D-01    |proj g|=  3.85593D-02\n",
      "\n",
      "At iterate    6    f=  1.69112D-01    |proj g|=  1.24120D-02\n",
      "\n",
      "At iterate    7    f=  1.32724D-01    |proj g|=  4.70357D-03\n",
      "\n",
      "At iterate    8    f=  1.07630D-01    |proj g|=  5.34023D-03\n",
      "\n",
      "At iterate    9    f=  9.04480D-02    |proj g|=  1.63745D-02\n",
      "\n",
      "At iterate   10    f=  7.26861D-02    |proj g|=  7.93247D-03\n",
      "\n",
      "At iterate   11    f=  6.11702D-02    |proj g|=  3.45101D-03\n",
      "\n",
      "At iterate   12    f=  5.23054D-02    |proj g|=  5.73432D-03\n",
      "\n",
      "At iterate   13    f=  4.48842D-02    |proj g|=  8.01422D-03\n",
      "\n",
      "At iterate   14    f=  4.00474D-02    |proj g|=  4.65719D-03\n",
      "\n",
      "At iterate   15    f=  3.70612D-02    |proj g|=  4.90008D-03\n",
      "\n",
      "At iterate   16    f=  3.37049D-02    |proj g|=  5.53282D-03\n",
      "\n",
      "At iterate   17    f=  2.72935D-02    |proj g|=  5.33123D-03\n",
      "\n",
      "At iterate   18    f=  1.51601D-02    |proj g|=  4.18587D-03\n",
      "\n",
      "At iterate   19    f=  5.31954D-03    |proj g|=  1.23375D-03\n",
      "\n",
      "At iterate   20    f=  3.46219D-03    |proj g|=  6.03817D-04\n",
      "\n",
      "At iterate   21    f=  2.03234D-03    |proj g|=  2.21536D-04\n",
      "\n",
      "At iterate   22    f=  1.10508D-03    |proj g|=  1.42003D-04\n",
      "\n",
      "At iterate   23    f=  5.82821D-04    |proj g|=  2.43569D-04\n",
      "\n",
      "At iterate   24    f=  2.56685D-04    |proj g|=  9.64286D-05\n",
      "\n",
      "At iterate   25    f=  1.47275D-04    |proj g|=  4.28336D-05\n",
      "\n",
      "At iterate   26    f=  7.05478D-05    |proj g|=  2.61978D-05\n",
      "\n",
      "At iterate   27    f=  4.39312D-05    |proj g|=  1.97264D-05\n",
      "\n",
      "At iterate   28    f=  2.28600D-05    |proj g|=  6.39120D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     28     31      1     0     0   6.391D-06   2.286D-05\n",
      "  F =   2.2859970010005976E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  6.74302D-02\n",
      "\n",
      "At iterate    1    f=  6.63765D-01    |proj g|=  1.09000D-01\n",
      "\n",
      "At iterate    2    f=  6.24806D-01    |proj g|=  7.74820D-02\n",
      "\n",
      "At iterate    3    f=  5.67898D-01    |proj g|=  5.36218D-02\n",
      "\n",
      "At iterate    4    f=  5.29587D-01    |proj g|=  5.26481D-02\n",
      "\n",
      "At iterate    5    f=  4.26220D-01    |proj g|=  5.30104D-02\n",
      "\n",
      "At iterate    6    f=  2.95303D-01    |proj g|=  3.46951D-02\n",
      "\n",
      "At iterate    7    f=  2.14997D-01    |proj g|=  1.49932D-02\n",
      "\n",
      "At iterate    8    f=  1.70461D-01    |proj g|=  5.57642D-03\n",
      "\n",
      "At iterate    9    f=  1.40150D-01    |proj g|=  5.52768D-03\n",
      "\n",
      "At iterate   10    f=  1.08379D-01    |proj g|=  5.02030D-03\n",
      "\n",
      "At iterate   11    f=  8.99522D-02    |proj g|=  1.24288D-02\n",
      "\n",
      "At iterate   12    f=  6.41856D-02    |proj g|=  5.45436D-03\n",
      "\n",
      "At iterate   13    f=  5.92323D-02    |proj g|=  1.11111D-02\n",
      "\n",
      "At iterate   14    f=  5.00501D-02    |proj g|=  2.88211D-03\n",
      "\n",
      "At iterate   15    f=  4.54268D-02    |proj g|=  2.51377D-03\n",
      "\n",
      "At iterate   16    f=  3.96230D-02    |proj g|=  2.19424D-03\n",
      "\n",
      "At iterate   17    f=  3.38448D-02    |proj g|=  7.25679D-03\n",
      "\n",
      "At iterate   18    f=  2.54710D-02    |proj g|=  3.96123D-03\n",
      "\n",
      "At iterate   19    f=  1.73302D-02    |proj g|=  1.93298D-03\n",
      "\n",
      "At iterate   20    f=  1.07775D-02    |proj g|=  1.38243D-03\n",
      "\n",
      "At iterate   21    f=  4.89957D-03    |proj g|=  7.30009D-04\n",
      "\n",
      "At iterate   22    f=  2.37051D-03    |proj g|=  3.60400D-04\n",
      "\n",
      "At iterate   23    f=  1.23549D-03    |proj g|=  3.31124D-04\n",
      "\n",
      "At iterate   24    f=  5.13874D-04    |proj g|=  1.56341D-04\n",
      "\n",
      "At iterate   25    f=  4.51717D-04    |proj g|=  1.43405D-04\n",
      "\n",
      "At iterate   26    f=  2.78093D-04    |proj g|=  4.82790D-05\n",
      "\n",
      "At iterate   27    f=  1.45348D-04    |proj g|=  1.92957D-05\n",
      "\n",
      "At iterate   28    f=  7.43990D-05    |proj g|=  9.82583D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     28     33      1     0     0   9.826D-06   7.440D-05\n",
      "  F =   7.4399006917024018E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  7.98428D-02\n",
      "\n",
      "At iterate    1    f=  6.76263D-01    |proj g|=  5.85073D-02\n",
      "\n",
      "At iterate    2    f=  6.46574D-01    |proj g|=  5.23450D-02\n",
      "\n",
      "At iterate    3    f=  5.42587D-01    |proj g|=  4.87067D-02\n",
      "\n",
      "At iterate    4    f=  4.33404D-01    |proj g|=  4.04370D-02\n",
      "\n",
      "At iterate    5    f=  3.30676D-01    |proj g|=  2.32406D-02\n",
      "\n",
      "At iterate    6    f=  2.79329D-01    |proj g|=  9.81474D-03\n",
      "\n",
      "At iterate    7    f=  2.64989D-01    |proj g|=  1.21469D-02\n",
      "\n",
      "At iterate    8    f=  2.49307D-01    |proj g|=  1.01453D-02\n",
      "\n",
      "At iterate    9    f=  2.37958D-01    |proj g|=  6.22987D-03\n",
      "\n",
      "At iterate   10    f=  2.31730D-01    |proj g|=  2.15688D-03\n",
      "\n",
      "At iterate   11    f=  2.30231D-01    |proj g|=  1.31687D-03\n",
      "\n",
      "At iterate   12    f=  2.29874D-01    |proj g|=  9.12842D-03\n",
      "\n",
      "At iterate   13    f=  2.28926D-01    |proj g|=  1.35432D-03\n",
      "\n",
      "At iterate   14    f=  2.28233D-01    |proj g|=  2.64476D-03\n",
      "\n",
      "At iterate   15    f=  2.27392D-01    |proj g|=  3.02830D-03\n",
      "\n",
      "At iterate   16    f=  2.26311D-01    |proj g|=  1.85428D-03\n",
      "\n",
      "At iterate   17    f=  2.25506D-01    |proj g|=  2.74466D-03\n",
      "\n",
      "At iterate   18    f=  2.25152D-01    |proj g|=  2.19884D-03\n",
      "\n",
      "At iterate   19    f=  2.24864D-01    |proj g|=  1.30515D-03\n",
      "\n",
      "At iterate   20    f=  2.24605D-01    |proj g|=  1.45456D-03\n",
      "\n",
      "At iterate   21    f=  2.24305D-01    |proj g|=  1.25843D-03\n",
      "\n",
      "At iterate   22    f=  2.24050D-01    |proj g|=  7.70043D-04\n",
      "\n",
      "At iterate   23    f=  2.23967D-01    |proj g|=  2.92420D-03\n",
      "\n",
      "At iterate   24    f=  2.23884D-01    |proj g|=  1.20125D-03\n",
      "\n",
      "At iterate   25    f=  2.23866D-01    |proj g|=  4.59375D-04\n",
      "\n",
      "At iterate   26    f=  2.23858D-01    |proj g|=  3.58652D-04\n",
      "\n",
      "At iterate   27    f=  2.23842D-01    |proj g|=  5.18833D-04\n",
      "\n",
      "At iterate   28    f=  2.23822D-01    |proj g|=  6.61044D-04\n",
      "\n",
      "At iterate   29    f=  2.23779D-01    |proj g|=  7.62662D-04\n",
      "\n",
      "At iterate   30    f=  2.23765D-01    |proj g|=  6.37123D-04\n",
      "\n",
      "At iterate   31    f=  2.23725D-01    |proj g|=  4.38567D-04\n",
      "\n",
      "At iterate   32    f=  2.23710D-01    |proj g|=  4.99423D-04\n",
      "\n",
      "At iterate   33    f=  2.23690D-01    |proj g|=  4.23387D-04\n",
      "\n",
      "At iterate   34    f=  2.23667D-01    |proj g|=  2.24983D-04\n",
      "\n",
      "At iterate   35    f=  2.23665D-01    |proj g|=  6.02006D-04\n",
      "\n",
      "At iterate   36    f=  2.23654D-01    |proj g|=  3.21891D-04\n",
      "\n",
      "At iterate   37    f=  2.23646D-01    |proj g|=  2.27245D-04\n",
      "\n",
      "At iterate   38    f=  2.23644D-01    |proj g|=  3.01951D-04\n",
      "\n",
      "At iterate   39    f=  2.23640D-01    |proj g|=  2.63500D-04\n",
      "\n",
      "At iterate   40    f=  2.23626D-01    |proj g|=  2.25013D-04\n",
      "\n",
      "At iterate   41    f=  2.23608D-01    |proj g|=  1.02762D-04\n",
      "\n",
      "At iterate   42    f=  2.23604D-01    |proj g|=  1.55637D-04\n",
      "\n",
      "At iterate   43    f=  2.23599D-01    |proj g|=  1.59075D-04\n",
      "\n",
      "At iterate   44    f=  2.23592D-01    |proj g|=  1.92679D-04\n",
      "\n",
      "At iterate   45    f=  2.23589D-01    |proj g|=  1.10217D-04\n",
      "\n",
      "At iterate   46    f=  2.23586D-01    |proj g|=  2.89527D-04\n",
      "\n",
      "At iterate   47    f=  2.23586D-01    |proj g|=  2.44659D-04\n",
      "\n",
      "At iterate   48    f=  2.23585D-01    |proj g|=  5.77010D-05\n",
      "\n",
      "At iterate   49    f=  2.23585D-01    |proj g|=  8.80105D-05\n",
      "\n",
      "At iterate   50    f=  2.23584D-01    |proj g|=  1.76597D-04\n",
      "\n",
      "At iterate   51    f=  2.23583D-01    |proj g|=  2.08864D-04\n",
      "\n",
      "At iterate   52    f=  2.23582D-01    |proj g|=  3.52798D-04\n",
      "\n",
      "At iterate   53    f=  2.23580D-01    |proj g|=  8.62172D-05\n",
      "\n",
      "At iterate   54    f=  2.23579D-01    |proj g|=  6.19477D-05\n",
      "\n",
      "At iterate   55    f=  2.23575D-01    |proj g|=  5.22333D-05\n",
      "\n",
      "At iterate   56    f=  2.23573D-01    |proj g|=  1.39775D-04\n",
      "\n",
      "At iterate   57    f=  2.23572D-01    |proj g|=  1.85525D-04\n",
      "\n",
      "At iterate   58    f=  2.23571D-01    |proj g|=  4.99929D-05\n",
      "\n",
      "At iterate   59    f=  2.23571D-01    |proj g|=  6.94152D-05\n",
      "\n",
      "At iterate   60    f=  2.23570D-01    |proj g|=  1.06065D-04\n",
      "\n",
      "At iterate   61    f=  2.23569D-01    |proj g|=  2.01441D-04\n",
      "\n",
      "At iterate   62    f=  2.23568D-01    |proj g|=  7.47786D-05\n",
      "\n",
      "At iterate   63    f=  2.23568D-01    |proj g|=  3.92354D-05\n",
      "\n",
      "At iterate   64    f=  2.23567D-01    |proj g|=  2.92598D-05\n",
      "\n",
      "At iterate   65    f=  2.23567D-01    |proj g|=  6.16436D-05\n",
      "\n",
      "At iterate   66    f=  2.23566D-01    |proj g|=  2.68877D-05\n",
      "\n",
      "At iterate   67    f=  2.23566D-01    |proj g|=  6.65357D-05\n",
      "\n",
      "At iterate   68    f=  2.23565D-01    |proj g|=  3.35155D-05\n",
      "\n",
      "At iterate   69    f=  2.23565D-01    |proj g|=  3.18139D-05\n",
      "\n",
      "At iterate   70    f=  2.23565D-01    |proj g|=  8.10042D-05\n",
      "\n",
      "At iterate   71    f=  2.23565D-01    |proj g|=  2.99540D-05\n",
      "\n",
      "At iterate   72    f=  2.23564D-01    |proj g|=  4.69299D-05\n",
      "\n",
      "At iterate   73    f=  2.23564D-01    |proj g|=  4.52610D-05\n",
      "\n",
      "At iterate   74    f=  2.23564D-01    |proj g|=  1.02840D-04\n",
      "\n",
      "At iterate   75    f=  2.23564D-01    |proj g|=  4.80838D-05\n",
      "\n",
      "At iterate   76    f=  2.23564D-01    |proj g|=  1.59451D-05\n",
      "\n",
      "At iterate   77    f=  2.23564D-01    |proj g|=  2.90312D-05\n",
      "\n",
      "At iterate   78    f=  2.23564D-01    |proj g|=  3.04431D-05\n",
      "\n",
      "At iterate   79    f=  2.23564D-01    |proj g|=  6.60350D-05\n",
      "\n",
      "At iterate   80    f=  2.23563D-01    |proj g|=  1.53648D-05\n",
      "\n",
      "At iterate   81    f=  2.23563D-01    |proj g|=  1.57738D-05\n",
      "\n",
      "At iterate   82    f=  2.23563D-01    |proj g|=  4.89682D-05\n",
      "\n",
      "At iterate   83    f=  2.23563D-01    |proj g|=  3.16096D-05\n",
      "\n",
      "At iterate   84    f=  2.23563D-01    |proj g|=  2.64529D-05\n",
      "\n",
      "At iterate   85    f=  2.23563D-01    |proj g|=  3.92740D-05\n",
      "\n",
      "At iterate   86    f=  2.23563D-01    |proj g|=  1.70743D-05\n",
      "\n",
      "At iterate   87    f=  2.23563D-01    |proj g|=  6.55816D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     87     97      1     0     0   6.558D-06   2.236D-01\n",
      "  F =  0.22356313591502294     \n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  7.09703D-02\n",
      "\n",
      "At iterate    1    f=  6.71310D-01    |proj g|=  1.05964D-01\n",
      "\n",
      "At iterate    2    f=  6.38655D-01    |proj g|=  9.38863D-02\n",
      "\n",
      "At iterate    3    f=  4.84567D-01    |proj g|=  5.08883D-02\n",
      "\n",
      "At iterate    4    f=  4.23013D-01    |proj g|=  4.28528D-02\n",
      "\n",
      "At iterate    5    f=  2.78278D-01    |proj g|=  1.97460D-02\n",
      "\n",
      "At iterate    6    f=  2.16697D-01    |proj g|=  2.75691D-02\n",
      "\n",
      "At iterate    7    f=  1.60173D-01    |proj g|=  8.41247D-03\n",
      "\n",
      "At iterate    8    f=  1.39601D-01    |proj g|=  8.09613D-03\n",
      "\n",
      "At iterate    9    f=  1.04535D-01    |proj g|=  5.06487D-03\n",
      "\n",
      "At iterate   10    f=  8.44412D-02    |proj g|=  8.66233D-03\n",
      "\n",
      "At iterate   11    f=  7.52069D-02    |proj g|=  6.74857D-03\n",
      "\n",
      "At iterate   12    f=  7.44881D-02    |proj g|=  2.06958D-03\n",
      "\n",
      "At iterate   13    f=  6.78339D-02    |proj g|=  1.05902D-03\n",
      "\n",
      "At iterate   14    f=  6.15132D-02    |proj g|=  3.40092D-03\n",
      "\n",
      "At iterate   15    f=  5.52986D-02    |proj g|=  1.87706D-03\n",
      "\n",
      "At iterate   16    f=  5.37962D-02    |proj g|=  1.72696D-03\n",
      "\n",
      "At iterate   17    f=  5.08216D-02    |proj g|=  3.24469D-03\n",
      "\n",
      "At iterate   18    f=  4.80014D-02    |proj g|=  1.40358D-03\n",
      "\n",
      "At iterate   19    f=  4.67552D-02    |proj g|=  1.61982D-03\n",
      "\n",
      "At iterate   20    f=  4.36618D-02    |proj g|=  1.56236D-03\n",
      "\n",
      "At iterate   21    f=  4.04248D-02    |proj g|=  1.49389D-03\n",
      "\n",
      "At iterate   22    f=  3.80177D-02    |proj g|=  2.26244D-03\n",
      "\n",
      "At iterate   23    f=  3.76772D-02    |proj g|=  1.97458D-03\n",
      "\n",
      "At iterate   24    f=  3.68812D-02    |proj g|=  1.40258D-03\n",
      "\n",
      "At iterate   25    f=  3.50316D-02    |proj g|=  9.16997D-04\n",
      "\n",
      "At iterate   26    f=  3.33284D-02    |proj g|=  7.84184D-04\n",
      "\n",
      "At iterate   27    f=  3.04350D-02    |proj g|=  2.90357D-03\n",
      "\n",
      "At iterate   28    f=  2.81916D-02    |proj g|=  1.03649D-03\n",
      "\n",
      "At iterate   29    f=  2.68690D-02    |proj g|=  1.12062D-03\n",
      "\n",
      "At iterate   30    f=  2.48619D-02    |proj g|=  1.74439D-03\n",
      "\n",
      "At iterate   31    f=  2.18402D-02    |proj g|=  2.29076D-03\n",
      "\n",
      "At iterate   32    f=  1.68493D-02    |proj g|=  2.18573D-03\n",
      "\n",
      "At iterate   33    f=  1.35919D-02    |proj g|=  3.58394D-03\n",
      "\n",
      "At iterate   34    f=  9.81038D-03    |proj g|=  2.90814D-03\n",
      "\n",
      "At iterate   35    f=  8.40730D-03    |proj g|=  1.46772D-03\n",
      "\n",
      "At iterate   36    f=  7.48917D-03    |proj g|=  8.14286D-04\n",
      "\n",
      "At iterate   37    f=  6.20073D-03    |proj g|=  7.69506D-04\n",
      "\n",
      "At iterate   38    f=  3.59395D-03    |proj g|=  4.17640D-04\n",
      "\n",
      "At iterate   39    f=  1.91514D-03    |proj g|=  2.30997D-04\n",
      "\n",
      "At iterate   40    f=  1.01268D-03    |proj g|=  3.89981D-04\n",
      "\n",
      "At iterate   41    f=  3.74066D-04    |proj g|=  6.18406D-05\n",
      "\n",
      "At iterate   42    f=  2.39404D-04    |proj g|=  3.83877D-05\n",
      "\n",
      "At iterate   43    f=  1.08442D-04    |proj g|=  2.31499D-05\n",
      "\n",
      "At iterate   44    f=  5.15848D-05    |proj g|=  1.35764D-05\n",
      "\n",
      "At iterate   45    f=  2.62072D-05    |proj g|=  4.45899D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     45     50      1     0     0   4.459D-06   2.621D-05\n",
      "  F =   2.6207169957896056E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.59181D-01\n",
      "\n",
      "At iterate    1    f=  6.44360D-01    |proj g|=  6.61716D-02\n",
      "\n",
      "At iterate    2    f=  6.04060D-01    |proj g|=  6.08584D-02\n",
      "\n",
      "At iterate    3    f=  5.09592D-01    |proj g|=  4.44723D-02\n",
      "\n",
      "At iterate    4    f=  4.28982D-01    |proj g|=  3.39084D-02\n",
      "\n",
      "At iterate    5    f=  2.72384D-01    |proj g|=  2.64362D-02\n",
      "\n",
      "At iterate    6    f=  2.16150D-01    |proj g|=  2.74969D-02\n",
      "\n",
      "At iterate    7    f=  1.69430D-01    |proj g|=  1.40892D-02\n",
      "\n",
      "At iterate    8    f=  1.41282D-01    |proj g|=  9.30571D-03\n",
      "\n",
      "At iterate    9    f=  1.19233D-01    |proj g|=  9.94252D-03\n",
      "\n",
      "At iterate   10    f=  9.18621D-02    |proj g|=  9.65956D-03\n",
      "\n",
      "At iterate   11    f=  8.00223D-02    |proj g|=  1.31881D-02\n",
      "\n",
      "At iterate   12    f=  7.45942D-02    |proj g|=  1.52008D-02\n",
      "\n",
      "At iterate   13    f=  5.77713D-02    |proj g|=  6.17878D-03\n",
      "\n",
      "At iterate   14    f=  5.07646D-02    |proj g|=  1.85416D-03\n",
      "\n",
      "At iterate   15    f=  4.65956D-02    |proj g|=  2.81292D-03\n",
      "\n",
      "At iterate   16    f=  3.87831D-02    |proj g|=  2.34578D-03\n",
      "\n",
      "At iterate   17    f=  3.20387D-02    |proj g|=  6.02228D-03\n",
      "\n",
      "At iterate   18    f=  1.64491D-02    |proj g|=  1.49945D-03\n",
      "\n",
      "At iterate   19    f=  1.28146D-02    |proj g|=  1.11932D-03\n",
      "\n",
      "At iterate   20    f=  7.31464D-03    |proj g|=  9.81781D-04\n",
      "\n",
      "At iterate   21    f=  3.87254D-03    |proj g|=  2.55803D-04\n",
      "\n",
      "At iterate   22    f=  2.09593D-03    |proj g|=  2.33248D-04\n",
      "\n",
      "At iterate   23    f=  2.00559D-03    |proj g|=  3.85302D-04\n",
      "\n",
      "At iterate   24    f=  1.04746D-03    |proj g|=  1.33596D-04\n",
      "\n",
      "At iterate   25    f=  5.50744D-04    |proj g|=  6.78056D-05\n",
      "\n",
      "At iterate   26    f=  3.18767D-04    |proj g|=  1.08735D-04\n",
      "\n",
      "At iterate   27    f=  1.23575D-04    |proj g|=  2.54152D-05\n",
      "\n",
      "At iterate   28    f=  7.35941D-05    |proj g|=  1.29517D-05\n",
      "\n",
      "At iterate   29    f=  3.48209D-05    |proj g|=  6.46789D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     29     34      1     0     0   6.468D-06   3.482D-05\n",
      "  F =   3.4820935042768584E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "MLM.fit_MergedLocalModels(10, classification=True, alpha=0, max_iter=10000, random_seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68af6765",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:00<00:00, 7229.16it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 272.07it/s]\n",
      "100%|██████████| 26/26 [00:00<00:00, 6060.80it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 311.57it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_epic_train = MLM.predict(X_train,  merged=True, \n",
    "                covariance_type='full', covariance_tied=False, uniform_prior=False)\n",
    "pred_epic_test = MLM.predict(X_test, merged=True, \n",
    "                covariance_type='full', covariance_tied=False, uniform_prior=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0113ad03",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLM-EPIC: Train RMSE:0.999 / Test RMSE:0.803\n"
     ]
    }
   ],
   "source": [
    "print('MLM-EPIC: Train RMSE:{:3.3f} / Test RMSE:{:3.3f}'.format(\n",
    "            roc_auc_score(y_train,np.array(pred_epic_train)),\n",
    "            roc_auc_score(y_test,np.array(pred_epic_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc12fb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "de6ee407",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x does not have column names.  The check that columns are provided in the same order when training and predicting will be skipped\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 (73,)\n",
      "# of CELL:13 / min size:11 / avg size:23.8 / max size:42 / # of singleton CELL:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of clf: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1/13 [00:00<00:01,  7.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: 0 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9b9d6b50>, dataframe=<capsule object NULL at 0x152d55cae280>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7    8   \\\n",
      "0    0.253333  0.458038  1.0  0.262136  0.069446  0.0  0.0  0.921053  0.0   \n",
      "1    0.800000  0.176952  1.0  0.097087  0.137836  0.0  0.0  1.000000  0.0   \n",
      "2    0.866667  0.050856  1.0  0.708738  0.146702  0.0  0.0  1.000000  0.0   \n",
      "3    0.293333  0.049687  1.0  0.000000  0.647704  0.0  0.0  1.000000  0.0   \n",
      "4    0.800000  0.086347  1.0  0.815534  0.000422  0.0  0.0  0.631579  0.0   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...  ...   \n",
      "305  0.906667  0.187056  1.0  0.495146  0.837150  0.0  0.0  0.921053  0.0   \n",
      "306  0.626667  0.148559  1.0  0.038835  0.956201  0.0  0.0  0.552632  0.0   \n",
      "307  0.280000  0.149478  1.0  0.902913  0.143852  0.0  0.0  0.263158  0.0   \n",
      "308  0.826667  0.131023  1.0  0.291262  0.167071  0.0  0.0  0.763158  0.0   \n",
      "309  0.600000  0.470564  1.0  0.378641  0.626807  0.0  0.0  0.105263  0.0   \n",
      "\n",
      "           9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "2    1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "3    1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "4    0.588235  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "..        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "306  0.500000  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "307  0.294118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "308  0.735294  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.147059  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "       0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "       1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
      "       0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
      "       1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
      "       1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0.,\n",
      "       0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "       1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
      "       0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
      "       1., 0., 1., 0.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n",
      "cluster: 1 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9b9d6b50>, dataframe=<capsule object NULL at 0x152d55cae280>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7    8   \\\n",
      "0    0.253333  0.458038  1.0  0.262136  0.069446  0.0  0.0  0.921053  0.0   \n",
      "1    0.800000  0.176952  1.0  0.097087  0.137836  0.0  0.0  1.000000  0.0   \n",
      "2    0.866667  0.050856  1.0  0.708738  0.146702  0.0  0.0  1.000000  0.0   \n",
      "3    0.293333  0.049687  1.0  0.000000  0.647704  0.0  0.0  1.000000  0.0   \n",
      "4    0.800000  0.086347  1.0  0.815534  0.000422  0.0  0.0  0.631579  0.0   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...  ...   \n",
      "305  0.906667  0.187056  1.0  0.495146  0.837150  0.0  0.0  0.921053  0.0   \n",
      "306  0.626667  0.148559  1.0  0.038835  0.956201  0.0  0.0  0.552632  0.0   \n",
      "307  0.280000  0.149478  1.0  0.902913  0.143852  0.0  0.0  0.263158  0.0   \n",
      "308  0.826667  0.131023  1.0  0.291262  0.167071  0.0  0.0  0.763158  0.0   \n",
      "309  0.600000  0.470564  1.0  0.378641  0.626807  0.0  0.0  0.105263  0.0   \n",
      "\n",
      "           9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "2    1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "3    1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "4    0.588235  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "..        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "306  0.500000  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "307  0.294118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "308  0.735294  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.147059  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "       0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "       1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
      "       0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
      "       1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
      "       1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0.,\n",
      "       0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "       1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
      "       0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
      "       1., 0., 1., 0.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 3/13 [00:00<00:01,  7.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: 2 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9b9d6b50>, dataframe=<capsule object NULL at 0x152d55cae280>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7    8   \\\n",
      "0    0.253333  0.458038  1.0  0.262136  0.069446  0.0  0.0  0.921053  0.0   \n",
      "1    0.800000  0.176952  1.0  0.097087  0.137836  0.0  0.0  1.000000  0.0   \n",
      "2    0.866667  0.050856  1.0  0.708738  0.146702  0.0  0.0  1.000000  0.0   \n",
      "3    0.293333  0.049687  1.0  0.000000  0.647704  0.0  0.0  1.000000  0.0   \n",
      "4    0.800000  0.086347  1.0  0.815534  0.000422  0.0  0.0  0.631579  0.0   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...  ...   \n",
      "305  0.906667  0.187056  1.0  0.495146  0.837150  0.0  0.0  0.921053  0.0   \n",
      "306  0.626667  0.148559  1.0  0.038835  0.956201  0.0  0.0  0.552632  0.0   \n",
      "307  0.280000  0.149478  1.0  0.902913  0.143852  0.0  0.0  0.263158  0.0   \n",
      "308  0.826667  0.131023  1.0  0.291262  0.167071  0.0  0.0  0.763158  0.0   \n",
      "309  0.600000  0.470564  1.0  0.378641  0.626807  0.0  0.0  0.105263  0.0   \n",
      "\n",
      "           9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "2    1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "3    1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "4    0.588235  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "..        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "306  0.500000  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "307  0.294118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "308  0.735294  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.147059  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "       0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "       1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
      "       0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
      "       1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
      "       1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0.,\n",
      "       0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "       1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
      "       0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
      "       1., 0., 1., 0.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n",
      "cluster: 3 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9b9d6b50>, dataframe=<capsule object NULL at 0x152d55cae280>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7    8   \\\n",
      "0    0.253333  0.458038  1.0  0.262136  0.069446  0.0  0.0  0.921053  0.0   \n",
      "1    0.800000  0.176952  1.0  0.097087  0.137836  0.0  0.0  1.000000  0.0   \n",
      "2    0.866667  0.050856  1.0  0.708738  0.146702  0.0  0.0  1.000000  0.0   \n",
      "3    0.293333  0.049687  1.0  0.000000  0.647704  0.0  0.0  1.000000  0.0   \n",
      "4    0.800000  0.086347  1.0  0.815534  0.000422  0.0  0.0  0.631579  0.0   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...  ...   \n",
      "305  0.906667  0.187056  1.0  0.495146  0.837150  0.0  0.0  0.921053  0.0   \n",
      "306  0.626667  0.148559  1.0  0.038835  0.956201  0.0  0.0  0.552632  0.0   \n",
      "307  0.280000  0.149478  1.0  0.902913  0.143852  0.0  0.0  0.263158  0.0   \n",
      "308  0.826667  0.131023  1.0  0.291262  0.167071  0.0  0.0  0.763158  0.0   \n",
      "309  0.600000  0.470564  1.0  0.378641  0.626807  0.0  0.0  0.105263  0.0   \n",
      "\n",
      "           9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "2    1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "3    1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "4    0.588235  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "..        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "306  0.500000  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "307  0.294118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "308  0.735294  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.147059  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "       0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "       1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
      "       0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
      "       1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
      "       1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0.,\n",
      "       0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "       1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
      "       0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
      "       1., 0., 1., 0.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 5/13 [00:00<00:01,  7.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: 4 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9b9d6b50>, dataframe=<capsule object NULL at 0x152d55cae280>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7    8   \\\n",
      "0    0.253333  0.458038  1.0  0.262136  0.069446  0.0  0.0  0.921053  0.0   \n",
      "1    0.800000  0.176952  1.0  0.097087  0.137836  0.0  0.0  1.000000  0.0   \n",
      "2    0.866667  0.050856  1.0  0.708738  0.146702  0.0  0.0  1.000000  0.0   \n",
      "3    0.293333  0.049687  1.0  0.000000  0.647704  0.0  0.0  1.000000  0.0   \n",
      "4    0.800000  0.086347  1.0  0.815534  0.000422  0.0  0.0  0.631579  0.0   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...  ...   \n",
      "305  0.906667  0.187056  1.0  0.495146  0.837150  0.0  0.0  0.921053  0.0   \n",
      "306  0.626667  0.148559  1.0  0.038835  0.956201  0.0  0.0  0.552632  0.0   \n",
      "307  0.280000  0.149478  1.0  0.902913  0.143852  0.0  0.0  0.263158  0.0   \n",
      "308  0.826667  0.131023  1.0  0.291262  0.167071  0.0  0.0  0.763158  0.0   \n",
      "309  0.600000  0.470564  1.0  0.378641  0.626807  0.0  0.0  0.105263  0.0   \n",
      "\n",
      "           9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "2    1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "3    1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "4    0.588235  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "..        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "306  0.500000  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "307  0.294118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "308  0.735294  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.147059  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "       0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "       1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
      "       0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
      "       1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
      "       1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0.,\n",
      "       0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "       1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
      "       0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
      "       1., 0., 1., 0.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n",
      "cluster: 5 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9b9d6b50>, dataframe=<capsule object NULL at 0x152d55cae280>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7    8   \\\n",
      "0    0.253333  0.458038  1.0  0.262136  0.069446  0.0  0.0  0.921053  0.0   \n",
      "1    0.800000  0.176952  1.0  0.097087  0.137836  0.0  0.0  1.000000  0.0   \n",
      "2    0.866667  0.050856  1.0  0.708738  0.146702  0.0  0.0  1.000000  0.0   \n",
      "3    0.293333  0.049687  1.0  0.000000  0.647704  0.0  0.0  1.000000  0.0   \n",
      "4    0.800000  0.086347  1.0  0.815534  0.000422  0.0  0.0  0.631579  0.0   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...  ...   \n",
      "305  0.906667  0.187056  1.0  0.495146  0.837150  0.0  0.0  0.921053  0.0   \n",
      "306  0.626667  0.148559  1.0  0.038835  0.956201  0.0  0.0  0.552632  0.0   \n",
      "307  0.280000  0.149478  1.0  0.902913  0.143852  0.0  0.0  0.263158  0.0   \n",
      "308  0.826667  0.131023  1.0  0.291262  0.167071  0.0  0.0  0.763158  0.0   \n",
      "309  0.600000  0.470564  1.0  0.378641  0.626807  0.0  0.0  0.105263  0.0   \n",
      "\n",
      "           9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "2    1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "3    1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "4    0.588235  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "..        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "306  0.500000  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "307  0.294118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "308  0.735294  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.147059  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "       0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "       1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
      "       0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
      "       1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
      "       1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0.,\n",
      "       0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "       1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
      "       0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
      "       1., 0., 1., 0.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 7/13 [00:00<00:00,  7.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: 6 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9b9d6b50>, dataframe=<capsule object NULL at 0x152d55cae280>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7    8   \\\n",
      "0    0.253333  0.458038  1.0  0.262136  0.069446  0.0  0.0  0.921053  0.0   \n",
      "1    0.800000  0.176952  1.0  0.097087  0.137836  0.0  0.0  1.000000  0.0   \n",
      "2    0.866667  0.050856  1.0  0.708738  0.146702  0.0  0.0  1.000000  0.0   \n",
      "3    0.293333  0.049687  1.0  0.000000  0.647704  0.0  0.0  1.000000  0.0   \n",
      "4    0.800000  0.086347  1.0  0.815534  0.000422  0.0  0.0  0.631579  0.0   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...  ...   \n",
      "305  0.906667  0.187056  1.0  0.495146  0.837150  0.0  0.0  0.921053  0.0   \n",
      "306  0.626667  0.148559  1.0  0.038835  0.956201  0.0  0.0  0.552632  0.0   \n",
      "307  0.280000  0.149478  1.0  0.902913  0.143852  0.0  0.0  0.263158  0.0   \n",
      "308  0.826667  0.131023  1.0  0.291262  0.167071  0.0  0.0  0.763158  0.0   \n",
      "309  0.600000  0.470564  1.0  0.378641  0.626807  0.0  0.0  0.105263  0.0   \n",
      "\n",
      "           9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "2    1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "3    1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "4    0.588235  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "..        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "306  0.500000  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "307  0.294118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "308  0.735294  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.147059  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "       0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "       1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
      "       0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
      "       1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
      "       1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0.,\n",
      "       0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "       1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
      "       0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
      "       1., 0., 1., 0.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n",
      "cluster: 7 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9b9d6b50>, dataframe=<capsule object NULL at 0x152d55cae280>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7    8   \\\n",
      "0    0.253333  0.458038  1.0  0.262136  0.069446  0.0  0.0  0.921053  0.0   \n",
      "1    0.800000  0.176952  1.0  0.097087  0.137836  0.0  0.0  1.000000  0.0   \n",
      "2    0.866667  0.050856  1.0  0.708738  0.146702  0.0  0.0  1.000000  0.0   \n",
      "3    0.293333  0.049687  1.0  0.000000  0.647704  0.0  0.0  1.000000  0.0   \n",
      "4    0.800000  0.086347  1.0  0.815534  0.000422  0.0  0.0  0.631579  0.0   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...  ...   \n",
      "305  0.906667  0.187056  1.0  0.495146  0.837150  0.0  0.0  0.921053  0.0   \n",
      "306  0.626667  0.148559  1.0  0.038835  0.956201  0.0  0.0  0.552632  0.0   \n",
      "307  0.280000  0.149478  1.0  0.902913  0.143852  0.0  0.0  0.263158  0.0   \n",
      "308  0.826667  0.131023  1.0  0.291262  0.167071  0.0  0.0  0.763158  0.0   \n",
      "309  0.600000  0.470564  1.0  0.378641  0.626807  0.0  0.0  0.105263  0.0   \n",
      "\n",
      "           9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "2    1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "3    1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "4    0.588235  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "..        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "306  0.500000  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "307  0.294118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "308  0.735294  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.147059  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "       0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "       1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
      "       0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
      "       1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
      "       1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0.,\n",
      "       0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "       1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
      "       0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
      "       1., 0., 1., 0.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 9/13 [00:01<00:00,  7.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: 8 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9b9d6b50>, dataframe=<capsule object NULL at 0x152d55cae280>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7    8   \\\n",
      "0    0.253333  0.458038  1.0  0.262136  0.069446  0.0  0.0  0.921053  0.0   \n",
      "1    0.800000  0.176952  1.0  0.097087  0.137836  0.0  0.0  1.000000  0.0   \n",
      "2    0.866667  0.050856  1.0  0.708738  0.146702  0.0  0.0  1.000000  0.0   \n",
      "3    0.293333  0.049687  1.0  0.000000  0.647704  0.0  0.0  1.000000  0.0   \n",
      "4    0.800000  0.086347  1.0  0.815534  0.000422  0.0  0.0  0.631579  0.0   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...  ...   \n",
      "305  0.906667  0.187056  1.0  0.495146  0.837150  0.0  0.0  0.921053  0.0   \n",
      "306  0.626667  0.148559  1.0  0.038835  0.956201  0.0  0.0  0.552632  0.0   \n",
      "307  0.280000  0.149478  1.0  0.902913  0.143852  0.0  0.0  0.263158  0.0   \n",
      "308  0.826667  0.131023  1.0  0.291262  0.167071  0.0  0.0  0.763158  0.0   \n",
      "309  0.600000  0.470564  1.0  0.378641  0.626807  0.0  0.0  0.105263  0.0   \n",
      "\n",
      "           9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "2    1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "3    1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "4    0.588235  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "..        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "306  0.500000  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "307  0.294118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "308  0.735294  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.147059  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "       0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "       1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
      "       0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
      "       1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
      "       1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0.,\n",
      "       0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "       1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
      "       0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
      "       1., 0., 1., 0.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n",
      "cluster: 9 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9b9d6b50>, dataframe=<capsule object NULL at 0x152d55cae280>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7    8   \\\n",
      "0    0.253333  0.458038  1.0  0.262136  0.069446  0.0  0.0  0.921053  0.0   \n",
      "1    0.800000  0.176952  1.0  0.097087  0.137836  0.0  0.0  1.000000  0.0   \n",
      "2    0.866667  0.050856  1.0  0.708738  0.146702  0.0  0.0  1.000000  0.0   \n",
      "3    0.293333  0.049687  1.0  0.000000  0.647704  0.0  0.0  1.000000  0.0   \n",
      "4    0.800000  0.086347  1.0  0.815534  0.000422  0.0  0.0  0.631579  0.0   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...  ...   \n",
      "305  0.906667  0.187056  1.0  0.495146  0.837150  0.0  0.0  0.921053  0.0   \n",
      "306  0.626667  0.148559  1.0  0.038835  0.956201  0.0  0.0  0.552632  0.0   \n",
      "307  0.280000  0.149478  1.0  0.902913  0.143852  0.0  0.0  0.263158  0.0   \n",
      "308  0.826667  0.131023  1.0  0.291262  0.167071  0.0  0.0  0.763158  0.0   \n",
      "309  0.600000  0.470564  1.0  0.378641  0.626807  0.0  0.0  0.105263  0.0   \n",
      "\n",
      "           9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "2    1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "3    1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "4    0.588235  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "..        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "306  0.500000  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "307  0.294118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "308  0.735294  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.147059  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "       0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "       1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
      "       0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
      "       1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
      "       1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0.,\n",
      "       0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "       1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
      "       0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
      "       1., 0., 1., 0.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 11/13 [00:01<00:00,  7.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: 10 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9b9d6b50>, dataframe=<capsule object NULL at 0x152d55cae280>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7    8   \\\n",
      "0    0.253333  0.458038  1.0  0.262136  0.069446  0.0  0.0  0.921053  0.0   \n",
      "1    0.800000  0.176952  1.0  0.097087  0.137836  0.0  0.0  1.000000  0.0   \n",
      "2    0.866667  0.050856  1.0  0.708738  0.146702  0.0  0.0  1.000000  0.0   \n",
      "3    0.293333  0.049687  1.0  0.000000  0.647704  0.0  0.0  1.000000  0.0   \n",
      "4    0.800000  0.086347  1.0  0.815534  0.000422  0.0  0.0  0.631579  0.0   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...  ...   \n",
      "305  0.906667  0.187056  1.0  0.495146  0.837150  0.0  0.0  0.921053  0.0   \n",
      "306  0.626667  0.148559  1.0  0.038835  0.956201  0.0  0.0  0.552632  0.0   \n",
      "307  0.280000  0.149478  1.0  0.902913  0.143852  0.0  0.0  0.263158  0.0   \n",
      "308  0.826667  0.131023  1.0  0.291262  0.167071  0.0  0.0  0.763158  0.0   \n",
      "309  0.600000  0.470564  1.0  0.378641  0.626807  0.0  0.0  0.105263  0.0   \n",
      "\n",
      "           9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "2    1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "3    1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "4    0.588235  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "..        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "306  0.500000  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "307  0.294118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "308  0.735294  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.147059  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "       0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "       1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
      "       0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
      "       1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
      "       1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0.,\n",
      "       0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "       1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
      "       0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
      "       1., 0., 1., 0.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n",
      "cluster: 11 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9b9d6b50>, dataframe=<capsule object NULL at 0x152d55cae280>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7    8   \\\n",
      "0    0.253333  0.458038  1.0  0.262136  0.069446  0.0  0.0  0.921053  0.0   \n",
      "1    0.800000  0.176952  1.0  0.097087  0.137836  0.0  0.0  1.000000  0.0   \n",
      "2    0.866667  0.050856  1.0  0.708738  0.146702  0.0  0.0  1.000000  0.0   \n",
      "3    0.293333  0.049687  1.0  0.000000  0.647704  0.0  0.0  1.000000  0.0   \n",
      "4    0.800000  0.086347  1.0  0.815534  0.000422  0.0  0.0  0.631579  0.0   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...  ...   \n",
      "305  0.906667  0.187056  1.0  0.495146  0.837150  0.0  0.0  0.921053  0.0   \n",
      "306  0.626667  0.148559  1.0  0.038835  0.956201  0.0  0.0  0.552632  0.0   \n",
      "307  0.280000  0.149478  1.0  0.902913  0.143852  0.0  0.0  0.263158  0.0   \n",
      "308  0.826667  0.131023  1.0  0.291262  0.167071  0.0  0.0  0.763158  0.0   \n",
      "309  0.600000  0.470564  1.0  0.378641  0.626807  0.0  0.0  0.105263  0.0   \n",
      "\n",
      "           9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "2    1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "3    1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "4    0.588235  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "..        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "306  0.500000  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "307  0.294118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "308  0.735294  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.147059  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "       0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "       1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
      "       0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
      "       1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
      "       1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0.,\n",
      "       0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "       1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
      "       0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
      "       1., 0., 1., 0.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00,  7.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: 12 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9b9d6b50>, dataframe=<capsule object NULL at 0x152d55cae280>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7    8   \\\n",
      "0    0.253333  0.458038  1.0  0.262136  0.069446  0.0  0.0  0.921053  0.0   \n",
      "1    0.800000  0.176952  1.0  0.097087  0.137836  0.0  0.0  1.000000  0.0   \n",
      "2    0.866667  0.050856  1.0  0.708738  0.146702  0.0  0.0  1.000000  0.0   \n",
      "3    0.293333  0.049687  1.0  0.000000  0.647704  0.0  0.0  1.000000  0.0   \n",
      "4    0.800000  0.086347  1.0  0.815534  0.000422  0.0  0.0  0.631579  0.0   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...  ...   \n",
      "305  0.906667  0.187056  1.0  0.495146  0.837150  0.0  0.0  0.921053  0.0   \n",
      "306  0.626667  0.148559  1.0  0.038835  0.956201  0.0  0.0  0.552632  0.0   \n",
      "307  0.280000  0.149478  1.0  0.902913  0.143852  0.0  0.0  0.263158  0.0   \n",
      "308  0.826667  0.131023  1.0  0.291262  0.167071  0.0  0.0  0.763158  0.0   \n",
      "309  0.600000  0.470564  1.0  0.378641  0.626807  0.0  0.0  0.105263  0.0   \n",
      "\n",
      "           9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "2    1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "3    1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "4    0.588235  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "..        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "306  0.500000  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "307  0.294118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "308  0.735294  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.147059  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "       0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "       1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
      "       0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
      "       1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
      "       1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0.,\n",
      "       0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "       1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
      "       0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
      "       1., 0., 1., 0.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13 [00:00<?, ?it/s] This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  7.30729D-02\n",
      "\n",
      "At iterate    1    f=  6.77374D-01    |proj g|=  8.30056D-02\n",
      "\n",
      "At iterate    2    f=  5.63690D-01    |proj g|=  6.95175D-02\n",
      "\n",
      "At iterate    3    f=  3.44115D-01    |proj g|=  2.49954D-02\n",
      "\n",
      "At iterate    4    f=  2.76657D-01    |proj g|=  1.75173D-02\n",
      "\n",
      "At iterate    5    f=  1.90430D-01    |proj g|=  1.17604D-02\n",
      "\n",
      "At iterate    6    f=  1.12342D-01    |proj g|=  6.05226D-03\n",
      "\n",
      "At iterate    7    f=  8.24771D-02    |proj g|=  1.16044D-02\n",
      "\n",
      "At iterate    8    f=  5.08808D-02    |proj g|=  4.81038D-03\n",
      "\n",
      "At iterate    9    f=  3.58195D-02    |proj g|=  1.14141D-03\n",
      "\n",
      "At iterate   10    f=  2.46541D-02    |proj g|=  1.03523D-03\n",
      "\n",
      "At iterate   11    f=  1.48353D-02    |proj g|=  8.19416D-04\n",
      "\n",
      "At iterate   12    f=  1.47733D-02    |proj g|=  7.04126D-04\n",
      "\n",
      "At iterate   13    f=  8.83396D-03    |proj g|=  4.71614D-04\n",
      "\n",
      "At iterate   14    f=  5.53137D-03    |proj g|=  1.77553D-03\n",
      "\n",
      "At iterate   15    f=  3.06145D-03    |proj g|=  8.56055D-04\n",
      "\n",
      "At iterate   16    f=  1.52587D-03    |proj g|=  3.77531D-04\n",
      "\n",
      "At iterate   17    f=  7.96853D-04    |proj g|=  1.70267D-04\n",
      "\n",
      "At iterate   18    f=  4.11658D-04    |proj g|=  7.21842D-05\n",
      "\n",
      "At iterate   19    f=  2.13702D-04    |proj g|=  3.19438D-05\n",
      "\n",
      "At iterate   20    f=  1.10404D-04    |proj g|=  1.46748D-05\n",
      "\n",
      "At iterate   21    f=  5.69246D-05    |proj g|=  7.03149D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     21     25      1     0     0   7.031D-06   5.692D-05\n",
      "  F =   5.6924587885566500E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  7.71800D-02\n",
      "\n",
      "At iterate    1    f=  6.81181D-01    |proj g|=  5.36121D-02\n",
      "\n",
      "At iterate    2    f=  5.59566D-01    |proj g|=  7.36729D-02\n",
      "\n",
      "At iterate    3    f=  3.63886D-01    |proj g|=  3.09281D-02\n",
      "\n",
      "At iterate    4    f=  3.01330D-01    |proj g|=  2.02867D-02\n",
      "\n",
      "At iterate    5    f=  2.32123D-01    |proj g|=  1.33279D-02\n",
      "\n",
      "At iterate    6    f=  1.57401D-01    |proj g|=  7.19897D-03\n",
      "\n",
      "At iterate    7    f=  8.28516D-02    |proj g|=  5.18887D-03\n",
      "\n",
      "At iterate    8    f=  4.71538D-02    |proj g|=  8.57547D-03\n",
      "\n",
      "At iterate    9    f=  2.64062D-02    |proj g|=  4.14522D-03\n",
      "\n",
      "At iterate   10    f=  1.53712D-02    |proj g|=  2.40897D-03\n",
      "\n",
      "At iterate   11    f=  7.52286D-03    |proj g|=  1.23964D-03\n",
      "\n",
      "At iterate   12    f=  4.77011D-03    |proj g|=  2.63163D-03\n",
      "\n",
      "At iterate   13    f=  1.48079D-03    |proj g|=  1.71544D-04\n",
      "\n",
      "At iterate   14    f=  9.72576D-04    |proj g|=  1.09487D-04\n",
      "\n",
      "At iterate   15    f=  4.38695D-04    |proj g|=  4.52844D-05\n",
      "\n",
      "At iterate   16    f=  2.27882D-04    |proj g|=  4.65379D-05\n",
      "\n",
      "At iterate   17    f=  9.05495D-05    |proj g|=  8.99216D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     17     21      1     0     0   8.992D-06   9.055D-05\n",
      "  F =   9.0549542945437785E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  2.71568D-01\n",
      "\n",
      "At iterate    1    f=  5.40189D-01    |proj g|=  4.17711D-02\n",
      "\n",
      "At iterate    2    f=  5.20624D-01    |proj g|=  4.02663D-02\n",
      "\n",
      "At iterate    3    f=  4.06158D-01    |proj g|=  1.28622D-01\n",
      "\n",
      "At iterate    4    f=  3.36694D-01    |proj g|=  1.07137D-01\n",
      "\n",
      "At iterate    5    f=  2.33174D-01    |proj g|=  4.92493D-02\n",
      "\n",
      "At iterate    6    f=  1.85867D-01    |proj g|=  2.26758D-02\n",
      "\n",
      "At iterate    7    f=  1.53577D-01    |proj g|=  7.72682D-03\n",
      "\n",
      "At iterate    8    f=  1.30143D-01    |proj g|=  7.60256D-03\n",
      "\n",
      "At iterate    9    f=  1.02928D-01    |proj g|=  7.22686D-03\n",
      "\n",
      "At iterate   10    f=  6.33833D-02    |proj g|=  7.45005D-03\n",
      "\n",
      "At iterate   11    f=  5.32624D-02    |proj g|=  4.82857D-03\n",
      "\n",
      "At iterate   12    f=  4.34862D-02    |proj g|=  1.85510D-03\n",
      "\n",
      "At iterate   13    f=  3.92107D-02    |proj g|=  1.79277D-03\n",
      "\n",
      "At iterate   14    f=  3.89816D-02    |proj g|=  1.28073D-03\n",
      "\n",
      "At iterate   15    f=  3.68965D-02    |proj g|=  1.61160D-03\n",
      "\n",
      "At iterate   16    f=  3.43308D-02    |proj g|=  1.84335D-03\n",
      "\n",
      "At iterate   17    f=  2.39807D-02    |proj g|=  4.41919D-03\n",
      "\n",
      "At iterate   18    f=  2.19479D-02    |proj g|=  9.79535D-04\n",
      "\n",
      "At iterate   19    f=  1.97288D-02    |proj g|=  1.00230D-03\n",
      "\n",
      "At iterate   20    f=  1.46992D-02    |proj g|=  1.29289D-03\n",
      "\n",
      "At iterate   21    f=  1.00147D-02    |proj g|=  1.92434D-03\n",
      "\n",
      "At iterate   22    f=  5.45129D-03    |proj g|=  7.14695D-04\n",
      "\n",
      "At iterate   23    f=  2.69451D-03    |proj g|=  5.53339D-04\n",
      "\n",
      "At iterate   24    f=  1.35168D-03    |proj g|=  2.83078D-04\n",
      "\n",
      "At iterate   25    f=  7.34828D-04    |proj g|=  3.99243D-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   26    f=  3.96694D-04    |proj g|=  3.59953D-04\n",
      "\n",
      "At iterate   27    f=  1.16343D-04    |proj g|=  5.08729D-05\n",
      "\n",
      "At iterate   28    f=  8.52376D-05    |proj g|=  3.13449D-05\n",
      "\n",
      "At iterate   29    f=  3.98356D-05    |proj g|=  8.67515D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     29     33      1     0     0   8.675D-06   3.984D-05\n",
      "  F =   3.9835631727010838E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  4.70948D-02\n",
      "\n",
      "At iterate    1    f=  6.84917D-01    |proj g|=  6.13633D-02\n",
      "\n",
      "At iterate    2    f=  6.30678D-01    |proj g|=  6.51997D-02\n",
      "\n",
      "At iterate    3    f=  4.09638D-01    |proj g|=  1.41465D-02\n",
      "\n",
      "At iterate    4    f=  3.70099D-01    |proj g|=  9.06713D-03\n",
      "\n",
      "At iterate    5    f=  3.03332D-01    |proj g|=  8.36408D-03\n",
      "\n",
      "At iterate    6    f=  2.54801D-01    |proj g|=  5.41568D-03\n",
      "\n",
      "At iterate    7    f=  2.21839D-01    |proj g|=  1.66676D-02\n",
      "\n",
      "At iterate    8    f=  1.86628D-01    |proj g|=  2.97199D-03\n",
      "\n",
      "At iterate    9    f=  1.75691D-01    |proj g|=  2.64217D-03\n",
      "\n",
      "At iterate   10    f=  1.50560D-01    |proj g|=  3.96994D-03\n",
      "\n",
      "At iterate   11    f=  1.34936D-01    |proj g|=  5.19836D-03\n",
      "\n",
      "At iterate   12    f=  1.34404D-01    |proj g|=  3.41595D-03\n",
      "\n",
      "At iterate   13    f=  1.25664D-01    |proj g|=  3.29941D-03\n",
      "\n",
      "At iterate   14    f=  1.15075D-01    |proj g|=  2.16075D-03\n",
      "\n",
      "At iterate   15    f=  1.04642D-01    |proj g|=  2.01821D-03\n",
      "\n",
      "At iterate   16    f=  9.67479D-02    |proj g|=  2.98326D-03\n",
      "\n",
      "At iterate   17    f=  9.17313D-02    |proj g|=  1.30701D-03\n",
      "\n",
      "At iterate   18    f=  9.00086D-02    |proj g|=  1.25895D-03\n",
      "\n",
      "At iterate   19    f=  8.76381D-02    |proj g|=  2.23063D-03\n",
      "\n",
      "At iterate   20    f=  8.65940D-02    |proj g|=  1.38180D-03\n",
      "\n",
      "At iterate   21    f=  8.47944D-02    |proj g|=  1.55354D-03\n",
      "\n",
      "At iterate   22    f=  8.42804D-02    |proj g|=  1.62202D-03\n",
      "\n",
      "At iterate   23    f=  8.42346D-02    |proj g|=  6.63754D-04\n",
      "\n",
      "At iterate   24    f=  8.39488D-02    |proj g|=  6.59421D-04\n",
      "\n",
      "At iterate   25    f=  8.30376D-02    |proj g|=  7.63667D-04\n",
      "\n",
      "At iterate   26    f=  8.17496D-02    |proj g|=  5.98894D-04\n",
      "\n",
      "At iterate   27    f=  8.04790D-02    |proj g|=  1.67891D-03\n",
      "\n",
      "At iterate   28    f=  7.94312D-02    |proj g|=  1.50785D-03\n",
      "\n",
      "At iterate   29    f=  7.90224D-02    |proj g|=  8.90329D-04\n",
      "\n",
      "At iterate   30    f=  7.85158D-02    |proj g|=  6.07779D-04\n",
      "\n",
      "At iterate   31    f=  7.80924D-02    |proj g|=  1.00950D-03\n",
      "\n",
      "At iterate   32    f=  7.75173D-02    |proj g|=  9.81147D-04\n",
      "\n",
      "At iterate   33    f=  7.67095D-02    |proj g|=  1.29059D-03\n",
      "\n",
      "At iterate   34    f=  7.65284D-02    |proj g|=  1.48007D-03\n",
      "\n",
      "At iterate   35    f=  7.63250D-02    |proj g|=  6.79205D-04\n",
      "\n",
      "At iterate   36    f=  7.62312D-02    |proj g|=  6.60229D-04\n",
      "\n",
      "At iterate   37    f=  7.60267D-02    |proj g|=  6.84455D-04\n",
      "\n",
      "At iterate   38    f=  7.55811D-02    |proj g|=  1.13836D-03\n",
      "\n",
      "At iterate   39    f=  7.49043D-02    |proj g|=  1.36447D-03\n",
      "\n",
      "At iterate   40    f=  7.35670D-02    |proj g|=  1.25115D-03\n",
      "\n",
      "At iterate   41    f=  7.29731D-02    |proj g|=  2.14354D-03\n",
      "\n",
      "At iterate   42    f=  7.13098D-02    |proj g|=  1.84075D-03\n",
      "\n",
      "At iterate   43    f=  6.94984D-02    |proj g|=  9.48622D-04\n",
      "\n",
      "At iterate   44    f=  6.88589D-02    |proj g|=  1.06784D-03\n",
      "\n",
      "At iterate   45    f=  6.87912D-02    |proj g|=  1.82008D-03\n",
      "\n",
      "At iterate   46    f=  6.84356D-02    |proj g|=  1.41537D-03\n",
      "\n",
      "At iterate   47    f=  6.78857D-02    |proj g|=  1.05124D-03\n",
      "\n",
      "At iterate   48    f=  6.75167D-02    |proj g|=  9.30444D-04\n",
      "\n",
      "At iterate   49    f=  6.64973D-02    |proj g|=  1.08946D-03\n",
      "\n",
      "At iterate   50    f=  6.55231D-02    |proj g|=  2.23606D-03\n",
      "\n",
      "At iterate   51    f=  6.44719D-02    |proj g|=  9.24688D-04\n",
      "\n",
      "At iterate   52    f=  6.41305D-02    |proj g|=  1.05565D-03\n",
      "\n",
      "At iterate   53    f=  6.39509D-02    |proj g|=  1.23379D-03\n",
      "\n",
      "At iterate   54    f=  6.34556D-02    |proj g|=  1.13299D-03\n",
      "\n",
      "At iterate   55    f=  6.31010D-02    |proj g|=  1.12090D-03\n",
      "\n",
      "At iterate   56    f=  6.29394D-02    |proj g|=  1.92144D-03\n",
      "\n",
      "At iterate   57    f=  6.26660D-02    |proj g|=  7.73639D-04\n",
      "\n",
      "At iterate   58    f=  6.25554D-02    |proj g|=  5.07999D-04\n",
      "\n",
      "At iterate   59    f=  6.24244D-02    |proj g|=  8.35215D-04\n",
      "\n",
      "At iterate   60    f=  6.22000D-02    |proj g|=  8.23523D-04\n",
      "\n",
      "At iterate   61    f=  6.15299D-02    |proj g|=  5.47997D-04\n",
      "\n",
      "At iterate   62    f=  6.13168D-02    |proj g|=  7.76753D-04\n",
      "\n",
      "At iterate   63    f=  6.12469D-02    |proj g|=  6.02637D-04\n",
      "\n",
      "At iterate   64    f=  6.11365D-02    |proj g|=  5.03156D-04\n",
      "\n",
      "At iterate   65    f=  6.10325D-02    |proj g|=  6.14278D-04\n",
      "\n",
      "At iterate   66    f=  6.07713D-02    |proj g|=  5.64120D-04\n",
      "\n",
      "At iterate   67    f=  6.06965D-02    |proj g|=  1.04188D-03\n",
      "\n",
      "At iterate   68    f=  6.05579D-02    |proj g|=  5.91200D-04\n",
      "\n",
      "At iterate   69    f=  6.04582D-02    |proj g|=  6.51132D-04\n",
      "\n",
      "At iterate   70    f=  6.03628D-02    |proj g|=  4.70456D-04\n",
      "\n",
      "At iterate   71    f=  6.03279D-02    |proj g|=  8.50804D-04\n",
      "\n",
      "At iterate   72    f=  6.02453D-02    |proj g|=  4.53608D-04\n",
      "\n",
      "At iterate   73    f=  6.01902D-02    |proj g|=  3.21381D-04\n",
      "\n",
      "At iterate   74    f=  6.01387D-02    |proj g|=  3.92815D-04\n",
      "\n",
      "At iterate   75    f=  6.00653D-02    |proj g|=  4.86867D-04\n",
      "\n",
      "At iterate   76    f=  5.99291D-02    |proj g|=  6.22102D-04\n",
      "\n",
      "At iterate   77    f=  5.98735D-02    |proj g|=  3.36217D-04\n",
      "\n",
      "At iterate   78    f=  5.98018D-02    |proj g|=  3.35745D-04\n",
      "\n",
      "At iterate   79    f=  5.97745D-02    |proj g|=  5.07521D-04\n",
      "\n",
      "At iterate   80    f=  5.96775D-02    |proj g|=  4.54086D-04\n",
      "\n",
      "At iterate   81    f=  5.95300D-02    |proj g|=  2.59086D-04\n",
      "\n",
      "At iterate   82    f=  5.93834D-02    |proj g|=  3.85184D-04\n",
      "\n",
      "At iterate   83    f=  5.93123D-02    |proj g|=  4.18725D-04\n",
      "\n",
      "At iterate   84    f=  5.92288D-02    |proj g|=  2.00557D-04\n",
      "\n",
      "At iterate   85    f=  5.91304D-02    |proj g|=  2.56293D-04\n",
      "\n",
      "At iterate   86    f=  5.90077D-02    |proj g|=  4.06494D-04\n",
      "\n",
      "At iterate   87    f=  5.87861D-02    |proj g|=  5.12441D-04\n",
      "\n",
      "At iterate   88    f=  5.87436D-02    |proj g|=  4.74825D-04\n",
      "\n",
      "At iterate   89    f=  5.85697D-02    |proj g|=  2.70327D-04\n",
      "\n",
      "At iterate   90    f=  5.85362D-02    |proj g|=  6.99423D-04\n",
      "\n",
      "At iterate   91    f=  5.84734D-02    |proj g|=  4.53210D-04\n",
      "\n",
      "At iterate   92    f=  5.84012D-02    |proj g|=  2.96994D-04\n",
      "\n",
      "At iterate   93    f=  5.82803D-02    |proj g|=  2.75681D-04\n",
      "\n",
      "At iterate   94    f=  5.81815D-02    |proj g|=  4.37790D-04\n",
      "\n",
      "At iterate   95    f=  5.80853D-02    |proj g|=  1.11665D-03\n",
      "\n",
      "At iterate   96    f=  5.78640D-02    |proj g|=  2.46005D-04\n",
      "\n",
      "At iterate   97    f=  5.77873D-02    |proj g|=  1.97353D-04\n",
      "\n",
      "At iterate   98    f=  5.75951D-02    |proj g|=  3.23420D-04\n",
      "\n",
      "At iterate   99    f=  5.75480D-02    |proj g|=  6.87751D-04\n",
      "\n",
      "At iterate  100    f=  5.73646D-02    |proj g|=  5.01808D-04\n",
      "\n",
      "At iterate  101    f=  5.73223D-02    |proj g|=  6.67263D-04\n",
      "\n",
      "At iterate  102    f=  5.71871D-02    |proj g|=  4.36912D-04\n",
      "\n",
      "At iterate  103    f=  5.70863D-02    |proj g|=  5.27355D-04\n",
      "\n",
      "At iterate  104    f=  5.69239D-02    |proj g|=  4.33138D-04\n",
      "\n",
      "At iterate  105    f=  5.65853D-02    |proj g|=  5.05769D-04\n",
      "\n",
      "At iterate  106    f=  5.63695D-02    |proj g|=  4.45989D-04\n",
      "\n",
      "At iterate  107    f=  5.60600D-02    |proj g|=  5.97084D-04\n",
      "\n",
      "At iterate  108    f=  5.59059D-02    |proj g|=  1.56628D-03\n",
      "\n",
      "At iterate  109    f=  5.55555D-02    |proj g|=  3.66768D-04\n",
      "\n",
      "At iterate  110    f=  5.54604D-02    |proj g|=  2.90934D-04\n",
      "\n",
      "At iterate  111    f=  5.54252D-02    |proj g|=  6.41777D-04\n",
      "\n",
      "At iterate  112    f=  5.53625D-02    |proj g|=  5.13144D-04\n",
      "\n",
      "At iterate  113    f=  5.53168D-02    |proj g|=  3.13202D-04\n",
      "\n",
      "At iterate  114    f=  5.52563D-02    |proj g|=  3.14370D-04\n",
      "\n",
      "At iterate  115    f=  5.51604D-02    |proj g|=  4.35168D-04\n",
      "\n",
      "At iterate  116    f=  5.49722D-02    |proj g|=  3.05802D-04\n",
      "\n",
      "At iterate  117    f=  5.48842D-02    |proj g|=  8.91482D-04\n",
      "\n",
      "At iterate  118    f=  5.46378D-02    |proj g|=  5.76925D-04\n",
      "\n",
      "At iterate  119    f=  5.42829D-02    |proj g|=  2.86349D-04\n",
      "\n",
      "At iterate  120    f=  5.41035D-02    |proj g|=  2.83980D-04\n",
      "\n",
      "At iterate  121    f=  5.39789D-02    |proj g|=  1.30103D-03\n",
      "\n",
      "At iterate  122    f=  5.37881D-02    |proj g|=  8.86367D-04\n",
      "\n",
      "At iterate  123    f=  5.34859D-02    |proj g|=  5.47287D-04\n",
      "\n",
      "At iterate  124    f=  5.33242D-02    |proj g|=  5.83635D-04\n",
      "\n",
      "At iterate  125    f=  5.32527D-02    |proj g|=  1.24412D-03\n",
      "\n",
      "At iterate  126    f=  5.30085D-02    |proj g|=  7.22932D-04\n",
      "\n",
      "At iterate  127    f=  5.28344D-02    |proj g|=  4.12954D-04\n",
      "\n",
      "At iterate  128    f=  5.26961D-02    |proj g|=  3.32050D-04\n",
      "\n",
      "At iterate  129    f=  5.25971D-02    |proj g|=  4.10133D-04\n",
      "\n",
      "At iterate  130    f=  5.24575D-02    |proj g|=  8.20040D-04\n",
      "\n",
      "At iterate  131    f=  5.21062D-02    |proj g|=  6.82827D-04\n",
      "\n",
      "At iterate  132    f=  5.20121D-02    |proj g|=  9.15660D-04\n",
      "\n",
      "At iterate  133    f=  5.15982D-02    |proj g|=  2.95504D-04\n",
      "\n",
      "At iterate  134    f=  5.15510D-02    |proj g|=  1.30020D-03\n",
      "\n",
      "At iterate  135    f=  5.14582D-02    |proj g|=  4.23769D-04\n",
      "\n",
      "At iterate  136    f=  5.14260D-02    |proj g|=  4.40390D-04\n",
      "\n",
      "At iterate  137    f=  5.13870D-02    |proj g|=  6.47883D-04\n",
      "\n",
      "At iterate  138    f=  5.13396D-02    |proj g|=  6.77598D-04\n",
      "\n",
      "At iterate  139    f=  5.10548D-02    |proj g|=  5.73096D-04\n",
      "\n",
      "At iterate  140    f=  5.09981D-02    |proj g|=  6.09144D-04\n",
      "\n",
      "At iterate  141    f=  5.06326D-02    |proj g|=  2.21857D-04\n",
      "\n",
      "At iterate  142    f=  5.04447D-02    |proj g|=  5.11384D-04\n",
      "\n",
      "At iterate  143    f=  5.02802D-02    |proj g|=  4.05040D-04\n",
      "\n",
      "At iterate  144    f=  5.02478D-02    |proj g|=  1.03960D-03\n",
      "\n",
      "At iterate  145    f=  5.01829D-02    |proj g|=  5.84047D-04\n",
      "\n",
      "At iterate  146    f=  5.00835D-02    |proj g|=  6.49737D-04\n",
      "\n",
      "At iterate  147    f=  5.00163D-02    |proj g|=  7.91338D-04\n",
      "\n",
      "At iterate  148    f=  4.99641D-02    |proj g|=  3.72611D-04\n",
      "\n",
      "At iterate  149    f=  4.98704D-02    |proj g|=  3.08690D-04\n",
      "\n",
      "At iterate  150    f=  4.97406D-02    |proj g|=  2.02771D-04\n",
      "\n",
      "At iterate  151    f=  4.95784D-02    |proj g|=  4.49245D-04\n",
      "\n",
      "At iterate  152    f=  4.93347D-02    |proj g|=  2.46815D-04\n",
      "\n",
      "At iterate  153    f=  4.91678D-02    |proj g|=  5.60091D-04\n",
      "\n",
      "At iterate  154    f=  4.90064D-02    |proj g|=  3.11587D-04\n",
      "\n",
      "At iterate  155    f=  4.86225D-02    |proj g|=  9.21387D-04\n",
      "\n",
      "At iterate  156    f=  4.85527D-02    |proj g|=  4.53020D-04\n",
      "\n",
      "At iterate  157    f=  4.85092D-02    |proj g|=  4.45381D-04\n",
      "\n",
      "At iterate  158    f=  4.83971D-02    |proj g|=  2.95356D-04\n",
      "\n",
      "At iterate  159    f=  4.82402D-02    |proj g|=  3.14094D-04\n",
      "\n",
      "At iterate  160    f=  4.80296D-02    |proj g|=  5.03213D-04\n",
      "\n",
      "At iterate  161    f=  4.79365D-02    |proj g|=  5.25456D-04\n",
      "\n",
      "At iterate  162    f=  4.78270D-02    |proj g|=  2.76333D-04\n",
      "\n",
      "At iterate  163    f=  4.77468D-02    |proj g|=  3.25508D-04\n",
      "\n",
      "At iterate  164    f=  4.75626D-02    |proj g|=  2.79313D-04\n",
      "\n",
      "At iterate  165    f=  4.73278D-02    |proj g|=  4.16078D-04\n",
      "\n",
      "At iterate  166    f=  4.71835D-02    |proj g|=  6.02912D-04\n",
      "\n",
      "At iterate  167    f=  4.70137D-02    |proj g|=  8.46954D-04\n",
      "\n",
      "At iterate  168    f=  4.68527D-02    |proj g|=  6.31083D-04\n",
      "\n",
      "At iterate  169    f=  4.67390D-02    |proj g|=  2.64895D-04\n",
      "\n",
      "At iterate  170    f=  4.66860D-02    |proj g|=  2.18823D-04\n",
      "\n",
      "At iterate  171    f=  4.64827D-02    |proj g|=  5.01189D-04\n",
      "\n",
      "At iterate  172    f=  4.64487D-02    |proj g|=  3.46416D-04\n",
      "\n",
      "At iterate  173    f=  4.63809D-02    |proj g|=  3.85846D-04\n",
      "\n",
      "At iterate  174    f=  4.62740D-02    |proj g|=  2.99208D-04\n",
      "\n",
      "At iterate  175    f=  4.60656D-02    |proj g|=  1.40280D-04\n",
      "\n",
      "At iterate  176    f=  4.58393D-02    |proj g|=  1.25428D-04\n",
      "\n",
      "At iterate  177    f=  4.54619D-02    |proj g|=  5.43111D-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate  178    f=  4.52845D-02    |proj g|=  3.28210D-04\n",
      "\n",
      "At iterate  179    f=  4.52124D-02    |proj g|=  4.03736D-04\n",
      "\n",
      "At iterate  180    f=  4.51609D-02    |proj g|=  2.74733D-04\n",
      "\n",
      "At iterate  181    f=  4.51412D-02    |proj g|=  1.90925D-04\n",
      "\n",
      "At iterate  182    f=  4.51332D-02    |proj g|=  1.32534D-04\n",
      "\n",
      "At iterate  183    f=  4.51081D-02    |proj g|=  1.12923D-04\n",
      "\n",
      "At iterate  184    f=  4.51006D-02    |proj g|=  4.04459D-04\n",
      "\n",
      "At iterate  185    f=  4.50824D-02    |proj g|=  2.73002D-04\n",
      "\n",
      "At iterate  186    f=  4.50598D-02    |proj g|=  1.57806D-04\n",
      "\n",
      "At iterate  187    f=  4.50490D-02    |proj g|=  2.20208D-04\n",
      "\n",
      "At iterate  188    f=  4.50185D-02    |proj g|=  2.93846D-04\n",
      "\n",
      "At iterate  189    f=  4.49650D-02    |proj g|=  2.87435D-04\n",
      "\n",
      "At iterate  190    f=  4.48375D-02    |proj g|=  7.04441D-04\n",
      "\n",
      "At iterate  191    f=  4.47746D-02    |proj g|=  3.73953D-04\n",
      "\n",
      "At iterate  192    f=  4.47248D-02    |proj g|=  3.83849D-04\n",
      "\n",
      "At iterate  193    f=  4.46097D-02    |proj g|=  2.19173D-04\n",
      "\n",
      "At iterate  194    f=  4.45722D-02    |proj g|=  2.99552D-04\n",
      "\n",
      "At iterate  195    f=  4.45656D-02    |proj g|=  2.05702D-04\n",
      "\n",
      "At iterate  196    f=  4.45358D-02    |proj g|=  2.00581D-04\n",
      "\n",
      "At iterate  197    f=  4.44811D-02    |proj g|=  3.72236D-04\n",
      "\n",
      "At iterate  198    f=  4.44162D-02    |proj g|=  2.12123D-04\n",
      "\n",
      "At iterate  199    f=  4.43885D-02    |proj g|=  1.93924D-04\n",
      "\n",
      "At iterate  200    f=  4.43445D-02    |proj g|=  1.23817D-04\n",
      "\n",
      "At iterate  201    f=  4.42838D-02    |proj g|=  4.51528D-04\n",
      "\n",
      "At iterate  202    f=  4.42533D-02    |proj g|=  2.37989D-04\n",
      "\n",
      "At iterate  203    f=  4.42159D-02    |proj g|=  2.11965D-04\n",
      "\n",
      "At iterate  204    f=  4.41500D-02    |proj g|=  1.73039D-04\n",
      "\n",
      "At iterate  205    f=  4.40793D-02    |proj g|=  2.26188D-04\n",
      "\n",
      "At iterate  206    f=  4.39896D-02    |proj g|=  3.45343D-04\n",
      "\n",
      "At iterate  207    f=  4.39738D-02    |proj g|=  1.85448D-04\n",
      "\n",
      "At iterate  208    f=  4.39430D-02    |proj g|=  1.80448D-04\n",
      "\n",
      "At iterate  209    f=  4.39316D-02    |proj g|=  1.58693D-04\n",
      "\n",
      "At iterate  210    f=  4.38965D-02    |proj g|=  3.20997D-04\n",
      "\n",
      "At iterate  211    f=  4.38556D-02    |proj g|=  1.99095D-04\n",
      "\n",
      "At iterate  212    f=  4.38111D-02    |proj g|=  1.81602D-04\n",
      "\n",
      "At iterate  213    f=  4.37907D-02    |proj g|=  2.64639D-04\n",
      "\n",
      "At iterate  214    f=  4.37590D-02    |proj g|=  1.87253D-04\n",
      "\n",
      "At iterate  215    f=  4.37210D-02    |proj g|=  3.69626D-04\n",
      "\n",
      "At iterate  216    f=  4.36764D-02    |proj g|=  2.19379D-04\n",
      "\n",
      "At iterate  217    f=  4.36442D-02    |proj g|=  2.35158D-04\n",
      "\n",
      "At iterate  218    f=  4.36218D-02    |proj g|=  2.65514D-04\n",
      "\n",
      "At iterate  219    f=  4.35909D-02    |proj g|=  4.93645D-04\n",
      "\n",
      "At iterate  220    f=  4.35632D-02    |proj g|=  1.93896D-04\n",
      "\n",
      "At iterate  221    f=  4.35457D-02    |proj g|=  2.12284D-04\n",
      "\n",
      "At iterate  222    f=  4.35385D-02    |proj g|=  1.46029D-04\n",
      "\n",
      "At iterate  223    f=  4.35244D-02    |proj g|=  1.11500D-04\n",
      "\n",
      "At iterate  224    f=  4.35128D-02    |proj g|=  2.57015D-04\n",
      "\n",
      "At iterate  225    f=  4.34990D-02    |proj g|=  2.32839D-04\n",
      "\n",
      "At iterate  226    f=  4.34528D-02    |proj g|=  1.76119D-04\n",
      "\n",
      "At iterate  227    f=  4.33944D-02    |proj g|=  1.77788D-04\n",
      "\n",
      "At iterate  228    f=  4.33370D-02    |proj g|=  1.81061D-04\n",
      "\n",
      "At iterate  229    f=  4.33302D-02    |proj g|=  2.15985D-04\n",
      "\n",
      "At iterate  230    f=  4.32900D-02    |proj g|=  1.19614D-04\n",
      "\n",
      "At iterate  231    f=  4.32724D-02    |proj g|=  3.64277D-04\n",
      "\n",
      "At iterate  232    f=  4.32478D-02    |proj g|=  1.45326D-04\n",
      "\n",
      "At iterate  233    f=  4.32259D-02    |proj g|=  1.17177D-04\n",
      "\n",
      "At iterate  234    f=  4.31676D-02    |proj g|=  1.41251D-04\n",
      "\n",
      "At iterate  235    f=  4.31512D-02    |proj g|=  2.92925D-04\n",
      "\n",
      "At iterate  236    f=  4.31160D-02    |proj g|=  2.54137D-04\n",
      "\n",
      "At iterate  237    f=  4.30454D-02    |proj g|=  4.29473D-04\n",
      "\n",
      "At iterate  238    f=  4.30027D-02    |proj g|=  2.85341D-04\n",
      "\n",
      "At iterate  239    f=  4.29607D-02    |proj g|=  9.66609D-05\n",
      "\n",
      "At iterate  240    f=  4.29408D-02    |proj g|=  1.58374D-04\n",
      "\n",
      "At iterate  241    f=  4.29313D-02    |proj g|=  1.56442D-04\n",
      "\n",
      "At iterate  242    f=  4.29170D-02    |proj g|=  2.63997D-04\n",
      "\n",
      "At iterate  243    f=  4.29100D-02    |proj g|=  1.20203D-04\n",
      "\n",
      "At iterate  244    f=  4.29017D-02    |proj g|=  1.00678D-04\n",
      "\n",
      "At iterate  245    f=  4.28752D-02    |proj g|=  1.25380D-04\n",
      "\n",
      "At iterate  246    f=  4.28556D-02    |proj g|=  1.81730D-04\n",
      "\n",
      "At iterate  247    f=  4.28332D-02    |proj g|=  4.39004D-04\n",
      "\n",
      "At iterate  248    f=  4.28076D-02    |proj g|=  2.74879D-04\n",
      "\n",
      "At iterate  249    f=  4.27640D-02    |proj g|=  1.72929D-04\n",
      "\n",
      "At iterate  250    f=  4.27529D-02    |proj g|=  1.18890D-04\n",
      "\n",
      "At iterate  251    f=  4.27400D-02    |proj g|=  1.23892D-04\n",
      "\n",
      "At iterate  252    f=  4.27217D-02    |proj g|=  2.23124D-04\n",
      "\n",
      "At iterate  253    f=  4.27079D-02    |proj g|=  1.81914D-04\n",
      "\n",
      "At iterate  254    f=  4.27022D-02    |proj g|=  3.62209D-04\n",
      "\n",
      "At iterate  255    f=  4.26932D-02    |proj g|=  2.15598D-04\n",
      "\n",
      "At iterate  256    f=  4.26799D-02    |proj g|=  2.70403D-04\n",
      "\n",
      "At iterate  257    f=  4.26653D-02    |proj g|=  4.10583D-04\n",
      "\n",
      "At iterate  258    f=  4.26429D-02    |proj g|=  5.49966D-04\n",
      "\n",
      "At iterate  259    f=  4.26344D-02    |proj g|=  5.12760D-04\n",
      "\n",
      "At iterate  260    f=  4.26096D-02    |proj g|=  4.09435D-04\n",
      "\n",
      "At iterate  261    f=  4.25909D-02    |proj g|=  2.10734D-04\n",
      "\n",
      "At iterate  262    f=  4.25750D-02    |proj g|=  8.73887D-05\n",
      "\n",
      "At iterate  263    f=  4.25541D-02    |proj g|=  3.32521D-04\n",
      "\n",
      "At iterate  264    f=  4.25419D-02    |proj g|=  3.56767D-04\n",
      "\n",
      "At iterate  265    f=  4.25252D-02    |proj g|=  3.99921D-04\n",
      "\n",
      "At iterate  266    f=  4.24891D-02    |proj g|=  2.58192D-04\n",
      "\n",
      "At iterate  267    f=  4.24584D-02    |proj g|=  2.23225D-04\n",
      "\n",
      "At iterate  268    f=  4.24337D-02    |proj g|=  3.39574D-04\n",
      "\n",
      "At iterate  269    f=  4.24278D-02    |proj g|=  1.05032D-04\n",
      "\n",
      "At iterate  270    f=  4.24215D-02    |proj g|=  1.36872D-04\n",
      "\n",
      "At iterate  271    f=  4.24128D-02    |proj g|=  1.87867D-04\n",
      "\n",
      "At iterate  272    f=  4.23838D-02    |proj g|=  2.66022D-04\n",
      "\n",
      "At iterate  273    f=  4.23606D-02    |proj g|=  1.83453D-04\n",
      "\n",
      "At iterate  274    f=  4.23187D-02    |proj g|=  2.11213D-04\n",
      "\n",
      "At iterate  275    f=  4.22916D-02    |proj g|=  2.80148D-04\n",
      "\n",
      "At iterate  276    f=  4.22611D-02    |proj g|=  3.10395D-04\n",
      "\n",
      "At iterate  277    f=  4.22516D-02    |proj g|=  5.92978D-04\n",
      "\n",
      "At iterate  278    f=  4.22149D-02    |proj g|=  3.66569D-04\n",
      "\n",
      "At iterate  279    f=  4.21915D-02    |proj g|=  1.18560D-04\n",
      "\n",
      "At iterate  280    f=  4.21842D-02    |proj g|=  1.35262D-04\n",
      "\n",
      "At iterate  281    f=  4.21775D-02    |proj g|=  1.81452D-04\n",
      "\n",
      "At iterate  282    f=  4.21586D-02    |proj g|=  1.99586D-04\n",
      "\n",
      "At iterate  283    f=  4.21318D-02    |proj g|=  1.74763D-04\n",
      "\n",
      "At iterate  284    f=  4.21073D-02    |proj g|=  1.55840D-04\n",
      "\n",
      "At iterate  285    f=  4.20679D-02    |proj g|=  2.83096D-04\n",
      "\n",
      "At iterate  286    f=  4.20566D-02    |proj g|=  1.69690D-04\n",
      "\n",
      "At iterate  287    f=  4.20500D-02    |proj g|=  1.53404D-04\n",
      "\n",
      "At iterate  288    f=  4.20338D-02    |proj g|=  2.29825D-04\n",
      "\n",
      "At iterate  289    f=  4.20182D-02    |proj g|=  2.79028D-04\n",
      "\n",
      "At iterate  290    f=  4.19968D-02    |proj g|=  1.71828D-04\n",
      "\n",
      "At iterate  291    f=  4.19744D-02    |proj g|=  1.32229D-04\n",
      "\n",
      "At iterate  292    f=  4.19566D-02    |proj g|=  9.02770D-05\n",
      "\n",
      "At iterate  293    f=  4.19391D-02    |proj g|=  2.49680D-04\n",
      "\n",
      "At iterate  294    f=  4.19293D-02    |proj g|=  2.70187D-04\n",
      "\n",
      "At iterate  295    f=  4.19210D-02    |proj g|=  1.14371D-04\n",
      "\n",
      "At iterate  296    f=  4.19168D-02    |proj g|=  1.23113D-04\n",
      "\n",
      "At iterate  297    f=  4.19124D-02    |proj g|=  1.53762D-04\n",
      "\n",
      "At iterate  298    f=  4.18997D-02    |proj g|=  1.70962D-04\n",
      "\n",
      "At iterate  299    f=  4.18852D-02    |proj g|=  2.20429D-04\n",
      "\n",
      "At iterate  300    f=  4.18831D-02    |proj g|=  1.09337D-04\n",
      "\n",
      "At iterate  301    f=  4.18751D-02    |proj g|=  1.02286D-04\n",
      "\n",
      "At iterate  302    f=  4.18225D-02    |proj g|=  6.68397D-05\n",
      "\n",
      "At iterate  303    f=  4.18091D-02    |proj g|=  9.79230D-05\n",
      "\n",
      "At iterate  304    f=  4.17873D-02    |proj g|=  6.68437D-05\n",
      "\n",
      "At iterate  305    f=  4.17823D-02    |proj g|=  2.40946D-04\n",
      "\n",
      "At iterate  306    f=  4.17711D-02    |proj g|=  1.14974D-04\n",
      "\n",
      "At iterate  307    f=  4.17647D-02    |proj g|=  1.30654D-04\n",
      "\n",
      "At iterate  308    f=  4.17544D-02    |proj g|=  1.58328D-04\n",
      "\n",
      "At iterate  309    f=  4.17392D-02    |proj g|=  2.23338D-04\n",
      "\n",
      "At iterate  310    f=  4.17072D-02    |proj g|=  2.68941D-04\n",
      "\n",
      "At iterate  311    f=  4.17033D-02    |proj g|=  4.68279D-04\n",
      "\n",
      "At iterate  312    f=  4.16745D-02    |proj g|=  3.45803D-04\n",
      "\n",
      "At iterate  313    f=  4.16477D-02    |proj g|=  1.88307D-04\n",
      "\n",
      "At iterate  314    f=  4.16271D-02    |proj g|=  1.62997D-04\n",
      "\n",
      "At iterate  315    f=  4.16156D-02    |proj g|=  1.89223D-04\n",
      "\n",
      "At iterate  316    f=  4.16025D-02    |proj g|=  8.11815D-05\n",
      "\n",
      "At iterate  317    f=  4.16001D-02    |proj g|=  3.56200D-04\n",
      "\n",
      "At iterate  318    f=  4.15859D-02    |proj g|=  1.03711D-04\n",
      "\n",
      "At iterate  319    f=  4.15825D-02    |proj g|=  1.16810D-04\n",
      "\n",
      "At iterate  320    f=  4.15782D-02    |proj g|=  1.50499D-04\n",
      "\n",
      "At iterate  321    f=  4.15714D-02    |proj g|=  1.34163D-04\n",
      "\n",
      "At iterate  322    f=  4.15692D-02    |proj g|=  2.68491D-04\n",
      "\n",
      "At iterate  323    f=  4.15567D-02    |proj g|=  1.49686D-04\n",
      "\n",
      "At iterate  324    f=  4.15475D-02    |proj g|=  1.00182D-04\n",
      "\n",
      "At iterate  325    f=  4.15398D-02    |proj g|=  1.68394D-04\n",
      "\n",
      "At iterate  326    f=  4.15312D-02    |proj g|=  2.31114D-04\n",
      "\n",
      "At iterate  327    f=  4.15184D-02    |proj g|=  3.30922D-04\n",
      "\n",
      "At iterate  328    f=  4.15056D-02    |proj g|=  2.22467D-04\n",
      "\n",
      "At iterate  329    f=  4.14889D-02    |proj g|=  1.74880D-04\n",
      "\n",
      "At iterate  330    f=  4.14692D-02    |proj g|=  1.02749D-04\n",
      "\n",
      "At iterate  331    f=  4.14657D-02    |proj g|=  1.46100D-04\n",
      "\n",
      "At iterate  332    f=  4.14541D-02    |proj g|=  9.59264D-05\n",
      "\n",
      "At iterate  333    f=  4.14449D-02    |proj g|=  2.97337D-04\n",
      "\n",
      "At iterate  334    f=  4.14358D-02    |proj g|=  1.19615D-04\n",
      "\n",
      "At iterate  335    f=  4.14291D-02    |proj g|=  1.19607D-04\n",
      "\n",
      "At iterate  336    f=  4.14053D-02    |proj g|=  3.32345D-04\n",
      "\n",
      "At iterate  337    f=  4.13888D-02    |proj g|=  3.18147D-04\n",
      "\n",
      "At iterate  338    f=  4.13757D-02    |proj g|=  6.07323D-04\n",
      "\n",
      "At iterate  339    f=  4.13560D-02    |proj g|=  2.02544D-04\n",
      "\n",
      "At iterate  340    f=  4.13477D-02    |proj g|=  1.10498D-04\n",
      "\n",
      "At iterate  341    f=  4.13445D-02    |proj g|=  1.20203D-04\n",
      "\n",
      "At iterate  342    f=  4.13384D-02    |proj g|=  1.44620D-04\n",
      "\n",
      "At iterate  343    f=  4.13315D-02    |proj g|=  1.72923D-04\n",
      "\n",
      "At iterate  344    f=  4.13112D-02    |proj g|=  1.97702D-04\n",
      "\n",
      "At iterate  345    f=  4.12916D-02    |proj g|=  6.95090D-04\n",
      "\n",
      "At iterate  346    f=  4.12619D-02    |proj g|=  1.65090D-04\n",
      "\n",
      "At iterate  347    f=  4.12539D-02    |proj g|=  7.14251D-05\n",
      "\n",
      "At iterate  348    f=  4.12469D-02    |proj g|=  1.22616D-04\n",
      "\n",
      "At iterate  349    f=  4.12357D-02    |proj g|=  1.25712D-04\n",
      "\n",
      "At iterate  350    f=  4.12184D-02    |proj g|=  2.24281D-04\n",
      "\n",
      "At iterate  351    f=  4.12132D-02    |proj g|=  9.99774D-05\n",
      "\n",
      "At iterate  352    f=  4.12064D-02    |proj g|=  8.27776D-05\n",
      "\n",
      "At iterate  353    f=  4.11956D-02    |proj g|=  5.62825D-05\n",
      "\n",
      "At iterate  354    f=  4.11790D-02    |proj g|=  9.19721D-05\n",
      "\n",
      "At iterate  355    f=  4.11593D-02    |proj g|=  9.39646D-05\n",
      "\n",
      "At iterate  356    f=  4.11499D-02    |proj g|=  2.18534D-04\n",
      "\n",
      "At iterate  357    f=  4.11414D-02    |proj g|=  2.23162D-04\n",
      "\n",
      "At iterate  358    f=  4.11307D-02    |proj g|=  6.26449D-05\n",
      "\n",
      "At iterate  359    f=  4.11287D-02    |proj g|=  3.75359D-05\n",
      "\n",
      "At iterate  360    f=  4.11277D-02    |proj g|=  5.02032D-05\n",
      "\n",
      "At iterate  361    f=  4.11228D-02    |proj g|=  9.84070D-05\n",
      "\n",
      "At iterate  362    f=  4.11097D-02    |proj g|=  1.03364D-04\n",
      "\n",
      "At iterate  363    f=  4.11050D-02    |proj g|=  2.21566D-04\n",
      "\n",
      "At iterate  364    f=  4.10961D-02    |proj g|=  2.01087D-04\n",
      "\n",
      "At iterate  365    f=  4.10850D-02    |proj g|=  1.56747D-04\n",
      "\n",
      "At iterate  366    f=  4.10788D-02    |proj g|=  1.32466D-04\n",
      "\n",
      "At iterate  367    f=  4.10569D-02    |proj g|=  1.61718D-04\n",
      "\n",
      "At iterate  368    f=  4.10545D-02    |proj g|=  1.69039D-04\n",
      "\n",
      "At iterate  369    f=  4.10416D-02    |proj g|=  1.07238D-04\n",
      "\n",
      "At iterate  370    f=  4.10319D-02    |proj g|=  1.45514D-04\n",
      "\n",
      "At iterate  371    f=  4.10250D-02    |proj g|=  7.80160D-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate  372    f=  4.10194D-02    |proj g|=  7.16190D-05\n",
      "\n",
      "At iterate  373    f=  4.10012D-02    |proj g|=  1.07012D-04\n",
      "\n",
      "At iterate  374    f=  4.09986D-02    |proj g|=  8.84133D-05\n",
      "\n",
      "At iterate  375    f=  4.09958D-02    |proj g|=  5.67225D-05\n",
      "\n",
      "At iterate  376    f=  4.09912D-02    |proj g|=  5.01136D-05\n",
      "\n",
      "At iterate  377    f=  4.09816D-02    |proj g|=  6.37577D-05\n",
      "\n",
      "At iterate  378    f=  4.09667D-02    |proj g|=  3.39881D-05\n",
      "\n",
      "At iterate  379    f=  4.09644D-02    |proj g|=  1.96262D-04\n",
      "\n",
      "At iterate  380    f=  4.09533D-02    |proj g|=  7.44139D-05\n",
      "\n",
      "At iterate  381    f=  4.09505D-02    |proj g|=  5.49437D-05\n",
      "\n",
      "At iterate  382    f=  4.09475D-02    |proj g|=  6.54156D-05\n",
      "\n",
      "At iterate  383    f=  4.09430D-02    |proj g|=  8.56367D-05\n",
      "\n",
      "At iterate  384    f=  4.09337D-02    |proj g|=  1.02400D-04\n",
      "\n",
      "At iterate  385    f=  4.09326D-02    |proj g|=  1.79950D-04\n",
      "\n",
      "At iterate  386    f=  4.09210D-02    |proj g|=  1.44750D-04\n",
      "\n",
      "At iterate  387    f=  4.09106D-02    |proj g|=  1.63528D-04\n",
      "\n",
      "At iterate  388    f=  4.09005D-02    |proj g|=  1.33130D-04\n",
      "\n",
      "At iterate  389    f=  4.08910D-02    |proj g|=  9.81165D-05\n",
      "\n",
      "At iterate  390    f=  4.08851D-02    |proj g|=  2.01364D-04\n",
      "\n",
      "At iterate  391    f=  4.08730D-02    |proj g|=  8.98420D-05\n",
      "\n",
      "At iterate  392    f=  4.08681D-02    |proj g|=  5.95896D-05\n",
      "\n",
      "At iterate  393    f=  4.08622D-02    |proj g|=  8.70939D-05\n",
      "\n",
      "At iterate  394    f=  4.08544D-02    |proj g|=  6.62402D-05\n",
      "\n",
      "At iterate  395    f=  4.08359D-02    |proj g|=  8.99602D-05\n",
      "\n",
      "At iterate  396    f=  4.08251D-02    |proj g|=  5.06231D-04\n",
      "\n",
      "At iterate  397    f=  4.07919D-02    |proj g|=  1.37325D-04\n",
      "\n",
      "At iterate  398    f=  4.07809D-02    |proj g|=  8.10016D-05\n",
      "\n",
      "At iterate  399    f=  4.07743D-02    |proj g|=  8.09376D-05\n",
      "\n",
      "At iterate  400    f=  4.07664D-02    |proj g|=  6.74423D-05\n",
      "\n",
      "At iterate  401    f=  4.07604D-02    |proj g|=  9.92484D-05\n",
      "\n",
      "At iterate  402    f=  4.07528D-02    |proj g|=  1.03238D-04\n",
      "\n",
      "At iterate  403    f=  4.07498D-02    |proj g|=  1.05297D-04\n",
      "\n",
      "At iterate  404    f=  4.07447D-02    |proj g|=  1.09628D-04\n",
      "\n",
      "At iterate  405    f=  4.07369D-02    |proj g|=  1.35576D-04\n",
      "\n",
      "At iterate  406    f=  4.07283D-02    |proj g|=  1.71401D-04\n",
      "\n",
      "At iterate  407    f=  4.07225D-02    |proj g|=  1.15751D-04\n",
      "\n",
      "At iterate  408    f=  4.07195D-02    |proj g|=  1.44912D-04\n",
      "\n",
      "At iterate  409    f=  4.07180D-02    |proj g|=  8.76434D-05\n",
      "\n",
      "At iterate  410    f=  4.07165D-02    |proj g|=  6.52145D-05\n",
      "\n",
      "At iterate  411    f=  4.07158D-02    |proj g|=  9.47295D-05\n",
      "\n",
      "At iterate  412    f=  4.07134D-02    |proj g|=  1.33662D-04\n",
      "\n",
      "At iterate  413    f=  4.07078D-02    |proj g|=  1.78692D-04\n",
      "\n",
      "At iterate  414    f=  4.07058D-02    |proj g|=  1.97030D-04\n",
      "\n",
      "At iterate  415    f=  4.06969D-02    |proj g|=  1.62824D-04\n",
      "\n",
      "At iterate  416    f=  4.06851D-02    |proj g|=  6.23082D-05\n",
      "\n",
      "At iterate  417    f=  4.06767D-02    |proj g|=  1.15025D-04\n",
      "\n",
      "At iterate  418    f=  4.06710D-02    |proj g|=  1.46098D-04\n",
      "\n",
      "At iterate  419    f=  4.06603D-02    |proj g|=  1.73290D-04\n",
      "\n",
      "At iterate  420    f=  4.06584D-02    |proj g|=  2.56897D-04\n",
      "\n",
      "At iterate  421    f=  4.06487D-02    |proj g|=  1.77417D-04\n",
      "\n",
      "At iterate  422    f=  4.06431D-02    |proj g|=  4.75614D-05\n",
      "\n",
      "At iterate  423    f=  4.06419D-02    |proj g|=  6.02241D-05\n",
      "\n",
      "At iterate  424    f=  4.06405D-02    |proj g|=  5.84605D-05\n",
      "\n",
      "At iterate  425    f=  4.06394D-02    |proj g|=  1.08018D-04\n",
      "\n",
      "At iterate  426    f=  4.06350D-02    |proj g|=  7.78797D-05\n",
      "\n",
      "At iterate  427    f=  4.06296D-02    |proj g|=  3.78236D-05\n",
      "\n",
      "At iterate  428    f=  4.06236D-02    |proj g|=  8.85524D-05\n",
      "\n",
      "At iterate  429    f=  4.06159D-02    |proj g|=  1.25017D-04\n",
      "\n",
      "At iterate  430    f=  4.06053D-02    |proj g|=  1.39925D-04\n",
      "\n",
      "At iterate  431    f=  4.06048D-02    |proj g|=  1.87821D-04\n",
      "\n",
      "At iterate  432    f=  4.05975D-02    |proj g|=  1.13195D-04\n",
      "\n",
      "At iterate  433    f=  4.05920D-02    |proj g|=  9.35956D-05\n",
      "\n",
      "At iterate  434    f=  4.05896D-02    |proj g|=  8.80006D-05\n",
      "\n",
      "At iterate  435    f=  4.05857D-02    |proj g|=  1.35599D-04\n",
      "\n",
      "At iterate  436    f=  4.05823D-02    |proj g|=  1.69459D-04\n",
      "\n",
      "At iterate  437    f=  4.05747D-02    |proj g|=  1.13714D-04\n",
      "\n",
      "At iterate  438    f=  4.05705D-02    |proj g|=  4.96342D-05\n",
      "\n",
      "At iterate  439    f=  4.05665D-02    |proj g|=  6.14442D-05\n",
      "\n",
      "At iterate  440    f=  4.05640D-02    |proj g|=  1.12890D-04\n",
      "\n",
      "At iterate  441    f=  4.05576D-02    |proj g|=  1.65947D-04\n",
      "\n",
      "At iterate  442    f=  4.05444D-02    |proj g|=  1.66867D-04\n",
      "\n",
      "At iterate  443    f=  4.05416D-02    |proj g|=  2.33735D-04\n",
      "\n",
      "At iterate  444    f=  4.05300D-02    |proj g|=  1.46515D-04\n",
      "\n",
      "At iterate  445    f=  4.05244D-02    |proj g|=  6.06283D-05\n",
      "\n",
      "At iterate  446    f=  4.05219D-02    |proj g|=  9.04166D-05\n",
      "\n",
      "At iterate  447    f=  4.05174D-02    |proj g|=  1.00367D-04\n",
      "\n",
      "At iterate  448    f=  4.05144D-02    |proj g|=  1.20471D-04\n",
      "\n",
      "At iterate  449    f=  4.05093D-02    |proj g|=  4.22014D-05\n",
      "\n",
      "At iterate  450    f=  4.05062D-02    |proj g|=  4.90228D-05\n",
      "\n",
      "At iterate  451    f=  4.04987D-02    |proj g|=  9.84453D-05\n",
      "\n",
      "At iterate  452    f=  4.04889D-02    |proj g|=  1.15325D-04\n",
      "\n",
      "At iterate  453    f=  4.04825D-02    |proj g|=  2.36040D-04\n",
      "\n",
      "At iterate  454    f=  4.04797D-02    |proj g|=  3.13106D-04\n",
      "\n",
      "At iterate  455    f=  4.04708D-02    |proj g|=  1.53030D-04\n",
      "\n",
      "At iterate  456    f=  4.04668D-02    |proj g|=  6.69737D-05\n",
      "\n",
      "At iterate  457    f=  4.04653D-02    |proj g|=  6.91967D-05\n",
      "\n",
      "At iterate  458    f=  4.04595D-02    |proj g|=  1.08574D-04\n",
      "\n",
      "At iterate  459    f=  4.04532D-02    |proj g|=  1.48481D-04\n",
      "\n",
      "At iterate  460    f=  4.04461D-02    |proj g|=  9.82599D-05\n",
      "\n",
      "At iterate  461    f=  4.04384D-02    |proj g|=  6.09587D-05\n",
      "\n",
      "At iterate  462    f=  4.04345D-02    |proj g|=  6.16111D-05\n",
      "\n",
      "At iterate  463    f=  4.04168D-02    |proj g|=  8.49395D-05\n",
      "\n",
      "At iterate  464    f=  4.04063D-02    |proj g|=  1.17313D-04\n",
      "\n",
      "At iterate  465    f=  4.04049D-02    |proj g|=  1.62862D-04\n",
      "\n",
      "At iterate  466    f=  4.04006D-02    |proj g|=  1.18033D-04\n",
      "\n",
      "At iterate  467    f=  4.03944D-02    |proj g|=  1.07075D-04\n",
      "\n",
      "At iterate  468    f=  4.03914D-02    |proj g|=  1.01756D-04\n",
      "\n",
      "At iterate  469    f=  4.03742D-02    |proj g|=  1.23043D-04\n",
      "\n",
      "At iterate  470    f=  4.03663D-02    |proj g|=  1.52744D-04\n",
      "\n",
      "At iterate  471    f=  4.03603D-02    |proj g|=  1.42028D-04\n",
      "\n",
      "At iterate  472    f=  4.03577D-02    |proj g|=  1.54838D-04\n",
      "\n",
      "At iterate  473    f=  4.03531D-02    |proj g|=  1.32964D-04\n",
      "\n",
      "At iterate  474    f=  4.03378D-02    |proj g|=  1.42394D-04\n",
      "\n",
      "At iterate  475    f=  4.03314D-02    |proj g|=  1.35035D-04\n",
      "\n",
      "At iterate  476    f=  4.03303D-02    |proj g|=  7.20217D-05\n",
      "\n",
      "At iterate  477    f=  4.03267D-02    |proj g|=  7.74400D-05\n",
      "\n",
      "At iterate  478    f=  4.03221D-02    |proj g|=  1.78804D-04\n",
      "\n",
      "At iterate  479    f=  4.03000D-02    |proj g|=  1.67060D-04\n",
      "\n",
      "At iterate  480    f=  4.02846D-02    |proj g|=  9.28196D-05\n",
      "\n",
      "At iterate  481    f=  4.02582D-02    |proj g|=  7.17717D-05\n",
      "\n",
      "At iterate  482    f=  4.02559D-02    |proj g|=  8.69779D-05\n",
      "\n",
      "At iterate  483    f=  4.02475D-02    |proj g|=  6.93871D-05\n",
      "\n",
      "At iterate  484    f=  4.02387D-02    |proj g|=  9.22896D-05\n",
      "\n",
      "At iterate  485    f=  4.02234D-02    |proj g|=  7.75969D-05\n",
      "\n",
      "At iterate  486    f=  4.02078D-02    |proj g|=  1.38418D-04\n",
      "\n",
      "At iterate  487    f=  4.02068D-02    |proj g|=  2.00238D-04\n",
      "\n",
      "At iterate  488    f=  4.02011D-02    |proj g|=  1.56967D-04\n",
      "\n",
      "At iterate  489    f=  4.01895D-02    |proj g|=  8.91236D-05\n",
      "\n",
      "At iterate  490    f=  4.01807D-02    |proj g|=  6.92452D-05\n",
      "\n",
      "At iterate  491    f=  4.01581D-02    |proj g|=  1.43402D-04\n",
      "\n",
      "At iterate  492    f=  4.01346D-02    |proj g|=  1.94540D-04\n",
      "\n",
      "At iterate  493    f=  4.01298D-02    |proj g|=  1.05479D-04\n",
      "\n",
      "At iterate  494    f=  4.01121D-02    |proj g|=  6.19295D-05\n",
      "\n",
      "At iterate  495    f=  4.01027D-02    |proj g|=  5.21168D-05\n",
      "\n",
      "At iterate  496    f=  4.00930D-02    |proj g|=  4.63127D-05\n",
      "\n",
      "At iterate  497    f=  4.00825D-02    |proj g|=  8.91245D-05\n",
      "\n",
      "At iterate  498    f=  4.00818D-02    |proj g|=  1.23886D-04\n",
      "\n",
      "At iterate  499    f=  4.00746D-02    |proj g|=  1.01779D-04\n",
      "\n",
      "At iterate  500    f=  4.00700D-02    |proj g|=  4.34626D-05\n",
      "\n",
      "At iterate  501    f=  4.00678D-02    |proj g|=  4.76819D-05\n",
      "\n",
      "At iterate  502    f=  4.00598D-02    |proj g|=  5.64245D-05\n",
      "\n",
      "At iterate  503    f=  4.00492D-02    |proj g|=  2.47026D-04\n",
      "\n",
      "At iterate  504    f=  4.00373D-02    |proj g|=  1.63074D-04\n",
      "\n",
      "At iterate  505    f=  4.00238D-02    |proj g|=  4.71668D-05\n",
      "\n",
      "At iterate  506    f=  4.00169D-02    |proj g|=  7.84091D-05\n",
      "\n",
      "At iterate  507    f=  4.00119D-02    |proj g|=  7.43154D-05\n",
      "\n",
      "At iterate  508    f=  3.99965D-02    |proj g|=  1.89165D-04\n",
      "\n",
      "At iterate  509    f=  3.99955D-02    |proj g|=  2.40098D-04\n",
      "\n",
      "At iterate  510    f=  3.99748D-02    |proj g|=  1.31119D-04\n",
      "\n",
      "At iterate  511    f=  3.99616D-02    |proj g|=  1.36876D-04\n",
      "\n",
      "At iterate  512    f=  3.99493D-02    |proj g|=  8.22576D-05\n",
      "\n",
      "At iterate  513    f=  3.99398D-02    |proj g|=  1.10284D-04\n",
      "\n",
      "At iterate  514    f=  3.99192D-02    |proj g|=  1.57027D-04\n",
      "\n",
      "At iterate  515    f=  3.99059D-02    |proj g|=  1.67387D-04\n",
      "\n",
      "At iterate  516    f=  3.98913D-02    |proj g|=  7.52163D-05\n",
      "\n",
      "At iterate  517    f=  3.98836D-02    |proj g|=  6.72028D-05\n",
      "\n",
      "At iterate  518    f=  3.98698D-02    |proj g|=  6.95687D-05\n",
      "\n",
      "At iterate  519    f=  3.98547D-02    |proj g|=  4.90138D-05\n",
      "\n",
      "At iterate  520    f=  3.98525D-02    |proj g|=  2.46649D-04\n",
      "\n",
      "At iterate  521    f=  3.98376D-02    |proj g|=  1.68398D-04\n",
      "\n",
      "At iterate  522    f=  3.98236D-02    |proj g|=  2.72727D-04\n",
      "\n",
      "At iterate  523    f=  3.98088D-02    |proj g|=  1.53396D-04\n",
      "\n",
      "At iterate  524    f=  3.97936D-02    |proj g|=  9.67678D-05\n",
      "\n",
      "At iterate  525    f=  3.97774D-02    |proj g|=  1.02165D-04\n",
      "\n",
      "At iterate  526    f=  3.97585D-02    |proj g|=  2.20133D-04\n",
      "\n",
      "At iterate  527    f=  3.97460D-02    |proj g|=  1.84914D-04\n",
      "\n",
      "At iterate  528    f=  3.97305D-02    |proj g|=  9.21902D-05\n",
      "\n",
      "At iterate  529    f=  3.97176D-02    |proj g|=  5.45099D-05\n",
      "\n",
      "At iterate  530    f=  3.96989D-02    |proj g|=  7.52133D-05\n",
      "\n",
      "At iterate  531    f=  3.96939D-02    |proj g|=  2.46163D-04\n",
      "\n",
      "At iterate  532    f=  3.96680D-02    |proj g|=  1.65112D-04\n",
      "\n",
      "At iterate  533    f=  3.96499D-02    |proj g|=  1.69171D-04\n",
      "\n",
      "At iterate  534    f=  3.96369D-02    |proj g|=  1.47526D-04\n",
      "\n",
      "At iterate  535    f=  3.96143D-02    |proj g|=  2.14963D-04\n",
      "\n",
      "At iterate  536    f=  3.95959D-02    |proj g|=  1.20330D-04\n",
      "\n",
      "At iterate  537    f=  3.95880D-02    |proj g|=  1.40397D-04\n",
      "\n",
      "At iterate  538    f=  3.95772D-02    |proj g|=  1.16597D-04\n",
      "\n",
      "At iterate  539    f=  3.95342D-02    |proj g|=  1.89824D-04\n",
      "\n",
      "At iterate  540    f=  3.94879D-02    |proj g|=  1.82086D-04\n",
      "\n",
      "At iterate  541    f=  3.94610D-02    |proj g|=  2.74703D-04\n",
      "\n",
      "At iterate  542    f=  3.94317D-02    |proj g|=  6.01221D-04\n",
      "\n",
      "At iterate  543    f=  3.94079D-02    |proj g|=  3.29987D-04\n",
      "\n",
      "At iterate  544    f=  3.93937D-02    |proj g|=  1.12459D-04\n",
      "\n",
      "At iterate  545    f=  3.93898D-02    |proj g|=  1.01920D-04\n",
      "\n",
      "At iterate  546    f=  3.93720D-02    |proj g|=  9.76686D-05\n",
      "\n",
      "At iterate  547    f=  3.93639D-02    |proj g|=  9.72432D-05\n",
      "\n",
      "At iterate  548    f=  3.93487D-02    |proj g|=  1.16272D-04\n",
      "\n",
      "At iterate  549    f=  3.93364D-02    |proj g|=  2.44209D-04\n",
      "\n",
      "At iterate  550    f=  3.93245D-02    |proj g|=  1.90090D-04\n",
      "\n",
      "At iterate  551    f=  3.92920D-02    |proj g|=  1.42485D-04\n",
      "\n",
      "At iterate  552    f=  3.92729D-02    |proj g|=  9.21887D-05\n",
      "\n",
      "At iterate  553    f=  3.92325D-02    |proj g|=  4.04849D-04\n",
      "\n",
      "At iterate  554    f=  3.92086D-02    |proj g|=  2.20293D-04\n",
      "\n",
      "At iterate  555    f=  3.91746D-02    |proj g|=  3.56076D-04\n",
      "\n",
      "At iterate  556    f=  3.91076D-02    |proj g|=  2.37958D-04\n",
      "\n",
      "At iterate  557    f=  3.90920D-02    |proj g|=  4.18238D-04\n",
      "\n",
      "At iterate  558    f=  3.90704D-02    |proj g|=  2.37251D-04\n",
      "\n",
      "At iterate  559    f=  3.90599D-02    |proj g|=  2.78410D-04\n",
      "\n",
      "At iterate  560    f=  3.90525D-02    |proj g|=  2.57029D-04\n",
      "\n",
      "At iterate  561    f=  3.90454D-02    |proj g|=  5.94471D-04\n",
      "\n",
      "At iterate  562    f=  3.90116D-02    |proj g|=  1.17292D-04\n",
      "\n",
      "At iterate  563    f=  3.89970D-02    |proj g|=  1.90708D-04\n",
      "\n",
      "At iterate  564    f=  3.89869D-02    |proj g|=  2.78766D-04\n",
      "\n",
      "At iterate  565    f=  3.89707D-02    |proj g|=  3.27279D-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate  566    f=  3.89599D-02    |proj g|=  3.41185D-04\n",
      "\n",
      "At iterate  567    f=  3.89400D-02    |proj g|=  1.60223D-04\n",
      "\n",
      "At iterate  568    f=  3.89250D-02    |proj g|=  1.47716D-04\n",
      "\n",
      "At iterate  569    f=  3.89175D-02    |proj g|=  9.50257D-04\n",
      "\n",
      "At iterate  570    f=  3.88784D-02    |proj g|=  3.51924D-04\n",
      "\n",
      "At iterate  571    f=  3.88668D-02    |proj g|=  3.23097D-04\n",
      "\n",
      "At iterate  572    f=  3.88030D-02    |proj g|=  3.34985D-04\n",
      "\n",
      "At iterate  573    f=  3.87532D-02    |proj g|=  2.73114D-04\n",
      "\n",
      "At iterate  574    f=  3.87339D-02    |proj g|=  3.70813D-04\n",
      "\n",
      "At iterate  575    f=  3.86956D-02    |proj g|=  2.02037D-04\n",
      "\n",
      "At iterate  576    f=  3.86864D-02    |proj g|=  1.14556D-04\n",
      "\n",
      "At iterate  577    f=  3.86705D-02    |proj g|=  1.07546D-04\n",
      "\n",
      "At iterate  578    f=  3.86242D-02    |proj g|=  1.46996D-04\n",
      "\n",
      "At iterate  579    f=  3.86101D-02    |proj g|=  4.87691D-04\n",
      "\n",
      "At iterate  580    f=  3.85873D-02    |proj g|=  2.96849D-04\n",
      "\n",
      "At iterate  581    f=  3.85752D-02    |proj g|=  2.41564D-04\n",
      "\n",
      "At iterate  582    f=  3.85643D-02    |proj g|=  2.64162D-04\n",
      "\n",
      "At iterate  583    f=  3.85252D-02    |proj g|=  1.75815D-04\n",
      "\n",
      "At iterate  584    f=  3.84997D-02    |proj g|=  3.89860D-04\n",
      "\n",
      "At iterate  585    f=  3.84730D-02    |proj g|=  1.06992D-04\n",
      "\n",
      "At iterate  586    f=  3.84677D-02    |proj g|=  1.04501D-04\n",
      "\n",
      "At iterate  587    f=  3.84532D-02    |proj g|=  2.49222D-04\n",
      "\n",
      "At iterate  588    f=  3.84248D-02    |proj g|=  2.23513D-04\n",
      "\n",
      "At iterate  589    f=  3.83664D-02    |proj g|=  2.44855D-04\n",
      "\n",
      "At iterate  590    f=  3.83608D-02    |proj g|=  1.94481D-04\n",
      "\n",
      "At iterate  591    f=  3.83418D-02    |proj g|=  1.10118D-04\n",
      "\n",
      "At iterate  592    f=  3.83384D-02    |proj g|=  2.40493D-04\n",
      "\n",
      "At iterate  593    f=  3.83283D-02    |proj g|=  1.91415D-04\n",
      "\n",
      "At iterate  594    f=  3.83073D-02    |proj g|=  1.30607D-04\n",
      "\n",
      "At iterate  595    f=  3.82730D-02    |proj g|=  8.12513D-05\n",
      "\n",
      "At iterate  596    f=  3.82690D-02    |proj g|=  1.82959D-04\n",
      "\n",
      "At iterate  597    f=  3.82585D-02    |proj g|=  1.26614D-04\n",
      "\n",
      "At iterate  598    f=  3.82463D-02    |proj g|=  1.54528D-04\n",
      "\n",
      "At iterate  599    f=  3.82377D-02    |proj g|=  1.36559D-04\n",
      "\n",
      "At iterate  600    f=  3.82094D-02    |proj g|=  1.40914D-04\n",
      "\n",
      "At iterate  601    f=  3.81974D-02    |proj g|=  2.91079D-04\n",
      "\n",
      "At iterate  602    f=  3.81787D-02    |proj g|=  1.27717D-04\n",
      "\n",
      "At iterate  603    f=  3.81737D-02    |proj g|=  1.10742D-04\n",
      "\n",
      "At iterate  604    f=  3.81723D-02    |proj g|=  1.53658D-04\n",
      "\n",
      "At iterate  605    f=  3.81691D-02    |proj g|=  1.85232D-04\n",
      "\n",
      "At iterate  606    f=  3.81525D-02    |proj g|=  1.90954D-04\n",
      "\n",
      "At iterate  607    f=  3.81216D-02    |proj g|=  1.63434D-04\n",
      "\n",
      "At iterate  608    f=  3.81167D-02    |proj g|=  2.40422D-04\n",
      "\n",
      "At iterate  609    f=  3.81037D-02    |proj g|=  1.82546D-04\n",
      "\n",
      "At iterate  610    f=  3.81001D-02    |proj g|=  1.32868D-04\n",
      "\n",
      "At iterate  611    f=  3.80907D-02    |proj g|=  9.83325D-05\n",
      "\n",
      "At iterate  612    f=  3.80809D-02    |proj g|=  1.46557D-04\n",
      "\n",
      "At iterate  613    f=  3.80667D-02    |proj g|=  1.55369D-04\n",
      "\n",
      "At iterate  614    f=  3.80636D-02    |proj g|=  2.10962D-04\n",
      "\n",
      "At iterate  615    f=  3.80558D-02    |proj g|=  1.27417D-04\n",
      "\n",
      "At iterate  616    f=  3.80499D-02    |proj g|=  1.16108D-04\n",
      "\n",
      "At iterate  617    f=  3.80383D-02    |proj g|=  1.55379D-04\n",
      "\n",
      "At iterate  618    f=  3.80252D-02    |proj g|=  1.35459D-04\n",
      "\n",
      "At iterate  619    f=  3.80203D-02    |proj g|=  9.12984D-05\n",
      "\n",
      "At iterate  620    f=  3.80171D-02    |proj g|=  9.84587D-05\n",
      "\n",
      "At iterate  621    f=  3.80098D-02    |proj g|=  1.72514D-04\n",
      "\n",
      "At iterate  622    f=  3.79958D-02    |proj g|=  1.77516D-04\n",
      "\n",
      "At iterate  623    f=  3.79658D-02    |proj g|=  3.43932D-04\n",
      "\n",
      "At iterate  624    f=  3.79433D-02    |proj g|=  2.14671D-04\n",
      "\n",
      "At iterate  625    f=  3.79353D-02    |proj g|=  1.30045D-04\n",
      "\n",
      "At iterate  626    f=  3.79300D-02    |proj g|=  1.61444D-04\n",
      "\n",
      "At iterate  627    f=  3.79249D-02    |proj g|=  1.23771D-04\n",
      "\n",
      "At iterate  628    f=  3.79185D-02    |proj g|=  3.54397D-04\n",
      "\n",
      "At iterate  629    f=  3.79069D-02    |proj g|=  2.37380D-04\n",
      "\n",
      "At iterate  630    f=  3.78820D-02    |proj g|=  1.88928D-04\n",
      "\n",
      "At iterate  631    f=  3.78755D-02    |proj g|=  1.59120D-04\n",
      "\n",
      "At iterate  632    f=  3.78564D-02    |proj g|=  2.05390D-04\n",
      "\n",
      "At iterate  633    f=  3.78273D-02    |proj g|=  1.94093D-04\n",
      "\n",
      "At iterate  634    f=  3.78120D-02    |proj g|=  1.27646D-04\n",
      "\n",
      "At iterate  635    f=  3.77997D-02    |proj g|=  1.68978D-04\n",
      "\n",
      "At iterate  636    f=  3.77948D-02    |proj g|=  1.39991D-04\n",
      "\n",
      "At iterate  637    f=  3.77923D-02    |proj g|=  9.64192D-05\n",
      "\n",
      "At iterate  638    f=  3.77849D-02    |proj g|=  1.04409D-04\n",
      "\n",
      "At iterate  639    f=  3.77621D-02    |proj g|=  6.28040D-05\n",
      "\n",
      "At iterate  640    f=  3.77593D-02    |proj g|=  2.50625D-04\n",
      "\n",
      "At iterate  641    f=  3.77388D-02    |proj g|=  1.55527D-04\n",
      "\n",
      "At iterate  642    f=  3.77232D-02    |proj g|=  8.76128D-05\n",
      "\n",
      "At iterate  643    f=  3.77182D-02    |proj g|=  5.36351D-05\n",
      "\n",
      "At iterate  644    f=  3.77144D-02    |proj g|=  2.00638D-04\n",
      "\n",
      "At iterate  645    f=  3.77035D-02    |proj g|=  1.01260D-04\n",
      "\n",
      "At iterate  646    f=  3.76940D-02    |proj g|=  7.98317D-05\n",
      "\n",
      "At iterate  647    f=  3.76890D-02    |proj g|=  9.75876D-05\n",
      "\n",
      "At iterate  648    f=  3.76850D-02    |proj g|=  9.46380D-05\n",
      "\n",
      "At iterate  649    f=  3.76772D-02    |proj g|=  9.24332D-05\n",
      "\n",
      "At iterate  650    f=  3.76632D-02    |proj g|=  7.23048D-05\n",
      "\n",
      "At iterate  651    f=  3.76507D-02    |proj g|=  3.79802D-04\n",
      "\n",
      "At iterate  652    f=  3.76252D-02    |proj g|=  1.17750D-04\n",
      "\n",
      "At iterate  653    f=  3.76224D-02    |proj g|=  9.93065D-05\n",
      "\n",
      "At iterate  654    f=  3.76185D-02    |proj g|=  9.73741D-05\n",
      "\n",
      "At iterate  655    f=  3.76085D-02    |proj g|=  1.05883D-04\n",
      "\n",
      "At iterate  656    f=  3.76056D-02    |proj g|=  2.20693D-04\n",
      "\n",
      "At iterate  657    f=  3.75956D-02    |proj g|=  1.85849D-04\n",
      "\n",
      "At iterate  658    f=  3.75788D-02    |proj g|=  1.41151D-04\n",
      "\n",
      "At iterate  659    f=  3.75707D-02    |proj g|=  2.17240D-04\n",
      "\n",
      "At iterate  660    f=  3.75517D-02    |proj g|=  1.88473D-04\n",
      "\n",
      "At iterate  661    f=  3.75419D-02    |proj g|=  1.25331D-04\n",
      "\n",
      "At iterate  662    f=  3.75330D-02    |proj g|=  2.93069D-04\n",
      "\n",
      "At iterate  663    f=  3.75277D-02    |proj g|=  1.19793D-04\n",
      "\n",
      "At iterate  664    f=  3.75256D-02    |proj g|=  8.11839D-05\n",
      "\n",
      "At iterate  665    f=  3.75218D-02    |proj g|=  8.42563D-05\n",
      "\n",
      "At iterate  666    f=  3.75159D-02    |proj g|=  1.10967D-04\n",
      "\n",
      "At iterate  667    f=  3.75116D-02    |proj g|=  1.41238D-04\n",
      "\n",
      "At iterate  668    f=  3.74941D-02    |proj g|=  1.10043D-04\n",
      "\n",
      "At iterate  669    f=  3.74792D-02    |proj g|=  1.45564D-04\n",
      "\n",
      "At iterate  670    f=  3.74524D-02    |proj g|=  1.10752D-04\n",
      "\n",
      "At iterate  671    f=  3.74493D-02    |proj g|=  1.49054D-04\n",
      "\n",
      "At iterate  672    f=  3.74420D-02    |proj g|=  1.10307D-04\n",
      "\n",
      "At iterate  673    f=  3.74327D-02    |proj g|=  1.06118D-04\n",
      "\n",
      "At iterate  674    f=  3.74305D-02    |proj g|=  1.93736D-04\n",
      "\n",
      "At iterate  675    f=  3.74220D-02    |proj g|=  1.85240D-04\n",
      "\n",
      "At iterate  676    f=  3.74136D-02    |proj g|=  1.21707D-04\n",
      "\n",
      "At iterate  677    f=  3.74082D-02    |proj g|=  8.03559D-05\n",
      "\n",
      "At iterate  678    f=  3.73981D-02    |proj g|=  9.41893D-05\n",
      "\n",
      "At iterate  679    f=  3.73815D-02    |proj g|=  1.30755D-04\n",
      "\n",
      "At iterate  680    f=  3.73750D-02    |proj g|=  2.05359D-04\n",
      "\n",
      "At iterate  681    f=  3.73674D-02    |proj g|=  7.89036D-05\n",
      "\n",
      "At iterate  682    f=  3.73652D-02    |proj g|=  3.85252D-05\n",
      "\n",
      "At iterate  683    f=  3.73636D-02    |proj g|=  7.21871D-05\n",
      "\n",
      "At iterate  684    f=  3.73597D-02    |proj g|=  9.91535D-05\n",
      "\n",
      "At iterate  685    f=  3.73584D-02    |proj g|=  2.05142D-04\n",
      "\n",
      "At iterate  686    f=  3.73517D-02    |proj g|=  1.63494D-04\n",
      "\n",
      "At iterate  687    f=  3.73436D-02    |proj g|=  9.14915D-05\n",
      "\n",
      "At iterate  688    f=  3.73381D-02    |proj g|=  7.97282D-05\n",
      "\n",
      "At iterate  689    f=  3.73337D-02    |proj g|=  1.29110D-04\n",
      "\n",
      "At iterate  690    f=  3.73283D-02    |proj g|=  1.17270D-04\n",
      "\n",
      "At iterate  691    f=  3.73216D-02    |proj g|=  2.21644D-04\n",
      "\n",
      "At iterate  692    f=  3.73084D-02    |proj g|=  1.02633D-04\n",
      "\n",
      "At iterate  693    f=  3.73036D-02    |proj g|=  5.84262D-05\n",
      "\n",
      "At iterate  694    f=  3.72996D-02    |proj g|=  7.38513D-05\n",
      "\n",
      "At iterate  695    f=  3.72966D-02    |proj g|=  9.82798D-05\n",
      "\n",
      "At iterate  696    f=  3.72892D-02    |proj g|=  9.88615D-05\n",
      "\n",
      "At iterate  697    f=  3.72868D-02    |proj g|=  1.97616D-04\n",
      "\n",
      "At iterate  698    f=  3.72795D-02    |proj g|=  1.07815D-04\n",
      "\n",
      "At iterate  699    f=  3.72732D-02    |proj g|=  7.84451D-05\n",
      "\n",
      "At iterate  700    f=  3.72710D-02    |proj g|=  8.16345D-05\n",
      "\n",
      "At iterate  701    f=  3.72616D-02    |proj g|=  1.07796D-04\n",
      "\n",
      "At iterate  702    f=  3.72422D-02    |proj g|=  1.19710D-04\n",
      "\n",
      "At iterate  703    f=  3.72382D-02    |proj g|=  2.03304D-04\n",
      "\n",
      "At iterate  704    f=  3.72276D-02    |proj g|=  1.22083D-04\n",
      "\n",
      "At iterate  705    f=  3.72197D-02    |proj g|=  8.12617D-05\n",
      "\n",
      "At iterate  706    f=  3.72175D-02    |proj g|=  8.92243D-05\n",
      "\n",
      "At iterate  707    f=  3.72131D-02    |proj g|=  6.38194D-05\n",
      "\n",
      "At iterate  708    f=  3.72044D-02    |proj g|=  8.21749D-05\n",
      "\n",
      "At iterate  709    f=  3.72032D-02    |proj g|=  2.02328D-04\n",
      "\n",
      "At iterate  710    f=  3.71899D-02    |proj g|=  1.44112D-04\n",
      "\n",
      "At iterate  711    f=  3.71773D-02    |proj g|=  1.06775D-04\n",
      "\n",
      "At iterate  712    f=  3.71700D-02    |proj g|=  1.00819D-04\n",
      "\n",
      "At iterate  713    f=  3.71591D-02    |proj g|=  7.68260D-05\n",
      "\n",
      "At iterate  714    f=  3.71555D-02    |proj g|=  1.33912D-04\n",
      "\n",
      "At iterate  715    f=  3.71495D-02    |proj g|=  9.69529D-05\n",
      "\n",
      "At iterate  716    f=  3.71414D-02    |proj g|=  8.52666D-05\n",
      "\n",
      "At iterate  717    f=  3.71267D-02    |proj g|=  6.43629D-05\n",
      "\n",
      "At iterate  718    f=  3.71241D-02    |proj g|=  8.84694D-05\n",
      "\n",
      "At iterate  719    f=  3.71180D-02    |proj g|=  4.15582D-05\n",
      "\n",
      "At iterate  720    f=  3.71114D-02    |proj g|=  1.16797D-04\n",
      "\n",
      "At iterate  721    f=  3.71078D-02    |proj g|=  1.26958D-04\n",
      "\n",
      "At iterate  722    f=  3.71022D-02    |proj g|=  1.09122D-04\n",
      "\n",
      "At iterate  723    f=  3.70986D-02    |proj g|=  8.38861D-05\n",
      "\n",
      "At iterate  724    f=  3.70910D-02    |proj g|=  9.17222D-05\n",
      "\n",
      "At iterate  725    f=  3.70865D-02    |proj g|=  2.25708D-04\n",
      "\n",
      "At iterate  726    f=  3.70761D-02    |proj g|=  1.61516D-04\n",
      "\n",
      "At iterate  727    f=  3.70672D-02    |proj g|=  1.05387D-04\n",
      "\n",
      "At iterate  728    f=  3.70596D-02    |proj g|=  1.21332D-04\n",
      "\n",
      "At iterate  729    f=  3.70529D-02    |proj g|=  1.36273D-04\n",
      "\n",
      "At iterate  730    f=  3.70461D-02    |proj g|=  8.31709D-05\n",
      "\n",
      "At iterate  731    f=  3.70431D-02    |proj g|=  1.43999D-04\n",
      "\n",
      "At iterate  732    f=  3.70405D-02    |proj g|=  4.53403D-05\n",
      "\n",
      "At iterate  733    f=  3.70392D-02    |proj g|=  6.59481D-05\n",
      "\n",
      "At iterate  734    f=  3.70377D-02    |proj g|=  6.48466D-05\n",
      "\n",
      "At iterate  735    f=  3.70344D-02    |proj g|=  1.25974D-04\n",
      "\n",
      "At iterate  736    f=  3.70324D-02    |proj g|=  1.16457D-04\n",
      "\n",
      "At iterate  737    f=  3.70310D-02    |proj g|=  9.09504D-05\n",
      "\n",
      "At iterate  738    f=  3.70278D-02    |proj g|=  9.05768D-05\n",
      "\n",
      "At iterate  739    f=  3.70239D-02    |proj g|=  8.00211D-05\n",
      "\n",
      "At iterate  740    f=  3.70201D-02    |proj g|=  1.00244D-04\n",
      "\n",
      "At iterate  741    f=  3.70124D-02    |proj g|=  1.38422D-04\n",
      "\n",
      "At iterate  742    f=  3.70095D-02    |proj g|=  8.94152D-05\n",
      "\n",
      "At iterate  743    f=  3.70069D-02    |proj g|=  9.62837D-05\n",
      "\n",
      "At iterate  744    f=  3.70020D-02    |proj g|=  1.08877D-04\n",
      "\n",
      "At iterate  745    f=  3.69935D-02    |proj g|=  1.60411D-04\n",
      "\n",
      "At iterate  746    f=  3.69836D-02    |proj g|=  1.21414D-04\n",
      "\n",
      "At iterate  747    f=  3.69814D-02    |proj g|=  3.65070D-04\n",
      "\n",
      "At iterate  748    f=  3.69734D-02    |proj g|=  1.67160D-04\n",
      "\n",
      "At iterate  749    f=  3.69631D-02    |proj g|=  1.18814D-04\n",
      "\n",
      "At iterate  750    f=  3.69574D-02    |proj g|=  1.68859D-04\n",
      "\n",
      "At iterate  751    f=  3.69507D-02    |proj g|=  1.24348D-04\n",
      "\n",
      "At iterate  752    f=  3.69416D-02    |proj g|=  7.82750D-05\n",
      "\n",
      "At iterate  753    f=  3.69366D-02    |proj g|=  1.29801D-04\n",
      "\n",
      "At iterate  754    f=  3.69322D-02    |proj g|=  1.21519D-04\n",
      "\n",
      "At iterate  755    f=  3.69305D-02    |proj g|=  1.08272D-04\n",
      "\n",
      "At iterate  756    f=  3.69253D-02    |proj g|=  1.14847D-04\n",
      "\n",
      "At iterate  757    f=  3.69233D-02    |proj g|=  1.07294D-04\n",
      "\n",
      "At iterate  758    f=  3.69024D-02    |proj g|=  1.14044D-04\n",
      "\n",
      "At iterate  759    f=  3.69009D-02    |proj g|=  1.74268D-04\n",
      "\n",
      "At iterate  760    f=  3.68840D-02    |proj g|=  1.24403D-04\n",
      "\n",
      "At iterate  761    f=  3.68767D-02    |proj g|=  9.97026D-05\n",
      "\n",
      "At iterate  762    f=  3.68564D-02    |proj g|=  9.66849D-05\n",
      "\n",
      "At iterate  763    f=  3.68423D-02    |proj g|=  1.27165D-04\n",
      "\n",
      "At iterate  764    f=  3.68407D-02    |proj g|=  1.85877D-04\n",
      "\n",
      "At iterate  765    f=  3.68316D-02    |proj g|=  4.33301D-05\n",
      "\n",
      "At iterate  766    f=  3.68292D-02    |proj g|=  2.88460D-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate  767    f=  3.68214D-02    |proj g|=  1.36990D-04\n",
      "\n",
      "At iterate  768    f=  3.68141D-02    |proj g|=  1.24523D-04\n",
      "\n",
      "At iterate  769    f=  3.68067D-02    |proj g|=  1.01213D-04\n",
      "\n",
      "At iterate  770    f=  3.67969D-02    |proj g|=  1.87499D-04\n",
      "\n",
      "At iterate  771    f=  3.67958D-02    |proj g|=  8.41338D-05\n",
      "\n",
      "At iterate  772    f=  3.67937D-02    |proj g|=  1.03590D-04\n",
      "\n",
      "At iterate  773    f=  3.67813D-02    |proj g|=  1.81572D-04\n",
      "\n",
      "At iterate  774    f=  3.67626D-02    |proj g|=  1.83967D-04\n",
      "\n",
      "At iterate  775    f=  3.67489D-02    |proj g|=  9.33185D-05\n",
      "\n",
      "At iterate  776    f=  3.67366D-02    |proj g|=  9.00417D-05\n",
      "\n",
      "At iterate  777    f=  3.67086D-02    |proj g|=  1.08435D-04\n",
      "\n",
      "At iterate  778    f=  3.66692D-02    |proj g|=  8.67018D-05\n",
      "\n",
      "At iterate  779    f=  3.66652D-02    |proj g|=  1.86642D-04\n",
      "\n",
      "At iterate  780    f=  3.66568D-02    |proj g|=  1.71416D-04\n",
      "\n",
      "At iterate  781    f=  3.66484D-02    |proj g|=  1.56322D-04\n",
      "\n",
      "At iterate  782    f=  3.66361D-02    |proj g|=  1.28578D-04\n",
      "\n",
      "At iterate  783    f=  3.66350D-02    |proj g|=  1.83925D-04\n",
      "\n",
      "At iterate  784    f=  3.66289D-02    |proj g|=  1.65102D-04\n",
      "\n",
      "At iterate  785    f=  3.66165D-02    |proj g|=  1.55965D-04\n",
      "\n",
      "At iterate  786    f=  3.66066D-02    |proj g|=  1.32855D-04\n",
      "\n",
      "At iterate  787    f=  3.65964D-02    |proj g|=  5.37333D-05\n",
      "\n",
      "At iterate  788    f=  3.65806D-02    |proj g|=  7.55495D-05\n",
      "\n",
      "At iterate  789    f=  3.65630D-02    |proj g|=  1.22350D-04\n",
      "\n",
      "At iterate  790    f=  3.65393D-02    |proj g|=  1.44448D-04\n",
      "\n",
      "At iterate  791    f=  3.65372D-02    |proj g|=  2.58052D-04\n",
      "\n",
      "At iterate  792    f=  3.65046D-02    |proj g|=  1.51802D-04\n",
      "\n",
      "At iterate  793    f=  3.64907D-02    |proj g|=  9.69661D-05\n",
      "\n",
      "At iterate  794    f=  3.64890D-02    |proj g|=  1.65278D-04\n",
      "\n",
      "At iterate  795    f=  3.64858D-02    |proj g|=  1.20158D-04\n",
      "\n",
      "At iterate  796    f=  3.64831D-02    |proj g|=  1.08097D-04\n",
      "\n",
      "At iterate  797    f=  3.64723D-02    |proj g|=  1.85062D-04\n",
      "\n",
      "At iterate  798    f=  3.64612D-02    |proj g|=  2.08490D-04\n",
      "\n",
      "At iterate  799    f=  3.64372D-02    |proj g|=  1.64064D-04\n",
      "\n",
      "At iterate  800    f=  3.64288D-02    |proj g|=  2.24865D-04\n",
      "\n",
      "At iterate  801    f=  3.63998D-02    |proj g|=  1.05717D-04\n",
      "\n",
      "At iterate  802    f=  3.63928D-02    |proj g|=  3.91412D-04\n",
      "\n",
      "At iterate  803    f=  3.63738D-02    |proj g|=  2.23122D-04\n",
      "\n",
      "At iterate  804    f=  3.63564D-02    |proj g|=  1.25059D-04\n",
      "\n",
      "At iterate  805    f=  3.63396D-02    |proj g|=  2.06619D-04\n",
      "\n",
      "At iterate  806    f=  3.63335D-02    |proj g|=  4.46898D-04\n",
      "\n",
      "At iterate  807    f=  3.63141D-02    |proj g|=  2.02370D-04\n",
      "\n",
      "At iterate  808    f=  3.63029D-02    |proj g|=  1.21866D-04\n",
      "\n",
      "At iterate  809    f=  3.62989D-02    |proj g|=  1.02894D-04\n",
      "\n",
      "At iterate  810    f=  3.62870D-02    |proj g|=  1.33360D-04\n",
      "\n",
      "At iterate  811    f=  3.62665D-02    |proj g|=  1.46322D-04\n",
      "\n",
      "At iterate  812    f=  3.62520D-02    |proj g|=  1.31867D-04\n",
      "\n",
      "At iterate  813    f=  3.62494D-02    |proj g|=  1.64699D-04\n",
      "\n",
      "At iterate  814    f=  3.62416D-02    |proj g|=  1.18619D-04\n",
      "\n",
      "At iterate  815    f=  3.62310D-02    |proj g|=  1.88154D-04\n",
      "\n",
      "At iterate  816    f=  3.62216D-02    |proj g|=  1.26648D-04\n",
      "\n",
      "At iterate  817    f=  3.62065D-02    |proj g|=  1.95371D-04\n",
      "\n",
      "At iterate  818    f=  3.62022D-02    |proj g|=  1.81118D-04\n",
      "\n",
      "At iterate  819    f=  3.61953D-02    |proj g|=  1.65751D-04\n",
      "\n",
      "At iterate  820    f=  3.61712D-02    |proj g|=  1.36672D-04\n",
      "\n",
      "At iterate  821    f=  3.61530D-02    |proj g|=  6.28072D-05\n",
      "\n",
      "At iterate  822    f=  3.61386D-02    |proj g|=  5.29929D-05\n",
      "\n",
      "At iterate  823    f=  3.61216D-02    |proj g|=  5.79841D-05\n",
      "\n",
      "At iterate  824    f=  3.61195D-02    |proj g|=  1.49758D-04\n",
      "\n",
      "At iterate  825    f=  3.60603D-02    |proj g|=  1.46173D-04\n",
      "\n",
      "At iterate  826    f=  3.60364D-02    |proj g|=  1.85093D-04\n",
      "\n",
      "At iterate  827    f=  3.60010D-02    |proj g|=  1.58780D-04\n",
      "\n",
      "At iterate  828    f=  3.59858D-02    |proj g|=  1.60719D-04\n",
      "\n",
      "At iterate  829    f=  3.59723D-02    |proj g|=  2.86921D-04\n",
      "\n",
      "At iterate  830    f=  3.59627D-02    |proj g|=  1.05325D-04\n",
      "\n",
      "At iterate  831    f=  3.59567D-02    |proj g|=  7.75877D-05\n",
      "\n",
      "At iterate  832    f=  3.59407D-02    |proj g|=  5.53613D-05\n",
      "\n",
      "At iterate  833    f=  3.59174D-02    |proj g|=  1.48620D-04\n",
      "\n",
      "At iterate  834    f=  3.58978D-02    |proj g|=  1.30145D-04\n",
      "\n",
      "At iterate  835    f=  3.58836D-02    |proj g|=  3.37132D-04\n",
      "\n",
      "At iterate  836    f=  3.58540D-02    |proj g|=  9.58882D-05\n",
      "\n",
      "At iterate  837    f=  3.58469D-02    |proj g|=  7.14005D-05\n",
      "\n",
      "At iterate  838    f=  3.58053D-02    |proj g|=  7.45907D-05\n",
      "\n",
      "At iterate  839    f=  3.57839D-02    |proj g|=  2.35609D-04\n",
      "\n",
      "At iterate  840    f=  3.57749D-02    |proj g|=  3.01431D-04\n",
      "\n",
      "At iterate  841    f=  3.57648D-02    |proj g|=  1.23371D-04\n",
      "\n",
      "At iterate  842    f=  3.57601D-02    |proj g|=  1.29759D-04\n",
      "\n",
      "At iterate  843    f=  3.57361D-02    |proj g|=  9.89130D-05\n",
      "\n",
      "At iterate  844    f=  3.57209D-02    |proj g|=  1.36454D-04\n",
      "\n",
      "At iterate  845    f=  3.57026D-02    |proj g|=  5.90534D-05\n",
      "\n",
      "At iterate  846    f=  3.57005D-02    |proj g|=  1.61869D-04\n",
      "\n",
      "At iterate  847    f=  3.56937D-02    |proj g|=  6.77157D-05\n",
      "\n",
      "At iterate  848    f=  3.56902D-02    |proj g|=  7.15965D-05\n",
      "\n",
      "At iterate  849    f=  3.56868D-02    |proj g|=  7.49970D-05\n",
      "\n",
      "At iterate  850    f=  3.56743D-02    |proj g|=  7.59356D-05\n",
      "\n",
      "At iterate  851    f=  3.56675D-02    |proj g|=  2.77535D-04\n",
      "\n",
      "At iterate  852    f=  3.56554D-02    |proj g|=  2.03181D-04\n",
      "\n",
      "At iterate  853    f=  3.56492D-02    |proj g|=  1.79096D-04\n",
      "\n",
      "At iterate  854    f=  3.56400D-02    |proj g|=  1.45786D-04\n",
      "\n",
      "At iterate  855    f=  3.56321D-02    |proj g|=  1.06462D-04\n",
      "\n",
      "At iterate  856    f=  3.56257D-02    |proj g|=  1.50837D-04\n",
      "\n",
      "At iterate  857    f=  3.56236D-02    |proj g|=  1.39519D-04\n",
      "\n",
      "At iterate  858    f=  3.56209D-02    |proj g|=  1.10784D-04\n",
      "\n",
      "At iterate  859    f=  3.56102D-02    |proj g|=  7.37795D-05\n",
      "\n",
      "At iterate  860    f=  3.56027D-02    |proj g|=  1.68851D-04\n",
      "\n",
      "At iterate  861    f=  3.55963D-02    |proj g|=  8.18815D-05\n",
      "\n",
      "At iterate  862    f=  3.55880D-02    |proj g|=  1.15089D-04\n",
      "\n",
      "At iterate  863    f=  3.55813D-02    |proj g|=  1.16695D-04\n",
      "\n",
      "At iterate  864    f=  3.55760D-02    |proj g|=  2.93293D-04\n",
      "\n",
      "At iterate  865    f=  3.55673D-02    |proj g|=  1.88936D-04\n",
      "\n",
      "At iterate  866    f=  3.55590D-02    |proj g|=  1.10492D-04\n",
      "\n",
      "At iterate  867    f=  3.55568D-02    |proj g|=  9.73876D-05\n",
      "\n",
      "At iterate  868    f=  3.55536D-02    |proj g|=  1.94129D-04\n",
      "\n",
      "At iterate  869    f=  3.55476D-02    |proj g|=  1.30791D-04\n",
      "\n",
      "At iterate  870    f=  3.55368D-02    |proj g|=  8.92981D-05\n",
      "\n",
      "At iterate  871    f=  3.55297D-02    |proj g|=  1.06451D-04\n",
      "\n",
      "At iterate  872    f=  3.55247D-02    |proj g|=  1.21814D-04\n",
      "\n",
      "At iterate  873    f=  3.55120D-02    |proj g|=  1.14002D-04\n",
      "\n",
      "At iterate  874    f=  3.54857D-02    |proj g|=  1.59228D-04\n",
      "\n",
      "At iterate  875    f=  3.54838D-02    |proj g|=  2.81941D-04\n",
      "\n",
      "At iterate  876    f=  3.54579D-02    |proj g|=  1.98833D-04\n",
      "\n",
      "At iterate  877    f=  3.54346D-02    |proj g|=  1.19284D-04\n",
      "\n",
      "At iterate  878    f=  3.54241D-02    |proj g|=  1.22323D-04\n",
      "\n",
      "At iterate  879    f=  3.54219D-02    |proj g|=  1.86381D-04\n",
      "\n",
      "At iterate  880    f=  3.54184D-02    |proj g|=  1.25479D-04\n",
      "\n",
      "At iterate  881    f=  3.54136D-02    |proj g|=  1.27776D-04\n",
      "\n",
      "At iterate  882    f=  3.54091D-02    |proj g|=  1.57045D-04\n",
      "\n",
      "At iterate  883    f=  3.54009D-02    |proj g|=  1.30764D-04\n",
      "\n",
      "At iterate  884    f=  3.53956D-02    |proj g|=  1.56925D-04\n",
      "\n",
      "At iterate  885    f=  3.53863D-02    |proj g|=  2.81854D-04\n",
      "\n",
      "At iterate  886    f=  3.53683D-02    |proj g|=  1.18088D-04\n",
      "\n",
      "At iterate  887    f=  3.53600D-02    |proj g|=  8.14744D-05\n",
      "\n",
      "At iterate  888    f=  3.53398D-02    |proj g|=  1.30941D-04\n",
      "\n",
      "At iterate  889    f=  3.53380D-02    |proj g|=  2.26787D-04\n",
      "\n",
      "At iterate  890    f=  3.53312D-02    |proj g|=  2.39455D-04\n",
      "\n",
      "At iterate  891    f=  3.53281D-02    |proj g|=  1.06969D-04\n",
      "\n",
      "At iterate  892    f=  3.53259D-02    |proj g|=  6.53288D-05\n",
      "\n",
      "At iterate  893    f=  3.53242D-02    |proj g|=  8.54681D-05\n",
      "\n",
      "At iterate  894    f=  3.53192D-02    |proj g|=  8.63438D-05\n",
      "\n",
      "At iterate  895    f=  3.53136D-02    |proj g|=  1.39068D-04\n",
      "\n",
      "At iterate  896    f=  3.53057D-02    |proj g|=  2.39666D-04\n",
      "\n",
      "At iterate  897    f=  3.52821D-02    |proj g|=  1.60235D-04\n",
      "\n",
      "At iterate  898    f=  3.52738D-02    |proj g|=  8.50662D-05\n",
      "\n",
      "At iterate  899    f=  3.52691D-02    |proj g|=  7.56175D-05\n",
      "\n",
      "At iterate  900    f=  3.52624D-02    |proj g|=  1.30340D-04\n",
      "\n",
      "At iterate  901    f=  3.52598D-02    |proj g|=  2.87569D-04\n",
      "\n",
      "At iterate  902    f=  3.52542D-02    |proj g|=  9.49978D-05\n",
      "\n",
      "At iterate  903    f=  3.52500D-02    |proj g|=  6.20894D-05\n",
      "\n",
      "At iterate  904    f=  3.52453D-02    |proj g|=  1.44401D-04\n",
      "\n",
      "At iterate  905    f=  3.52421D-02    |proj g|=  1.26141D-04\n",
      "\n",
      "At iterate  906    f=  3.52414D-02    |proj g|=  1.21978D-04\n",
      "\n",
      "At iterate  907    f=  3.52285D-02    |proj g|=  9.34563D-05\n",
      "\n",
      "At iterate  908    f=  3.52182D-02    |proj g|=  6.55123D-05\n",
      "\n",
      "At iterate  909    f=  3.52140D-02    |proj g|=  3.89539D-05\n",
      "\n",
      "At iterate  910    f=  3.52106D-02    |proj g|=  5.36295D-05\n",
      "\n",
      "At iterate  911    f=  3.51962D-02    |proj g|=  1.30840D-04\n",
      "\n",
      "At iterate  912    f=  3.51899D-02    |proj g|=  2.31371D-04\n",
      "\n",
      "At iterate  913    f=  3.51782D-02    |proj g|=  1.38924D-04\n",
      "\n",
      "At iterate  914    f=  3.51728D-02    |proj g|=  1.44879D-04\n",
      "\n",
      "At iterate  915    f=  3.51709D-02    |proj g|=  4.99940D-05\n",
      "\n",
      "At iterate  916    f=  3.51658D-02    |proj g|=  9.27610D-05\n",
      "\n",
      "At iterate  917    f=  3.51576D-02    |proj g|=  1.22756D-04\n",
      "\n",
      "At iterate  918    f=  3.51438D-02    |proj g|=  7.79236D-05\n",
      "\n",
      "At iterate  919    f=  3.51359D-02    |proj g|=  1.18675D-04\n",
      "\n",
      "At iterate  920    f=  3.51270D-02    |proj g|=  7.03661D-05\n",
      "\n",
      "At iterate  921    f=  3.51216D-02    |proj g|=  1.06369D-04\n",
      "\n",
      "At iterate  922    f=  3.51145D-02    |proj g|=  1.12251D-04\n",
      "\n",
      "At iterate  923    f=  3.51014D-02    |proj g|=  1.86711D-04\n",
      "\n",
      "At iterate  924    f=  3.50835D-02    |proj g|=  1.32007D-04\n",
      "\n",
      "At iterate  925    f=  3.50705D-02    |proj g|=  2.15394D-04\n",
      "\n",
      "At iterate  926    f=  3.50639D-02    |proj g|=  1.02436D-04\n",
      "\n",
      "At iterate  927    f=  3.50604D-02    |proj g|=  6.44326D-05\n",
      "\n",
      "At iterate  928    f=  3.50512D-02    |proj g|=  6.11878D-05\n",
      "\n",
      "At iterate  929    f=  3.50368D-02    |proj g|=  6.52075D-05\n",
      "\n",
      "At iterate  930    f=  3.50287D-02    |proj g|=  2.91065D-04\n",
      "\n",
      "At iterate  931    f=  3.50141D-02    |proj g|=  8.71955D-05\n",
      "\n",
      "At iterate  932    f=  3.50105D-02    |proj g|=  8.81341D-05\n",
      "\n",
      "At iterate  933    f=  3.50063D-02    |proj g|=  6.64693D-05\n",
      "\n",
      "At iterate  934    f=  3.50045D-02    |proj g|=  1.27379D-04\n",
      "\n",
      "At iterate  935    f=  3.49952D-02    |proj g|=  8.67730D-05\n",
      "\n",
      "At iterate  936    f=  3.49903D-02    |proj g|=  1.43825D-04\n",
      "\n",
      "At iterate  937    f=  3.49827D-02    |proj g|=  7.84822D-05\n",
      "\n",
      "At iterate  938    f=  3.49792D-02    |proj g|=  8.19281D-05\n",
      "\n",
      "At iterate  939    f=  3.49702D-02    |proj g|=  3.02729D-04\n",
      "\n",
      "At iterate  940    f=  3.49603D-02    |proj g|=  2.27947D-04\n",
      "\n",
      "At iterate  941    f=  3.49398D-02    |proj g|=  4.07459D-05\n",
      "\n",
      "At iterate  942    f=  3.49380D-02    |proj g|=  4.11584D-05\n",
      "\n",
      "At iterate  943    f=  3.49365D-02    |proj g|=  1.19156D-04\n",
      "\n",
      "At iterate  944    f=  3.49324D-02    |proj g|=  7.65511D-05\n",
      "\n",
      "At iterate  945    f=  3.49182D-02    |proj g|=  6.42340D-05\n",
      "\n",
      "At iterate  946    f=  3.48965D-02    |proj g|=  2.31532D-04\n",
      "\n",
      "At iterate  947    f=  3.48910D-02    |proj g|=  2.52060D-04\n",
      "\n",
      "At iterate  948    f=  3.48797D-02    |proj g|=  2.31201D-04\n",
      "\n",
      "At iterate  949    f=  3.48651D-02    |proj g|=  1.50538D-04\n",
      "\n",
      "At iterate  950    f=  3.48585D-02    |proj g|=  2.23874D-04\n",
      "\n",
      "At iterate  951    f=  3.48517D-02    |proj g|=  7.27498D-05\n",
      "\n",
      "At iterate  952    f=  3.48472D-02    |proj g|=  8.11386D-05\n",
      "\n",
      "At iterate  953    f=  3.48391D-02    |proj g|=  1.58371D-04\n",
      "\n",
      "At iterate  954    f=  3.48317D-02    |proj g|=  1.43945D-04\n",
      "\n",
      "At iterate  955    f=  3.48282D-02    |proj g|=  1.93688D-04\n",
      "\n",
      "At iterate  956    f=  3.48132D-02    |proj g|=  1.57525D-04\n",
      "\n",
      "At iterate  957    f=  3.47854D-02    |proj g|=  2.25374D-04\n",
      "\n",
      "At iterate  958    f=  3.47687D-02    |proj g|=  1.54919D-04\n",
      "\n",
      "At iterate  959    f=  3.47666D-02    |proj g|=  2.85726D-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate  960    f=  3.47575D-02    |proj g|=  2.01202D-04\n",
      "\n",
      "At iterate  961    f=  3.47519D-02    |proj g|=  1.27182D-04\n",
      "\n",
      "At iterate  962    f=  3.47466D-02    |proj g|=  1.74041D-04\n",
      "\n",
      "At iterate  963    f=  3.47399D-02    |proj g|=  2.42913D-04\n",
      "\n",
      "At iterate  964    f=  3.47276D-02    |proj g|=  2.32461D-04\n",
      "\n",
      "At iterate  965    f=  3.47211D-02    |proj g|=  4.09703D-04\n",
      "\n",
      "At iterate  966    f=  3.46998D-02    |proj g|=  3.29960D-04\n",
      "\n",
      "At iterate  967    f=  3.46781D-02    |proj g|=  2.41516D-04\n",
      "\n",
      "At iterate  968    f=  3.46617D-02    |proj g|=  2.23431D-04\n",
      "\n",
      "At iterate  969    f=  3.46479D-02    |proj g|=  1.93294D-04\n",
      "\n",
      "At iterate  970    f=  3.46300D-02    |proj g|=  2.17722D-04\n",
      "\n",
      "At iterate  971    f=  3.46160D-02    |proj g|=  1.43307D-04\n",
      "\n",
      "At iterate  972    f=  3.46075D-02    |proj g|=  1.39166D-04\n",
      "\n",
      "At iterate  973    f=  3.45984D-02    |proj g|=  2.25831D-04\n",
      "\n",
      "At iterate  974    f=  3.45845D-02    |proj g|=  1.40999D-04\n",
      "\n",
      "At iterate  975    f=  3.45728D-02    |proj g|=  1.26011D-04\n",
      "\n",
      "At iterate  976    f=  3.45679D-02    |proj g|=  1.99838D-04\n",
      "\n",
      "At iterate  977    f=  3.45592D-02    |proj g|=  4.04552D-04\n",
      "\n",
      "At iterate  978    f=  3.45478D-02    |proj g|=  1.45761D-04\n",
      "\n",
      "At iterate  979    f=  3.45406D-02    |proj g|=  8.28805D-05\n",
      "\n",
      "At iterate  980    f=  3.45208D-02    |proj g|=  1.86043D-04\n",
      "\n",
      "At iterate  981    f=  3.45053D-02    |proj g|=  3.01086D-04\n",
      "\n",
      "At iterate  982    f=  3.44997D-02    |proj g|=  1.69937D-04\n",
      "\n",
      "At iterate  983    f=  3.44910D-02    |proj g|=  1.48956D-04\n",
      "\n",
      "At iterate  984    f=  3.44839D-02    |proj g|=  1.19388D-04\n",
      "\n",
      "At iterate  985    f=  3.44729D-02    |proj g|=  4.53088D-04\n",
      "\n",
      "At iterate  986    f=  3.44595D-02    |proj g|=  3.62547D-04\n",
      "\n",
      "At iterate  987    f=  3.44273D-02    |proj g|=  1.27121D-04\n",
      "\n",
      "At iterate  988    f=  3.44176D-02    |proj g|=  1.53743D-04\n",
      "\n",
      "At iterate  989    f=  3.44091D-02    |proj g|=  1.12902D-04\n",
      "\n",
      "At iterate  990    f=  3.44037D-02    |proj g|=  9.90430D-05\n",
      "\n",
      "At iterate  991    f=  3.43996D-02    |proj g|=  2.01675D-04\n",
      "\n",
      "At iterate  992    f=  3.43905D-02    |proj g|=  1.32077D-04\n",
      "\n",
      "At iterate  993    f=  3.43815D-02    |proj g|=  1.15325D-04\n",
      "\n",
      "At iterate  994    f=  3.43685D-02    |proj g|=  1.39119D-04\n",
      "\n",
      "At iterate  995    f=  3.43617D-02    |proj g|=  2.79067D-04\n",
      "\n",
      "At iterate  996    f=  3.43424D-02    |proj g|=  1.50674D-04\n",
      "\n",
      "At iterate  997    f=  3.43231D-02    |proj g|=  1.91571D-04\n",
      "\n",
      "At iterate  998    f=  3.43145D-02    |proj g|=  2.29620D-04\n",
      "\n",
      "At iterate  999    f=  3.43074D-02    |proj g|=  1.89625D-04\n",
      "\n",
      "At iterate 1000    f=  3.42951D-02    |proj g|=  9.07935D-05\n",
      "\n",
      "At iterate 1001    f=  3.42898D-02    |proj g|=  8.54811D-05\n",
      "\n",
      "At iterate 1002    f=  3.42737D-02    |proj g|=  8.63306D-05\n",
      "\n",
      "At iterate 1003    f=  3.42661D-02    |proj g|=  1.83565D-04\n",
      "\n",
      "At iterate 1004    f=  3.42406D-02    |proj g|=  1.17436D-04\n",
      "\n",
      "At iterate 1005    f=  3.42309D-02    |proj g|=  5.77943D-05\n",
      "\n",
      "At iterate 1006    f=  3.42256D-02    |proj g|=  2.10297D-04\n",
      "\n",
      "At iterate 1007    f=  3.42091D-02    |proj g|=  1.18687D-04\n",
      "\n",
      "At iterate 1008    f=  3.41990D-02    |proj g|=  1.18676D-04\n",
      "\n",
      "At iterate 1009    f=  3.41814D-02    |proj g|=  2.41782D-04\n",
      "\n",
      "At iterate 1010    f=  3.41767D-02    |proj g|=  1.09958D-04\n",
      "\n",
      "At iterate 1011    f=  3.41712D-02    |proj g|=  1.34197D-04\n",
      "\n",
      "At iterate 1012    f=  3.41609D-02    |proj g|=  1.16527D-04\n",
      "\n",
      "At iterate 1013    f=  3.41390D-02    |proj g|=  2.62371D-04\n",
      "\n",
      "At iterate 1014    f=  3.41213D-02    |proj g|=  1.26997D-04\n",
      "\n",
      "At iterate 1015    f=  3.41051D-02    |proj g|=  1.25707D-04\n",
      "\n",
      "At iterate 1016    f=  3.40952D-02    |proj g|=  9.51442D-05\n",
      "\n",
      "At iterate 1017    f=  3.40732D-02    |proj g|=  1.95331D-04\n",
      "\n",
      "At iterate 1018    f=  3.40645D-02    |proj g|=  2.17833D-04\n",
      "\n",
      "At iterate 1019    f=  3.40591D-02    |proj g|=  1.65391D-04\n",
      "\n",
      "At iterate 1020    f=  3.40447D-02    |proj g|=  1.51682D-04\n",
      "\n",
      "At iterate 1021    f=  3.40366D-02    |proj g|=  3.68717D-04\n",
      "\n",
      "At iterate 1022    f=  3.40202D-02    |proj g|=  1.05239D-04\n",
      "\n",
      "At iterate 1023    f=  3.40160D-02    |proj g|=  5.66713D-05\n",
      "\n",
      "At iterate 1024    f=  3.40092D-02    |proj g|=  7.12300D-05\n",
      "\n",
      "At iterate 1025    f=  3.40029D-02    |proj g|=  1.20260D-04\n",
      "\n",
      "At iterate 1026    f=  3.39906D-02    |proj g|=  1.16865D-04\n",
      "\n",
      "At iterate 1027    f=  3.39659D-02    |proj g|=  1.37987D-04\n",
      "\n",
      "At iterate 1028    f=  3.39500D-02    |proj g|=  1.16857D-04\n",
      "\n",
      "At iterate 1029    f=  3.39473D-02    |proj g|=  1.77275D-04\n",
      "\n",
      "At iterate 1030    f=  3.39374D-02    |proj g|=  1.69103D-04\n",
      "\n",
      "At iterate 1031    f=  3.39183D-02    |proj g|=  1.16236D-04\n",
      "\n",
      "At iterate 1032    f=  3.39063D-02    |proj g|=  2.49447D-04\n",
      "\n",
      "At iterate 1033    f=  3.39042D-02    |proj g|=  1.89096D-04\n",
      "\n",
      "At iterate 1034    f=  3.39019D-02    |proj g|=  1.61527D-04\n",
      "\n",
      "At iterate 1035    f=  3.38776D-02    |proj g|=  6.98304D-05\n",
      "\n",
      "At iterate 1036    f=  3.38495D-02    |proj g|=  2.54794D-04\n",
      "\n",
      "At iterate 1037    f=  3.38385D-02    |proj g|=  1.26991D-04\n",
      "\n",
      "At iterate 1038    f=  3.38283D-02    |proj g|=  1.23866D-04\n",
      "\n",
      "At iterate 1039    f=  3.38209D-02    |proj g|=  1.09571D-04\n",
      "\n",
      "At iterate 1040    f=  3.38133D-02    |proj g|=  2.42333D-04\n",
      "\n",
      "At iterate 1041    f=  3.37898D-02    |proj g|=  1.12306D-04\n",
      "\n",
      "At iterate 1042    f=  3.37727D-02    |proj g|=  8.47923D-05\n",
      "\n",
      "At iterate 1043    f=  3.37489D-02    |proj g|=  1.16588D-04\n",
      "\n",
      "At iterate 1044    f=  3.37465D-02    |proj g|=  1.20020D-04\n",
      "\n",
      "At iterate 1045    f=  3.37394D-02    |proj g|=  8.09073D-05\n",
      "\n",
      "At iterate 1046    f=  3.37283D-02    |proj g|=  7.23590D-05\n",
      "\n",
      "At iterate 1047    f=  3.37057D-02    |proj g|=  6.14931D-05\n",
      "\n",
      "At iterate 1048    f=  3.36823D-02    |proj g|=  1.03824D-04\n",
      "\n",
      "At iterate 1049    f=  3.36339D-02    |proj g|=  2.00227D-04\n",
      "\n",
      "At iterate 1050    f=  3.35984D-02    |proj g|=  1.61952D-04\n",
      "\n",
      "At iterate 1051    f=  3.35799D-02    |proj g|=  4.77700D-04\n",
      "\n",
      "At iterate 1052    f=  3.35508D-02    |proj g|=  2.44550D-04\n",
      "\n",
      "At iterate 1053    f=  3.35386D-02    |proj g|=  1.31932D-04\n",
      "\n",
      "At iterate 1054    f=  3.35300D-02    |proj g|=  1.00983D-04\n",
      "\n",
      "At iterate 1055    f=  3.35178D-02    |proj g|=  3.73450D-04\n",
      "\n",
      "At iterate 1056    f=  3.35116D-02    |proj g|=  3.88514D-04\n",
      "\n",
      "At iterate 1057    f=  3.34940D-02    |proj g|=  1.40913D-04\n",
      "\n",
      "At iterate 1058    f=  3.34899D-02    |proj g|=  1.13656D-04\n",
      "\n",
      "At iterate 1059    f=  3.34747D-02    |proj g|=  1.89035D-04\n",
      "\n",
      "At iterate 1060    f=  3.34634D-02    |proj g|=  2.63251D-04\n",
      "\n",
      "At iterate 1061    f=  3.34316D-02    |proj g|=  3.00641D-04\n",
      "\n",
      "At iterate 1062    f=  3.34286D-02    |proj g|=  3.79169D-04\n",
      "\n",
      "At iterate 1063    f=  3.33973D-02    |proj g|=  2.15841D-04\n",
      "\n",
      "At iterate 1064    f=  3.33798D-02    |proj g|=  1.16324D-04\n",
      "\n",
      "At iterate 1065    f=  3.33667D-02    |proj g|=  2.05309D-04\n",
      "\n",
      "At iterate 1066    f=  3.33498D-02    |proj g|=  2.35121D-04\n",
      "\n",
      "At iterate 1067    f=  3.33377D-02    |proj g|=  4.34622D-04\n",
      "\n",
      "At iterate 1068    f=  3.32989D-02    |proj g|=  3.79834D-04\n",
      "\n",
      "At iterate 1069    f=  3.32486D-02    |proj g|=  3.66144D-04\n",
      "\n",
      "At iterate 1070    f=  3.32385D-02    |proj g|=  2.28414D-04\n",
      "\n",
      "At iterate 1071    f=  3.32192D-02    |proj g|=  1.73296D-04\n",
      "\n",
      "At iterate 1072    f=  3.31991D-02    |proj g|=  1.72612D-04\n",
      "\n",
      "At iterate 1073    f=  3.31342D-02    |proj g|=  2.44313D-04\n",
      "\n",
      "At iterate 1074    f=  3.31009D-02    |proj g|=  4.33198D-04\n",
      "\n",
      "At iterate 1075    f=  3.30542D-02    |proj g|=  4.16642D-04\n",
      "\n",
      "At iterate 1076    f=  3.30216D-02    |proj g|=  2.47890D-04\n",
      "\n",
      "At iterate 1077    f=  3.29985D-02    |proj g|=  1.51510D-04\n",
      "\n",
      "At iterate 1078    f=  3.29844D-02    |proj g|=  2.94182D-04\n",
      "\n",
      "At iterate 1079    f=  3.29503D-02    |proj g|=  3.81920D-04\n",
      "\n",
      "At iterate 1080    f=  3.29148D-02    |proj g|=  2.90739D-04\n",
      "\n",
      "At iterate 1081    f=  3.29005D-02    |proj g|=  2.85174D-04\n",
      "\n",
      "At iterate 1082    f=  3.28710D-02    |proj g|=  1.09537D-04\n",
      "\n",
      "At iterate 1083    f=  3.28606D-02    |proj g|=  1.06060D-04\n",
      "\n",
      "At iterate 1084    f=  3.28037D-02    |proj g|=  9.20562D-05\n",
      "\n",
      "At iterate 1085    f=  3.27594D-02    |proj g|=  5.59966D-04\n",
      "\n",
      "At iterate 1086    f=  3.27276D-02    |proj g|=  4.65267D-04\n",
      "\n",
      "At iterate 1087    f=  3.26990D-02    |proj g|=  1.26930D-04\n",
      "\n",
      "At iterate 1088    f=  3.26898D-02    |proj g|=  1.48704D-04\n",
      "\n",
      "At iterate 1089    f=  3.26848D-02    |proj g|=  1.56716D-04\n",
      "\n",
      "At iterate 1090    f=  3.26806D-02    |proj g|=  1.46357D-04\n",
      "\n",
      "At iterate 1091    f=  3.26669D-02    |proj g|=  8.23329D-05\n",
      "\n",
      "At iterate 1092    f=  3.26583D-02    |proj g|=  1.78488D-04\n",
      "\n",
      "At iterate 1093    f=  3.26393D-02    |proj g|=  2.16742D-04\n",
      "\n",
      "At iterate 1094    f=  3.26253D-02    |proj g|=  2.26402D-04\n",
      "\n",
      "At iterate 1095    f=  3.25990D-02    |proj g|=  1.51576D-04\n",
      "\n",
      "At iterate 1096    f=  3.25788D-02    |proj g|=  2.27144D-04\n",
      "\n",
      "At iterate 1097    f=  3.25669D-02    |proj g|=  4.13214D-04\n",
      "\n",
      "At iterate 1098    f=  3.25427D-02    |proj g|=  3.09728D-04\n",
      "\n",
      "At iterate 1099    f=  3.25112D-02    |proj g|=  1.43569D-04\n",
      "\n",
      "At iterate 1100    f=  3.25032D-02    |proj g|=  8.81895D-05\n",
      "\n",
      "At iterate 1101    f=  3.24959D-02    |proj g|=  1.82080D-04\n",
      "\n",
      "At iterate 1102    f=  3.24800D-02    |proj g|=  1.31397D-04\n",
      "\n",
      "At iterate 1103    f=  3.24536D-02    |proj g|=  1.43631D-04\n",
      "\n",
      "At iterate 1104    f=  3.24250D-02    |proj g|=  1.78781D-04\n",
      "\n",
      "At iterate 1105    f=  3.24015D-02    |proj g|=  1.88035D-04\n",
      "\n",
      "At iterate 1106    f=  3.23656D-02    |proj g|=  2.16675D-04\n",
      "\n",
      "At iterate 1107    f=  3.23301D-02    |proj g|=  1.98832D-04\n",
      "\n",
      "At iterate 1108    f=  3.23222D-02    |proj g|=  1.66540D-04\n",
      "\n",
      "At iterate 1109    f=  3.23131D-02    |proj g|=  1.54127D-04\n",
      "\n",
      "At iterate 1110    f=  3.22956D-02    |proj g|=  9.86940D-05\n",
      "\n",
      "At iterate 1111    f=  3.22688D-02    |proj g|=  1.18623D-04\n",
      "\n",
      "At iterate 1112    f=  3.22286D-02    |proj g|=  4.59713D-04\n",
      "\n",
      "At iterate 1113    f=  3.21869D-02    |proj g|=  2.69190D-04\n",
      "\n",
      "At iterate 1114    f=  3.21638D-02    |proj g|=  1.82212D-04\n",
      "\n",
      "At iterate 1115    f=  3.21541D-02    |proj g|=  1.51069D-04\n",
      "\n",
      "At iterate 1116    f=  3.21293D-02    |proj g|=  1.50832D-04\n",
      "\n",
      "At iterate 1117    f=  3.21128D-02    |proj g|=  1.63341D-04\n",
      "\n",
      "At iterate 1118    f=  3.20979D-02    |proj g|=  2.07851D-04\n",
      "\n",
      "At iterate 1119    f=  3.20804D-02    |proj g|=  3.98885D-04\n",
      "\n",
      "At iterate 1120    f=  3.20622D-02    |proj g|=  2.89987D-04\n",
      "\n",
      "At iterate 1121    f=  3.20500D-02    |proj g|=  1.85487D-04\n",
      "\n",
      "At iterate 1122    f=  3.20234D-02    |proj g|=  1.53770D-04\n",
      "\n",
      "At iterate 1123    f=  3.19947D-02    |proj g|=  2.02340D-04\n",
      "\n",
      "At iterate 1124    f=  3.19572D-02    |proj g|=  3.51684D-04\n",
      "\n",
      "At iterate 1125    f=  3.19322D-02    |proj g|=  1.57522D-04\n",
      "\n",
      "At iterate 1126    f=  3.19064D-02    |proj g|=  1.10077D-04\n",
      "\n",
      "At iterate 1127    f=  3.18682D-02    |proj g|=  2.37452D-04\n",
      "\n",
      "At iterate 1128    f=  3.18441D-02    |proj g|=  1.48918D-04\n",
      "\n",
      "At iterate 1129    f=  3.18299D-02    |proj g|=  1.65016D-04\n",
      "\n",
      "At iterate 1130    f=  3.18192D-02    |proj g|=  2.31230D-04\n",
      "\n",
      "At iterate 1131    f=  3.18099D-02    |proj g|=  1.24838D-04\n",
      "\n",
      "At iterate 1132    f=  3.18036D-02    |proj g|=  1.02821D-04\n",
      "\n",
      "At iterate 1133    f=  3.17774D-02    |proj g|=  3.31096D-04\n",
      "\n",
      "At iterate 1134    f=  3.17600D-02    |proj g|=  1.16556D-04\n",
      "\n",
      "At iterate 1135    f=  3.17550D-02    |proj g|=  4.88872D-05\n",
      "\n",
      "At iterate 1136    f=  3.17536D-02    |proj g|=  6.35349D-05\n",
      "\n",
      "At iterate 1137    f=  3.17453D-02    |proj g|=  6.25650D-05\n",
      "\n",
      "At iterate 1138    f=  3.17385D-02    |proj g|=  2.17345D-04\n",
      "\n",
      "At iterate 1139    f=  3.17266D-02    |proj g|=  1.38097D-04\n",
      "\n",
      "At iterate 1140    f=  3.17116D-02    |proj g|=  1.40636D-04\n",
      "\n",
      "At iterate 1141    f=  3.16998D-02    |proj g|=  1.58907D-04\n",
      "\n",
      "At iterate 1142    f=  3.16970D-02    |proj g|=  2.62104D-04\n",
      "\n",
      "At iterate 1143    f=  3.16864D-02    |proj g|=  1.80289D-04\n",
      "\n",
      "At iterate 1144    f=  3.16763D-02    |proj g|=  1.52218D-04\n",
      "\n",
      "At iterate 1145    f=  3.16703D-02    |proj g|=  1.71773D-04\n",
      "\n",
      "At iterate 1146    f=  3.16500D-02    |proj g|=  1.88966D-04\n",
      "\n",
      "At iterate 1147    f=  3.16354D-02    |proj g|=  1.48863D-04\n",
      "\n",
      "At iterate 1148    f=  3.16240D-02    |proj g|=  1.80141D-04\n",
      "\n",
      "At iterate 1149    f=  3.16174D-02    |proj g|=  1.65161D-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate 1150    f=  3.16049D-02    |proj g|=  1.84376D-04\n",
      "\n",
      "At iterate 1151    f=  3.15927D-02    |proj g|=  2.25373D-04\n",
      "\n",
      "At iterate 1152    f=  3.15700D-02    |proj g|=  2.35490D-04\n",
      "\n",
      "At iterate 1153    f=  3.15400D-02    |proj g|=  1.38967D-04\n",
      "\n",
      "At iterate 1154    f=  3.15262D-02    |proj g|=  4.17603D-04\n",
      "\n",
      "At iterate 1155    f=  3.15031D-02    |proj g|=  2.17615D-04\n",
      "\n",
      "At iterate 1156    f=  3.14985D-02    |proj g|=  1.23564D-04\n",
      "\n",
      "At iterate 1157    f=  3.14940D-02    |proj g|=  1.21901D-04\n",
      "\n",
      "At iterate 1158    f=  3.14908D-02    |proj g|=  1.29958D-04\n",
      "\n",
      "At iterate 1159    f=  3.14829D-02    |proj g|=  1.33220D-04\n",
      "\n",
      "At iterate 1160    f=  3.14789D-02    |proj g|=  2.44309D-04\n",
      "\n",
      "At iterate 1161    f=  3.14636D-02    |proj g|=  1.85570D-04\n",
      "\n",
      "At iterate 1162    f=  3.14445D-02    |proj g|=  8.10920D-05\n",
      "\n",
      "At iterate 1163    f=  3.14295D-02    |proj g|=  1.77816D-04\n",
      "\n",
      "At iterate 1164    f=  3.14201D-02    |proj g|=  2.25932D-04\n",
      "\n",
      "At iterate 1165    f=  3.14032D-02    |proj g|=  2.25065D-04\n",
      "\n",
      "At iterate 1166    f=  3.13972D-02    |proj g|=  4.48005D-04\n",
      "\n",
      "At iterate 1167    f=  3.13734D-02    |proj g|=  2.58360D-04\n",
      "\n",
      "At iterate 1168    f=  3.13603D-02    |proj g|=  1.26524D-04\n",
      "\n",
      "At iterate 1169    f=  3.13551D-02    |proj g|=  1.74942D-04\n",
      "\n",
      "At iterate 1170    f=  3.13499D-02    |proj g|=  2.06618D-04\n",
      "\n",
      "At iterate 1171    f=  3.13390D-02    |proj g|=  2.09075D-04\n",
      "\n",
      "At iterate 1172    f=  3.13348D-02    |proj g|=  1.66713D-04\n",
      "\n",
      "At iterate 1173    f=  3.13264D-02    |proj g|=  1.30961D-04\n",
      "\n",
      "At iterate 1174    f=  3.13158D-02    |proj g|=  9.63464D-05\n",
      "\n",
      "At iterate 1175    f=  3.13003D-02    |proj g|=  1.27030D-04\n",
      "\n",
      "At iterate 1176    f=  3.12627D-02    |proj g|=  1.37442D-04\n",
      "\n",
      "At iterate 1177    f=  3.12443D-02    |proj g|=  8.62241D-05\n",
      "\n",
      "At iterate 1178    f=  3.12415D-02    |proj g|=  2.21836D-04\n",
      "\n",
      "At iterate 1179    f=  3.12308D-02    |proj g|=  1.28697D-04\n",
      "\n",
      "At iterate 1180    f=  3.12170D-02    |proj g|=  1.62657D-04\n",
      "\n",
      "At iterate 1181    f=  3.12015D-02    |proj g|=  1.55812D-04\n",
      "\n",
      "At iterate 1182    f=  3.11892D-02    |proj g|=  2.49956D-04\n",
      "\n",
      "At iterate 1183    f=  3.11838D-02    |proj g|=  8.07336D-05\n",
      "\n",
      "At iterate 1184    f=  3.11813D-02    |proj g|=  1.14681D-04\n",
      "\n",
      "At iterate 1185    f=  3.11690D-02    |proj g|=  1.26741D-04\n",
      "\n",
      "At iterate 1186    f=  3.11286D-02    |proj g|=  1.46292D-04\n",
      "\n",
      "At iterate 1187    f=  3.10759D-02    |proj g|=  1.92833D-04\n",
      "\n",
      "At iterate 1188    f=  3.10237D-02    |proj g|=  2.32610D-04\n",
      "\n",
      "At iterate 1189    f=  3.10128D-02    |proj g|=  4.22681D-04\n",
      "\n",
      "At iterate 1190    f=  3.09811D-02    |proj g|=  6.97626D-05\n",
      "\n",
      "At iterate 1191    f=  3.09744D-02    |proj g|=  5.17535D-05\n",
      "\n",
      "At iterate 1192    f=  3.09629D-02    |proj g|=  8.44688D-05\n",
      "\n",
      "At iterate 1193    f=  3.09444D-02    |proj g|=  4.08668D-04\n",
      "\n",
      "At iterate 1194    f=  3.09334D-02    |proj g|=  5.05225D-04\n",
      "\n",
      "At iterate 1195    f=  3.09103D-02    |proj g|=  2.47917D-04\n",
      "\n",
      "At iterate 1196    f=  3.08988D-02    |proj g|=  2.64128D-04\n",
      "\n",
      "At iterate 1197    f=  3.08805D-02    |proj g|=  3.47453D-04\n",
      "\n",
      "At iterate 1198    f=  3.08450D-02    |proj g|=  3.21294D-04\n",
      "\n",
      "At iterate 1199    f=  3.07939D-02    |proj g|=  3.57303D-04\n",
      "\n",
      "At iterate 1200    f=  3.07865D-02    |proj g|=  4.33930D-04\n",
      "\n",
      "At iterate 1201    f=  3.07822D-02    |proj g|=  2.68489D-04\n",
      "\n",
      "At iterate 1202    f=  3.07707D-02    |proj g|=  1.22092D-04\n",
      "\n",
      "At iterate 1203    f=  3.07655D-02    |proj g|=  1.36585D-04\n",
      "\n",
      "At iterate 1204    f=  3.07535D-02    |proj g|=  1.73657D-04\n",
      "\n",
      "At iterate 1205    f=  3.07456D-02    |proj g|=  1.49153D-04\n",
      "\n",
      "At iterate 1206    f=  3.07262D-02    |proj g|=  2.49007D-04\n",
      "\n",
      "At iterate 1207    f=  3.07119D-02    |proj g|=  1.48758D-04\n",
      "\n",
      "At iterate 1208    f=  3.06912D-02    |proj g|=  1.71129D-04\n",
      "\n",
      "At iterate 1209    f=  3.06785D-02    |proj g|=  1.61669D-04\n",
      "\n",
      "At iterate 1210    f=  3.06631D-02    |proj g|=  2.19549D-04\n",
      "\n",
      "At iterate 1211    f=  3.06519D-02    |proj g|=  1.93731D-04\n",
      "\n",
      "At iterate 1212    f=  3.06387D-02    |proj g|=  1.90739D-04\n",
      "\n",
      "At iterate 1213    f=  3.06187D-02    |proj g|=  1.02770D-04\n",
      "\n",
      "At iterate 1214    f=  3.06040D-02    |proj g|=  1.49960D-04\n",
      "\n",
      "At iterate 1215    f=  3.05897D-02    |proj g|=  1.49551D-04\n",
      "\n",
      "At iterate 1216    f=  3.05579D-02    |proj g|=  1.08040D-04\n",
      "\n",
      "At iterate 1217    f=  3.05494D-02    |proj g|=  3.04099D-04\n",
      "\n",
      "At iterate 1218    f=  3.05196D-02    |proj g|=  1.22840D-04\n",
      "\n",
      "At iterate 1219    f=  3.05033D-02    |proj g|=  1.56693D-04\n",
      "\n",
      "At iterate 1220    f=  3.04855D-02    |proj g|=  2.10660D-04\n",
      "\n",
      "At iterate 1221    f=  3.04589D-02    |proj g|=  1.78991D-04\n",
      "\n",
      "At iterate 1222    f=  3.04247D-02    |proj g|=  2.62918D-04\n",
      "\n",
      "At iterate 1223    f=  3.04013D-02    |proj g|=  3.07444D-04\n",
      "\n",
      "At iterate 1224    f=  3.03813D-02    |proj g|=  3.70250D-04\n",
      "\n",
      "At iterate 1225    f=  3.03511D-02    |proj g|=  2.01050D-04\n",
      "\n",
      "At iterate 1226    f=  3.03250D-02    |proj g|=  2.23742D-04\n",
      "\n",
      "At iterate 1227    f=  3.02729D-02    |proj g|=  1.15748D-04\n",
      "\n",
      "At iterate 1228    f=  3.02102D-02    |proj g|=  2.19854D-04\n",
      "\n",
      "At iterate 1229    f=  3.01915D-02    |proj g|=  4.78412D-04\n",
      "\n",
      "At iterate 1230    f=  3.01552D-02    |proj g|=  4.13595D-04\n",
      "\n",
      "At iterate 1231    f=  3.00407D-02    |proj g|=  4.58892D-04\n",
      "\n",
      "At iterate 1232    f=  3.00024D-02    |proj g|=  1.72131D-04\n",
      "\n",
      "At iterate 1233    f=  2.99393D-02    |proj g|=  1.52971D-04\n",
      "\n",
      "At iterate 1234    f=  2.99245D-02    |proj g|=  3.41352D-04\n",
      "\n",
      "At iterate 1235    f=  2.98936D-02    |proj g|=  1.69453D-04\n",
      "\n",
      "At iterate 1236    f=  2.98723D-02    |proj g|=  1.08245D-04\n",
      "\n",
      "At iterate 1237    f=  2.98507D-02    |proj g|=  1.87257D-04\n",
      "\n",
      "At iterate 1238    f=  2.98072D-02    |proj g|=  2.63392D-04\n",
      "\n",
      "At iterate 1239    f=  2.97082D-02    |proj g|=  6.47061D-04\n",
      "\n",
      "At iterate 1240    f=  2.96833D-02    |proj g|=  4.22022D-04\n",
      "\n",
      "At iterate 1241    f=  2.95920D-02    |proj g|=  4.74146D-04\n",
      "\n",
      "At iterate 1242    f=  2.94394D-02    |proj g|=  3.01165D-04\n",
      "\n",
      "At iterate 1243    f=  2.93053D-02    |proj g|=  2.63259D-04\n",
      "\n",
      "At iterate 1244    f=  2.92949D-02    |proj g|=  1.20412D-04\n",
      "\n",
      "At iterate 1245    f=  2.92788D-02    |proj g|=  1.64932D-04\n",
      "\n",
      "At iterate 1246    f=  2.92691D-02    |proj g|=  3.79730D-04\n",
      "\n",
      "At iterate 1247    f=  2.92285D-02    |proj g|=  3.17983D-04\n",
      "\n",
      "At iterate 1248    f=  2.91157D-02    |proj g|=  1.62884D-04\n",
      "\n",
      "At iterate 1249    f=  2.90602D-02    |proj g|=  1.99499D-04\n",
      "\n",
      "At iterate 1250    f=  2.89668D-02    |proj g|=  2.18602D-04\n",
      "\n",
      "At iterate 1251    f=  2.89488D-02    |proj g|=  6.31343D-04\n",
      "\n",
      "At iterate 1252    f=  2.88393D-02    |proj g|=  3.70277D-04\n",
      "\n",
      "At iterate 1253    f=  2.87829D-02    |proj g|=  2.52806D-04\n",
      "\n",
      "At iterate 1254    f=  2.87236D-02    |proj g|=  2.41137D-04\n",
      "\n",
      "At iterate 1255    f=  2.87053D-02    |proj g|=  2.01708D-04\n",
      "\n",
      "At iterate 1256    f=  2.86812D-02    |proj g|=  2.33040D-04\n",
      "\n",
      "At iterate 1257    f=  2.86687D-02    |proj g|=  4.71680D-04\n",
      "\n",
      "At iterate 1258    f=  2.86335D-02    |proj g|=  2.84806D-04\n",
      "\n",
      "At iterate 1259    f=  2.86171D-02    |proj g|=  2.17619D-04\n",
      "\n",
      "At iterate 1260    f=  2.85810D-02    |proj g|=  1.22260D-04\n",
      "\n",
      "At iterate 1261    f=  2.85476D-02    |proj g|=  2.29485D-04\n",
      "\n",
      "At iterate 1262    f=  2.84981D-02    |proj g|=  2.18242D-04\n",
      "\n",
      "At iterate 1263    f=  2.84951D-02    |proj g|=  3.66052D-04\n",
      "\n",
      "At iterate 1264    f=  2.84655D-02    |proj g|=  2.57922D-04\n",
      "\n",
      "At iterate 1265    f=  2.84479D-02    |proj g|=  1.13531D-04\n",
      "\n",
      "At iterate 1266    f=  2.84396D-02    |proj g|=  1.66775D-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/discrete/discrete_model.py:1819: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate 1267    f=  2.84289D-02    |proj g|=  1.93148D-04\n",
      "\n",
      "At iterate 1268    f=  2.83956D-02    |proj g|=  2.04507D-04\n",
      "\n",
      "At iterate 1269    f=  2.83888D-02    |proj g|=  1.95393D-04\n",
      "\n",
      "At iterate 1270    f=  2.83469D-02    |proj g|=  1.93295D-04\n",
      "\n",
      "At iterate 1271    f=  2.83068D-02    |proj g|=  2.77108D-04\n",
      "\n",
      "At iterate 1272    f=  2.82576D-02    |proj g|=  1.66382D-04\n",
      "\n",
      "At iterate 1273    f=  2.81870D-02    |proj g|=  1.64370D-04\n",
      "\n",
      "At iterate 1274    f=  2.81834D-02    |proj g|=  3.28435D-04\n",
      "\n",
      "At iterate 1275    f=  2.81618D-02    |proj g|=  2.73727D-04\n",
      "\n",
      "At iterate 1276    f=  2.81477D-02    |proj g|=  1.88745D-04\n",
      "\n",
      "At iterate 1277    f=  2.81283D-02    |proj g|=  1.55537D-04\n",
      "\n",
      "At iterate 1278    f=  2.80884D-02    |proj g|=  2.39682D-04\n",
      "\n",
      "At iterate 1279    f=  2.80219D-02    |proj g|=  2.55190D-04\n",
      "\n",
      "At iterate 1280    f=  2.80059D-02    |proj g|=  4.31659D-04\n",
      "\n",
      "At iterate 1281    f=  2.79504D-02    |proj g|=  2.25250D-04\n",
      "\n",
      "At iterate 1282    f=  2.79131D-02    |proj g|=  1.29986D-04\n",
      "\n",
      "At iterate 1283    f=  2.78923D-02    |proj g|=  1.10707D-04\n",
      "\n",
      "At iterate 1284    f=  2.78708D-02    |proj g|=  1.73200D-04\n",
      "\n",
      "At iterate 1285    f=  2.78608D-02    |proj g|=  2.26443D-04\n",
      "\n",
      "At iterate 1286    f=  2.78267D-02    |proj g|=  1.17446D-04\n",
      "\n",
      "At iterate 1287    f=  2.77948D-02    |proj g|=  1.34202D-04\n",
      "\n",
      "At iterate 1288    f=  2.77546D-02    |proj g|=  1.23157D-04\n",
      "\n",
      "At iterate 1289    f=  2.77329D-02    |proj g|=  3.40160D-04\n",
      "\n",
      "At iterate 1290    f=  2.77059D-02    |proj g|=  2.25112D-04\n",
      "\n",
      "At iterate 1291    f=  2.76691D-02    |proj g|=  2.04704D-04\n",
      "\n",
      "At iterate 1292    f=  2.76512D-02    |proj g|=  1.70337D-04\n",
      "\n",
      "At iterate 1293    f=  2.76324D-02    |proj g|=  4.55568D-04\n",
      "\n",
      "At iterate 1294    f=  2.75899D-02    |proj g|=  3.14819D-04\n",
      "\n",
      "At iterate 1295    f=  2.75375D-02    |proj g|=  1.86824D-04\n",
      "\n",
      "At iterate 1296    f=  2.75169D-02    |proj g|=  2.08743D-04\n",
      "\n",
      "At iterate 1297    f=  2.74781D-02    |proj g|=  4.45045D-04\n",
      "\n",
      "At iterate 1298    f=  2.73909D-02    |proj g|=  4.99164D-04\n",
      "\n",
      "At iterate 1299    f=  2.72847D-02    |proj g|=  4.67557D-04\n",
      "\n",
      "At iterate 1300    f=  2.72382D-02    |proj g|=  5.09927D-04\n",
      "\n",
      "At iterate 1301    f=  2.71234D-02    |proj g|=  1.68714D-04\n",
      "\n",
      "At iterate 1302    f=  2.70819D-02    |proj g|=  1.70398D-04\n",
      "\n",
      "At iterate 1303    f=  2.69771D-02    |proj g|=  1.60473D-04\n",
      "\n",
      "At iterate 1304    f=  2.69130D-02    |proj g|=  4.56415D-04\n",
      "\n",
      "At iterate 1305    f=  2.67782D-02    |proj g|=  5.75565D-04\n",
      "\n",
      "At iterate 1306    f=  2.67091D-02    |proj g|=  3.93035D-04\n",
      "\n",
      "At iterate 1307    f=  2.66811D-02    |proj g|=  2.17488D-04\n",
      "\n",
      "At iterate 1308    f=  2.66675D-02    |proj g|=  1.94271D-04\n",
      "\n",
      "At iterate 1309    f=  2.66446D-02    |proj g|=  4.09215D-04\n",
      "\n",
      "At iterate 1310    f=  2.65689D-02    |proj g|=  3.07601D-04\n",
      "\n",
      "At iterate 1311    f=  2.63597D-02    |proj g|=  2.20956D-04\n",
      "\n",
      "At iterate 1312    f=  2.63035D-02    |proj g|=  9.20589D-04\n",
      "\n",
      "At iterate 1313    f=  2.61921D-02    |proj g|=  5.38334D-04\n",
      "\n",
      "At iterate 1314    f=  2.61303D-02    |proj g|=  3.21455D-04\n",
      "\n",
      "At iterate 1315    f=  2.60991D-02    |proj g|=  1.66972D-04\n",
      "\n",
      "At iterate 1316    f=  2.60772D-02    |proj g|=  2.15553D-04\n",
      "\n",
      "At iterate 1317    f=  2.60478D-02    |proj g|=  1.49539D-04\n",
      "\n",
      "At iterate 1318    f=  2.60096D-02    |proj g|=  5.51974D-04\n",
      "\n",
      "At iterate 1319    f=  2.59665D-02    |proj g|=  2.55615D-04\n",
      "\n",
      "At iterate 1320    f=  2.59452D-02    |proj g|=  1.50912D-04\n",
      "\n",
      "At iterate 1321    f=  2.59330D-02    |proj g|=  2.02891D-04\n",
      "\n",
      "At iterate 1322    f=  2.59141D-02    |proj g|=  2.48205D-04\n",
      "\n",
      "At iterate 1323    f=  2.59026D-02    |proj g|=  1.19077D-03\n",
      "\n",
      "At iterate 1324    f=  2.58318D-02    |proj g|=  5.71951D-04\n",
      "\n",
      "At iterate 1325    f=  2.57755D-02    |proj g|=  2.66959D-04\n",
      "\n",
      "At iterate 1326    f=  2.57222D-02    |proj g|=  3.62687D-04\n",
      "\n",
      "At iterate 1327    f=  2.56941D-02    |proj g|=  2.52142D-04\n",
      "\n",
      "At iterate 1328    f=  2.56525D-02    |proj g|=  5.14031D-04\n",
      "\n",
      "At iterate 1329    f=  2.56331D-02    |proj g|=  4.03478D-04\n",
      "\n",
      "At iterate 1330    f=  2.56017D-02    |proj g|=  3.42658D-04\n",
      "\n",
      "At iterate 1331    f=  2.55923D-02    |proj g|=  3.69041D-04\n",
      "\n",
      "At iterate 1332    f=  2.55589D-02    |proj g|=  5.06787D-04\n",
      "\n",
      "At iterate 1333    f=  2.54995D-02    |proj g|=  4.69067D-04\n",
      "\n",
      "At iterate 1334    f=  2.54302D-02    |proj g|=  5.43651D-04\n",
      "\n",
      "At iterate 1335    f=  2.52587D-02    |proj g|=  1.05394D-03\n",
      "\n",
      "At iterate 1336    f=  2.51440D-02    |proj g|=  3.81954D-04\n",
      "\n",
      "At iterate 1337    f=  2.50701D-02    |proj g|=  4.88679D-04\n",
      "\n",
      "At iterate 1338    f=  2.50420D-02    |proj g|=  1.42737D-04\n",
      "\n",
      "At iterate 1339    f=  2.50351D-02    |proj g|=  1.65534D-04\n",
      "\n",
      "At iterate 1340    f=  2.50216D-02    |proj g|=  9.27018D-05\n",
      "\n",
      "At iterate 1341    f=  2.50093D-02    |proj g|=  2.57743D-04\n",
      "\n",
      "At iterate 1342    f=  2.49943D-02    |proj g|=  1.32965D-04\n",
      "\n",
      "At iterate 1343    f=  2.49780D-02    |proj g|=  1.52505D-04\n",
      "\n",
      "At iterate 1344    f=  2.49534D-02    |proj g|=  1.68268D-04\n",
      "\n",
      "At iterate 1345    f=  2.49107D-02    |proj g|=  2.36907D-04\n",
      "\n",
      "At iterate 1346    f=  2.48386D-02    |proj g|=  2.66387D-04\n",
      "\n",
      "At iterate 1347    f=  2.48237D-02    |proj g|=  5.44911D-04\n",
      "\n",
      "At iterate 1348    f=  2.47428D-02    |proj g|=  3.68045D-04\n",
      "\n",
      "At iterate 1349    f=  2.47205D-02    |proj g|=  2.14027D-04\n",
      "\n",
      "At iterate 1350    f=  2.46946D-02    |proj g|=  1.38459D-04\n",
      "\n",
      "At iterate 1351    f=  2.46660D-02    |proj g|=  6.64441D-05\n",
      "\n",
      "At iterate 1352    f=  2.46558D-02    |proj g|=  4.01807D-04\n",
      "\n",
      "At iterate 1353    f=  2.46394D-02    |proj g|=  1.21568D-04\n",
      "\n",
      "At iterate 1354    f=  2.46338D-02    |proj g|=  1.08215D-04\n",
      "\n",
      "At iterate 1355    f=  2.46134D-02    |proj g|=  1.50792D-04\n",
      "\n",
      "At iterate 1356    f=  2.45636D-02    |proj g|=  1.78621D-04\n",
      "\n",
      "At iterate 1357    f=  2.44764D-02    |proj g|=  2.17161D-04\n",
      "\n",
      "At iterate 1358    f=  2.44650D-02    |proj g|=  3.62678D-04\n",
      "\n",
      "At iterate 1359    f=  2.43898D-02    |proj g|=  2.01597D-04\n",
      "\n",
      "At iterate 1360    f=  2.42540D-02    |proj g|=  5.02342D-04\n",
      "\n",
      "At iterate 1361    f=  2.42308D-02    |proj g|=  6.35535D-04\n",
      "\n",
      "At iterate 1362    f=  2.41979D-02    |proj g|=  4.50583D-04\n",
      "\n",
      "At iterate 1363    f=  2.40795D-02    |proj g|=  4.06096D-04\n",
      "\n",
      "At iterate 1364    f=  2.40026D-02    |proj g|=  5.21648D-04\n",
      "\n",
      "At iterate 1365    f=  2.39407D-02    |proj g|=  4.92189D-04\n",
      "\n",
      "At iterate 1366    f=  2.39019D-02    |proj g|=  6.74615D-04\n",
      "\n",
      "At iterate 1367    f=  2.38132D-02    |proj g|=  5.77432D-04\n",
      "\n",
      "At iterate 1368    f=  2.36516D-02    |proj g|=  2.68329D-04\n",
      "\n",
      "At iterate 1369    f=  2.35770D-02    |proj g|=  3.41019D-04\n",
      "\n",
      "At iterate 1370    f=  2.32671D-02    |proj g|=  8.51278D-04\n",
      "\n",
      "At iterate 1371    f=  2.31552D-02    |proj g|=  4.54974D-04\n",
      "\n",
      "At iterate 1372    f=  2.31192D-02    |proj g|=  5.45409D-04\n",
      "\n",
      "At iterate 1373    f=  2.30490D-02    |proj g|=  5.84764D-04\n",
      "\n",
      "At iterate 1374    f=  2.29230D-02    |proj g|=  4.29634D-04\n",
      "\n",
      "At iterate 1375    f=  2.28884D-02    |proj g|=  8.78726D-04\n",
      "\n",
      "At iterate 1376    f=  2.28138D-02    |proj g|=  4.64587D-04\n",
      "\n",
      "At iterate 1377    f=  2.27539D-02    |proj g|=  4.08096D-04\n",
      "\n",
      "At iterate 1378    f=  2.26807D-02    |proj g|=  6.95442D-04\n",
      "\n",
      "At iterate 1379    f=  2.25206D-02    |proj g|=  1.03943D-03\n",
      "\n",
      "At iterate 1380    f=  2.22421D-02    |proj g|=  1.30407D-03\n",
      "\n",
      "At iterate 1381    f=  2.19258D-02    |proj g|=  8.90430D-04\n",
      "\n",
      "At iterate 1382    f=  2.17772D-02    |proj g|=  1.40045D-03\n",
      "\n",
      "At iterate 1383    f=  2.17649D-02    |proj g|=  6.62837D-04\n",
      "\n",
      "At iterate 1384    f=  2.16160D-02    |proj g|=  5.28608D-04\n",
      "\n",
      "At iterate 1385    f=  2.15645D-02    |proj g|=  5.63847D-04\n",
      "\n",
      "At iterate 1386    f=  2.13047D-02    |proj g|=  6.99764D-04\n",
      "\n",
      "At iterate 1387    f=  2.12239D-02    |proj g|=  1.18807D-03\n",
      "\n",
      "At iterate 1388    f=  2.11057D-02    |proj g|=  3.90732D-04\n",
      "\n",
      "At iterate 1389    f=  2.10564D-02    |proj g|=  4.60237D-04\n",
      "\n",
      "At iterate 1390    f=  2.10038D-02    |proj g|=  6.97104D-04\n",
      "\n",
      "At iterate 1391    f=  2.09021D-02    |proj g|=  6.09639D-04\n",
      "\n",
      "At iterate 1392    f=  2.08183D-02    |proj g|=  9.54859D-04\n",
      "\n",
      "At iterate 1393    f=  2.06746D-02    |proj g|=  5.86275D-04\n",
      "\n",
      "At iterate 1394    f=  2.05957D-02    |proj g|=  4.23012D-04\n",
      "\n",
      "At iterate 1395    f=  2.04759D-02    |proj g|=  6.53082D-04\n",
      "\n",
      "At iterate 1396    f=  2.03993D-02    |proj g|=  6.34067D-04\n",
      "\n",
      "At iterate 1397    f=  2.01402D-02    |proj g|=  2.97619D-04\n",
      "\n",
      "At iterate 1398    f=  1.99786D-02    |proj g|=  3.88256D-04\n",
      "\n",
      "At iterate 1399    f=  1.99045D-02    |proj g|=  6.13646D-04\n",
      "\n",
      "At iterate 1400    f=  1.97445D-02    |proj g|=  3.36511D-04\n",
      "\n",
      "At iterate 1401    f=  1.96597D-02    |proj g|=  3.66395D-04\n",
      "\n",
      "At iterate 1402    f=  1.95933D-02    |proj g|=  3.16840D-04\n",
      "\n",
      "At iterate 1403    f=  1.91412D-02    |proj g|=  7.80533D-04\n",
      "\n",
      "At iterate 1404    f=  1.84711D-02    |proj g|=  1.50158D-03\n",
      "\n",
      "At iterate 1405    f=  1.77352D-02    |proj g|=  1.67748D-03\n",
      "\n",
      "At iterate 1406    f=  1.76871D-02    |proj g|=  1.72749D-03\n",
      "\n",
      "At iterate 1407    f=  1.71543D-02    |proj g|=  8.60225D-04\n",
      "\n",
      "At iterate 1408    f=  1.69846D-02    |proj g|=  5.86249D-04\n",
      "\n",
      "At iterate 1409    f=  1.68909D-02    |proj g|=  4.70242D-04\n",
      "\n",
      "At iterate 1410    f=  1.66934D-02    |proj g|=  3.13535D-04\n",
      "\n",
      "At iterate 1411    f=  1.63945D-02    |proj g|=  4.41043D-04\n",
      "\n",
      "At iterate 1412    f=  1.63097D-02    |proj g|=  1.07094D-03\n",
      "\n",
      "At iterate 1413    f=  1.59681D-02    |proj g|=  8.62787D-04\n",
      "\n",
      "At iterate 1414    f=  1.57011D-02    |proj g|=  6.14228D-04\n",
      "\n",
      "At iterate 1415    f=  1.54146D-02    |proj g|=  2.16559D-04\n",
      "\n",
      "At iterate 1416    f=  1.52998D-02    |proj g|=  3.12593D-04\n",
      "\n",
      "At iterate 1417    f=  1.52520D-02    |proj g|=  8.69347D-04\n",
      "\n",
      "At iterate 1418    f=  1.51316D-02    |proj g|=  6.80177D-04\n",
      "\n",
      "At iterate 1419    f=  1.49950D-02    |proj g|=  5.45912D-04\n",
      "\n",
      "At iterate 1420    f=  1.48623D-02    |proj g|=  5.49588D-04\n",
      "\n",
      "At iterate 1421    f=  1.46350D-02    |proj g|=  6.65494D-04\n",
      "\n",
      "At iterate 1422    f=  1.43494D-02    |proj g|=  5.19165D-04\n",
      "\n",
      "At iterate 1423    f=  1.37311D-02    |proj g|=  1.02402D-03\n",
      "\n",
      "At iterate 1424    f=  1.33807D-02    |proj g|=  1.14394D-03\n",
      "\n",
      "At iterate 1425    f=  1.29852D-02    |proj g|=  1.09231D-03\n",
      "\n",
      "At iterate 1426    f=  1.24865D-02    |proj g|=  3.93100D-04\n",
      "\n",
      "At iterate 1427    f=  1.23125D-02    |proj g|=  3.42553D-04\n",
      "\n",
      "At iterate 1428    f=  1.21024D-02    |proj g|=  4.70467D-04\n",
      "\n",
      "At iterate 1429    f=  1.15890D-02    |proj g|=  3.61249D-04\n",
      "\n",
      "At iterate 1430    f=  1.13678D-02    |proj g|=  1.23910D-03\n",
      "\n",
      "At iterate 1431    f=  1.12864D-02    |proj g|=  7.46735D-04\n",
      "\n",
      "At iterate 1432    f=  1.11977D-02    |proj g|=  2.73149D-04\n",
      "\n",
      "At iterate 1433    f=  1.11714D-02    |proj g|=  4.26869D-04\n",
      "\n",
      "At iterate 1434    f=  1.11146D-02    |proj g|=  5.03507D-04\n",
      "\n",
      "At iterate 1435    f=  1.08312D-02    |proj g|=  4.60887D-04\n",
      "\n",
      "At iterate 1436    f=  1.03893D-02    |proj g|=  5.28138D-04\n",
      "\n",
      "At iterate 1437    f=  1.02185D-02    |proj g|=  5.63916D-04\n",
      "\n",
      "At iterate 1438    f=  9.97449D-03    |proj g|=  2.53282D-04\n",
      "\n",
      "At iterate 1439    f=  9.78662D-03    |proj g|=  2.58072D-04\n",
      "\n",
      "At iterate 1440    f=  9.58128D-03    |proj g|=  4.25994D-04\n",
      "\n",
      "At iterate 1441    f=  9.40245D-03    |proj g|=  1.12926D-03\n",
      "\n",
      "At iterate 1442    f=  9.13640D-03    |proj g|=  8.15040D-04\n",
      "\n",
      "At iterate 1443    f=  9.01051D-03    |proj g|=  5.07768D-04\n",
      "\n",
      "At iterate 1444    f=  8.87181D-03    |proj g|=  4.27757D-04\n",
      "\n",
      "At iterate 1445    f=  8.59645D-03    |proj g|=  4.30828D-04\n",
      "\n",
      "At iterate 1446    f=  8.07851D-03    |proj g|=  5.17166D-04\n",
      "\n",
      "At iterate 1447    f=  7.71575D-03    |proj g|=  2.78781D-04\n",
      "\n",
      "At iterate 1448    f=  7.36940D-03    |proj g|=  4.15717D-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "\r",
      " 31%|███       | 4/13 [00:01<00:03,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate 1449    f=  6.71519D-03    |proj g|=  6.17945D-04\n",
      "\n",
      "At iterate 1450    f=  6.39759D-03    |proj g|=  5.08470D-04\n",
      "\n",
      "At iterate 1451    f=  5.87160D-03    |proj g|=  3.56687D-04\n",
      "\n",
      "At iterate 1452    f=  5.60696D-03    |proj g|=  4.17758D-04\n",
      "\n",
      "At iterate 1453    f=  5.17387D-03    |proj g|=  2.92000D-04\n",
      "\n",
      "At iterate 1454    f=  5.00752D-03    |proj g|=  9.30339D-04\n",
      "\n",
      "At iterate 1455    f=  4.50875D-03    |proj g|=  2.83078D-04\n",
      "\n",
      "At iterate 1456    f=  4.41599D-03    |proj g|=  2.76059D-04\n",
      "\n",
      "At iterate 1457    f=  4.29958D-03    |proj g|=  1.74689D-04\n",
      "\n",
      "At iterate 1458    f=  4.14514D-03    |proj g|=  1.49621D-04\n",
      "\n",
      "At iterate 1459    f=  3.80676D-03    |proj g|=  1.31002D-04\n",
      "\n",
      "At iterate 1460    f=  3.64969D-03    |proj g|=  6.42480D-04\n",
      "\n",
      "At iterate 1461    f=  3.41941D-03    |proj g|=  3.09781D-04\n",
      "\n",
      "At iterate 1462    f=  3.29355D-03    |proj g|=  1.25754D-04\n",
      "\n",
      "At iterate 1463    f=  3.16682D-03    |proj g|=  2.43491D-04\n",
      "\n",
      "At iterate 1464    f=  3.10674D-03    |proj g|=  2.09570D-04\n",
      "\n",
      "At iterate 1465    f=  2.88515D-03    |proj g|=  1.68754D-04\n",
      "\n",
      "At iterate 1466    f=  2.63941D-03    |proj g|=  4.17046D-04\n",
      "\n",
      "At iterate 1467    f=  2.38820D-03    |proj g|=  4.83164D-04\n",
      "\n",
      "At iterate 1468    f=  2.31101D-03    |proj g|=  7.72104D-04\n",
      "\n",
      "At iterate 1469    f=  1.99310D-03    |proj g|=  6.16690D-04\n",
      "\n",
      "At iterate 1470    f=  1.51666D-03    |proj g|=  3.39208D-04\n",
      "\n",
      "At iterate 1471    f=  1.29720D-03    |proj g|=  3.76680D-04\n",
      "\n",
      "At iterate 1472    f=  1.24028D-03    |proj g|=  1.55805D-04\n",
      "\n",
      "At iterate 1473    f=  1.19122D-03    |proj g|=  1.44494D-04\n",
      "\n",
      "At iterate 1474    f=  1.10576D-03    |proj g|=  3.62636D-04\n",
      "\n",
      "At iterate 1475    f=  9.90171D-04    |proj g|=  2.09538D-04\n",
      "\n",
      "At iterate 1476    f=  9.18723D-04    |proj g|=  1.00831D-04\n",
      "\n",
      "At iterate 1477    f=  8.38345D-04    |proj g|=  8.32086D-05\n",
      "\n",
      "At iterate 1478    f=  6.96404D-04    |proj g|=  1.53473D-04\n",
      "\n",
      "At iterate 1479    f=  6.53461D-04    |proj g|=  2.78758D-04\n",
      "\n",
      "At iterate 1480    f=  4.92983D-04    |proj g|=  3.42178D-04\n",
      "\n",
      "At iterate 1481    f=  3.92493D-04    |proj g|=  9.53663D-05\n",
      "\n",
      "At iterate 1482    f=  3.07016D-04    |proj g|=  3.57845D-05\n",
      "\n",
      "At iterate 1483    f=  2.46790D-04    |proj g|=  2.38049D-05\n",
      "\n",
      "At iterate 1484    f=  2.22940D-04    |proj g|=  1.10995D-04\n",
      "\n",
      "At iterate 1485    f=  1.53420D-04    |proj g|=  2.27433D-05\n",
      "\n",
      "At iterate 1486    f=  1.15474D-04    |proj g|=  1.47929D-05\n",
      "\n",
      "At iterate 1487    f=  6.25472D-05    |proj g|=  1.93317D-05\n",
      "\n",
      "At iterate 1488    f=  3.36623D-05    |proj g|=  3.85006D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74   1488   1743      1     0     0   3.850D-06   3.366D-05\n",
      "  F =   3.3662329980512469E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.29768D-01\n",
      "\n",
      "At iterate    1    f=  6.62110D-01    |proj g|=  7.21341D-02\n",
      "\n",
      "At iterate    2    f=  5.31209D-01    |proj g|=  5.94053D-02\n",
      "\n",
      "At iterate    3    f=  2.71985D-01    |proj g|=  2.63084D-02\n",
      "\n",
      "At iterate    4    f=  1.96949D-01    |proj g|=  2.38061D-02\n",
      "\n",
      "At iterate    5    f=  1.08614D-01    |proj g|=  1.55783D-02\n",
      "\n",
      "At iterate    6    f=  6.39229D-02    |proj g|=  8.79677D-03\n",
      "\n",
      "At iterate    7    f=  3.66611D-02    |proj g|=  5.95986D-03\n",
      "\n",
      "At iterate    8    f=  1.98943D-02    |proj g|=  3.70735D-03\n",
      "\n",
      "At iterate    9    f=  1.05383D-02    |proj g|=  2.07587D-03\n",
      "\n",
      "At iterate   10    f=  5.49355D-03    |proj g|=  1.12176D-03\n",
      "\n",
      "At iterate   11    f=  2.81554D-03    |proj g|=  5.75528D-04\n",
      "\n",
      "At iterate   12    f=  1.43123D-03    |proj g|=  2.58578D-04\n",
      "\n",
      "At iterate   13    f=  8.79777D-04    |proj g|=  6.45613D-04\n",
      "\n",
      "At iterate   14    f=  1.62615D-04    |proj g|=  8.42933D-05\n",
      "\n",
      "At iterate   15    f=  1.16671D-04    |proj g|=  4.98981D-05\n",
      "\n",
      "At iterate   16    f=  5.59409D-05    |proj g|=  1.34855D-05\n",
      "\n",
      "At iterate   17    f=  3.10242D-05    |proj g|=  3.40163D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     17     21      1     0     0   3.402D-06   3.102D-05\n",
      "  F =   3.1024172077190563E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.73988D-01\n",
      "\n",
      "At iterate    1    f=  6.40218D-01    |proj g|=  6.41361D-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "\r",
      " 62%|██████▏   | 8/13 [00:01<00:00,  5.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate    2    f=  5.37000D-01    |proj g|=  5.15819D-02\n",
      "\n",
      "At iterate    3    f=  3.17889D-01    |proj g|=  4.43235D-02\n",
      "\n",
      "At iterate    4    f=  2.45469D-01    |proj g|=  3.11906D-02\n",
      "\n",
      "At iterate    5    f=  1.72768D-01    |proj g|=  1.50510D-02\n",
      "\n",
      "At iterate    6    f=  1.32388D-01    |proj g|=  5.40183D-03\n",
      "\n",
      "At iterate    7    f=  1.03301D-01    |proj g|=  5.29130D-03\n",
      "\n",
      "At iterate    8    f=  7.88208D-02    |proj g|=  6.61178D-03\n",
      "\n",
      "At iterate    9    f=  5.59934D-02    |proj g|=  2.72985D-03\n",
      "\n",
      "At iterate   10    f=  4.43817D-02    |proj g|=  1.01392D-03\n",
      "\n",
      "At iterate   11    f=  3.64508D-02    |proj g|=  1.55063D-03\n",
      "\n",
      "At iterate   12    f=  3.59548D-02    |proj g|=  3.36474D-03\n",
      "\n",
      "At iterate   13    f=  3.23629D-02    |proj g|=  1.82744D-03\n",
      "\n",
      "At iterate   14    f=  3.03235D-02    |proj g|=  1.55511D-03\n",
      "\n",
      "At iterate   15    f=  2.73785D-02    |proj g|=  1.70080D-03\n",
      "\n",
      "At iterate   16    f=  2.27639D-02    |proj g|=  1.55998D-03\n",
      "\n",
      "At iterate   17    f=  1.66592D-02    |proj g|=  1.75764D-03\n",
      "\n",
      "At iterate   18    f=  1.09027D-02    |proj g|=  1.69678D-03\n",
      "\n",
      "At iterate   19    f=  8.40197D-03    |proj g|=  1.19809D-03\n",
      "\n",
      "At iterate   20    f=  5.94169D-03    |proj g|=  8.06345D-04\n",
      "\n",
      "At iterate   21    f=  3.93396D-03    |proj g|=  5.45241D-04\n",
      "\n",
      "At iterate   22    f=  1.89602D-03    |proj g|=  2.05150D-04\n",
      "\n",
      "At iterate   23    f=  1.06710D-03    |proj g|=  7.01665D-04\n",
      "\n",
      "At iterate   24    f=  4.55763D-04    |proj g|=  2.22309D-04\n",
      "\n",
      "At iterate   25    f=  2.49507D-04    |proj g|=  9.97279D-05\n",
      "\n",
      "At iterate   26    f=  1.22360D-04    |proj g|=  3.87524D-05\n",
      "\n",
      "At iterate   27    f=  6.23678D-05    |proj g|=  1.56776D-05\n",
      "\n",
      "At iterate   28    f=  3.12386D-05    |proj g|=  6.09156D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     28     33      1     0     0   6.092D-06   3.124D-05\n",
      "  F =   3.1238585357170255E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.74205D-01\n",
      "\n",
      "At iterate    1    f=  6.33445D-01    |proj g|=  6.01594D-02\n",
      "\n",
      "At iterate    2    f=  5.46259D-01    |proj g|=  5.99139D-02\n",
      "\n",
      "At iterate    3    f=  3.58419D-01    |proj g|=  9.69215D-02\n",
      "\n",
      "At iterate    4    f=  2.69906D-01    |proj g|=  6.54063D-02\n",
      "\n",
      "At iterate    5    f=  2.00065D-01    |proj g|=  3.54794D-02\n",
      "\n",
      "At iterate    6    f=  1.56483D-01    |proj g|=  1.84257D-02\n",
      "\n",
      "At iterate    7    f=  1.21896D-01    |proj g|=  8.90098D-03\n",
      "\n",
      "At iterate    8    f=  9.15505D-02    |proj g|=  8.25807D-03\n",
      "\n",
      "At iterate    9    f=  6.29335D-02    |proj g|=  7.66284D-03\n",
      "\n",
      "At iterate   10    f=  3.32248D-02    |proj g|=  7.71830D-03\n",
      "\n",
      "At iterate   11    f=  1.65206D-02    |proj g|=  2.53041D-03\n",
      "\n",
      "At iterate   12    f=  1.08881D-02    |proj g|=  1.03271D-03\n",
      "\n",
      "At iterate   13    f=  6.40089D-03    |proj g|=  1.81014D-03\n",
      "\n",
      "At iterate   14    f=  3.53778D-03    |proj g|=  2.83184D-03\n",
      "\n",
      "At iterate   15    f=  2.55601D-03    |proj g|=  1.88011D-03\n",
      "\n",
      "At iterate   16    f=  1.48596D-03    |proj g|=  8.64291D-04\n",
      "\n",
      "At iterate   17    f=  8.97164D-04    |proj g|=  3.83792D-04\n",
      "\n",
      "At iterate   18    f=  5.04706D-04    |proj g|=  1.58859D-04\n",
      "\n",
      "At iterate   19    f=  2.74377D-04    |proj g|=  8.08123D-05\n",
      "\n",
      "At iterate   20    f=  1.45097D-04    |proj g|=  3.82196D-05\n",
      "\n",
      "At iterate   21    f=  7.58823D-05    |proj g|=  2.69625D-05\n",
      "\n",
      "At iterate   22    f=  3.97137D-05    |proj g|=  1.43117D-05\n",
      "\n",
      "At iterate   23    f=  2.94115D-05    |proj g|=  1.58027D-05\n",
      "\n",
      "At iterate   24    f=  9.72348D-06    |proj g|=  3.85241D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     24     29      1     0     0   3.852D-06   9.723D-06\n",
      "  F =   9.7234849399117114E-006\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  2.47181D-01\n",
      "\n",
      "At iterate    1    f=  5.65619D-01    |proj g|=  4.81331D-02\n",
      "\n",
      "At iterate    2    f=  5.47840D-01    |proj g|=  4.41768D-02\n",
      "\n",
      "At iterate    3    f=  4.33690D-01    |proj g|=  1.23273D-01\n",
      "\n",
      "At iterate    4    f=  3.68018D-01    |proj g|=  1.05855D-01\n",
      "\n",
      "At iterate    5    f=  2.79796D-01    |proj g|=  5.00323D-02\n",
      "\n",
      "At iterate    6    f=  2.40163D-01    |proj g|=  2.04329D-02\n",
      "\n",
      "At iterate    7    f=  2.16806D-01    |proj g|=  7.97392D-03\n",
      "\n",
      "At iterate    8    f=  2.00608D-01    |proj g|=  1.27387D-02\n",
      "\n",
      "At iterate    9    f=  1.78374D-01    |proj g|=  1.29683D-02\n",
      "\n",
      "At iterate   10    f=  9.84168D-02    |proj g|=  1.23131D-02\n",
      "\n",
      "At iterate   11    f=  8.21376D-02    |proj g|=  1.61180D-02\n",
      "\n",
      "At iterate   12    f=  6.12099D-02    |proj g|=  3.30863D-03\n",
      "\n",
      "At iterate   13    f=  5.10210D-02    |proj g|=  4.15829D-03\n",
      "\n",
      "At iterate   14    f=  4.94019D-02    |proj g|=  7.44855D-03\n",
      "\n",
      "At iterate   15    f=  4.20501D-02    |proj g|=  2.91397D-03\n",
      "\n",
      "At iterate   16    f=  3.83874D-02    |proj g|=  2.42695D-03\n",
      "\n",
      "At iterate   17    f=  3.47419D-02    |proj g|=  2.57023D-03\n",
      "\n",
      "At iterate   18    f=  2.97067D-02    |proj g|=  2.36063D-03\n",
      "\n",
      "At iterate   19    f=  1.80828D-02    |proj g|=  2.93732D-03\n",
      "\n",
      "At iterate   20    f=  1.13155D-02    |proj g|=  7.56464D-04\n",
      "\n",
      "At iterate   21    f=  7.26076D-03    |proj g|=  1.49047D-03\n",
      "\n",
      "At iterate   22    f=  2.78810D-03    |proj g|=  8.00926D-04\n",
      "\n",
      "At iterate   23    f=  1.41072D-03    |proj g|=  2.18022D-04\n",
      "\n",
      "At iterate   24    f=  8.54244D-04    |proj g|=  1.17036D-04\n",
      "\n",
      "At iterate   25    f=  4.55111D-04    |proj g|=  2.36649D-04\n",
      "\n",
      "At iterate   26    f=  2.44553D-04    |proj g|=  2.26310D-04\n",
      "\n",
      "At iterate   27    f=  8.97598D-05    |proj g|=  6.62625D-05\n",
      "\n",
      "At iterate   28    f=  5.98536D-05    |proj g|=  3.77137D-05\n",
      "\n",
      "At iterate   29    f=  3.24085D-05    |proj g|=  1.38707D-05\n",
      "\n",
      "At iterate   30    f=  1.93112D-05    |proj g|=  5.25311D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     30     34      1     0     0   5.253D-06   1.931D-05\n",
      "  F =   1.9311158954527789E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.31276D-01\n",
      "\n",
      "At iterate    1    f=  6.56791D-01    |proj g|=  6.09391D-02\n",
      "\n",
      "At iterate    2    f=  5.71625D-01    |proj g|=  5.35923D-02\n",
      "\n",
      "At iterate    3    f=  3.08743D-01    |proj g|=  1.84558D-02\n",
      "\n",
      "At iterate    4    f=  2.31305D-01    |proj g|=  1.06483D-02\n",
      "\n",
      "At iterate    5    f=  1.43816D-01    |proj g|=  8.84851D-03\n",
      "\n",
      "At iterate    6    f=  9.98887D-02    |proj g|=  8.19919D-03\n",
      "\n",
      "At iterate    7    f=  4.78916D-02    |proj g|=  7.11216D-03\n",
      "\n",
      "At iterate    8    f=  2.95951D-02    |proj g|=  5.33925D-03\n",
      "\n",
      "At iterate    9    f=  1.54050D-02    |proj g|=  3.99772D-03\n",
      "\n",
      "At iterate   10    f=  7.88294D-03    |proj g|=  2.15641D-03\n",
      "\n",
      "At iterate   11    f=  4.33228D-03    |proj g|=  1.23304D-03\n",
      "\n",
      "At iterate   12    f=  2.30468D-03    |proj g|=  2.82087D-04\n",
      "\n",
      "At iterate   13    f=  9.26479D-04    |proj g|=  5.38712D-04\n",
      "\n",
      "At iterate   14    f=  4.45941D-04    |proj g|=  1.36411D-04\n",
      "\n",
      "At iterate   15    f=  2.59836D-04    |proj g|=  4.76669D-05\n",
      "\n",
      "At iterate   16    f=  1.31079D-04    |proj g|=  1.50633D-05\n",
      "\n",
      "At iterate   17    f=  6.74760D-05    |proj g|=  7.79402D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     17     21      1     0     0   7.794D-06   6.748D-05\n",
      "  F =   6.7476030505156617E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.66604D-01\n",
      "\n",
      "At iterate    1    f=  6.40347D-01    |proj g|=  5.99802D-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate    2    f=  5.34024D-01    |proj g|=  4.77427D-02\n",
      "\n",
      "At iterate    3    f=  2.96223D-01    |proj g|=  4.29150D-02\n",
      "\n",
      "At iterate    4    f=  2.18280D-01    |proj g|=  2.92367D-02\n",
      "\n",
      "At iterate    5    f=  1.39797D-01    |proj g|=  1.65513D-02\n",
      "\n",
      "At iterate    6    f=  9.21239D-02    |proj g|=  9.53971D-03\n",
      "\n",
      "At iterate    7    f=  5.41394D-02    |proj g|=  5.04803D-03\n",
      "\n",
      "At iterate    8    f=  2.79887D-02    |proj g|=  2.22795D-03\n",
      "\n",
      "At iterate    9    f=  1.38546D-02    |proj g|=  8.86151D-04\n",
      "\n",
      "At iterate   10    f=  6.36663D-03    |proj g|=  3.55070D-04\n",
      "\n",
      "At iterate   11    f=  2.94228D-03    |proj g|=  1.88463D-04\n",
      "\n",
      "At iterate   12    f=  2.86955D-03    |proj g|=  1.11943D-03\n",
      "\n",
      "At iterate   13    f=  1.63798D-03    |proj g|=  3.85944D-04\n",
      "\n",
      "At iterate   14    f=  8.41126D-04    |proj g|=  1.86201D-04\n",
      "\n",
      "At iterate   15    f=  4.24317D-04    |proj g|=  9.17684D-05\n",
      "\n",
      "At iterate   16    f=  2.13154D-04    |proj g|=  4.60932D-05\n",
      "\n",
      "At iterate   17    f=  1.06886D-04    |proj g|=  2.31331D-05\n",
      "\n",
      "At iterate   18    f=  5.35546D-05    |proj g|=  1.17518D-05\n",
      "\n",
      "At iterate   19    f=  2.68192D-05    |proj g|=  5.94882D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     19     24      1     0     0   5.949D-06   2.682D-05\n",
      "  F =   2.6819245783309316E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  2.51121D-01\n",
      "\n",
      "At iterate    1    f=  5.72384D-01    |proj g|=  4.35441D-02\n",
      "\n",
      "At iterate    2    f=  5.48845D-01    |proj g|=  3.80456D-02\n",
      "\n",
      "At iterate    3    f=  3.83706D-01    |proj g|=  1.25463D-01\n",
      "\n",
      "At iterate    4    f=  3.21314D-01    |proj g|=  9.04082D-02\n",
      "\n",
      "At iterate    5    f=  2.52710D-01    |proj g|=  4.45081D-02\n",
      "\n",
      "At iterate    6    f=  2.23160D-01    |proj g|=  2.48497D-02\n",
      "\n",
      "At iterate    7    f=  2.02071D-01    |proj g|=  1.34930D-02\n",
      "\n",
      "At iterate    8    f=  1.81666D-01    |proj g|=  9.90115D-03\n",
      "\n",
      "At iterate    9    f=  1.49897D-01    |proj g|=  8.63170D-03\n",
      "\n",
      "At iterate   10    f=  9.55217D-02    |proj g|=  4.95461D-03\n",
      "\n",
      "At iterate   11    f=  4.84597D-02    |proj g|=  1.41264D-02\n",
      "\n",
      "At iterate   12    f=  3.04537D-02    |proj g|=  4.75719D-03\n",
      "\n",
      "At iterate   13    f=  1.89022D-02    |proj g|=  3.03987D-03\n",
      "\n",
      "At iterate   14    f=  1.20737D-02    |proj g|=  1.01686D-02\n",
      "\n",
      "At iterate   15    f=  6.64852D-03    |proj g|=  4.94792D-03\n",
      "\n",
      "At iterate   16    f=  3.81090D-03    |proj g|=  2.28485D-03\n",
      "\n",
      "At iterate   17    f=  2.16206D-03    |proj g|=  9.44810D-04\n",
      "\n",
      "At iterate   18    f=  1.20330D-03    |proj g|=  3.35326D-04\n",
      "\n",
      "At iterate   19    f=  6.48707D-04    |proj g|=  1.01909D-04\n",
      "\n",
      "At iterate   20    f=  3.40276D-04    |proj g|=  4.59574D-05\n",
      "\n",
      "At iterate   21    f=  1.74701D-04    |proj g|=  2.91947D-05\n",
      "\n",
      "At iterate   22    f=  8.87104D-05    |proj g|=  2.41673D-05\n",
      "\n",
      "At iterate   23    f=  4.93964D-05    |proj g|=  1.74326D-05\n",
      "\n",
      "At iterate   24    f=  1.92957D-05    |proj g|=  3.26226D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     24     27      1     0     0   3.262D-06   1.930D-05\n",
      "  F =   1.9295744838038304E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  6.59940D-02\n",
      "\n",
      "At iterate    1    f=  6.84499D-01    |proj g|=  5.50444D-02\n",
      "\n",
      "At iterate    2    f=  5.47291D-01    |proj g|=  7.71852D-02\n",
      "\n",
      "At iterate    3    f=  3.85073D-01    |proj g|=  2.28054D-02\n",
      "\n",
      "At iterate    4    f=  3.37079D-01    |proj g|=  1.54392D-02\n",
      "\n",
      "At iterate    5    f=  2.92476D-01    |proj g|=  1.09970D-02\n",
      "\n",
      "At iterate    6    f=  2.30636D-01    |proj g|=  8.25944D-03\n",
      "\n",
      "At iterate    7    f=  1.50463D-01    |proj g|=  9.39590D-03\n",
      "\n",
      "At iterate    8    f=  1.04652D-01    |proj g|=  2.25266D-02\n",
      "\n",
      "At iterate    9    f=  7.43618D-02    |proj g|=  1.11880D-02\n",
      "\n",
      "At iterate   10    f=  5.33614D-02    |proj g|=  4.60389D-03\n",
      "\n",
      "At iterate   11    f=  3.98121D-02    |proj g|=  2.76770D-03\n",
      "\n",
      "At iterate   12    f=  3.17422D-02    |proj g|=  1.31173D-02\n",
      "\n",
      "At iterate   13    f=  2.00915D-02    |proj g|=  3.06520D-03\n",
      "\n",
      "At iterate   14    f=  1.72211D-02    |proj g|=  1.87529D-03\n",
      "\n",
      "At iterate   15    f=  1.44960D-02    |proj g|=  1.21606D-03\n",
      "\n",
      "At iterate   16    f=  1.25099D-02    |proj g|=  1.62623D-03\n",
      "\n",
      "At iterate   17    f=  9.88674D-03    |proj g|=  1.84574D-03\n",
      "\n",
      "At iterate   18    f=  6.14745D-03    |proj g|=  1.55408D-03\n",
      "\n",
      "At iterate   19    f=  5.65092D-03    |proj g|=  3.85947D-03\n",
      "\n",
      "At iterate   20    f=  1.91394D-03    |proj g|=  1.24520D-03\n",
      "\n",
      "At iterate   21    f=  1.02103D-03    |proj g|=  5.11325D-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " 92%|█████████▏| 12/13 [00:01<00:00,  8.50it/s] This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "100%|██████████| 13/13 [00:01<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   22    f=  5.35243D-04    |proj g|=  1.82246D-04\n",
      "\n",
      "At iterate   23    f=  2.75157D-04    |proj g|=  6.33532D-05\n",
      "\n",
      "At iterate   24    f=  1.36779D-04    |proj g|=  2.59654D-05\n",
      "\n",
      "At iterate   25    f=  6.83724D-05    |proj g|=  1.25109D-05\n",
      "\n",
      "At iterate   26    f=  3.41491D-05    |proj g|=  4.73596D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     26     30      1     0     0   4.736D-06   3.415D-05\n",
      "  F =   3.4149085808641755E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  5.24768D-02\n",
      "\n",
      "At iterate    1    f=  6.83820D-01    |proj g|=  6.62656D-02\n",
      "\n",
      "At iterate    2    f=  6.35147D-01    |proj g|=  6.72977D-02\n",
      "\n",
      "At iterate    3    f=  3.38419D-01    |proj g|=  2.98141D-02\n",
      "\n",
      "At iterate    4    f=  2.66468D-01    |proj g|=  2.13265D-02\n",
      "\n",
      "At iterate    5    f=  1.69269D-01    |proj g|=  1.58115D-02\n",
      "\n",
      "At iterate    6    f=  1.21175D-01    |proj g|=  8.36962D-03\n",
      "\n",
      "At iterate    7    f=  8.53282D-02    |proj g|=  5.67945D-03\n",
      "\n",
      "At iterate    8    f=  5.83487D-02    |proj g|=  3.07476D-03\n",
      "\n",
      "At iterate    9    f=  4.13723D-02    |proj g|=  1.85197D-03\n",
      "\n",
      "At iterate   10    f=  2.68739D-02    |proj g|=  7.02263D-03\n",
      "\n",
      "At iterate   11    f=  1.79105D-02    |proj g|=  3.52474D-03\n",
      "\n",
      "At iterate   12    f=  1.02888D-02    |proj g|=  1.48886D-03\n",
      "\n",
      "At iterate   13    f=  6.11187D-03    |proj g|=  7.68635D-04\n",
      "\n",
      "At iterate   14    f=  4.52788D-03    |proj g|=  3.29875D-03\n",
      "\n",
      "At iterate   15    f=  1.75788D-03    |proj g|=  7.42232D-04\n",
      "\n",
      "At iterate   16    f=  1.14742D-03    |proj g|=  3.79646D-04\n",
      "\n",
      "At iterate   17    f=  6.04911D-04    |proj g|=  1.49361D-04\n",
      "\n",
      "At iterate   18    f=  3.51962D-04    |proj g|=  8.84713D-05\n",
      "\n",
      "At iterate   19    f=  1.90710D-04    |proj g|=  4.57090D-05\n",
      "\n",
      "At iterate   20    f=  9.43440D-05    |proj g|=  2.10106D-05\n",
      "\n",
      "At iterate   21    f=  4.85841D-05    |proj g|=  1.50095D-05\n",
      "\n",
      "At iterate   22    f=  3.26805D-05    |proj g|=  9.65051D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     22     26      1     0     0   9.651D-06   3.268D-05\n",
      "  F =   3.2680535651212315E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 5405.03it/s]\n",
      "100%|██████████| 13/13 [00:00<00:00, 768.50it/s]\n",
      "100%|██████████| 13/13 [00:00<00:00, 5629.94it/s]\n",
      "100%|██████████| 13/13 [00:00<00:00, 877.22it/s]\n",
      "100%|██████████| 13/13 [00:00<00:00, 260.09it/s]\n",
      "/storage/work/eak5582/Research/generalized_mlm_2.py:404: ClusterWarning: scipy.cluster: The symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix\n",
      "  LocalModelsTree = linkage(self.dist_mat_avg, 'ward')\n",
      "  0%|          | 0/10 [00:00<?, ?it/s] This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " 40%|████      | 4/10 [00:00<00:00, 28.39it/s] This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.48179D-01\n",
      "\n",
      "At iterate    1    f=  6.50415D-01    |proj g|=  5.96962D-02\n",
      "\n",
      "At iterate    2    f=  5.76751D-01    |proj g|=  5.53494D-02\n",
      "\n",
      "At iterate    3    f=  3.39796D-01    |proj g|=  2.11798D-02\n",
      "\n",
      "At iterate    4    f=  2.73100D-01    |proj g|=  2.67846D-02\n",
      "\n",
      "At iterate    5    f=  2.20228D-01    |proj g|=  1.72437D-02\n",
      "\n",
      "At iterate    6    f=  1.73048D-01    |proj g|=  8.36011D-03\n",
      "\n",
      "At iterate    7    f=  1.31968D-01    |proj g|=  3.82543D-03\n",
      "\n",
      "At iterate    8    f=  8.96456D-02    |proj g|=  3.30405D-03\n",
      "\n",
      "At iterate    9    f=  6.42313D-02    |proj g|=  4.37359D-03\n",
      "\n",
      "At iterate   10    f=  3.87982D-02    |proj g|=  2.92639D-03\n",
      "\n",
      "At iterate   11    f=  2.31543D-02    |proj g|=  1.26086D-03\n",
      "\n",
      "At iterate   12    f=  2.10386D-02    |proj g|=  7.79790D-03\n",
      "\n",
      "At iterate   13    f=  1.36636D-02    |proj g|=  3.34584D-03\n",
      "\n",
      "At iterate   14    f=  8.67574D-03    |proj g|=  1.64229D-03\n",
      "\n",
      "At iterate   15    f=  4.79655D-03    |proj g|=  5.41599D-04\n",
      "\n",
      "At iterate   16    f=  2.48887D-03    |proj g|=  2.76158D-04\n",
      "\n",
      "At iterate   17    f=  1.30191D-03    |proj g|=  3.20780D-04\n",
      "\n",
      "At iterate   18    f=  4.87634D-04    |proj g|=  5.64031D-05\n",
      "\n",
      "At iterate   19    f=  2.93136D-04    |proj g|=  3.31680D-05\n",
      "\n",
      "At iterate   20    f=  1.36979D-04    |proj g|=  1.53826D-05\n",
      "\n",
      "At iterate   21    f=  7.05359D-05    |proj g|=  8.08794D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     21     26      1     0     0   8.088D-06   7.054D-05\n",
      "  F =   7.0535859274670245E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.29768D-01\n",
      "\n",
      "At iterate    1    f=  6.62110D-01    |proj g|=  7.21341D-02\n",
      "\n",
      "At iterate    2    f=  5.31209D-01    |proj g|=  5.94053D-02\n",
      "\n",
      "At iterate    3    f=  2.71985D-01    |proj g|=  2.63084D-02\n",
      "\n",
      "At iterate    4    f=  1.96949D-01    |proj g|=  2.38061D-02\n",
      "\n",
      "At iterate    5    f=  1.08614D-01    |proj g|=  1.55783D-02\n",
      "\n",
      "At iterate    6    f=  6.39229D-02    |proj g|=  8.79677D-03\n",
      "\n",
      "At iterate    7    f=  3.66611D-02    |proj g|=  5.95986D-03\n",
      "\n",
      "At iterate    8    f=  1.98943D-02    |proj g|=  3.70735D-03\n",
      "\n",
      "At iterate    9    f=  1.05383D-02    |proj g|=  2.07587D-03\n",
      "\n",
      "At iterate   10    f=  5.49355D-03    |proj g|=  1.12176D-03\n",
      "\n",
      "At iterate   11    f=  2.81554D-03    |proj g|=  5.75528D-04\n",
      "\n",
      "At iterate   12    f=  1.43123D-03    |proj g|=  2.58578D-04\n",
      "\n",
      "At iterate   13    f=  8.79777D-04    |proj g|=  6.45613D-04\n",
      "\n",
      "At iterate   14    f=  1.62615D-04    |proj g|=  8.42933D-05\n",
      "\n",
      "At iterate   15    f=  1.16671D-04    |proj g|=  4.98981D-05\n",
      "\n",
      "At iterate   16    f=  5.59409D-05    |proj g|=  1.34855D-05\n",
      "\n",
      "At iterate   17    f=  3.10242D-05    |proj g|=  3.40163D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     17     21      1     0     0   3.402D-06   3.102D-05\n",
      "  F =   3.1024172077190563E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  2.71568D-01\n",
      "\n",
      "At iterate    1    f=  5.40189D-01    |proj g|=  4.17711D-02\n",
      "\n",
      "At iterate    2    f=  5.20624D-01    |proj g|=  4.02663D-02\n",
      "\n",
      "At iterate    3    f=  4.06158D-01    |proj g|=  1.28622D-01\n",
      "\n",
      "At iterate    4    f=  3.36694D-01    |proj g|=  1.07137D-01\n",
      "\n",
      "At iterate    5    f=  2.33174D-01    |proj g|=  4.92493D-02\n",
      "\n",
      "At iterate    6    f=  1.85867D-01    |proj g|=  2.26758D-02\n",
      "\n",
      "At iterate    7    f=  1.53577D-01    |proj g|=  7.72682D-03\n",
      "\n",
      "At iterate    8    f=  1.30143D-01    |proj g|=  7.60256D-03\n",
      "\n",
      "At iterate    9    f=  1.02928D-01    |proj g|=  7.22686D-03\n",
      "\n",
      "At iterate   10    f=  6.33833D-02    |proj g|=  7.45005D-03\n",
      "\n",
      "At iterate   11    f=  5.32624D-02    |proj g|=  4.82857D-03\n",
      "\n",
      "At iterate   12    f=  4.34862D-02    |proj g|=  1.85510D-03\n",
      "\n",
      "At iterate   13    f=  3.92107D-02    |proj g|=  1.79277D-03\n",
      "\n",
      "At iterate   14    f=  3.89816D-02    |proj g|=  1.28073D-03\n",
      "\n",
      "At iterate   15    f=  3.68965D-02    |proj g|=  1.61160D-03\n",
      "\n",
      "At iterate   16    f=  3.43308D-02    |proj g|=  1.84335D-03\n",
      "\n",
      "At iterate   17    f=  2.39807D-02    |proj g|=  4.41919D-03\n",
      "\n",
      "At iterate   18    f=  2.19479D-02    |proj g|=  9.79535D-04\n",
      "\n",
      "At iterate   19    f=  1.97288D-02    |proj g|=  1.00230D-03\n",
      "\n",
      "At iterate   20    f=  1.46992D-02    |proj g|=  1.29289D-03\n",
      "\n",
      "At iterate   21    f=  1.00147D-02    |proj g|=  1.92434D-03\n",
      "\n",
      "At iterate   22    f=  5.45129D-03    |proj g|=  7.14695D-04\n",
      "\n",
      "At iterate   23    f=  2.69451D-03    |proj g|=  5.53339D-04\n",
      "\n",
      "At iterate   24    f=  1.35168D-03    |proj g|=  2.83078D-04\n",
      "\n",
      "At iterate   25    f=  7.34828D-04    |proj g|=  3.99243D-04\n",
      "\n",
      "At iterate   26    f=  3.96694D-04    |proj g|=  3.59953D-04\n",
      "\n",
      "At iterate   27    f=  1.16343D-04    |proj g|=  5.08729D-05\n",
      "\n",
      "At iterate   28    f=  8.52376D-05    |proj g|=  3.13449D-05\n",
      "\n",
      "At iterate   29    f=  3.98356D-05    |proj g|=  8.67515D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     29     33      1     0     0   8.675D-06   3.984D-05\n",
      "  F =   3.9835631727010838E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.48354D-01\n",
      "\n",
      "At iterate    1    f=  6.53635D-01    |proj g|=  6.60170D-02\n",
      "\n",
      "At iterate    2    f=  6.13120D-01    |proj g|=  4.72671D-02\n",
      "\n",
      "At iterate    3    f=  5.36305D-01    |proj g|=  4.01945D-02\n",
      "\n",
      "At iterate    4    f=  4.55996D-01    |proj g|=  4.77213D-02\n",
      "\n",
      "At iterate    5    f=  3.29483D-01    |proj g|=  2.33675D-02\n",
      "\n",
      "At iterate    6    f=  2.63772D-01    |proj g|=  1.46635D-02\n",
      "\n",
      "At iterate    7    f=  2.17834D-01    |proj g|=  1.27199D-02\n",
      "\n",
      "At iterate    8    f=  1.72949D-01    |proj g|=  1.01105D-02\n",
      "\n",
      "At iterate    9    f=  1.33799D-01    |proj g|=  9.72212D-03\n",
      "\n",
      "At iterate   10    f=  1.01558D-01    |proj g|=  4.86223D-03\n",
      "\n",
      "At iterate   11    f=  9.02168D-02    |proj g|=  3.42338D-03\n",
      "\n",
      "At iterate   12    f=  7.17776D-02    |proj g|=  1.17763D-02\n",
      "\n",
      "At iterate   13    f=  7.13404D-02    |proj g|=  2.70581D-02\n",
      "\n",
      "At iterate   14    f=  5.71982D-02    |proj g|=  7.35003D-03\n",
      "\n",
      "At iterate   15    f=  5.49960D-02    |proj g|=  2.26508D-03\n",
      "\n",
      "At iterate   16    f=  5.36533D-02    |proj g|=  2.77800D-03\n",
      "\n",
      "At iterate   17    f=  5.21725D-02    |proj g|=  3.24479D-03\n",
      "\n",
      "At iterate   18    f=  4.65786D-02    |proj g|=  2.63165D-03\n",
      "\n",
      "At iterate   19    f=  4.40849D-02    |proj g|=  5.65063D-03\n",
      "\n",
      "At iterate   20    f=  3.90504D-02    |proj g|=  1.64339D-03\n",
      "\n",
      "At iterate   21    f=  3.75451D-02    |proj g|=  1.24187D-03\n",
      "\n",
      "At iterate   22    f=  3.64916D-02    |proj g|=  1.26604D-03\n",
      "\n",
      "At iterate   23    f=  3.46562D-02    |proj g|=  8.54715D-04\n",
      "\n",
      "At iterate   24    f=  3.27846D-02    |proj g|=  1.58893D-03\n",
      "\n",
      "At iterate   25    f=  3.04404D-02    |proj g|=  1.86860D-03\n",
      "\n",
      "At iterate   26    f=  3.00879D-02    |proj g|=  9.80265D-04\n",
      "\n",
      "At iterate   27    f=  2.95586D-02    |proj g|=  8.35306D-04\n",
      "\n",
      "At iterate   28    f=  2.83229D-02    |proj g|=  7.38043D-04\n",
      "\n",
      "At iterate   29    f=  2.35101D-02    |proj g|=  1.74094D-03\n",
      "\n",
      "At iterate   30    f=  2.06580D-02    |proj g|=  1.66744D-03\n",
      "\n",
      "At iterate   31    f=  1.88359D-02    |proj g|=  9.72835D-04\n",
      "\n",
      "At iterate   32    f=  1.61407D-02    |proj g|=  1.16207D-03\n",
      "\n",
      "At iterate   33    f=  1.34321D-02    |proj g|=  1.53785D-03\n",
      "\n",
      "At iterate   34    f=  1.10223D-02    |proj g|=  3.04891D-03\n",
      "\n",
      "At iterate   35    f=  5.73447D-03    |proj g|=  9.19602D-04\n",
      "\n",
      "At iterate   36    f=  4.42077D-03    |proj g|=  1.21628D-03\n",
      "\n",
      "At iterate   37    f=  2.14984D-03    |proj g|=  5.41746D-04\n",
      "\n",
      "At iterate   38    f=  1.22709D-03    |proj g|=  2.67877D-04\n",
      "\n",
      "At iterate   39    f=  6.07863D-04    |proj g|=  1.27119D-04\n",
      "\n",
      "At iterate   40    f=  3.09039D-04    |proj g|=  6.63854D-05\n",
      "\n",
      "At iterate   41    f=  1.55839D-04    |proj g|=  3.43489D-05\n",
      "\n",
      "At iterate   42    f=  7.81950D-05    |proj g|=  2.64543D-05\n",
      "\n",
      "At iterate   43    f=  3.64450D-05    |proj g|=  7.66479D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     43     48      1     0     0   7.665D-06   3.644D-05\n",
      "  F =   3.6444951174735735E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.73988D-01\n",
      "\n",
      "At iterate    1    f=  6.40218D-01    |proj g|=  6.41361D-02\n",
      "\n",
      "At iterate    2    f=  5.37000D-01    |proj g|=  5.15819D-02\n",
      "\n",
      "At iterate    3    f=  3.17889D-01    |proj g|=  4.43235D-02\n",
      "\n",
      "At iterate    4    f=  2.45469D-01    |proj g|=  3.11906D-02\n",
      "\n",
      "At iterate    5    f=  1.72768D-01    |proj g|=  1.50510D-02\n",
      "\n",
      "At iterate    6    f=  1.32388D-01    |proj g|=  5.40183D-03\n",
      "\n",
      "At iterate    7    f=  1.03301D-01    |proj g|=  5.29130D-03\n",
      "\n",
      "At iterate    8    f=  7.88208D-02    |proj g|=  6.61178D-03\n",
      "\n",
      "At iterate    9    f=  5.59934D-02    |proj g|=  2.72985D-03\n",
      "\n",
      "At iterate   10    f=  4.43817D-02    |proj g|=  1.01392D-03\n",
      "\n",
      "At iterate   11    f=  3.64508D-02    |proj g|=  1.55063D-03\n",
      "\n",
      "At iterate   12    f=  3.59548D-02    |proj g|=  3.36474D-03\n",
      "\n",
      "At iterate   13    f=  3.23629D-02    |proj g|=  1.82744D-03\n",
      "\n",
      "At iterate   14    f=  3.03235D-02    |proj g|=  1.55511D-03\n",
      "\n",
      "At iterate   15    f=  2.73785D-02    |proj g|=  1.70080D-03\n",
      "\n",
      "At iterate   16    f=  2.27639D-02    |proj g|=  1.55998D-03\n",
      "\n",
      "At iterate   17    f=  1.66592D-02    |proj g|=  1.75764D-03\n",
      "\n",
      "At iterate   18    f=  1.09027D-02    |proj g|=  1.69678D-03\n",
      "\n",
      "At iterate   19    f=  8.40197D-03    |proj g|=  1.19809D-03\n",
      "\n",
      "At iterate   20    f=  5.94169D-03    |proj g|=  8.06345D-04\n",
      "\n",
      "At iterate   21    f=  3.93396D-03    |proj g|=  5.45241D-04\n",
      "\n",
      "At iterate   22    f=  1.89602D-03    |proj g|=  2.05150D-04\n",
      "\n",
      "At iterate   23    f=  1.06710D-03    |proj g|=  7.01665D-04\n",
      "\n",
      "At iterate   24    f=  4.55763D-04    |proj g|=  2.22309D-04\n",
      "\n",
      "At iterate   25    f=  2.49507D-04    |proj g|=  9.97279D-05\n",
      "\n",
      "At iterate   26    f=  1.22360D-04    |proj g|=  3.87524D-05\n",
      "\n",
      "At iterate   27    f=  6.23678D-05    |proj g|=  1.56776D-05\n",
      "\n",
      "At iterate   28    f=  3.12386D-05    |proj g|=  6.09156D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     28     33      1     0     0   6.092D-06   3.124D-05\n",
      "  F =   3.1238585357170255E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  4.70948D-02\n",
      "\n",
      "At iterate    1    f=  6.84917D-01    |proj g|=  6.13633D-02\n",
      "\n",
      "At iterate    2    f=  6.30678D-01    |proj g|=  6.51997D-02\n",
      "\n",
      "At iterate    3    f=  4.09638D-01    |proj g|=  1.41465D-02\n",
      "\n",
      "At iterate    4    f=  3.70099D-01    |proj g|=  9.06713D-03\n",
      "\n",
      "At iterate    5    f=  3.03332D-01    |proj g|=  8.36408D-03\n",
      "\n",
      "At iterate    6    f=  2.54801D-01    |proj g|=  5.41568D-03\n",
      "\n",
      "At iterate    7    f=  2.21839D-01    |proj g|=  1.66676D-02\n",
      "\n",
      "At iterate    8    f=  1.86628D-01    |proj g|=  2.97199D-03\n",
      "\n",
      "At iterate    9    f=  1.75691D-01    |proj g|=  2.64217D-03\n",
      "\n",
      "At iterate   10    f=  1.50560D-01    |proj g|=  3.96994D-03\n",
      "\n",
      "At iterate   11    f=  1.34936D-01    |proj g|=  5.19836D-03\n",
      "\n",
      "At iterate   12    f=  1.34404D-01    |proj g|=  3.41595D-03\n",
      "\n",
      "At iterate   13    f=  1.25664D-01    |proj g|=  3.29941D-03\n",
      "\n",
      "At iterate   14    f=  1.15075D-01    |proj g|=  2.16075D-03\n",
      "\n",
      "At iterate   15    f=  1.04642D-01    |proj g|=  2.01821D-03\n",
      "\n",
      "At iterate   16    f=  9.67479D-02    |proj g|=  2.98326D-03\n",
      "\n",
      "At iterate   17    f=  9.17313D-02    |proj g|=  1.30701D-03\n",
      "\n",
      "At iterate   18    f=  9.00086D-02    |proj g|=  1.25895D-03\n",
      "\n",
      "At iterate   19    f=  8.76381D-02    |proj g|=  2.23063D-03\n",
      "\n",
      "At iterate   20    f=  8.65940D-02    |proj g|=  1.38180D-03\n",
      "\n",
      "At iterate   21    f=  8.47944D-02    |proj g|=  1.55354D-03\n",
      "\n",
      "At iterate   22    f=  8.42804D-02    |proj g|=  1.62202D-03\n",
      "\n",
      "At iterate   23    f=  8.42346D-02    |proj g|=  6.63754D-04\n",
      "\n",
      "At iterate   24    f=  8.39488D-02    |proj g|=  6.59421D-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   25    f=  8.30376D-02    |proj g|=  7.63667D-04\n",
      "\n",
      "At iterate   26    f=  8.17496D-02    |proj g|=  5.98894D-04\n",
      "\n",
      "At iterate   27    f=  8.04790D-02    |proj g|=  1.67891D-03\n",
      "\n",
      "At iterate   28    f=  7.94312D-02    |proj g|=  1.50785D-03\n",
      "\n",
      "At iterate   29    f=  7.90224D-02    |proj g|=  8.90329D-04\n",
      "\n",
      "At iterate   30    f=  7.85158D-02    |proj g|=  6.07779D-04\n",
      "\n",
      "At iterate   31    f=  7.80924D-02    |proj g|=  1.00950D-03\n",
      "\n",
      "At iterate   32    f=  7.75173D-02    |proj g|=  9.81147D-04\n",
      "\n",
      "At iterate   33    f=  7.67095D-02    |proj g|=  1.29059D-03\n",
      "\n",
      "At iterate   34    f=  7.65284D-02    |proj g|=  1.48007D-03\n",
      "\n",
      "At iterate   35    f=  7.63250D-02    |proj g|=  6.79205D-04\n",
      "\n",
      "At iterate   36    f=  7.62312D-02    |proj g|=  6.60229D-04\n",
      "\n",
      "At iterate   37    f=  7.60267D-02    |proj g|=  6.84455D-04\n",
      "\n",
      "At iterate   38    f=  7.55811D-02    |proj g|=  1.13836D-03\n",
      "\n",
      "At iterate   39    f=  7.49043D-02    |proj g|=  1.36447D-03\n",
      "\n",
      "At iterate   40    f=  7.35670D-02    |proj g|=  1.25115D-03\n",
      "\n",
      "At iterate   41    f=  7.29731D-02    |proj g|=  2.14354D-03\n",
      "\n",
      "At iterate   42    f=  7.13098D-02    |proj g|=  1.84075D-03\n",
      "\n",
      "At iterate   43    f=  6.94984D-02    |proj g|=  9.48622D-04\n",
      "\n",
      "At iterate   44    f=  6.88589D-02    |proj g|=  1.06784D-03\n",
      "\n",
      "At iterate   45    f=  6.87912D-02    |proj g|=  1.82008D-03\n",
      "\n",
      "At iterate   46    f=  6.84356D-02    |proj g|=  1.41537D-03\n",
      "\n",
      "At iterate   47    f=  6.78857D-02    |proj g|=  1.05124D-03\n",
      "\n",
      "At iterate   48    f=  6.75167D-02    |proj g|=  9.30444D-04\n",
      "\n",
      "At iterate   49    f=  6.64973D-02    |proj g|=  1.08946D-03\n",
      "\n",
      "At iterate   50    f=  6.55231D-02    |proj g|=  2.23606D-03\n",
      "\n",
      "At iterate   51    f=  6.44719D-02    |proj g|=  9.24688D-04\n",
      "\n",
      "At iterate   52    f=  6.41305D-02    |proj g|=  1.05565D-03\n",
      "\n",
      "At iterate   53    f=  6.39509D-02    |proj g|=  1.23379D-03\n",
      "\n",
      "At iterate   54    f=  6.34556D-02    |proj g|=  1.13299D-03\n",
      "\n",
      "At iterate   55    f=  6.31010D-02    |proj g|=  1.12090D-03\n",
      "\n",
      "At iterate   56    f=  6.29394D-02    |proj g|=  1.92144D-03\n",
      "\n",
      "At iterate   57    f=  6.26660D-02    |proj g|=  7.73639D-04\n",
      "\n",
      "At iterate   58    f=  6.25554D-02    |proj g|=  5.07999D-04\n",
      "\n",
      "At iterate   59    f=  6.24244D-02    |proj g|=  8.35215D-04\n",
      "\n",
      "At iterate   60    f=  6.22000D-02    |proj g|=  8.23523D-04\n",
      "\n",
      "At iterate   61    f=  6.15299D-02    |proj g|=  5.47997D-04\n",
      "\n",
      "At iterate   62    f=  6.13168D-02    |proj g|=  7.76753D-04\n",
      "\n",
      "At iterate   63    f=  6.12469D-02    |proj g|=  6.02637D-04\n",
      "\n",
      "At iterate   64    f=  6.11365D-02    |proj g|=  5.03156D-04\n",
      "\n",
      "At iterate   65    f=  6.10325D-02    |proj g|=  6.14278D-04\n",
      "\n",
      "At iterate   66    f=  6.07713D-02    |proj g|=  5.64120D-04\n",
      "\n",
      "At iterate   67    f=  6.06965D-02    |proj g|=  1.04188D-03\n",
      "\n",
      "At iterate   68    f=  6.05579D-02    |proj g|=  5.91200D-04\n",
      "\n",
      "At iterate   69    f=  6.04582D-02    |proj g|=  6.51132D-04\n",
      "\n",
      "At iterate   70    f=  6.03628D-02    |proj g|=  4.70456D-04\n",
      "\n",
      "At iterate   71    f=  6.03279D-02    |proj g|=  8.50804D-04\n",
      "\n",
      "At iterate   72    f=  6.02453D-02    |proj g|=  4.53608D-04\n",
      "\n",
      "At iterate   73    f=  6.01902D-02    |proj g|=  3.21381D-04\n",
      "\n",
      "At iterate   74    f=  6.01387D-02    |proj g|=  3.92815D-04\n",
      "\n",
      "At iterate   75    f=  6.00653D-02    |proj g|=  4.86867D-04\n",
      "\n",
      "At iterate   76    f=  5.99291D-02    |proj g|=  6.22102D-04\n",
      "\n",
      "At iterate   77    f=  5.98735D-02    |proj g|=  3.36217D-04\n",
      "\n",
      "At iterate   78    f=  5.98018D-02    |proj g|=  3.35745D-04\n",
      "\n",
      "At iterate   79    f=  5.97745D-02    |proj g|=  5.07521D-04\n",
      "\n",
      "At iterate   80    f=  5.96775D-02    |proj g|=  4.54086D-04\n",
      "\n",
      "At iterate   81    f=  5.95300D-02    |proj g|=  2.59086D-04\n",
      "\n",
      "At iterate   82    f=  5.93834D-02    |proj g|=  3.85184D-04\n",
      "\n",
      "At iterate   83    f=  5.93123D-02    |proj g|=  4.18725D-04\n",
      "\n",
      "At iterate   84    f=  5.92288D-02    |proj g|=  2.00557D-04\n",
      "\n",
      "At iterate   85    f=  5.91304D-02    |proj g|=  2.56293D-04\n",
      "\n",
      "At iterate   86    f=  5.90077D-02    |proj g|=  4.06494D-04\n",
      "\n",
      "At iterate   87    f=  5.87861D-02    |proj g|=  5.12441D-04\n",
      "\n",
      "At iterate   88    f=  5.87436D-02    |proj g|=  4.74825D-04\n",
      "\n",
      "At iterate   89    f=  5.85697D-02    |proj g|=  2.70327D-04\n",
      "\n",
      "At iterate   90    f=  5.85362D-02    |proj g|=  6.99423D-04\n",
      "\n",
      "At iterate   91    f=  5.84734D-02    |proj g|=  4.53210D-04\n",
      "\n",
      "At iterate   92    f=  5.84012D-02    |proj g|=  2.96994D-04\n",
      "\n",
      "At iterate   93    f=  5.82803D-02    |proj g|=  2.75681D-04\n",
      "\n",
      "At iterate   94    f=  5.81815D-02    |proj g|=  4.37790D-04\n",
      "\n",
      "At iterate   95    f=  5.80853D-02    |proj g|=  1.11665D-03\n",
      "\n",
      "At iterate   96    f=  5.78640D-02    |proj g|=  2.46005D-04\n",
      "\n",
      "At iterate   97    f=  5.77873D-02    |proj g|=  1.97353D-04\n",
      "\n",
      "At iterate   98    f=  5.75951D-02    |proj g|=  3.23420D-04\n",
      "\n",
      "At iterate   99    f=  5.75480D-02    |proj g|=  6.87751D-04\n",
      "\n",
      "At iterate  100    f=  5.73646D-02    |proj g|=  5.01808D-04\n",
      "\n",
      "At iterate  101    f=  5.73223D-02    |proj g|=  6.67263D-04\n",
      "\n",
      "At iterate  102    f=  5.71871D-02    |proj g|=  4.36912D-04\n",
      "\n",
      "At iterate  103    f=  5.70863D-02    |proj g|=  5.27355D-04\n",
      "\n",
      "At iterate  104    f=  5.69239D-02    |proj g|=  4.33138D-04\n",
      "\n",
      "At iterate  105    f=  5.65853D-02    |proj g|=  5.05769D-04\n",
      "\n",
      "At iterate  106    f=  5.63695D-02    |proj g|=  4.45989D-04\n",
      "\n",
      "At iterate  107    f=  5.60600D-02    |proj g|=  5.97084D-04\n",
      "\n",
      "At iterate  108    f=  5.59059D-02    |proj g|=  1.56628D-03\n",
      "\n",
      "At iterate  109    f=  5.55555D-02    |proj g|=  3.66768D-04\n",
      "\n",
      "At iterate  110    f=  5.54604D-02    |proj g|=  2.90934D-04\n",
      "\n",
      "At iterate  111    f=  5.54252D-02    |proj g|=  6.41777D-04\n",
      "\n",
      "At iterate  112    f=  5.53625D-02    |proj g|=  5.13144D-04\n",
      "\n",
      "At iterate  113    f=  5.53168D-02    |proj g|=  3.13202D-04\n",
      "\n",
      "At iterate  114    f=  5.52563D-02    |proj g|=  3.14370D-04\n",
      "\n",
      "At iterate  115    f=  5.51604D-02    |proj g|=  4.35168D-04\n",
      "\n",
      "At iterate  116    f=  5.49722D-02    |proj g|=  3.05802D-04\n",
      "\n",
      "At iterate  117    f=  5.48842D-02    |proj g|=  8.91482D-04\n",
      "\n",
      "At iterate  118    f=  5.46378D-02    |proj g|=  5.76925D-04\n",
      "\n",
      "At iterate  119    f=  5.42829D-02    |proj g|=  2.86349D-04\n",
      "\n",
      "At iterate  120    f=  5.41035D-02    |proj g|=  2.83980D-04\n",
      "\n",
      "At iterate  121    f=  5.39789D-02    |proj g|=  1.30103D-03\n",
      "\n",
      "At iterate  122    f=  5.37881D-02    |proj g|=  8.86367D-04\n",
      "\n",
      "At iterate  123    f=  5.34859D-02    |proj g|=  5.47287D-04\n",
      "\n",
      "At iterate  124    f=  5.33242D-02    |proj g|=  5.83635D-04\n",
      "\n",
      "At iterate  125    f=  5.32527D-02    |proj g|=  1.24412D-03\n",
      "\n",
      "At iterate  126    f=  5.30085D-02    |proj g|=  7.22932D-04\n",
      "\n",
      "At iterate  127    f=  5.28344D-02    |proj g|=  4.12954D-04\n",
      "\n",
      "At iterate  128    f=  5.26961D-02    |proj g|=  3.32050D-04\n",
      "\n",
      "At iterate  129    f=  5.25971D-02    |proj g|=  4.10133D-04\n",
      "\n",
      "At iterate  130    f=  5.24575D-02    |proj g|=  8.20040D-04\n",
      "\n",
      "At iterate  131    f=  5.21062D-02    |proj g|=  6.82827D-04\n",
      "\n",
      "At iterate  132    f=  5.20121D-02    |proj g|=  9.15660D-04\n",
      "\n",
      "At iterate  133    f=  5.15982D-02    |proj g|=  2.95504D-04\n",
      "\n",
      "At iterate  134    f=  5.15510D-02    |proj g|=  1.30020D-03\n",
      "\n",
      "At iterate  135    f=  5.14582D-02    |proj g|=  4.23769D-04\n",
      "\n",
      "At iterate  136    f=  5.14260D-02    |proj g|=  4.40390D-04\n",
      "\n",
      "At iterate  137    f=  5.13870D-02    |proj g|=  6.47883D-04\n",
      "\n",
      "At iterate  138    f=  5.13396D-02    |proj g|=  6.77598D-04\n",
      "\n",
      "At iterate  139    f=  5.10548D-02    |proj g|=  5.73096D-04\n",
      "\n",
      "At iterate  140    f=  5.09981D-02    |proj g|=  6.09144D-04\n",
      "\n",
      "At iterate  141    f=  5.06326D-02    |proj g|=  2.21857D-04\n",
      "\n",
      "At iterate  142    f=  5.04447D-02    |proj g|=  5.11384D-04\n",
      "\n",
      "At iterate  143    f=  5.02802D-02    |proj g|=  4.05040D-04\n",
      "\n",
      "At iterate  144    f=  5.02478D-02    |proj g|=  1.03960D-03\n",
      "\n",
      "At iterate  145    f=  5.01829D-02    |proj g|=  5.84047D-04\n",
      "\n",
      "At iterate  146    f=  5.00835D-02    |proj g|=  6.49737D-04\n",
      "\n",
      "At iterate  147    f=  5.00163D-02    |proj g|=  7.91338D-04\n",
      "\n",
      "At iterate  148    f=  4.99641D-02    |proj g|=  3.72611D-04\n",
      "\n",
      "At iterate  149    f=  4.98704D-02    |proj g|=  3.08690D-04\n",
      "\n",
      "At iterate  150    f=  4.97406D-02    |proj g|=  2.02771D-04\n",
      "\n",
      "At iterate  151    f=  4.95784D-02    |proj g|=  4.49245D-04\n",
      "\n",
      "At iterate  152    f=  4.93347D-02    |proj g|=  2.46815D-04\n",
      "\n",
      "At iterate  153    f=  4.91678D-02    |proj g|=  5.60091D-04\n",
      "\n",
      "At iterate  154    f=  4.90064D-02    |proj g|=  3.11587D-04\n",
      "\n",
      "At iterate  155    f=  4.86225D-02    |proj g|=  9.21387D-04\n",
      "\n",
      "At iterate  156    f=  4.85527D-02    |proj g|=  4.53020D-04\n",
      "\n",
      "At iterate  157    f=  4.85092D-02    |proj g|=  4.45381D-04\n",
      "\n",
      "At iterate  158    f=  4.83971D-02    |proj g|=  2.95356D-04\n",
      "\n",
      "At iterate  159    f=  4.82402D-02    |proj g|=  3.14094D-04\n",
      "\n",
      "At iterate  160    f=  4.80296D-02    |proj g|=  5.03213D-04\n",
      "\n",
      "At iterate  161    f=  4.79365D-02    |proj g|=  5.25456D-04\n",
      "\n",
      "At iterate  162    f=  4.78270D-02    |proj g|=  2.76333D-04\n",
      "\n",
      "At iterate  163    f=  4.77468D-02    |proj g|=  3.25508D-04\n",
      "\n",
      "At iterate  164    f=  4.75626D-02    |proj g|=  2.79313D-04\n",
      "\n",
      "At iterate  165    f=  4.73278D-02    |proj g|=  4.16078D-04\n",
      "\n",
      "At iterate  166    f=  4.71835D-02    |proj g|=  6.02912D-04\n",
      "\n",
      "At iterate  167    f=  4.70137D-02    |proj g|=  8.46954D-04\n",
      "\n",
      "At iterate  168    f=  4.68527D-02    |proj g|=  6.31083D-04\n",
      "\n",
      "At iterate  169    f=  4.67390D-02    |proj g|=  2.64895D-04\n",
      "\n",
      "At iterate  170    f=  4.66860D-02    |proj g|=  2.18823D-04\n",
      "\n",
      "At iterate  171    f=  4.64827D-02    |proj g|=  5.01189D-04\n",
      "\n",
      "At iterate  172    f=  4.64487D-02    |proj g|=  3.46416D-04\n",
      "\n",
      "At iterate  173    f=  4.63809D-02    |proj g|=  3.85846D-04\n",
      "\n",
      "At iterate  174    f=  4.62740D-02    |proj g|=  2.99208D-04\n",
      "\n",
      "At iterate  175    f=  4.60656D-02    |proj g|=  1.40280D-04\n",
      "\n",
      "At iterate  176    f=  4.58393D-02    |proj g|=  1.25428D-04\n",
      "\n",
      "At iterate  177    f=  4.54619D-02    |proj g|=  5.43111D-04\n",
      "\n",
      "At iterate  178    f=  4.52845D-02    |proj g|=  3.28210D-04\n",
      "\n",
      "At iterate  179    f=  4.52124D-02    |proj g|=  4.03736D-04\n",
      "\n",
      "At iterate  180    f=  4.51609D-02    |proj g|=  2.74733D-04\n",
      "\n",
      "At iterate  181    f=  4.51412D-02    |proj g|=  1.90925D-04\n",
      "\n",
      "At iterate  182    f=  4.51332D-02    |proj g|=  1.32534D-04\n",
      "\n",
      "At iterate  183    f=  4.51081D-02    |proj g|=  1.12923D-04\n",
      "\n",
      "At iterate  184    f=  4.51006D-02    |proj g|=  4.04459D-04\n",
      "\n",
      "At iterate  185    f=  4.50824D-02    |proj g|=  2.73002D-04\n",
      "\n",
      "At iterate  186    f=  4.50598D-02    |proj g|=  1.57806D-04\n",
      "\n",
      "At iterate  187    f=  4.50490D-02    |proj g|=  2.20208D-04\n",
      "\n",
      "At iterate  188    f=  4.50185D-02    |proj g|=  2.93846D-04\n",
      "\n",
      "At iterate  189    f=  4.49650D-02    |proj g|=  2.87435D-04\n",
      "\n",
      "At iterate  190    f=  4.48375D-02    |proj g|=  7.04441D-04\n",
      "\n",
      "At iterate  191    f=  4.47746D-02    |proj g|=  3.73953D-04\n",
      "\n",
      "At iterate  192    f=  4.47248D-02    |proj g|=  3.83849D-04\n",
      "\n",
      "At iterate  193    f=  4.46097D-02    |proj g|=  2.19173D-04\n",
      "\n",
      "At iterate  194    f=  4.45722D-02    |proj g|=  2.99552D-04\n",
      "\n",
      "At iterate  195    f=  4.45656D-02    |proj g|=  2.05702D-04\n",
      "\n",
      "At iterate  196    f=  4.45358D-02    |proj g|=  2.00581D-04\n",
      "\n",
      "At iterate  197    f=  4.44811D-02    |proj g|=  3.72236D-04\n",
      "\n",
      "At iterate  198    f=  4.44162D-02    |proj g|=  2.12123D-04\n",
      "\n",
      "At iterate  199    f=  4.43885D-02    |proj g|=  1.93924D-04\n",
      "\n",
      "At iterate  200    f=  4.43445D-02    |proj g|=  1.23817D-04\n",
      "\n",
      "At iterate  201    f=  4.42838D-02    |proj g|=  4.51528D-04\n",
      "\n",
      "At iterate  202    f=  4.42533D-02    |proj g|=  2.37989D-04\n",
      "\n",
      "At iterate  203    f=  4.42159D-02    |proj g|=  2.11965D-04\n",
      "\n",
      "At iterate  204    f=  4.41500D-02    |proj g|=  1.73039D-04\n",
      "\n",
      "At iterate  205    f=  4.40793D-02    |proj g|=  2.26188D-04\n",
      "\n",
      "At iterate  206    f=  4.39896D-02    |proj g|=  3.45343D-04\n",
      "\n",
      "At iterate  207    f=  4.39738D-02    |proj g|=  1.85448D-04\n",
      "\n",
      "At iterate  208    f=  4.39430D-02    |proj g|=  1.80448D-04\n",
      "\n",
      "At iterate  209    f=  4.39316D-02    |proj g|=  1.58693D-04\n",
      "\n",
      "At iterate  210    f=  4.38965D-02    |proj g|=  3.20997D-04\n",
      "\n",
      "At iterate  211    f=  4.38556D-02    |proj g|=  1.99095D-04\n",
      "\n",
      "At iterate  212    f=  4.38111D-02    |proj g|=  1.81602D-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate  213    f=  4.37907D-02    |proj g|=  2.64639D-04\n",
      "\n",
      "At iterate  214    f=  4.37590D-02    |proj g|=  1.87253D-04\n",
      "\n",
      "At iterate  215    f=  4.37210D-02    |proj g|=  3.69626D-04\n",
      "\n",
      "At iterate  216    f=  4.36764D-02    |proj g|=  2.19379D-04\n",
      "\n",
      "At iterate  217    f=  4.36442D-02    |proj g|=  2.35158D-04\n",
      "\n",
      "At iterate  218    f=  4.36218D-02    |proj g|=  2.65514D-04\n",
      "\n",
      "At iterate  219    f=  4.35909D-02    |proj g|=  4.93645D-04\n",
      "\n",
      "At iterate  220    f=  4.35632D-02    |proj g|=  1.93896D-04\n",
      "\n",
      "At iterate  221    f=  4.35457D-02    |proj g|=  2.12284D-04\n",
      "\n",
      "At iterate  222    f=  4.35385D-02    |proj g|=  1.46029D-04\n",
      "\n",
      "At iterate  223    f=  4.35244D-02    |proj g|=  1.11500D-04\n",
      "\n",
      "At iterate  224    f=  4.35128D-02    |proj g|=  2.57015D-04\n",
      "\n",
      "At iterate  225    f=  4.34990D-02    |proj g|=  2.32839D-04\n",
      "\n",
      "At iterate  226    f=  4.34528D-02    |proj g|=  1.76119D-04\n",
      "\n",
      "At iterate  227    f=  4.33944D-02    |proj g|=  1.77788D-04\n",
      "\n",
      "At iterate  228    f=  4.33370D-02    |proj g|=  1.81061D-04\n",
      "\n",
      "At iterate  229    f=  4.33302D-02    |proj g|=  2.15985D-04\n",
      "\n",
      "At iterate  230    f=  4.32900D-02    |proj g|=  1.19614D-04\n",
      "\n",
      "At iterate  231    f=  4.32724D-02    |proj g|=  3.64277D-04\n",
      "\n",
      "At iterate  232    f=  4.32478D-02    |proj g|=  1.45326D-04\n",
      "\n",
      "At iterate  233    f=  4.32259D-02    |proj g|=  1.17177D-04\n",
      "\n",
      "At iterate  234    f=  4.31676D-02    |proj g|=  1.41251D-04\n",
      "\n",
      "At iterate  235    f=  4.31512D-02    |proj g|=  2.92925D-04\n",
      "\n",
      "At iterate  236    f=  4.31160D-02    |proj g|=  2.54137D-04\n",
      "\n",
      "At iterate  237    f=  4.30454D-02    |proj g|=  4.29473D-04\n",
      "\n",
      "At iterate  238    f=  4.30027D-02    |proj g|=  2.85341D-04\n",
      "\n",
      "At iterate  239    f=  4.29607D-02    |proj g|=  9.66609D-05\n",
      "\n",
      "At iterate  240    f=  4.29408D-02    |proj g|=  1.58374D-04\n",
      "\n",
      "At iterate  241    f=  4.29313D-02    |proj g|=  1.56442D-04\n",
      "\n",
      "At iterate  242    f=  4.29170D-02    |proj g|=  2.63997D-04\n",
      "\n",
      "At iterate  243    f=  4.29100D-02    |proj g|=  1.20203D-04\n",
      "\n",
      "At iterate  244    f=  4.29017D-02    |proj g|=  1.00678D-04\n",
      "\n",
      "At iterate  245    f=  4.28752D-02    |proj g|=  1.25380D-04\n",
      "\n",
      "At iterate  246    f=  4.28556D-02    |proj g|=  1.81730D-04\n",
      "\n",
      "At iterate  247    f=  4.28332D-02    |proj g|=  4.39004D-04\n",
      "\n",
      "At iterate  248    f=  4.28076D-02    |proj g|=  2.74879D-04\n",
      "\n",
      "At iterate  249    f=  4.27640D-02    |proj g|=  1.72929D-04\n",
      "\n",
      "At iterate  250    f=  4.27529D-02    |proj g|=  1.18890D-04\n",
      "\n",
      "At iterate  251    f=  4.27400D-02    |proj g|=  1.23892D-04\n",
      "\n",
      "At iterate  252    f=  4.27217D-02    |proj g|=  2.23124D-04\n",
      "\n",
      "At iterate  253    f=  4.27079D-02    |proj g|=  1.81914D-04\n",
      "\n",
      "At iterate  254    f=  4.27022D-02    |proj g|=  3.62209D-04\n",
      "\n",
      "At iterate  255    f=  4.26932D-02    |proj g|=  2.15598D-04\n",
      "\n",
      "At iterate  256    f=  4.26799D-02    |proj g|=  2.70403D-04\n",
      "\n",
      "At iterate  257    f=  4.26653D-02    |proj g|=  4.10583D-04\n",
      "\n",
      "At iterate  258    f=  4.26429D-02    |proj g|=  5.49966D-04\n",
      "\n",
      "At iterate  259    f=  4.26344D-02    |proj g|=  5.12760D-04\n",
      "\n",
      "At iterate  260    f=  4.26096D-02    |proj g|=  4.09435D-04\n",
      "\n",
      "At iterate  261    f=  4.25909D-02    |proj g|=  2.10734D-04\n",
      "\n",
      "At iterate  262    f=  4.25750D-02    |proj g|=  8.73887D-05\n",
      "\n",
      "At iterate  263    f=  4.25541D-02    |proj g|=  3.32521D-04\n",
      "\n",
      "At iterate  264    f=  4.25419D-02    |proj g|=  3.56767D-04\n",
      "\n",
      "At iterate  265    f=  4.25252D-02    |proj g|=  3.99921D-04\n",
      "\n",
      "At iterate  266    f=  4.24891D-02    |proj g|=  2.58192D-04\n",
      "\n",
      "At iterate  267    f=  4.24584D-02    |proj g|=  2.23225D-04\n",
      "\n",
      "At iterate  268    f=  4.24337D-02    |proj g|=  3.39574D-04\n",
      "\n",
      "At iterate  269    f=  4.24278D-02    |proj g|=  1.05032D-04\n",
      "\n",
      "At iterate  270    f=  4.24215D-02    |proj g|=  1.36872D-04\n",
      "\n",
      "At iterate  271    f=  4.24128D-02    |proj g|=  1.87867D-04\n",
      "\n",
      "At iterate  272    f=  4.23838D-02    |proj g|=  2.66022D-04\n",
      "\n",
      "At iterate  273    f=  4.23606D-02    |proj g|=  1.83453D-04\n",
      "\n",
      "At iterate  274    f=  4.23187D-02    |proj g|=  2.11213D-04\n",
      "\n",
      "At iterate  275    f=  4.22916D-02    |proj g|=  2.80148D-04\n",
      "\n",
      "At iterate  276    f=  4.22611D-02    |proj g|=  3.10395D-04\n",
      "\n",
      "At iterate  277    f=  4.22516D-02    |proj g|=  5.92978D-04\n",
      "\n",
      "At iterate  278    f=  4.22149D-02    |proj g|=  3.66569D-04\n",
      "\n",
      "At iterate  279    f=  4.21915D-02    |proj g|=  1.18560D-04\n",
      "\n",
      "At iterate  280    f=  4.21842D-02    |proj g|=  1.35262D-04\n",
      "\n",
      "At iterate  281    f=  4.21775D-02    |proj g|=  1.81452D-04\n",
      "\n",
      "At iterate  282    f=  4.21586D-02    |proj g|=  1.99586D-04\n",
      "\n",
      "At iterate  283    f=  4.21318D-02    |proj g|=  1.74763D-04\n",
      "\n",
      "At iterate  284    f=  4.21073D-02    |proj g|=  1.55840D-04\n",
      "\n",
      "At iterate  285    f=  4.20679D-02    |proj g|=  2.83096D-04\n",
      "\n",
      "At iterate  286    f=  4.20566D-02    |proj g|=  1.69690D-04\n",
      "\n",
      "At iterate  287    f=  4.20500D-02    |proj g|=  1.53404D-04\n",
      "\n",
      "At iterate  288    f=  4.20338D-02    |proj g|=  2.29825D-04\n",
      "\n",
      "At iterate  289    f=  4.20182D-02    |proj g|=  2.79028D-04\n",
      "\n",
      "At iterate  290    f=  4.19968D-02    |proj g|=  1.71828D-04\n",
      "\n",
      "At iterate  291    f=  4.19744D-02    |proj g|=  1.32229D-04\n",
      "\n",
      "At iterate  292    f=  4.19566D-02    |proj g|=  9.02770D-05\n",
      "\n",
      "At iterate  293    f=  4.19391D-02    |proj g|=  2.49680D-04\n",
      "\n",
      "At iterate  294    f=  4.19293D-02    |proj g|=  2.70187D-04\n",
      "\n",
      "At iterate  295    f=  4.19210D-02    |proj g|=  1.14371D-04\n",
      "\n",
      "At iterate  296    f=  4.19168D-02    |proj g|=  1.23113D-04\n",
      "\n",
      "At iterate  297    f=  4.19124D-02    |proj g|=  1.53762D-04\n",
      "\n",
      "At iterate  298    f=  4.18997D-02    |proj g|=  1.70962D-04\n",
      "\n",
      "At iterate  299    f=  4.18852D-02    |proj g|=  2.20429D-04\n",
      "\n",
      "At iterate  300    f=  4.18831D-02    |proj g|=  1.09337D-04\n",
      "\n",
      "At iterate  301    f=  4.18751D-02    |proj g|=  1.02286D-04\n",
      "\n",
      "At iterate  302    f=  4.18225D-02    |proj g|=  6.68397D-05\n",
      "\n",
      "At iterate  303    f=  4.18091D-02    |proj g|=  9.79230D-05\n",
      "\n",
      "At iterate  304    f=  4.17873D-02    |proj g|=  6.68437D-05\n",
      "\n",
      "At iterate  305    f=  4.17823D-02    |proj g|=  2.40946D-04\n",
      "\n",
      "At iterate  306    f=  4.17711D-02    |proj g|=  1.14974D-04\n",
      "\n",
      "At iterate  307    f=  4.17647D-02    |proj g|=  1.30654D-04\n",
      "\n",
      "At iterate  308    f=  4.17544D-02    |proj g|=  1.58328D-04\n",
      "\n",
      "At iterate  309    f=  4.17392D-02    |proj g|=  2.23338D-04\n",
      "\n",
      "At iterate  310    f=  4.17072D-02    |proj g|=  2.68941D-04\n",
      "\n",
      "At iterate  311    f=  4.17033D-02    |proj g|=  4.68279D-04\n",
      "\n",
      "At iterate  312    f=  4.16745D-02    |proj g|=  3.45803D-04\n",
      "\n",
      "At iterate  313    f=  4.16477D-02    |proj g|=  1.88307D-04\n",
      "\n",
      "At iterate  314    f=  4.16271D-02    |proj g|=  1.62997D-04\n",
      "\n",
      "At iterate  315    f=  4.16156D-02    |proj g|=  1.89223D-04\n",
      "\n",
      "At iterate  316    f=  4.16025D-02    |proj g|=  8.11815D-05\n",
      "\n",
      "At iterate  317    f=  4.16001D-02    |proj g|=  3.56200D-04\n",
      "\n",
      "At iterate  318    f=  4.15859D-02    |proj g|=  1.03711D-04\n",
      "\n",
      "At iterate  319    f=  4.15825D-02    |proj g|=  1.16810D-04\n",
      "\n",
      "At iterate  320    f=  4.15782D-02    |proj g|=  1.50499D-04\n",
      "\n",
      "At iterate  321    f=  4.15714D-02    |proj g|=  1.34163D-04\n",
      "\n",
      "At iterate  322    f=  4.15692D-02    |proj g|=  2.68491D-04\n",
      "\n",
      "At iterate  323    f=  4.15567D-02    |proj g|=  1.49686D-04\n",
      "\n",
      "At iterate  324    f=  4.15475D-02    |proj g|=  1.00182D-04\n",
      "\n",
      "At iterate  325    f=  4.15398D-02    |proj g|=  1.68394D-04\n",
      "\n",
      "At iterate  326    f=  4.15312D-02    |proj g|=  2.31114D-04\n",
      "\n",
      "At iterate  327    f=  4.15184D-02    |proj g|=  3.30922D-04\n",
      "\n",
      "At iterate  328    f=  4.15056D-02    |proj g|=  2.22467D-04\n",
      "\n",
      "At iterate  329    f=  4.14889D-02    |proj g|=  1.74880D-04\n",
      "\n",
      "At iterate  330    f=  4.14692D-02    |proj g|=  1.02749D-04\n",
      "\n",
      "At iterate  331    f=  4.14657D-02    |proj g|=  1.46100D-04\n",
      "\n",
      "At iterate  332    f=  4.14541D-02    |proj g|=  9.59264D-05\n",
      "\n",
      "At iterate  333    f=  4.14449D-02    |proj g|=  2.97337D-04\n",
      "\n",
      "At iterate  334    f=  4.14358D-02    |proj g|=  1.19615D-04\n",
      "\n",
      "At iterate  335    f=  4.14291D-02    |proj g|=  1.19607D-04\n",
      "\n",
      "At iterate  336    f=  4.14053D-02    |proj g|=  3.32345D-04\n",
      "\n",
      "At iterate  337    f=  4.13888D-02    |proj g|=  3.18147D-04\n",
      "\n",
      "At iterate  338    f=  4.13757D-02    |proj g|=  6.07323D-04\n",
      "\n",
      "At iterate  339    f=  4.13560D-02    |proj g|=  2.02544D-04\n",
      "\n",
      "At iterate  340    f=  4.13477D-02    |proj g|=  1.10498D-04\n",
      "\n",
      "At iterate  341    f=  4.13445D-02    |proj g|=  1.20203D-04\n",
      "\n",
      "At iterate  342    f=  4.13384D-02    |proj g|=  1.44620D-04\n",
      "\n",
      "At iterate  343    f=  4.13315D-02    |proj g|=  1.72923D-04\n",
      "\n",
      "At iterate  344    f=  4.13112D-02    |proj g|=  1.97702D-04\n",
      "\n",
      "At iterate  345    f=  4.12916D-02    |proj g|=  6.95090D-04\n",
      "\n",
      "At iterate  346    f=  4.12619D-02    |proj g|=  1.65090D-04\n",
      "\n",
      "At iterate  347    f=  4.12539D-02    |proj g|=  7.14251D-05\n",
      "\n",
      "At iterate  348    f=  4.12469D-02    |proj g|=  1.22616D-04\n",
      "\n",
      "At iterate  349    f=  4.12357D-02    |proj g|=  1.25712D-04\n",
      "\n",
      "At iterate  350    f=  4.12184D-02    |proj g|=  2.24281D-04\n",
      "\n",
      "At iterate  351    f=  4.12132D-02    |proj g|=  9.99774D-05\n",
      "\n",
      "At iterate  352    f=  4.12064D-02    |proj g|=  8.27776D-05\n",
      "\n",
      "At iterate  353    f=  4.11956D-02    |proj g|=  5.62825D-05\n",
      "\n",
      "At iterate  354    f=  4.11790D-02    |proj g|=  9.19721D-05\n",
      "\n",
      "At iterate  355    f=  4.11593D-02    |proj g|=  9.39646D-05\n",
      "\n",
      "At iterate  356    f=  4.11499D-02    |proj g|=  2.18534D-04\n",
      "\n",
      "At iterate  357    f=  4.11414D-02    |proj g|=  2.23162D-04\n",
      "\n",
      "At iterate  358    f=  4.11307D-02    |proj g|=  6.26449D-05\n",
      "\n",
      "At iterate  359    f=  4.11287D-02    |proj g|=  3.75359D-05\n",
      "\n",
      "At iterate  360    f=  4.11277D-02    |proj g|=  5.02032D-05\n",
      "\n",
      "At iterate  361    f=  4.11228D-02    |proj g|=  9.84070D-05\n",
      "\n",
      "At iterate  362    f=  4.11097D-02    |proj g|=  1.03364D-04\n",
      "\n",
      "At iterate  363    f=  4.11050D-02    |proj g|=  2.21566D-04\n",
      "\n",
      "At iterate  364    f=  4.10961D-02    |proj g|=  2.01087D-04\n",
      "\n",
      "At iterate  365    f=  4.10850D-02    |proj g|=  1.56747D-04\n",
      "\n",
      "At iterate  366    f=  4.10788D-02    |proj g|=  1.32466D-04\n",
      "\n",
      "At iterate  367    f=  4.10569D-02    |proj g|=  1.61718D-04\n",
      "\n",
      "At iterate  368    f=  4.10545D-02    |proj g|=  1.69039D-04\n",
      "\n",
      "At iterate  369    f=  4.10416D-02    |proj g|=  1.07238D-04\n",
      "\n",
      "At iterate  370    f=  4.10319D-02    |proj g|=  1.45514D-04\n",
      "\n",
      "At iterate  371    f=  4.10250D-02    |proj g|=  7.80160D-05\n",
      "\n",
      "At iterate  372    f=  4.10194D-02    |proj g|=  7.16190D-05\n",
      "\n",
      "At iterate  373    f=  4.10012D-02    |proj g|=  1.07012D-04\n",
      "\n",
      "At iterate  374    f=  4.09986D-02    |proj g|=  8.84133D-05\n",
      "\n",
      "At iterate  375    f=  4.09958D-02    |proj g|=  5.67225D-05\n",
      "\n",
      "At iterate  376    f=  4.09912D-02    |proj g|=  5.01136D-05\n",
      "\n",
      "At iterate  377    f=  4.09816D-02    |proj g|=  6.37577D-05\n",
      "\n",
      "At iterate  378    f=  4.09667D-02    |proj g|=  3.39881D-05\n",
      "\n",
      "At iterate  379    f=  4.09644D-02    |proj g|=  1.96262D-04\n",
      "\n",
      "At iterate  380    f=  4.09533D-02    |proj g|=  7.44139D-05\n",
      "\n",
      "At iterate  381    f=  4.09505D-02    |proj g|=  5.49437D-05\n",
      "\n",
      "At iterate  382    f=  4.09475D-02    |proj g|=  6.54156D-05\n",
      "\n",
      "At iterate  383    f=  4.09430D-02    |proj g|=  8.56367D-05\n",
      "\n",
      "At iterate  384    f=  4.09337D-02    |proj g|=  1.02400D-04\n",
      "\n",
      "At iterate  385    f=  4.09326D-02    |proj g|=  1.79950D-04\n",
      "\n",
      "At iterate  386    f=  4.09210D-02    |proj g|=  1.44750D-04\n",
      "\n",
      "At iterate  387    f=  4.09106D-02    |proj g|=  1.63528D-04\n",
      "\n",
      "At iterate  388    f=  4.09005D-02    |proj g|=  1.33130D-04\n",
      "\n",
      "At iterate  389    f=  4.08910D-02    |proj g|=  9.81165D-05\n",
      "\n",
      "At iterate  390    f=  4.08851D-02    |proj g|=  2.01364D-04\n",
      "\n",
      "At iterate  391    f=  4.08730D-02    |proj g|=  8.98420D-05\n",
      "\n",
      "At iterate  392    f=  4.08681D-02    |proj g|=  5.95896D-05\n",
      "\n",
      "At iterate  393    f=  4.08622D-02    |proj g|=  8.70939D-05\n",
      "\n",
      "At iterate  394    f=  4.08544D-02    |proj g|=  6.62402D-05\n",
      "\n",
      "At iterate  395    f=  4.08359D-02    |proj g|=  8.99602D-05\n",
      "\n",
      "At iterate  396    f=  4.08251D-02    |proj g|=  5.06231D-04\n",
      "\n",
      "At iterate  397    f=  4.07919D-02    |proj g|=  1.37325D-04\n",
      "\n",
      "At iterate  398    f=  4.07809D-02    |proj g|=  8.10016D-05\n",
      "\n",
      "At iterate  399    f=  4.07743D-02    |proj g|=  8.09376D-05\n",
      "\n",
      "At iterate  400    f=  4.07664D-02    |proj g|=  6.74423D-05\n",
      "\n",
      "At iterate  401    f=  4.07604D-02    |proj g|=  9.92484D-05\n",
      "\n",
      "At iterate  402    f=  4.07528D-02    |proj g|=  1.03238D-04\n",
      "\n",
      "At iterate  403    f=  4.07498D-02    |proj g|=  1.05297D-04\n",
      "\n",
      "At iterate  404    f=  4.07447D-02    |proj g|=  1.09628D-04\n",
      "\n",
      "At iterate  405    f=  4.07369D-02    |proj g|=  1.35576D-04\n",
      "\n",
      "At iterate  406    f=  4.07283D-02    |proj g|=  1.71401D-04\n",
      "\n",
      "At iterate  407    f=  4.07225D-02    |proj g|=  1.15751D-04\n",
      "\n",
      "At iterate  408    f=  4.07195D-02    |proj g|=  1.44912D-04\n",
      "\n",
      "At iterate  409    f=  4.07180D-02    |proj g|=  8.76434D-05\n",
      "\n",
      "At iterate  410    f=  4.07165D-02    |proj g|=  6.52145D-05\n",
      "\n",
      "At iterate  411    f=  4.07158D-02    |proj g|=  9.47295D-05\n",
      "\n",
      "At iterate  412    f=  4.07134D-02    |proj g|=  1.33662D-04\n",
      "\n",
      "At iterate  413    f=  4.07078D-02    |proj g|=  1.78692D-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate  414    f=  4.07058D-02    |proj g|=  1.97030D-04\n",
      "\n",
      "At iterate  415    f=  4.06969D-02    |proj g|=  1.62824D-04\n",
      "\n",
      "At iterate  416    f=  4.06851D-02    |proj g|=  6.23082D-05\n",
      "\n",
      "At iterate  417    f=  4.06767D-02    |proj g|=  1.15025D-04\n",
      "\n",
      "At iterate  418    f=  4.06710D-02    |proj g|=  1.46098D-04\n",
      "\n",
      "At iterate  419    f=  4.06603D-02    |proj g|=  1.73290D-04\n",
      "\n",
      "At iterate  420    f=  4.06584D-02    |proj g|=  2.56897D-04\n",
      "\n",
      "At iterate  421    f=  4.06487D-02    |proj g|=  1.77417D-04\n",
      "\n",
      "At iterate  422    f=  4.06431D-02    |proj g|=  4.75614D-05\n",
      "\n",
      "At iterate  423    f=  4.06419D-02    |proj g|=  6.02241D-05\n",
      "\n",
      "At iterate  424    f=  4.06405D-02    |proj g|=  5.84605D-05\n",
      "\n",
      "At iterate  425    f=  4.06394D-02    |proj g|=  1.08018D-04\n",
      "\n",
      "At iterate  426    f=  4.06350D-02    |proj g|=  7.78797D-05\n",
      "\n",
      "At iterate  427    f=  4.06296D-02    |proj g|=  3.78236D-05\n",
      "\n",
      "At iterate  428    f=  4.06236D-02    |proj g|=  8.85524D-05\n",
      "\n",
      "At iterate  429    f=  4.06159D-02    |proj g|=  1.25017D-04\n",
      "\n",
      "At iterate  430    f=  4.06053D-02    |proj g|=  1.39925D-04\n",
      "\n",
      "At iterate  431    f=  4.06048D-02    |proj g|=  1.87821D-04\n",
      "\n",
      "At iterate  432    f=  4.05975D-02    |proj g|=  1.13195D-04\n",
      "\n",
      "At iterate  433    f=  4.05920D-02    |proj g|=  9.35956D-05\n",
      "\n",
      "At iterate  434    f=  4.05896D-02    |proj g|=  8.80006D-05\n",
      "\n",
      "At iterate  435    f=  4.05857D-02    |proj g|=  1.35599D-04\n",
      "\n",
      "At iterate  436    f=  4.05823D-02    |proj g|=  1.69459D-04\n",
      "\n",
      "At iterate  437    f=  4.05747D-02    |proj g|=  1.13714D-04\n",
      "\n",
      "At iterate  438    f=  4.05705D-02    |proj g|=  4.96342D-05\n",
      "\n",
      "At iterate  439    f=  4.05665D-02    |proj g|=  6.14442D-05\n",
      "\n",
      "At iterate  440    f=  4.05640D-02    |proj g|=  1.12890D-04\n",
      "\n",
      "At iterate  441    f=  4.05576D-02    |proj g|=  1.65947D-04\n",
      "\n",
      "At iterate  442    f=  4.05444D-02    |proj g|=  1.66867D-04\n",
      "\n",
      "At iterate  443    f=  4.05416D-02    |proj g|=  2.33735D-04\n",
      "\n",
      "At iterate  444    f=  4.05300D-02    |proj g|=  1.46515D-04\n",
      "\n",
      "At iterate  445    f=  4.05244D-02    |proj g|=  6.06283D-05\n",
      "\n",
      "At iterate  446    f=  4.05219D-02    |proj g|=  9.04166D-05\n",
      "\n",
      "At iterate  447    f=  4.05174D-02    |proj g|=  1.00367D-04\n",
      "\n",
      "At iterate  448    f=  4.05144D-02    |proj g|=  1.20471D-04\n",
      "\n",
      "At iterate  449    f=  4.05093D-02    |proj g|=  4.22014D-05\n",
      "\n",
      "At iterate  450    f=  4.05062D-02    |proj g|=  4.90228D-05\n",
      "\n",
      "At iterate  451    f=  4.04987D-02    |proj g|=  9.84453D-05\n",
      "\n",
      "At iterate  452    f=  4.04889D-02    |proj g|=  1.15325D-04\n",
      "\n",
      "At iterate  453    f=  4.04825D-02    |proj g|=  2.36040D-04\n",
      "\n",
      "At iterate  454    f=  4.04797D-02    |proj g|=  3.13106D-04\n",
      "\n",
      "At iterate  455    f=  4.04708D-02    |proj g|=  1.53030D-04\n",
      "\n",
      "At iterate  456    f=  4.04668D-02    |proj g|=  6.69737D-05\n",
      "\n",
      "At iterate  457    f=  4.04653D-02    |proj g|=  6.91967D-05\n",
      "\n",
      "At iterate  458    f=  4.04595D-02    |proj g|=  1.08574D-04\n",
      "\n",
      "At iterate  459    f=  4.04532D-02    |proj g|=  1.48481D-04\n",
      "\n",
      "At iterate  460    f=  4.04461D-02    |proj g|=  9.82599D-05\n",
      "\n",
      "At iterate  461    f=  4.04384D-02    |proj g|=  6.09587D-05\n",
      "\n",
      "At iterate  462    f=  4.04345D-02    |proj g|=  6.16111D-05\n",
      "\n",
      "At iterate  463    f=  4.04168D-02    |proj g|=  8.49395D-05\n",
      "\n",
      "At iterate  464    f=  4.04063D-02    |proj g|=  1.17313D-04\n",
      "\n",
      "At iterate  465    f=  4.04049D-02    |proj g|=  1.62862D-04\n",
      "\n",
      "At iterate  466    f=  4.04006D-02    |proj g|=  1.18033D-04\n",
      "\n",
      "At iterate  467    f=  4.03944D-02    |proj g|=  1.07075D-04\n",
      "\n",
      "At iterate  468    f=  4.03914D-02    |proj g|=  1.01756D-04\n",
      "\n",
      "At iterate  469    f=  4.03742D-02    |proj g|=  1.23043D-04\n",
      "\n",
      "At iterate  470    f=  4.03663D-02    |proj g|=  1.52744D-04\n",
      "\n",
      "At iterate  471    f=  4.03603D-02    |proj g|=  1.42028D-04\n",
      "\n",
      "At iterate  472    f=  4.03577D-02    |proj g|=  1.54838D-04\n",
      "\n",
      "At iterate  473    f=  4.03531D-02    |proj g|=  1.32964D-04\n",
      "\n",
      "At iterate  474    f=  4.03378D-02    |proj g|=  1.42394D-04\n",
      "\n",
      "At iterate  475    f=  4.03314D-02    |proj g|=  1.35035D-04\n",
      "\n",
      "At iterate  476    f=  4.03303D-02    |proj g|=  7.20217D-05\n",
      "\n",
      "At iterate  477    f=  4.03267D-02    |proj g|=  7.74400D-05\n",
      "\n",
      "At iterate  478    f=  4.03221D-02    |proj g|=  1.78804D-04\n",
      "\n",
      "At iterate  479    f=  4.03000D-02    |proj g|=  1.67060D-04\n",
      "\n",
      "At iterate  480    f=  4.02846D-02    |proj g|=  9.28196D-05\n",
      "\n",
      "At iterate  481    f=  4.02582D-02    |proj g|=  7.17717D-05\n",
      "\n",
      "At iterate  482    f=  4.02559D-02    |proj g|=  8.69779D-05\n",
      "\n",
      "At iterate  483    f=  4.02475D-02    |proj g|=  6.93871D-05\n",
      "\n",
      "At iterate  484    f=  4.02387D-02    |proj g|=  9.22896D-05\n",
      "\n",
      "At iterate  485    f=  4.02234D-02    |proj g|=  7.75969D-05\n",
      "\n",
      "At iterate  486    f=  4.02078D-02    |proj g|=  1.38418D-04\n",
      "\n",
      "At iterate  487    f=  4.02068D-02    |proj g|=  2.00238D-04\n",
      "\n",
      "At iterate  488    f=  4.02011D-02    |proj g|=  1.56967D-04\n",
      "\n",
      "At iterate  489    f=  4.01895D-02    |proj g|=  8.91236D-05\n",
      "\n",
      "At iterate  490    f=  4.01807D-02    |proj g|=  6.92452D-05\n",
      "\n",
      "At iterate  491    f=  4.01581D-02    |proj g|=  1.43402D-04\n",
      "\n",
      "At iterate  492    f=  4.01346D-02    |proj g|=  1.94540D-04\n",
      "\n",
      "At iterate  493    f=  4.01298D-02    |proj g|=  1.05479D-04\n",
      "\n",
      "At iterate  494    f=  4.01121D-02    |proj g|=  6.19295D-05\n",
      "\n",
      "At iterate  495    f=  4.01027D-02    |proj g|=  5.21168D-05\n",
      "\n",
      "At iterate  496    f=  4.00930D-02    |proj g|=  4.63127D-05\n",
      "\n",
      "At iterate  497    f=  4.00825D-02    |proj g|=  8.91245D-05\n",
      "\n",
      "At iterate  498    f=  4.00818D-02    |proj g|=  1.23886D-04\n",
      "\n",
      "At iterate  499    f=  4.00746D-02    |proj g|=  1.01779D-04\n",
      "\n",
      "At iterate  500    f=  4.00700D-02    |proj g|=  4.34626D-05\n",
      "\n",
      "At iterate  501    f=  4.00678D-02    |proj g|=  4.76819D-05\n",
      "\n",
      "At iterate  502    f=  4.00598D-02    |proj g|=  5.64245D-05\n",
      "\n",
      "At iterate  503    f=  4.00492D-02    |proj g|=  2.47026D-04\n",
      "\n",
      "At iterate  504    f=  4.00373D-02    |proj g|=  1.63074D-04\n",
      "\n",
      "At iterate  505    f=  4.00238D-02    |proj g|=  4.71668D-05\n",
      "\n",
      "At iterate  506    f=  4.00169D-02    |proj g|=  7.84091D-05\n",
      "\n",
      "At iterate  507    f=  4.00119D-02    |proj g|=  7.43154D-05\n",
      "\n",
      "At iterate  508    f=  3.99965D-02    |proj g|=  1.89165D-04\n",
      "\n",
      "At iterate  509    f=  3.99955D-02    |proj g|=  2.40098D-04\n",
      "\n",
      "At iterate  510    f=  3.99748D-02    |proj g|=  1.31119D-04\n",
      "\n",
      "At iterate  511    f=  3.99616D-02    |proj g|=  1.36876D-04\n",
      "\n",
      "At iterate  512    f=  3.99493D-02    |proj g|=  8.22576D-05\n",
      "\n",
      "At iterate  513    f=  3.99398D-02    |proj g|=  1.10284D-04\n",
      "\n",
      "At iterate  514    f=  3.99192D-02    |proj g|=  1.57027D-04\n",
      "\n",
      "At iterate  515    f=  3.99059D-02    |proj g|=  1.67387D-04\n",
      "\n",
      "At iterate  516    f=  3.98913D-02    |proj g|=  7.52163D-05\n",
      "\n",
      "At iterate  517    f=  3.98836D-02    |proj g|=  6.72028D-05\n",
      "\n",
      "At iterate  518    f=  3.98698D-02    |proj g|=  6.95687D-05\n",
      "\n",
      "At iterate  519    f=  3.98547D-02    |proj g|=  4.90138D-05\n",
      "\n",
      "At iterate  520    f=  3.98525D-02    |proj g|=  2.46649D-04\n",
      "\n",
      "At iterate  521    f=  3.98376D-02    |proj g|=  1.68398D-04\n",
      "\n",
      "At iterate  522    f=  3.98236D-02    |proj g|=  2.72727D-04\n",
      "\n",
      "At iterate  523    f=  3.98088D-02    |proj g|=  1.53396D-04\n",
      "\n",
      "At iterate  524    f=  3.97936D-02    |proj g|=  9.67678D-05\n",
      "\n",
      "At iterate  525    f=  3.97774D-02    |proj g|=  1.02165D-04\n",
      "\n",
      "At iterate  526    f=  3.97585D-02    |proj g|=  2.20133D-04\n",
      "\n",
      "At iterate  527    f=  3.97460D-02    |proj g|=  1.84914D-04\n",
      "\n",
      "At iterate  528    f=  3.97305D-02    |proj g|=  9.21902D-05\n",
      "\n",
      "At iterate  529    f=  3.97176D-02    |proj g|=  5.45099D-05\n",
      "\n",
      "At iterate  530    f=  3.96989D-02    |proj g|=  7.52133D-05\n",
      "\n",
      "At iterate  531    f=  3.96939D-02    |proj g|=  2.46163D-04\n",
      "\n",
      "At iterate  532    f=  3.96680D-02    |proj g|=  1.65112D-04\n",
      "\n",
      "At iterate  533    f=  3.96499D-02    |proj g|=  1.69171D-04\n",
      "\n",
      "At iterate  534    f=  3.96369D-02    |proj g|=  1.47526D-04\n",
      "\n",
      "At iterate  535    f=  3.96143D-02    |proj g|=  2.14963D-04\n",
      "\n",
      "At iterate  536    f=  3.95959D-02    |proj g|=  1.20330D-04\n",
      "\n",
      "At iterate  537    f=  3.95880D-02    |proj g|=  1.40397D-04\n",
      "\n",
      "At iterate  538    f=  3.95772D-02    |proj g|=  1.16597D-04\n",
      "\n",
      "At iterate  539    f=  3.95342D-02    |proj g|=  1.89824D-04\n",
      "\n",
      "At iterate  540    f=  3.94879D-02    |proj g|=  1.82086D-04\n",
      "\n",
      "At iterate  541    f=  3.94610D-02    |proj g|=  2.74703D-04\n",
      "\n",
      "At iterate  542    f=  3.94317D-02    |proj g|=  6.01221D-04\n",
      "\n",
      "At iterate  543    f=  3.94079D-02    |proj g|=  3.29987D-04\n",
      "\n",
      "At iterate  544    f=  3.93937D-02    |proj g|=  1.12459D-04\n",
      "\n",
      "At iterate  545    f=  3.93898D-02    |proj g|=  1.01920D-04\n",
      "\n",
      "At iterate  546    f=  3.93720D-02    |proj g|=  9.76686D-05\n",
      "\n",
      "At iterate  547    f=  3.93639D-02    |proj g|=  9.72432D-05\n",
      "\n",
      "At iterate  548    f=  3.93487D-02    |proj g|=  1.16272D-04\n",
      "\n",
      "At iterate  549    f=  3.93364D-02    |proj g|=  2.44209D-04\n",
      "\n",
      "At iterate  550    f=  3.93245D-02    |proj g|=  1.90090D-04\n",
      "\n",
      "At iterate  551    f=  3.92920D-02    |proj g|=  1.42485D-04\n",
      "\n",
      "At iterate  552    f=  3.92729D-02    |proj g|=  9.21887D-05\n",
      "\n",
      "At iterate  553    f=  3.92325D-02    |proj g|=  4.04849D-04\n",
      "\n",
      "At iterate  554    f=  3.92086D-02    |proj g|=  2.20293D-04\n",
      "\n",
      "At iterate  555    f=  3.91746D-02    |proj g|=  3.56076D-04\n",
      "\n",
      "At iterate  556    f=  3.91076D-02    |proj g|=  2.37958D-04\n",
      "\n",
      "At iterate  557    f=  3.90920D-02    |proj g|=  4.18238D-04\n",
      "\n",
      "At iterate  558    f=  3.90704D-02    |proj g|=  2.37251D-04\n",
      "\n",
      "At iterate  559    f=  3.90599D-02    |proj g|=  2.78410D-04\n",
      "\n",
      "At iterate  560    f=  3.90525D-02    |proj g|=  2.57029D-04\n",
      "\n",
      "At iterate  561    f=  3.90454D-02    |proj g|=  5.94471D-04\n",
      "\n",
      "At iterate  562    f=  3.90116D-02    |proj g|=  1.17292D-04\n",
      "\n",
      "At iterate  563    f=  3.89970D-02    |proj g|=  1.90708D-04\n",
      "\n",
      "At iterate  564    f=  3.89869D-02    |proj g|=  2.78766D-04\n",
      "\n",
      "At iterate  565    f=  3.89707D-02    |proj g|=  3.27279D-04\n",
      "\n",
      "At iterate  566    f=  3.89599D-02    |proj g|=  3.41185D-04\n",
      "\n",
      "At iterate  567    f=  3.89400D-02    |proj g|=  1.60223D-04\n",
      "\n",
      "At iterate  568    f=  3.89250D-02    |proj g|=  1.47716D-04\n",
      "\n",
      "At iterate  569    f=  3.89175D-02    |proj g|=  9.50257D-04\n",
      "\n",
      "At iterate  570    f=  3.88784D-02    |proj g|=  3.51924D-04\n",
      "\n",
      "At iterate  571    f=  3.88668D-02    |proj g|=  3.23097D-04\n",
      "\n",
      "At iterate  572    f=  3.88030D-02    |proj g|=  3.34985D-04\n",
      "\n",
      "At iterate  573    f=  3.87532D-02    |proj g|=  2.73114D-04\n",
      "\n",
      "At iterate  574    f=  3.87339D-02    |proj g|=  3.70813D-04\n",
      "\n",
      "At iterate  575    f=  3.86956D-02    |proj g|=  2.02037D-04\n",
      "\n",
      "At iterate  576    f=  3.86864D-02    |proj g|=  1.14556D-04\n",
      "\n",
      "At iterate  577    f=  3.86705D-02    |proj g|=  1.07546D-04\n",
      "\n",
      "At iterate  578    f=  3.86242D-02    |proj g|=  1.46996D-04\n",
      "\n",
      "At iterate  579    f=  3.86101D-02    |proj g|=  4.87691D-04\n",
      "\n",
      "At iterate  580    f=  3.85873D-02    |proj g|=  2.96849D-04\n",
      "\n",
      "At iterate  581    f=  3.85752D-02    |proj g|=  2.41564D-04\n",
      "\n",
      "At iterate  582    f=  3.85643D-02    |proj g|=  2.64162D-04\n",
      "\n",
      "At iterate  583    f=  3.85252D-02    |proj g|=  1.75815D-04\n",
      "\n",
      "At iterate  584    f=  3.84997D-02    |proj g|=  3.89860D-04\n",
      "\n",
      "At iterate  585    f=  3.84730D-02    |proj g|=  1.06992D-04\n",
      "\n",
      "At iterate  586    f=  3.84677D-02    |proj g|=  1.04501D-04\n",
      "\n",
      "At iterate  587    f=  3.84532D-02    |proj g|=  2.49222D-04\n",
      "\n",
      "At iterate  588    f=  3.84248D-02    |proj g|=  2.23513D-04\n",
      "\n",
      "At iterate  589    f=  3.83664D-02    |proj g|=  2.44855D-04\n",
      "\n",
      "At iterate  590    f=  3.83608D-02    |proj g|=  1.94481D-04\n",
      "\n",
      "At iterate  591    f=  3.83418D-02    |proj g|=  1.10118D-04\n",
      "\n",
      "At iterate  592    f=  3.83384D-02    |proj g|=  2.40493D-04\n",
      "\n",
      "At iterate  593    f=  3.83283D-02    |proj g|=  1.91415D-04\n",
      "\n",
      "At iterate  594    f=  3.83073D-02    |proj g|=  1.30607D-04\n",
      "\n",
      "At iterate  595    f=  3.82730D-02    |proj g|=  8.12513D-05\n",
      "\n",
      "At iterate  596    f=  3.82690D-02    |proj g|=  1.82959D-04\n",
      "\n",
      "At iterate  597    f=  3.82585D-02    |proj g|=  1.26614D-04\n",
      "\n",
      "At iterate  598    f=  3.82463D-02    |proj g|=  1.54528D-04\n",
      "\n",
      "At iterate  599    f=  3.82377D-02    |proj g|=  1.36559D-04\n",
      "\n",
      "At iterate  600    f=  3.82094D-02    |proj g|=  1.40914D-04\n",
      "\n",
      "At iterate  601    f=  3.81974D-02    |proj g|=  2.91079D-04\n",
      "\n",
      "At iterate  602    f=  3.81787D-02    |proj g|=  1.27717D-04\n",
      "\n",
      "At iterate  603    f=  3.81737D-02    |proj g|=  1.10742D-04\n",
      "\n",
      "At iterate  604    f=  3.81723D-02    |proj g|=  1.53658D-04\n",
      "\n",
      "At iterate  605    f=  3.81691D-02    |proj g|=  1.85232D-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate  606    f=  3.81525D-02    |proj g|=  1.90954D-04\n",
      "\n",
      "At iterate  607    f=  3.81216D-02    |proj g|=  1.63434D-04\n",
      "\n",
      "At iterate  608    f=  3.81167D-02    |proj g|=  2.40422D-04\n",
      "\n",
      "At iterate  609    f=  3.81037D-02    |proj g|=  1.82546D-04\n",
      "\n",
      "At iterate  610    f=  3.81001D-02    |proj g|=  1.32868D-04\n",
      "\n",
      "At iterate  611    f=  3.80907D-02    |proj g|=  9.83325D-05\n",
      "\n",
      "At iterate  612    f=  3.80809D-02    |proj g|=  1.46557D-04\n",
      "\n",
      "At iterate  613    f=  3.80667D-02    |proj g|=  1.55369D-04\n",
      "\n",
      "At iterate  614    f=  3.80636D-02    |proj g|=  2.10962D-04\n",
      "\n",
      "At iterate  615    f=  3.80558D-02    |proj g|=  1.27417D-04\n",
      "\n",
      "At iterate  616    f=  3.80499D-02    |proj g|=  1.16108D-04\n",
      "\n",
      "At iterate  617    f=  3.80383D-02    |proj g|=  1.55379D-04\n",
      "\n",
      "At iterate  618    f=  3.80252D-02    |proj g|=  1.35459D-04\n",
      "\n",
      "At iterate  619    f=  3.80203D-02    |proj g|=  9.12984D-05\n",
      "\n",
      "At iterate  620    f=  3.80171D-02    |proj g|=  9.84587D-05\n",
      "\n",
      "At iterate  621    f=  3.80098D-02    |proj g|=  1.72514D-04\n",
      "\n",
      "At iterate  622    f=  3.79958D-02    |proj g|=  1.77516D-04\n",
      "\n",
      "At iterate  623    f=  3.79658D-02    |proj g|=  3.43932D-04\n",
      "\n",
      "At iterate  624    f=  3.79433D-02    |proj g|=  2.14671D-04\n",
      "\n",
      "At iterate  625    f=  3.79353D-02    |proj g|=  1.30045D-04\n",
      "\n",
      "At iterate  626    f=  3.79300D-02    |proj g|=  1.61444D-04\n",
      "\n",
      "At iterate  627    f=  3.79249D-02    |proj g|=  1.23771D-04\n",
      "\n",
      "At iterate  628    f=  3.79185D-02    |proj g|=  3.54397D-04\n",
      "\n",
      "At iterate  629    f=  3.79069D-02    |proj g|=  2.37380D-04\n",
      "\n",
      "At iterate  630    f=  3.78820D-02    |proj g|=  1.88928D-04\n",
      "\n",
      "At iterate  631    f=  3.78755D-02    |proj g|=  1.59120D-04\n",
      "\n",
      "At iterate  632    f=  3.78564D-02    |proj g|=  2.05390D-04\n",
      "\n",
      "At iterate  633    f=  3.78273D-02    |proj g|=  1.94093D-04\n",
      "\n",
      "At iterate  634    f=  3.78120D-02    |proj g|=  1.27646D-04\n",
      "\n",
      "At iterate  635    f=  3.77997D-02    |proj g|=  1.68978D-04\n",
      "\n",
      "At iterate  636    f=  3.77948D-02    |proj g|=  1.39991D-04\n",
      "\n",
      "At iterate  637    f=  3.77923D-02    |proj g|=  9.64192D-05\n",
      "\n",
      "At iterate  638    f=  3.77849D-02    |proj g|=  1.04409D-04\n",
      "\n",
      "At iterate  639    f=  3.77621D-02    |proj g|=  6.28040D-05\n",
      "\n",
      "At iterate  640    f=  3.77593D-02    |proj g|=  2.50625D-04\n",
      "\n",
      "At iterate  641    f=  3.77388D-02    |proj g|=  1.55527D-04\n",
      "\n",
      "At iterate  642    f=  3.77232D-02    |proj g|=  8.76128D-05\n",
      "\n",
      "At iterate  643    f=  3.77182D-02    |proj g|=  5.36351D-05\n",
      "\n",
      "At iterate  644    f=  3.77144D-02    |proj g|=  2.00638D-04\n",
      "\n",
      "At iterate  645    f=  3.77035D-02    |proj g|=  1.01260D-04\n",
      "\n",
      "At iterate  646    f=  3.76940D-02    |proj g|=  7.98317D-05\n",
      "\n",
      "At iterate  647    f=  3.76890D-02    |proj g|=  9.75876D-05\n",
      "\n",
      "At iterate  648    f=  3.76850D-02    |proj g|=  9.46380D-05\n",
      "\n",
      "At iterate  649    f=  3.76772D-02    |proj g|=  9.24332D-05\n",
      "\n",
      "At iterate  650    f=  3.76632D-02    |proj g|=  7.23048D-05\n",
      "\n",
      "At iterate  651    f=  3.76507D-02    |proj g|=  3.79802D-04\n",
      "\n",
      "At iterate  652    f=  3.76252D-02    |proj g|=  1.17750D-04\n",
      "\n",
      "At iterate  653    f=  3.76224D-02    |proj g|=  9.93065D-05\n",
      "\n",
      "At iterate  654    f=  3.76185D-02    |proj g|=  9.73741D-05\n",
      "\n",
      "At iterate  655    f=  3.76085D-02    |proj g|=  1.05883D-04\n",
      "\n",
      "At iterate  656    f=  3.76056D-02    |proj g|=  2.20693D-04\n",
      "\n",
      "At iterate  657    f=  3.75956D-02    |proj g|=  1.85849D-04\n",
      "\n",
      "At iterate  658    f=  3.75788D-02    |proj g|=  1.41151D-04\n",
      "\n",
      "At iterate  659    f=  3.75707D-02    |proj g|=  2.17240D-04\n",
      "\n",
      "At iterate  660    f=  3.75517D-02    |proj g|=  1.88473D-04\n",
      "\n",
      "At iterate  661    f=  3.75419D-02    |proj g|=  1.25331D-04\n",
      "\n",
      "At iterate  662    f=  3.75330D-02    |proj g|=  2.93069D-04\n",
      "\n",
      "At iterate  663    f=  3.75277D-02    |proj g|=  1.19793D-04\n",
      "\n",
      "At iterate  664    f=  3.75256D-02    |proj g|=  8.11839D-05\n",
      "\n",
      "At iterate  665    f=  3.75218D-02    |proj g|=  8.42563D-05\n",
      "\n",
      "At iterate  666    f=  3.75159D-02    |proj g|=  1.10967D-04\n",
      "\n",
      "At iterate  667    f=  3.75116D-02    |proj g|=  1.41238D-04\n",
      "\n",
      "At iterate  668    f=  3.74941D-02    |proj g|=  1.10043D-04\n",
      "\n",
      "At iterate  669    f=  3.74792D-02    |proj g|=  1.45564D-04\n",
      "\n",
      "At iterate  670    f=  3.74524D-02    |proj g|=  1.10752D-04\n",
      "\n",
      "At iterate  671    f=  3.74493D-02    |proj g|=  1.49054D-04\n",
      "\n",
      "At iterate  672    f=  3.74420D-02    |proj g|=  1.10307D-04\n",
      "\n",
      "At iterate  673    f=  3.74327D-02    |proj g|=  1.06118D-04\n",
      "\n",
      "At iterate  674    f=  3.74305D-02    |proj g|=  1.93736D-04\n",
      "\n",
      "At iterate  675    f=  3.74220D-02    |proj g|=  1.85240D-04\n",
      "\n",
      "At iterate  676    f=  3.74136D-02    |proj g|=  1.21707D-04\n",
      "\n",
      "At iterate  677    f=  3.74082D-02    |proj g|=  8.03559D-05\n",
      "\n",
      "At iterate  678    f=  3.73981D-02    |proj g|=  9.41893D-05\n",
      "\n",
      "At iterate  679    f=  3.73815D-02    |proj g|=  1.30755D-04\n",
      "\n",
      "At iterate  680    f=  3.73750D-02    |proj g|=  2.05359D-04\n",
      "\n",
      "At iterate  681    f=  3.73674D-02    |proj g|=  7.89036D-05\n",
      "\n",
      "At iterate  682    f=  3.73652D-02    |proj g|=  3.85252D-05\n",
      "\n",
      "At iterate  683    f=  3.73636D-02    |proj g|=  7.21871D-05\n",
      "\n",
      "At iterate  684    f=  3.73597D-02    |proj g|=  9.91535D-05\n",
      "\n",
      "At iterate  685    f=  3.73584D-02    |proj g|=  2.05142D-04\n",
      "\n",
      "At iterate  686    f=  3.73517D-02    |proj g|=  1.63494D-04\n",
      "\n",
      "At iterate  687    f=  3.73436D-02    |proj g|=  9.14915D-05\n",
      "\n",
      "At iterate  688    f=  3.73381D-02    |proj g|=  7.97282D-05\n",
      "\n",
      "At iterate  689    f=  3.73337D-02    |proj g|=  1.29110D-04\n",
      "\n",
      "At iterate  690    f=  3.73283D-02    |proj g|=  1.17270D-04\n",
      "\n",
      "At iterate  691    f=  3.73216D-02    |proj g|=  2.21644D-04\n",
      "\n",
      "At iterate  692    f=  3.73084D-02    |proj g|=  1.02633D-04\n",
      "\n",
      "At iterate  693    f=  3.73036D-02    |proj g|=  5.84262D-05\n",
      "\n",
      "At iterate  694    f=  3.72996D-02    |proj g|=  7.38513D-05\n",
      "\n",
      "At iterate  695    f=  3.72966D-02    |proj g|=  9.82798D-05\n",
      "\n",
      "At iterate  696    f=  3.72892D-02    |proj g|=  9.88615D-05\n",
      "\n",
      "At iterate  697    f=  3.72868D-02    |proj g|=  1.97616D-04\n",
      "\n",
      "At iterate  698    f=  3.72795D-02    |proj g|=  1.07815D-04\n",
      "\n",
      "At iterate  699    f=  3.72732D-02    |proj g|=  7.84451D-05\n",
      "\n",
      "At iterate  700    f=  3.72710D-02    |proj g|=  8.16345D-05\n",
      "\n",
      "At iterate  701    f=  3.72616D-02    |proj g|=  1.07796D-04\n",
      "\n",
      "At iterate  702    f=  3.72422D-02    |proj g|=  1.19710D-04\n",
      "\n",
      "At iterate  703    f=  3.72382D-02    |proj g|=  2.03304D-04\n",
      "\n",
      "At iterate  704    f=  3.72276D-02    |proj g|=  1.22083D-04\n",
      "\n",
      "At iterate  705    f=  3.72197D-02    |proj g|=  8.12617D-05\n",
      "\n",
      "At iterate  706    f=  3.72175D-02    |proj g|=  8.92243D-05\n",
      "\n",
      "At iterate  707    f=  3.72131D-02    |proj g|=  6.38194D-05\n",
      "\n",
      "At iterate  708    f=  3.72044D-02    |proj g|=  8.21749D-05\n",
      "\n",
      "At iterate  709    f=  3.72032D-02    |proj g|=  2.02328D-04\n",
      "\n",
      "At iterate  710    f=  3.71899D-02    |proj g|=  1.44112D-04\n",
      "\n",
      "At iterate  711    f=  3.71773D-02    |proj g|=  1.06775D-04\n",
      "\n",
      "At iterate  712    f=  3.71700D-02    |proj g|=  1.00819D-04\n",
      "\n",
      "At iterate  713    f=  3.71591D-02    |proj g|=  7.68260D-05\n",
      "\n",
      "At iterate  714    f=  3.71555D-02    |proj g|=  1.33912D-04\n",
      "\n",
      "At iterate  715    f=  3.71495D-02    |proj g|=  9.69529D-05\n",
      "\n",
      "At iterate  716    f=  3.71414D-02    |proj g|=  8.52666D-05\n",
      "\n",
      "At iterate  717    f=  3.71267D-02    |proj g|=  6.43629D-05\n",
      "\n",
      "At iterate  718    f=  3.71241D-02    |proj g|=  8.84694D-05\n",
      "\n",
      "At iterate  719    f=  3.71180D-02    |proj g|=  4.15582D-05\n",
      "\n",
      "At iterate  720    f=  3.71114D-02    |proj g|=  1.16797D-04\n",
      "\n",
      "At iterate  721    f=  3.71078D-02    |proj g|=  1.26958D-04\n",
      "\n",
      "At iterate  722    f=  3.71022D-02    |proj g|=  1.09122D-04\n",
      "\n",
      "At iterate  723    f=  3.70986D-02    |proj g|=  8.38861D-05\n",
      "\n",
      "At iterate  724    f=  3.70910D-02    |proj g|=  9.17222D-05\n",
      "\n",
      "At iterate  725    f=  3.70865D-02    |proj g|=  2.25708D-04\n",
      "\n",
      "At iterate  726    f=  3.70761D-02    |proj g|=  1.61516D-04\n",
      "\n",
      "At iterate  727    f=  3.70672D-02    |proj g|=  1.05387D-04\n",
      "\n",
      "At iterate  728    f=  3.70596D-02    |proj g|=  1.21332D-04\n",
      "\n",
      "At iterate  729    f=  3.70529D-02    |proj g|=  1.36273D-04\n",
      "\n",
      "At iterate  730    f=  3.70461D-02    |proj g|=  8.31709D-05\n",
      "\n",
      "At iterate  731    f=  3.70431D-02    |proj g|=  1.43999D-04\n",
      "\n",
      "At iterate  732    f=  3.70405D-02    |proj g|=  4.53403D-05\n",
      "\n",
      "At iterate  733    f=  3.70392D-02    |proj g|=  6.59481D-05\n",
      "\n",
      "At iterate  734    f=  3.70377D-02    |proj g|=  6.48466D-05\n",
      "\n",
      "At iterate  735    f=  3.70344D-02    |proj g|=  1.25974D-04\n",
      "\n",
      "At iterate  736    f=  3.70324D-02    |proj g|=  1.16457D-04\n",
      "\n",
      "At iterate  737    f=  3.70310D-02    |proj g|=  9.09504D-05\n",
      "\n",
      "At iterate  738    f=  3.70278D-02    |proj g|=  9.05768D-05\n",
      "\n",
      "At iterate  739    f=  3.70239D-02    |proj g|=  8.00211D-05\n",
      "\n",
      "At iterate  740    f=  3.70201D-02    |proj g|=  1.00244D-04\n",
      "\n",
      "At iterate  741    f=  3.70124D-02    |proj g|=  1.38422D-04\n",
      "\n",
      "At iterate  742    f=  3.70095D-02    |proj g|=  8.94152D-05\n",
      "\n",
      "At iterate  743    f=  3.70069D-02    |proj g|=  9.62837D-05\n",
      "\n",
      "At iterate  744    f=  3.70020D-02    |proj g|=  1.08877D-04\n",
      "\n",
      "At iterate  745    f=  3.69935D-02    |proj g|=  1.60411D-04\n",
      "\n",
      "At iterate  746    f=  3.69836D-02    |proj g|=  1.21414D-04\n",
      "\n",
      "At iterate  747    f=  3.69814D-02    |proj g|=  3.65070D-04\n",
      "\n",
      "At iterate  748    f=  3.69734D-02    |proj g|=  1.67160D-04\n",
      "\n",
      "At iterate  749    f=  3.69631D-02    |proj g|=  1.18814D-04\n",
      "\n",
      "At iterate  750    f=  3.69574D-02    |proj g|=  1.68859D-04\n",
      "\n",
      "At iterate  751    f=  3.69507D-02    |proj g|=  1.24348D-04\n",
      "\n",
      "At iterate  752    f=  3.69416D-02    |proj g|=  7.82750D-05\n",
      "\n",
      "At iterate  753    f=  3.69366D-02    |proj g|=  1.29801D-04\n",
      "\n",
      "At iterate  754    f=  3.69322D-02    |proj g|=  1.21519D-04\n",
      "\n",
      "At iterate  755    f=  3.69305D-02    |proj g|=  1.08272D-04\n",
      "\n",
      "At iterate  756    f=  3.69253D-02    |proj g|=  1.14847D-04\n",
      "\n",
      "At iterate  757    f=  3.69233D-02    |proj g|=  1.07294D-04\n",
      "\n",
      "At iterate  758    f=  3.69024D-02    |proj g|=  1.14044D-04\n",
      "\n",
      "At iterate  759    f=  3.69009D-02    |proj g|=  1.74268D-04\n",
      "\n",
      "At iterate  760    f=  3.68840D-02    |proj g|=  1.24403D-04\n",
      "\n",
      "At iterate  761    f=  3.68767D-02    |proj g|=  9.97026D-05\n",
      "\n",
      "At iterate  762    f=  3.68564D-02    |proj g|=  9.66849D-05\n",
      "\n",
      "At iterate  763    f=  3.68423D-02    |proj g|=  1.27165D-04\n",
      "\n",
      "At iterate  764    f=  3.68407D-02    |proj g|=  1.85877D-04\n",
      "\n",
      "At iterate  765    f=  3.68316D-02    |proj g|=  4.33301D-05\n",
      "\n",
      "At iterate  766    f=  3.68292D-02    |proj g|=  2.88460D-05\n",
      "\n",
      "At iterate  767    f=  3.68214D-02    |proj g|=  1.36990D-04\n",
      "\n",
      "At iterate  768    f=  3.68141D-02    |proj g|=  1.24523D-04\n",
      "\n",
      "At iterate  769    f=  3.68067D-02    |proj g|=  1.01213D-04\n",
      "\n",
      "At iterate  770    f=  3.67969D-02    |proj g|=  1.87499D-04\n",
      "\n",
      "At iterate  771    f=  3.67958D-02    |proj g|=  8.41338D-05\n",
      "\n",
      "At iterate  772    f=  3.67937D-02    |proj g|=  1.03590D-04\n",
      "\n",
      "At iterate  773    f=  3.67813D-02    |proj g|=  1.81572D-04\n",
      "\n",
      "At iterate  774    f=  3.67626D-02    |proj g|=  1.83967D-04\n",
      "\n",
      "At iterate  775    f=  3.67489D-02    |proj g|=  9.33185D-05\n",
      "\n",
      "At iterate  776    f=  3.67366D-02    |proj g|=  9.00417D-05\n",
      "\n",
      "At iterate  777    f=  3.67086D-02    |proj g|=  1.08435D-04\n",
      "\n",
      "At iterate  778    f=  3.66692D-02    |proj g|=  8.67018D-05\n",
      "\n",
      "At iterate  779    f=  3.66652D-02    |proj g|=  1.86642D-04\n",
      "\n",
      "At iterate  780    f=  3.66568D-02    |proj g|=  1.71416D-04\n",
      "\n",
      "At iterate  781    f=  3.66484D-02    |proj g|=  1.56322D-04\n",
      "\n",
      "At iterate  782    f=  3.66361D-02    |proj g|=  1.28578D-04\n",
      "\n",
      "At iterate  783    f=  3.66350D-02    |proj g|=  1.83925D-04\n",
      "\n",
      "At iterate  784    f=  3.66289D-02    |proj g|=  1.65102D-04\n",
      "\n",
      "At iterate  785    f=  3.66165D-02    |proj g|=  1.55965D-04\n",
      "\n",
      "At iterate  786    f=  3.66066D-02    |proj g|=  1.32855D-04\n",
      "\n",
      "At iterate  787    f=  3.65964D-02    |proj g|=  5.37333D-05\n",
      "\n",
      "At iterate  788    f=  3.65806D-02    |proj g|=  7.55495D-05\n",
      "\n",
      "At iterate  789    f=  3.65630D-02    |proj g|=  1.22350D-04\n",
      "\n",
      "At iterate  790    f=  3.65393D-02    |proj g|=  1.44448D-04\n",
      "\n",
      "At iterate  791    f=  3.65372D-02    |proj g|=  2.58052D-04\n",
      "\n",
      "At iterate  792    f=  3.65046D-02    |proj g|=  1.51802D-04\n",
      "\n",
      "At iterate  793    f=  3.64907D-02    |proj g|=  9.69661D-05\n",
      "\n",
      "At iterate  794    f=  3.64890D-02    |proj g|=  1.65278D-04\n",
      "\n",
      "At iterate  795    f=  3.64858D-02    |proj g|=  1.20158D-04\n",
      "\n",
      "At iterate  796    f=  3.64831D-02    |proj g|=  1.08097D-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate  797    f=  3.64723D-02    |proj g|=  1.85062D-04\n",
      "\n",
      "At iterate  798    f=  3.64612D-02    |proj g|=  2.08490D-04\n",
      "\n",
      "At iterate  799    f=  3.64372D-02    |proj g|=  1.64064D-04\n",
      "\n",
      "At iterate  800    f=  3.64288D-02    |proj g|=  2.24865D-04\n",
      "\n",
      "At iterate  801    f=  3.63998D-02    |proj g|=  1.05717D-04\n",
      "\n",
      "At iterate  802    f=  3.63928D-02    |proj g|=  3.91412D-04\n",
      "\n",
      "At iterate  803    f=  3.63738D-02    |proj g|=  2.23122D-04\n",
      "\n",
      "At iterate  804    f=  3.63564D-02    |proj g|=  1.25059D-04\n",
      "\n",
      "At iterate  805    f=  3.63396D-02    |proj g|=  2.06619D-04\n",
      "\n",
      "At iterate  806    f=  3.63335D-02    |proj g|=  4.46898D-04\n",
      "\n",
      "At iterate  807    f=  3.63141D-02    |proj g|=  2.02370D-04\n",
      "\n",
      "At iterate  808    f=  3.63029D-02    |proj g|=  1.21866D-04\n",
      "\n",
      "At iterate  809    f=  3.62989D-02    |proj g|=  1.02894D-04\n",
      "\n",
      "At iterate  810    f=  3.62870D-02    |proj g|=  1.33360D-04\n",
      "\n",
      "At iterate  811    f=  3.62665D-02    |proj g|=  1.46322D-04\n",
      "\n",
      "At iterate  812    f=  3.62520D-02    |proj g|=  1.31867D-04\n",
      "\n",
      "At iterate  813    f=  3.62494D-02    |proj g|=  1.64699D-04\n",
      "\n",
      "At iterate  814    f=  3.62416D-02    |proj g|=  1.18619D-04\n",
      "\n",
      "At iterate  815    f=  3.62310D-02    |proj g|=  1.88154D-04\n",
      "\n",
      "At iterate  816    f=  3.62216D-02    |proj g|=  1.26648D-04\n",
      "\n",
      "At iterate  817    f=  3.62065D-02    |proj g|=  1.95371D-04\n",
      "\n",
      "At iterate  818    f=  3.62022D-02    |proj g|=  1.81118D-04\n",
      "\n",
      "At iterate  819    f=  3.61953D-02    |proj g|=  1.65751D-04\n",
      "\n",
      "At iterate  820    f=  3.61712D-02    |proj g|=  1.36672D-04\n",
      "\n",
      "At iterate  821    f=  3.61530D-02    |proj g|=  6.28072D-05\n",
      "\n",
      "At iterate  822    f=  3.61386D-02    |proj g|=  5.29929D-05\n",
      "\n",
      "At iterate  823    f=  3.61216D-02    |proj g|=  5.79841D-05\n",
      "\n",
      "At iterate  824    f=  3.61195D-02    |proj g|=  1.49758D-04\n",
      "\n",
      "At iterate  825    f=  3.60603D-02    |proj g|=  1.46173D-04\n",
      "\n",
      "At iterate  826    f=  3.60364D-02    |proj g|=  1.85093D-04\n",
      "\n",
      "At iterate  827    f=  3.60010D-02    |proj g|=  1.58780D-04\n",
      "\n",
      "At iterate  828    f=  3.59858D-02    |proj g|=  1.60719D-04\n",
      "\n",
      "At iterate  829    f=  3.59723D-02    |proj g|=  2.86921D-04\n",
      "\n",
      "At iterate  830    f=  3.59627D-02    |proj g|=  1.05325D-04\n",
      "\n",
      "At iterate  831    f=  3.59567D-02    |proj g|=  7.75877D-05\n",
      "\n",
      "At iterate  832    f=  3.59407D-02    |proj g|=  5.53613D-05\n",
      "\n",
      "At iterate  833    f=  3.59174D-02    |proj g|=  1.48620D-04\n",
      "\n",
      "At iterate  834    f=  3.58978D-02    |proj g|=  1.30145D-04\n",
      "\n",
      "At iterate  835    f=  3.58836D-02    |proj g|=  3.37132D-04\n",
      "\n",
      "At iterate  836    f=  3.58540D-02    |proj g|=  9.58882D-05\n",
      "\n",
      "At iterate  837    f=  3.58469D-02    |proj g|=  7.14005D-05\n",
      "\n",
      "At iterate  838    f=  3.58053D-02    |proj g|=  7.45907D-05\n",
      "\n",
      "At iterate  839    f=  3.57839D-02    |proj g|=  2.35609D-04\n",
      "\n",
      "At iterate  840    f=  3.57749D-02    |proj g|=  3.01431D-04\n",
      "\n",
      "At iterate  841    f=  3.57648D-02    |proj g|=  1.23371D-04\n",
      "\n",
      "At iterate  842    f=  3.57601D-02    |proj g|=  1.29759D-04\n",
      "\n",
      "At iterate  843    f=  3.57361D-02    |proj g|=  9.89130D-05\n",
      "\n",
      "At iterate  844    f=  3.57209D-02    |proj g|=  1.36454D-04\n",
      "\n",
      "At iterate  845    f=  3.57026D-02    |proj g|=  5.90534D-05\n",
      "\n",
      "At iterate  846    f=  3.57005D-02    |proj g|=  1.61869D-04\n",
      "\n",
      "At iterate  847    f=  3.56937D-02    |proj g|=  6.77157D-05\n",
      "\n",
      "At iterate  848    f=  3.56902D-02    |proj g|=  7.15965D-05\n",
      "\n",
      "At iterate  849    f=  3.56868D-02    |proj g|=  7.49970D-05\n",
      "\n",
      "At iterate  850    f=  3.56743D-02    |proj g|=  7.59356D-05\n",
      "\n",
      "At iterate  851    f=  3.56675D-02    |proj g|=  2.77535D-04\n",
      "\n",
      "At iterate  852    f=  3.56554D-02    |proj g|=  2.03181D-04\n",
      "\n",
      "At iterate  853    f=  3.56492D-02    |proj g|=  1.79096D-04\n",
      "\n",
      "At iterate  854    f=  3.56400D-02    |proj g|=  1.45786D-04\n",
      "\n",
      "At iterate  855    f=  3.56321D-02    |proj g|=  1.06462D-04\n",
      "\n",
      "At iterate  856    f=  3.56257D-02    |proj g|=  1.50837D-04\n",
      "\n",
      "At iterate  857    f=  3.56236D-02    |proj g|=  1.39519D-04\n",
      "\n",
      "At iterate  858    f=  3.56209D-02    |proj g|=  1.10784D-04\n",
      "\n",
      "At iterate  859    f=  3.56102D-02    |proj g|=  7.37795D-05\n",
      "\n",
      "At iterate  860    f=  3.56027D-02    |proj g|=  1.68851D-04\n",
      "\n",
      "At iterate  861    f=  3.55963D-02    |proj g|=  8.18815D-05\n",
      "\n",
      "At iterate  862    f=  3.55880D-02    |proj g|=  1.15089D-04\n",
      "\n",
      "At iterate  863    f=  3.55813D-02    |proj g|=  1.16695D-04\n",
      "\n",
      "At iterate  864    f=  3.55760D-02    |proj g|=  2.93293D-04\n",
      "\n",
      "At iterate  865    f=  3.55673D-02    |proj g|=  1.88936D-04\n",
      "\n",
      "At iterate  866    f=  3.55590D-02    |proj g|=  1.10492D-04\n",
      "\n",
      "At iterate  867    f=  3.55568D-02    |proj g|=  9.73876D-05\n",
      "\n",
      "At iterate  868    f=  3.55536D-02    |proj g|=  1.94129D-04\n",
      "\n",
      "At iterate  869    f=  3.55476D-02    |proj g|=  1.30791D-04\n",
      "\n",
      "At iterate  870    f=  3.55368D-02    |proj g|=  8.92981D-05\n",
      "\n",
      "At iterate  871    f=  3.55297D-02    |proj g|=  1.06451D-04\n",
      "\n",
      "At iterate  872    f=  3.55247D-02    |proj g|=  1.21814D-04\n",
      "\n",
      "At iterate  873    f=  3.55120D-02    |proj g|=  1.14002D-04\n",
      "\n",
      "At iterate  874    f=  3.54857D-02    |proj g|=  1.59228D-04\n",
      "\n",
      "At iterate  875    f=  3.54838D-02    |proj g|=  2.81941D-04\n",
      "\n",
      "At iterate  876    f=  3.54579D-02    |proj g|=  1.98833D-04\n",
      "\n",
      "At iterate  877    f=  3.54346D-02    |proj g|=  1.19284D-04\n",
      "\n",
      "At iterate  878    f=  3.54241D-02    |proj g|=  1.22323D-04\n",
      "\n",
      "At iterate  879    f=  3.54219D-02    |proj g|=  1.86381D-04\n",
      "\n",
      "At iterate  880    f=  3.54184D-02    |proj g|=  1.25479D-04\n",
      "\n",
      "At iterate  881    f=  3.54136D-02    |proj g|=  1.27776D-04\n",
      "\n",
      "At iterate  882    f=  3.54091D-02    |proj g|=  1.57045D-04\n",
      "\n",
      "At iterate  883    f=  3.54009D-02    |proj g|=  1.30764D-04\n",
      "\n",
      "At iterate  884    f=  3.53956D-02    |proj g|=  1.56925D-04\n",
      "\n",
      "At iterate  885    f=  3.53863D-02    |proj g|=  2.81854D-04\n",
      "\n",
      "At iterate  886    f=  3.53683D-02    |proj g|=  1.18088D-04\n",
      "\n",
      "At iterate  887    f=  3.53600D-02    |proj g|=  8.14744D-05\n",
      "\n",
      "At iterate  888    f=  3.53398D-02    |proj g|=  1.30941D-04\n",
      "\n",
      "At iterate  889    f=  3.53380D-02    |proj g|=  2.26787D-04\n",
      "\n",
      "At iterate  890    f=  3.53312D-02    |proj g|=  2.39455D-04\n",
      "\n",
      "At iterate  891    f=  3.53281D-02    |proj g|=  1.06969D-04\n",
      "\n",
      "At iterate  892    f=  3.53259D-02    |proj g|=  6.53288D-05\n",
      "\n",
      "At iterate  893    f=  3.53242D-02    |proj g|=  8.54681D-05\n",
      "\n",
      "At iterate  894    f=  3.53192D-02    |proj g|=  8.63438D-05\n",
      "\n",
      "At iterate  895    f=  3.53136D-02    |proj g|=  1.39068D-04\n",
      "\n",
      "At iterate  896    f=  3.53057D-02    |proj g|=  2.39666D-04\n",
      "\n",
      "At iterate  897    f=  3.52821D-02    |proj g|=  1.60235D-04\n",
      "\n",
      "At iterate  898    f=  3.52738D-02    |proj g|=  8.50662D-05\n",
      "\n",
      "At iterate  899    f=  3.52691D-02    |proj g|=  7.56175D-05\n",
      "\n",
      "At iterate  900    f=  3.52624D-02    |proj g|=  1.30340D-04\n",
      "\n",
      "At iterate  901    f=  3.52598D-02    |proj g|=  2.87569D-04\n",
      "\n",
      "At iterate  902    f=  3.52542D-02    |proj g|=  9.49978D-05\n",
      "\n",
      "At iterate  903    f=  3.52500D-02    |proj g|=  6.20894D-05\n",
      "\n",
      "At iterate  904    f=  3.52453D-02    |proj g|=  1.44401D-04\n",
      "\n",
      "At iterate  905    f=  3.52421D-02    |proj g|=  1.26141D-04\n",
      "\n",
      "At iterate  906    f=  3.52414D-02    |proj g|=  1.21978D-04\n",
      "\n",
      "At iterate  907    f=  3.52285D-02    |proj g|=  9.34563D-05\n",
      "\n",
      "At iterate  908    f=  3.52182D-02    |proj g|=  6.55123D-05\n",
      "\n",
      "At iterate  909    f=  3.52140D-02    |proj g|=  3.89539D-05\n",
      "\n",
      "At iterate  910    f=  3.52106D-02    |proj g|=  5.36295D-05\n",
      "\n",
      "At iterate  911    f=  3.51962D-02    |proj g|=  1.30840D-04\n",
      "\n",
      "At iterate  912    f=  3.51899D-02    |proj g|=  2.31371D-04\n",
      "\n",
      "At iterate  913    f=  3.51782D-02    |proj g|=  1.38924D-04\n",
      "\n",
      "At iterate  914    f=  3.51728D-02    |proj g|=  1.44879D-04\n",
      "\n",
      "At iterate  915    f=  3.51709D-02    |proj g|=  4.99940D-05\n",
      "\n",
      "At iterate  916    f=  3.51658D-02    |proj g|=  9.27610D-05\n",
      "\n",
      "At iterate  917    f=  3.51576D-02    |proj g|=  1.22756D-04\n",
      "\n",
      "At iterate  918    f=  3.51438D-02    |proj g|=  7.79236D-05\n",
      "\n",
      "At iterate  919    f=  3.51359D-02    |proj g|=  1.18675D-04\n",
      "\n",
      "At iterate  920    f=  3.51270D-02    |proj g|=  7.03661D-05\n",
      "\n",
      "At iterate  921    f=  3.51216D-02    |proj g|=  1.06369D-04\n",
      "\n",
      "At iterate  922    f=  3.51145D-02    |proj g|=  1.12251D-04\n",
      "\n",
      "At iterate  923    f=  3.51014D-02    |proj g|=  1.86711D-04\n",
      "\n",
      "At iterate  924    f=  3.50835D-02    |proj g|=  1.32007D-04\n",
      "\n",
      "At iterate  925    f=  3.50705D-02    |proj g|=  2.15394D-04\n",
      "\n",
      "At iterate  926    f=  3.50639D-02    |proj g|=  1.02436D-04\n",
      "\n",
      "At iterate  927    f=  3.50604D-02    |proj g|=  6.44326D-05\n",
      "\n",
      "At iterate  928    f=  3.50512D-02    |proj g|=  6.11878D-05\n",
      "\n",
      "At iterate  929    f=  3.50368D-02    |proj g|=  6.52075D-05\n",
      "\n",
      "At iterate  930    f=  3.50287D-02    |proj g|=  2.91065D-04\n",
      "\n",
      "At iterate  931    f=  3.50141D-02    |proj g|=  8.71955D-05\n",
      "\n",
      "At iterate  932    f=  3.50105D-02    |proj g|=  8.81341D-05\n",
      "\n",
      "At iterate  933    f=  3.50063D-02    |proj g|=  6.64693D-05\n",
      "\n",
      "At iterate  934    f=  3.50045D-02    |proj g|=  1.27379D-04\n",
      "\n",
      "At iterate  935    f=  3.49952D-02    |proj g|=  8.67730D-05\n",
      "\n",
      "At iterate  936    f=  3.49903D-02    |proj g|=  1.43825D-04\n",
      "\n",
      "At iterate  937    f=  3.49827D-02    |proj g|=  7.84822D-05\n",
      "\n",
      "At iterate  938    f=  3.49792D-02    |proj g|=  8.19281D-05\n",
      "\n",
      "At iterate  939    f=  3.49702D-02    |proj g|=  3.02729D-04\n",
      "\n",
      "At iterate  940    f=  3.49603D-02    |proj g|=  2.27947D-04\n",
      "\n",
      "At iterate  941    f=  3.49398D-02    |proj g|=  4.07459D-05\n",
      "\n",
      "At iterate  942    f=  3.49380D-02    |proj g|=  4.11584D-05\n",
      "\n",
      "At iterate  943    f=  3.49365D-02    |proj g|=  1.19156D-04\n",
      "\n",
      "At iterate  944    f=  3.49324D-02    |proj g|=  7.65511D-05\n",
      "\n",
      "At iterate  945    f=  3.49182D-02    |proj g|=  6.42340D-05\n",
      "\n",
      "At iterate  946    f=  3.48965D-02    |proj g|=  2.31532D-04\n",
      "\n",
      "At iterate  947    f=  3.48910D-02    |proj g|=  2.52060D-04\n",
      "\n",
      "At iterate  948    f=  3.48797D-02    |proj g|=  2.31201D-04\n",
      "\n",
      "At iterate  949    f=  3.48651D-02    |proj g|=  1.50538D-04\n",
      "\n",
      "At iterate  950    f=  3.48585D-02    |proj g|=  2.23874D-04\n",
      "\n",
      "At iterate  951    f=  3.48517D-02    |proj g|=  7.27498D-05\n",
      "\n",
      "At iterate  952    f=  3.48472D-02    |proj g|=  8.11386D-05\n",
      "\n",
      "At iterate  953    f=  3.48391D-02    |proj g|=  1.58371D-04\n",
      "\n",
      "At iterate  954    f=  3.48317D-02    |proj g|=  1.43945D-04\n",
      "\n",
      "At iterate  955    f=  3.48282D-02    |proj g|=  1.93688D-04\n",
      "\n",
      "At iterate  956    f=  3.48132D-02    |proj g|=  1.57525D-04\n",
      "\n",
      "At iterate  957    f=  3.47854D-02    |proj g|=  2.25374D-04\n",
      "\n",
      "At iterate  958    f=  3.47687D-02    |proj g|=  1.54919D-04\n",
      "\n",
      "At iterate  959    f=  3.47666D-02    |proj g|=  2.85726D-04\n",
      "\n",
      "At iterate  960    f=  3.47575D-02    |proj g|=  2.01202D-04\n",
      "\n",
      "At iterate  961    f=  3.47519D-02    |proj g|=  1.27182D-04\n",
      "\n",
      "At iterate  962    f=  3.47466D-02    |proj g|=  1.74041D-04\n",
      "\n",
      "At iterate  963    f=  3.47399D-02    |proj g|=  2.42913D-04\n",
      "\n",
      "At iterate  964    f=  3.47276D-02    |proj g|=  2.32461D-04\n",
      "\n",
      "At iterate  965    f=  3.47211D-02    |proj g|=  4.09703D-04\n",
      "\n",
      "At iterate  966    f=  3.46998D-02    |proj g|=  3.29960D-04\n",
      "\n",
      "At iterate  967    f=  3.46781D-02    |proj g|=  2.41516D-04\n",
      "\n",
      "At iterate  968    f=  3.46617D-02    |proj g|=  2.23431D-04\n",
      "\n",
      "At iterate  969    f=  3.46479D-02    |proj g|=  1.93294D-04\n",
      "\n",
      "At iterate  970    f=  3.46300D-02    |proj g|=  2.17722D-04\n",
      "\n",
      "At iterate  971    f=  3.46160D-02    |proj g|=  1.43307D-04\n",
      "\n",
      "At iterate  972    f=  3.46075D-02    |proj g|=  1.39166D-04\n",
      "\n",
      "At iterate  973    f=  3.45984D-02    |proj g|=  2.25831D-04\n",
      "\n",
      "At iterate  974    f=  3.45845D-02    |proj g|=  1.40999D-04\n",
      "\n",
      "At iterate  975    f=  3.45728D-02    |proj g|=  1.26011D-04\n",
      "\n",
      "At iterate  976    f=  3.45679D-02    |proj g|=  1.99838D-04\n",
      "\n",
      "At iterate  977    f=  3.45592D-02    |proj g|=  4.04552D-04\n",
      "\n",
      "At iterate  978    f=  3.45478D-02    |proj g|=  1.45761D-04\n",
      "\n",
      "At iterate  979    f=  3.45406D-02    |proj g|=  8.28805D-05\n",
      "\n",
      "At iterate  980    f=  3.45208D-02    |proj g|=  1.86043D-04\n",
      "\n",
      "At iterate  981    f=  3.45053D-02    |proj g|=  3.01086D-04\n",
      "\n",
      "At iterate  982    f=  3.44997D-02    |proj g|=  1.69937D-04\n",
      "\n",
      "At iterate  983    f=  3.44910D-02    |proj g|=  1.48956D-04\n",
      "\n",
      "At iterate  984    f=  3.44839D-02    |proj g|=  1.19388D-04\n",
      "\n",
      "At iterate  985    f=  3.44729D-02    |proj g|=  4.53088D-04\n",
      "\n",
      "At iterate  986    f=  3.44595D-02    |proj g|=  3.62547D-04\n",
      "\n",
      "At iterate  987    f=  3.44273D-02    |proj g|=  1.27121D-04\n",
      "\n",
      "At iterate  988    f=  3.44176D-02    |proj g|=  1.53743D-04\n",
      "\n",
      "At iterate  989    f=  3.44091D-02    |proj g|=  1.12902D-04\n",
      "\n",
      "At iterate  990    f=  3.44037D-02    |proj g|=  9.90430D-05\n",
      "\n",
      "At iterate  991    f=  3.43996D-02    |proj g|=  2.01675D-04\n",
      "\n",
      "At iterate  992    f=  3.43905D-02    |proj g|=  1.32077D-04\n",
      "\n",
      "At iterate  993    f=  3.43815D-02    |proj g|=  1.15325D-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate  994    f=  3.43685D-02    |proj g|=  1.39119D-04\n",
      "\n",
      "At iterate  995    f=  3.43617D-02    |proj g|=  2.79067D-04\n",
      "\n",
      "At iterate  996    f=  3.43424D-02    |proj g|=  1.50674D-04\n",
      "\n",
      "At iterate  997    f=  3.43231D-02    |proj g|=  1.91571D-04\n",
      "\n",
      "At iterate  998    f=  3.43145D-02    |proj g|=  2.29620D-04\n",
      "\n",
      "At iterate  999    f=  3.43074D-02    |proj g|=  1.89625D-04\n",
      "\n",
      "At iterate 1000    f=  3.42951D-02    |proj g|=  9.07935D-05\n",
      "\n",
      "At iterate 1001    f=  3.42898D-02    |proj g|=  8.54811D-05\n",
      "\n",
      "At iterate 1002    f=  3.42737D-02    |proj g|=  8.63306D-05\n",
      "\n",
      "At iterate 1003    f=  3.42661D-02    |proj g|=  1.83565D-04\n",
      "\n",
      "At iterate 1004    f=  3.42406D-02    |proj g|=  1.17436D-04\n",
      "\n",
      "At iterate 1005    f=  3.42309D-02    |proj g|=  5.77943D-05\n",
      "\n",
      "At iterate 1006    f=  3.42256D-02    |proj g|=  2.10297D-04\n",
      "\n",
      "At iterate 1007    f=  3.42091D-02    |proj g|=  1.18687D-04\n",
      "\n",
      "At iterate 1008    f=  3.41990D-02    |proj g|=  1.18676D-04\n",
      "\n",
      "At iterate 1009    f=  3.41814D-02    |proj g|=  2.41782D-04\n",
      "\n",
      "At iterate 1010    f=  3.41767D-02    |proj g|=  1.09958D-04\n",
      "\n",
      "At iterate 1011    f=  3.41712D-02    |proj g|=  1.34197D-04\n",
      "\n",
      "At iterate 1012    f=  3.41609D-02    |proj g|=  1.16527D-04\n",
      "\n",
      "At iterate 1013    f=  3.41390D-02    |proj g|=  2.62371D-04\n",
      "\n",
      "At iterate 1014    f=  3.41213D-02    |proj g|=  1.26997D-04\n",
      "\n",
      "At iterate 1015    f=  3.41051D-02    |proj g|=  1.25707D-04\n",
      "\n",
      "At iterate 1016    f=  3.40952D-02    |proj g|=  9.51442D-05\n",
      "\n",
      "At iterate 1017    f=  3.40732D-02    |proj g|=  1.95331D-04\n",
      "\n",
      "At iterate 1018    f=  3.40645D-02    |proj g|=  2.17833D-04\n",
      "\n",
      "At iterate 1019    f=  3.40591D-02    |proj g|=  1.65391D-04\n",
      "\n",
      "At iterate 1020    f=  3.40447D-02    |proj g|=  1.51682D-04\n",
      "\n",
      "At iterate 1021    f=  3.40366D-02    |proj g|=  3.68717D-04\n",
      "\n",
      "At iterate 1022    f=  3.40202D-02    |proj g|=  1.05239D-04\n",
      "\n",
      "At iterate 1023    f=  3.40160D-02    |proj g|=  5.66713D-05\n",
      "\n",
      "At iterate 1024    f=  3.40092D-02    |proj g|=  7.12300D-05\n",
      "\n",
      "At iterate 1025    f=  3.40029D-02    |proj g|=  1.20260D-04\n",
      "\n",
      "At iterate 1026    f=  3.39906D-02    |proj g|=  1.16865D-04\n",
      "\n",
      "At iterate 1027    f=  3.39659D-02    |proj g|=  1.37987D-04\n",
      "\n",
      "At iterate 1028    f=  3.39500D-02    |proj g|=  1.16857D-04\n",
      "\n",
      "At iterate 1029    f=  3.39473D-02    |proj g|=  1.77275D-04\n",
      "\n",
      "At iterate 1030    f=  3.39374D-02    |proj g|=  1.69103D-04\n",
      "\n",
      "At iterate 1031    f=  3.39183D-02    |proj g|=  1.16236D-04\n",
      "\n",
      "At iterate 1032    f=  3.39063D-02    |proj g|=  2.49447D-04\n",
      "\n",
      "At iterate 1033    f=  3.39042D-02    |proj g|=  1.89096D-04\n",
      "\n",
      "At iterate 1034    f=  3.39019D-02    |proj g|=  1.61527D-04\n",
      "\n",
      "At iterate 1035    f=  3.38776D-02    |proj g|=  6.98304D-05\n",
      "\n",
      "At iterate 1036    f=  3.38495D-02    |proj g|=  2.54794D-04\n",
      "\n",
      "At iterate 1037    f=  3.38385D-02    |proj g|=  1.26991D-04\n",
      "\n",
      "At iterate 1038    f=  3.38283D-02    |proj g|=  1.23866D-04\n",
      "\n",
      "At iterate 1039    f=  3.38209D-02    |proj g|=  1.09571D-04\n",
      "\n",
      "At iterate 1040    f=  3.38133D-02    |proj g|=  2.42333D-04\n",
      "\n",
      "At iterate 1041    f=  3.37898D-02    |proj g|=  1.12306D-04\n",
      "\n",
      "At iterate 1042    f=  3.37727D-02    |proj g|=  8.47923D-05\n",
      "\n",
      "At iterate 1043    f=  3.37489D-02    |proj g|=  1.16588D-04\n",
      "\n",
      "At iterate 1044    f=  3.37465D-02    |proj g|=  1.20020D-04\n",
      "\n",
      "At iterate 1045    f=  3.37394D-02    |proj g|=  8.09073D-05\n",
      "\n",
      "At iterate 1046    f=  3.37283D-02    |proj g|=  7.23590D-05\n",
      "\n",
      "At iterate 1047    f=  3.37057D-02    |proj g|=  6.14931D-05\n",
      "\n",
      "At iterate 1048    f=  3.36823D-02    |proj g|=  1.03824D-04\n",
      "\n",
      "At iterate 1049    f=  3.36339D-02    |proj g|=  2.00227D-04\n",
      "\n",
      "At iterate 1050    f=  3.35984D-02    |proj g|=  1.61952D-04\n",
      "\n",
      "At iterate 1051    f=  3.35799D-02    |proj g|=  4.77700D-04\n",
      "\n",
      "At iterate 1052    f=  3.35508D-02    |proj g|=  2.44550D-04\n",
      "\n",
      "At iterate 1053    f=  3.35386D-02    |proj g|=  1.31932D-04\n",
      "\n",
      "At iterate 1054    f=  3.35300D-02    |proj g|=  1.00983D-04\n",
      "\n",
      "At iterate 1055    f=  3.35178D-02    |proj g|=  3.73450D-04\n",
      "\n",
      "At iterate 1056    f=  3.35116D-02    |proj g|=  3.88514D-04\n",
      "\n",
      "At iterate 1057    f=  3.34940D-02    |proj g|=  1.40913D-04\n",
      "\n",
      "At iterate 1058    f=  3.34899D-02    |proj g|=  1.13656D-04\n",
      "\n",
      "At iterate 1059    f=  3.34747D-02    |proj g|=  1.89035D-04\n",
      "\n",
      "At iterate 1060    f=  3.34634D-02    |proj g|=  2.63251D-04\n",
      "\n",
      "At iterate 1061    f=  3.34316D-02    |proj g|=  3.00641D-04\n",
      "\n",
      "At iterate 1062    f=  3.34286D-02    |proj g|=  3.79169D-04\n",
      "\n",
      "At iterate 1063    f=  3.33973D-02    |proj g|=  2.15841D-04\n",
      "\n",
      "At iterate 1064    f=  3.33798D-02    |proj g|=  1.16324D-04\n",
      "\n",
      "At iterate 1065    f=  3.33667D-02    |proj g|=  2.05309D-04\n",
      "\n",
      "At iterate 1066    f=  3.33498D-02    |proj g|=  2.35121D-04\n",
      "\n",
      "At iterate 1067    f=  3.33377D-02    |proj g|=  4.34622D-04\n",
      "\n",
      "At iterate 1068    f=  3.32989D-02    |proj g|=  3.79834D-04\n",
      "\n",
      "At iterate 1069    f=  3.32486D-02    |proj g|=  3.66144D-04\n",
      "\n",
      "At iterate 1070    f=  3.32385D-02    |proj g|=  2.28414D-04\n",
      "\n",
      "At iterate 1071    f=  3.32192D-02    |proj g|=  1.73296D-04\n",
      "\n",
      "At iterate 1072    f=  3.31991D-02    |proj g|=  1.72612D-04\n",
      "\n",
      "At iterate 1073    f=  3.31342D-02    |proj g|=  2.44313D-04\n",
      "\n",
      "At iterate 1074    f=  3.31009D-02    |proj g|=  4.33198D-04\n",
      "\n",
      "At iterate 1075    f=  3.30542D-02    |proj g|=  4.16642D-04\n",
      "\n",
      "At iterate 1076    f=  3.30216D-02    |proj g|=  2.47890D-04\n",
      "\n",
      "At iterate 1077    f=  3.29985D-02    |proj g|=  1.51510D-04\n",
      "\n",
      "At iterate 1078    f=  3.29844D-02    |proj g|=  2.94182D-04\n",
      "\n",
      "At iterate 1079    f=  3.29503D-02    |proj g|=  3.81920D-04\n",
      "\n",
      "At iterate 1080    f=  3.29148D-02    |proj g|=  2.90739D-04\n",
      "\n",
      "At iterate 1081    f=  3.29005D-02    |proj g|=  2.85174D-04\n",
      "\n",
      "At iterate 1082    f=  3.28710D-02    |proj g|=  1.09537D-04\n",
      "\n",
      "At iterate 1083    f=  3.28606D-02    |proj g|=  1.06060D-04\n",
      "\n",
      "At iterate 1084    f=  3.28037D-02    |proj g|=  9.20562D-05\n",
      "\n",
      "At iterate 1085    f=  3.27594D-02    |proj g|=  5.59966D-04\n",
      "\n",
      "At iterate 1086    f=  3.27276D-02    |proj g|=  4.65267D-04\n",
      "\n",
      "At iterate 1087    f=  3.26990D-02    |proj g|=  1.26930D-04\n",
      "\n",
      "At iterate 1088    f=  3.26898D-02    |proj g|=  1.48704D-04\n",
      "\n",
      "At iterate 1089    f=  3.26848D-02    |proj g|=  1.56716D-04\n",
      "\n",
      "At iterate 1090    f=  3.26806D-02    |proj g|=  1.46357D-04\n",
      "\n",
      "At iterate 1091    f=  3.26669D-02    |proj g|=  8.23329D-05\n",
      "\n",
      "At iterate 1092    f=  3.26583D-02    |proj g|=  1.78488D-04\n",
      "\n",
      "At iterate 1093    f=  3.26393D-02    |proj g|=  2.16742D-04\n",
      "\n",
      "At iterate 1094    f=  3.26253D-02    |proj g|=  2.26402D-04\n",
      "\n",
      "At iterate 1095    f=  3.25990D-02    |proj g|=  1.51576D-04\n",
      "\n",
      "At iterate 1096    f=  3.25788D-02    |proj g|=  2.27144D-04\n",
      "\n",
      "At iterate 1097    f=  3.25669D-02    |proj g|=  4.13214D-04\n",
      "\n",
      "At iterate 1098    f=  3.25427D-02    |proj g|=  3.09728D-04\n",
      "\n",
      "At iterate 1099    f=  3.25112D-02    |proj g|=  1.43569D-04\n",
      "\n",
      "At iterate 1100    f=  3.25032D-02    |proj g|=  8.81895D-05\n",
      "\n",
      "At iterate 1101    f=  3.24959D-02    |proj g|=  1.82080D-04\n",
      "\n",
      "At iterate 1102    f=  3.24800D-02    |proj g|=  1.31397D-04\n",
      "\n",
      "At iterate 1103    f=  3.24536D-02    |proj g|=  1.43631D-04\n",
      "\n",
      "At iterate 1104    f=  3.24250D-02    |proj g|=  1.78781D-04\n",
      "\n",
      "At iterate 1105    f=  3.24015D-02    |proj g|=  1.88035D-04\n",
      "\n",
      "At iterate 1106    f=  3.23656D-02    |proj g|=  2.16675D-04\n",
      "\n",
      "At iterate 1107    f=  3.23301D-02    |proj g|=  1.98832D-04\n",
      "\n",
      "At iterate 1108    f=  3.23222D-02    |proj g|=  1.66540D-04\n",
      "\n",
      "At iterate 1109    f=  3.23131D-02    |proj g|=  1.54127D-04\n",
      "\n",
      "At iterate 1110    f=  3.22956D-02    |proj g|=  9.86940D-05\n",
      "\n",
      "At iterate 1111    f=  3.22688D-02    |proj g|=  1.18623D-04\n",
      "\n",
      "At iterate 1112    f=  3.22286D-02    |proj g|=  4.59713D-04\n",
      "\n",
      "At iterate 1113    f=  3.21869D-02    |proj g|=  2.69190D-04\n",
      "\n",
      "At iterate 1114    f=  3.21638D-02    |proj g|=  1.82212D-04\n",
      "\n",
      "At iterate 1115    f=  3.21541D-02    |proj g|=  1.51069D-04\n",
      "\n",
      "At iterate 1116    f=  3.21293D-02    |proj g|=  1.50832D-04\n",
      "\n",
      "At iterate 1117    f=  3.21128D-02    |proj g|=  1.63341D-04\n",
      "\n",
      "At iterate 1118    f=  3.20979D-02    |proj g|=  2.07851D-04\n",
      "\n",
      "At iterate 1119    f=  3.20804D-02    |proj g|=  3.98885D-04\n",
      "\n",
      "At iterate 1120    f=  3.20622D-02    |proj g|=  2.89987D-04\n",
      "\n",
      "At iterate 1121    f=  3.20500D-02    |proj g|=  1.85487D-04\n",
      "\n",
      "At iterate 1122    f=  3.20234D-02    |proj g|=  1.53770D-04\n",
      "\n",
      "At iterate 1123    f=  3.19947D-02    |proj g|=  2.02340D-04\n",
      "\n",
      "At iterate 1124    f=  3.19572D-02    |proj g|=  3.51684D-04\n",
      "\n",
      "At iterate 1125    f=  3.19322D-02    |proj g|=  1.57522D-04\n",
      "\n",
      "At iterate 1126    f=  3.19064D-02    |proj g|=  1.10077D-04\n",
      "\n",
      "At iterate 1127    f=  3.18682D-02    |proj g|=  2.37452D-04\n",
      "\n",
      "At iterate 1128    f=  3.18441D-02    |proj g|=  1.48918D-04\n",
      "\n",
      "At iterate 1129    f=  3.18299D-02    |proj g|=  1.65016D-04\n",
      "\n",
      "At iterate 1130    f=  3.18192D-02    |proj g|=  2.31230D-04\n",
      "\n",
      "At iterate 1131    f=  3.18099D-02    |proj g|=  1.24838D-04\n",
      "\n",
      "At iterate 1132    f=  3.18036D-02    |proj g|=  1.02821D-04\n",
      "\n",
      "At iterate 1133    f=  3.17774D-02    |proj g|=  3.31096D-04\n",
      "\n",
      "At iterate 1134    f=  3.17600D-02    |proj g|=  1.16556D-04\n",
      "\n",
      "At iterate 1135    f=  3.17550D-02    |proj g|=  4.88872D-05\n",
      "\n",
      "At iterate 1136    f=  3.17536D-02    |proj g|=  6.35349D-05\n",
      "\n",
      "At iterate 1137    f=  3.17453D-02    |proj g|=  6.25650D-05\n",
      "\n",
      "At iterate 1138    f=  3.17385D-02    |proj g|=  2.17345D-04\n",
      "\n",
      "At iterate 1139    f=  3.17266D-02    |proj g|=  1.38097D-04\n",
      "\n",
      "At iterate 1140    f=  3.17116D-02    |proj g|=  1.40636D-04\n",
      "\n",
      "At iterate 1141    f=  3.16998D-02    |proj g|=  1.58907D-04\n",
      "\n",
      "At iterate 1142    f=  3.16970D-02    |proj g|=  2.62104D-04\n",
      "\n",
      "At iterate 1143    f=  3.16864D-02    |proj g|=  1.80289D-04\n",
      "\n",
      "At iterate 1144    f=  3.16763D-02    |proj g|=  1.52218D-04\n",
      "\n",
      "At iterate 1145    f=  3.16703D-02    |proj g|=  1.71773D-04\n",
      "\n",
      "At iterate 1146    f=  3.16500D-02    |proj g|=  1.88966D-04\n",
      "\n",
      "At iterate 1147    f=  3.16354D-02    |proj g|=  1.48863D-04\n",
      "\n",
      "At iterate 1148    f=  3.16240D-02    |proj g|=  1.80141D-04\n",
      "\n",
      "At iterate 1149    f=  3.16174D-02    |proj g|=  1.65161D-04\n",
      "\n",
      "At iterate 1150    f=  3.16049D-02    |proj g|=  1.84376D-04\n",
      "\n",
      "At iterate 1151    f=  3.15927D-02    |proj g|=  2.25373D-04\n",
      "\n",
      "At iterate 1152    f=  3.15700D-02    |proj g|=  2.35490D-04\n",
      "\n",
      "At iterate 1153    f=  3.15400D-02    |proj g|=  1.38967D-04\n",
      "\n",
      "At iterate 1154    f=  3.15262D-02    |proj g|=  4.17603D-04\n",
      "\n",
      "At iterate 1155    f=  3.15031D-02    |proj g|=  2.17615D-04\n",
      "\n",
      "At iterate 1156    f=  3.14985D-02    |proj g|=  1.23564D-04\n",
      "\n",
      "At iterate 1157    f=  3.14940D-02    |proj g|=  1.21901D-04\n",
      "\n",
      "At iterate 1158    f=  3.14908D-02    |proj g|=  1.29958D-04\n",
      "\n",
      "At iterate 1159    f=  3.14829D-02    |proj g|=  1.33220D-04\n",
      "\n",
      "At iterate 1160    f=  3.14789D-02    |proj g|=  2.44309D-04\n",
      "\n",
      "At iterate 1161    f=  3.14636D-02    |proj g|=  1.85570D-04\n",
      "\n",
      "At iterate 1162    f=  3.14445D-02    |proj g|=  8.10920D-05\n",
      "\n",
      "At iterate 1163    f=  3.14295D-02    |proj g|=  1.77816D-04\n",
      "\n",
      "At iterate 1164    f=  3.14201D-02    |proj g|=  2.25932D-04\n",
      "\n",
      "At iterate 1165    f=  3.14032D-02    |proj g|=  2.25065D-04\n",
      "\n",
      "At iterate 1166    f=  3.13972D-02    |proj g|=  4.48005D-04\n",
      "\n",
      "At iterate 1167    f=  3.13734D-02    |proj g|=  2.58360D-04\n",
      "\n",
      "At iterate 1168    f=  3.13603D-02    |proj g|=  1.26524D-04\n",
      "\n",
      "At iterate 1169    f=  3.13551D-02    |proj g|=  1.74942D-04\n",
      "\n",
      "At iterate 1170    f=  3.13499D-02    |proj g|=  2.06618D-04\n",
      "\n",
      "At iterate 1171    f=  3.13390D-02    |proj g|=  2.09075D-04\n",
      "\n",
      "At iterate 1172    f=  3.13348D-02    |proj g|=  1.66713D-04\n",
      "\n",
      "At iterate 1173    f=  3.13264D-02    |proj g|=  1.30961D-04\n",
      "\n",
      "At iterate 1174    f=  3.13158D-02    |proj g|=  9.63464D-05\n",
      "\n",
      "At iterate 1175    f=  3.13003D-02    |proj g|=  1.27030D-04\n",
      "\n",
      "At iterate 1176    f=  3.12627D-02    |proj g|=  1.37442D-04\n",
      "\n",
      "At iterate 1177    f=  3.12443D-02    |proj g|=  8.62241D-05\n",
      "\n",
      "At iterate 1178    f=  3.12415D-02    |proj g|=  2.21836D-04\n",
      "\n",
      "At iterate 1179    f=  3.12308D-02    |proj g|=  1.28697D-04\n",
      "\n",
      "At iterate 1180    f=  3.12170D-02    |proj g|=  1.62657D-04\n",
      "\n",
      "At iterate 1181    f=  3.12015D-02    |proj g|=  1.55812D-04\n",
      "\n",
      "At iterate 1182    f=  3.11892D-02    |proj g|=  2.49956D-04\n",
      "\n",
      "At iterate 1183    f=  3.11838D-02    |proj g|=  8.07336D-05\n",
      "\n",
      "At iterate 1184    f=  3.11813D-02    |proj g|=  1.14681D-04\n",
      "\n",
      "At iterate 1185    f=  3.11690D-02    |proj g|=  1.26741D-04\n",
      "\n",
      "At iterate 1186    f=  3.11286D-02    |proj g|=  1.46292D-04\n",
      "\n",
      "At iterate 1187    f=  3.10759D-02    |proj g|=  1.92833D-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate 1188    f=  3.10237D-02    |proj g|=  2.32610D-04\n",
      "\n",
      "At iterate 1189    f=  3.10128D-02    |proj g|=  4.22681D-04\n",
      "\n",
      "At iterate 1190    f=  3.09811D-02    |proj g|=  6.97626D-05\n",
      "\n",
      "At iterate 1191    f=  3.09744D-02    |proj g|=  5.17535D-05\n",
      "\n",
      "At iterate 1192    f=  3.09629D-02    |proj g|=  8.44688D-05\n",
      "\n",
      "At iterate 1193    f=  3.09444D-02    |proj g|=  4.08668D-04\n",
      "\n",
      "At iterate 1194    f=  3.09334D-02    |proj g|=  5.05225D-04\n",
      "\n",
      "At iterate 1195    f=  3.09103D-02    |proj g|=  2.47917D-04\n",
      "\n",
      "At iterate 1196    f=  3.08988D-02    |proj g|=  2.64128D-04\n",
      "\n",
      "At iterate 1197    f=  3.08805D-02    |proj g|=  3.47453D-04\n",
      "\n",
      "At iterate 1198    f=  3.08450D-02    |proj g|=  3.21294D-04\n",
      "\n",
      "At iterate 1199    f=  3.07939D-02    |proj g|=  3.57303D-04\n",
      "\n",
      "At iterate 1200    f=  3.07865D-02    |proj g|=  4.33930D-04\n",
      "\n",
      "At iterate 1201    f=  3.07822D-02    |proj g|=  2.68489D-04\n",
      "\n",
      "At iterate 1202    f=  3.07707D-02    |proj g|=  1.22092D-04\n",
      "\n",
      "At iterate 1203    f=  3.07655D-02    |proj g|=  1.36585D-04\n",
      "\n",
      "At iterate 1204    f=  3.07535D-02    |proj g|=  1.73657D-04\n",
      "\n",
      "At iterate 1205    f=  3.07456D-02    |proj g|=  1.49153D-04\n",
      "\n",
      "At iterate 1206    f=  3.07262D-02    |proj g|=  2.49007D-04\n",
      "\n",
      "At iterate 1207    f=  3.07119D-02    |proj g|=  1.48758D-04\n",
      "\n",
      "At iterate 1208    f=  3.06912D-02    |proj g|=  1.71129D-04\n",
      "\n",
      "At iterate 1209    f=  3.06785D-02    |proj g|=  1.61669D-04\n",
      "\n",
      "At iterate 1210    f=  3.06631D-02    |proj g|=  2.19549D-04\n",
      "\n",
      "At iterate 1211    f=  3.06519D-02    |proj g|=  1.93731D-04\n",
      "\n",
      "At iterate 1212    f=  3.06387D-02    |proj g|=  1.90739D-04\n",
      "\n",
      "At iterate 1213    f=  3.06187D-02    |proj g|=  1.02770D-04\n",
      "\n",
      "At iterate 1214    f=  3.06040D-02    |proj g|=  1.49960D-04\n",
      "\n",
      "At iterate 1215    f=  3.05897D-02    |proj g|=  1.49551D-04\n",
      "\n",
      "At iterate 1216    f=  3.05579D-02    |proj g|=  1.08040D-04\n",
      "\n",
      "At iterate 1217    f=  3.05494D-02    |proj g|=  3.04099D-04\n",
      "\n",
      "At iterate 1218    f=  3.05196D-02    |proj g|=  1.22840D-04\n",
      "\n",
      "At iterate 1219    f=  3.05033D-02    |proj g|=  1.56693D-04\n",
      "\n",
      "At iterate 1220    f=  3.04855D-02    |proj g|=  2.10660D-04\n",
      "\n",
      "At iterate 1221    f=  3.04589D-02    |proj g|=  1.78991D-04\n",
      "\n",
      "At iterate 1222    f=  3.04247D-02    |proj g|=  2.62918D-04\n",
      "\n",
      "At iterate 1223    f=  3.04013D-02    |proj g|=  3.07444D-04\n",
      "\n",
      "At iterate 1224    f=  3.03813D-02    |proj g|=  3.70250D-04\n",
      "\n",
      "At iterate 1225    f=  3.03511D-02    |proj g|=  2.01050D-04\n",
      "\n",
      "At iterate 1226    f=  3.03250D-02    |proj g|=  2.23742D-04\n",
      "\n",
      "At iterate 1227    f=  3.02729D-02    |proj g|=  1.15748D-04\n",
      "\n",
      "At iterate 1228    f=  3.02102D-02    |proj g|=  2.19854D-04\n",
      "\n",
      "At iterate 1229    f=  3.01915D-02    |proj g|=  4.78412D-04\n",
      "\n",
      "At iterate 1230    f=  3.01552D-02    |proj g|=  4.13595D-04\n",
      "\n",
      "At iterate 1231    f=  3.00407D-02    |proj g|=  4.58892D-04\n",
      "\n",
      "At iterate 1232    f=  3.00024D-02    |proj g|=  1.72131D-04\n",
      "\n",
      "At iterate 1233    f=  2.99393D-02    |proj g|=  1.52971D-04\n",
      "\n",
      "At iterate 1234    f=  2.99245D-02    |proj g|=  3.41352D-04\n",
      "\n",
      "At iterate 1235    f=  2.98936D-02    |proj g|=  1.69453D-04\n",
      "\n",
      "At iterate 1236    f=  2.98723D-02    |proj g|=  1.08245D-04\n",
      "\n",
      "At iterate 1237    f=  2.98507D-02    |proj g|=  1.87257D-04\n",
      "\n",
      "At iterate 1238    f=  2.98072D-02    |proj g|=  2.63392D-04\n",
      "\n",
      "At iterate 1239    f=  2.97082D-02    |proj g|=  6.47061D-04\n",
      "\n",
      "At iterate 1240    f=  2.96833D-02    |proj g|=  4.22022D-04\n",
      "\n",
      "At iterate 1241    f=  2.95920D-02    |proj g|=  4.74146D-04\n",
      "\n",
      "At iterate 1242    f=  2.94394D-02    |proj g|=  3.01165D-04\n",
      "\n",
      "At iterate 1243    f=  2.93053D-02    |proj g|=  2.63259D-04\n",
      "\n",
      "At iterate 1244    f=  2.92949D-02    |proj g|=  1.20412D-04\n",
      "\n",
      "At iterate 1245    f=  2.92788D-02    |proj g|=  1.64932D-04\n",
      "\n",
      "At iterate 1246    f=  2.92691D-02    |proj g|=  3.79730D-04\n",
      "\n",
      "At iterate 1247    f=  2.92285D-02    |proj g|=  3.17983D-04\n",
      "\n",
      "At iterate 1248    f=  2.91157D-02    |proj g|=  1.62884D-04\n",
      "\n",
      "At iterate 1249    f=  2.90602D-02    |proj g|=  1.99499D-04\n",
      "\n",
      "At iterate 1250    f=  2.89668D-02    |proj g|=  2.18602D-04\n",
      "\n",
      "At iterate 1251    f=  2.89488D-02    |proj g|=  6.31343D-04\n",
      "\n",
      "At iterate 1252    f=  2.88393D-02    |proj g|=  3.70277D-04\n",
      "\n",
      "At iterate 1253    f=  2.87829D-02    |proj g|=  2.52806D-04\n",
      "\n",
      "At iterate 1254    f=  2.87236D-02    |proj g|=  2.41137D-04\n",
      "\n",
      "At iterate 1255    f=  2.87053D-02    |proj g|=  2.01708D-04\n",
      "\n",
      "At iterate 1256    f=  2.86812D-02    |proj g|=  2.33040D-04\n",
      "\n",
      "At iterate 1257    f=  2.86687D-02    |proj g|=  4.71680D-04\n",
      "\n",
      "At iterate 1258    f=  2.86335D-02    |proj g|=  2.84806D-04\n",
      "\n",
      "At iterate 1259    f=  2.86171D-02    |proj g|=  2.17619D-04\n",
      "\n",
      "At iterate 1260    f=  2.85810D-02    |proj g|=  1.22260D-04\n",
      "\n",
      "At iterate 1261    f=  2.85476D-02    |proj g|=  2.29485D-04\n",
      "\n",
      "At iterate 1262    f=  2.84981D-02    |proj g|=  2.18242D-04\n",
      "\n",
      "At iterate 1263    f=  2.84951D-02    |proj g|=  3.66052D-04\n",
      "\n",
      "At iterate 1264    f=  2.84655D-02    |proj g|=  2.57922D-04\n",
      "\n",
      "At iterate 1265    f=  2.84479D-02    |proj g|=  1.13531D-04\n",
      "\n",
      "At iterate 1266    f=  2.84396D-02    |proj g|=  1.66775D-04\n",
      "\n",
      "At iterate 1267    f=  2.84289D-02    |proj g|=  1.93148D-04\n",
      "\n",
      "At iterate 1268    f=  2.83956D-02    |proj g|=  2.04507D-04\n",
      "\n",
      "At iterate 1269    f=  2.83888D-02    |proj g|=  1.95393D-04\n",
      "\n",
      "At iterate 1270    f=  2.83469D-02    |proj g|=  1.93295D-04\n",
      "\n",
      "At iterate 1271    f=  2.83068D-02    |proj g|=  2.77108D-04\n",
      "\n",
      "At iterate 1272    f=  2.82576D-02    |proj g|=  1.66382D-04\n",
      "\n",
      "At iterate 1273    f=  2.81870D-02    |proj g|=  1.64370D-04\n",
      "\n",
      "At iterate 1274    f=  2.81834D-02    |proj g|=  3.28435D-04\n",
      "\n",
      "At iterate 1275    f=  2.81618D-02    |proj g|=  2.73727D-04\n",
      "\n",
      "At iterate 1276    f=  2.81477D-02    |proj g|=  1.88745D-04\n",
      "\n",
      "At iterate 1277    f=  2.81283D-02    |proj g|=  1.55537D-04\n",
      "\n",
      "At iterate 1278    f=  2.80884D-02    |proj g|=  2.39682D-04\n",
      "\n",
      "At iterate 1279    f=  2.80219D-02    |proj g|=  2.55190D-04\n",
      "\n",
      "At iterate 1280    f=  2.80059D-02    |proj g|=  4.31659D-04\n",
      "\n",
      "At iterate 1281    f=  2.79504D-02    |proj g|=  2.25250D-04\n",
      "\n",
      "At iterate 1282    f=  2.79131D-02    |proj g|=  1.29986D-04\n",
      "\n",
      "At iterate 1283    f=  2.78923D-02    |proj g|=  1.10707D-04\n",
      "\n",
      "At iterate 1284    f=  2.78708D-02    |proj g|=  1.73200D-04\n",
      "\n",
      "At iterate 1285    f=  2.78608D-02    |proj g|=  2.26443D-04\n",
      "\n",
      "At iterate 1286    f=  2.78267D-02    |proj g|=  1.17446D-04\n",
      "\n",
      "At iterate 1287    f=  2.77948D-02    |proj g|=  1.34202D-04\n",
      "\n",
      "At iterate 1288    f=  2.77546D-02    |proj g|=  1.23157D-04\n",
      "\n",
      "At iterate 1289    f=  2.77329D-02    |proj g|=  3.40160D-04\n",
      "\n",
      "At iterate 1290    f=  2.77059D-02    |proj g|=  2.25112D-04\n",
      "\n",
      "At iterate 1291    f=  2.76691D-02    |proj g|=  2.04704D-04\n",
      "\n",
      "At iterate 1292    f=  2.76512D-02    |proj g|=  1.70337D-04\n",
      "\n",
      "At iterate 1293    f=  2.76324D-02    |proj g|=  4.55568D-04\n",
      "\n",
      "At iterate 1294    f=  2.75899D-02    |proj g|=  3.14819D-04\n",
      "\n",
      "At iterate 1295    f=  2.75375D-02    |proj g|=  1.86824D-04\n",
      "\n",
      "At iterate 1296    f=  2.75169D-02    |proj g|=  2.08743D-04\n",
      "\n",
      "At iterate 1297    f=  2.74781D-02    |proj g|=  4.45045D-04\n",
      "\n",
      "At iterate 1298    f=  2.73909D-02    |proj g|=  4.99164D-04\n",
      "\n",
      "At iterate 1299    f=  2.72847D-02    |proj g|=  4.67557D-04\n",
      "\n",
      "At iterate 1300    f=  2.72382D-02    |proj g|=  5.09927D-04\n",
      "\n",
      "At iterate 1301    f=  2.71234D-02    |proj g|=  1.68714D-04\n",
      "\n",
      "At iterate 1302    f=  2.70819D-02    |proj g|=  1.70398D-04\n",
      "\n",
      "At iterate 1303    f=  2.69771D-02    |proj g|=  1.60473D-04\n",
      "\n",
      "At iterate 1304    f=  2.69130D-02    |proj g|=  4.56415D-04\n",
      "\n",
      "At iterate 1305    f=  2.67782D-02    |proj g|=  5.75565D-04\n",
      "\n",
      "At iterate 1306    f=  2.67091D-02    |proj g|=  3.93035D-04\n",
      "\n",
      "At iterate 1307    f=  2.66811D-02    |proj g|=  2.17488D-04\n",
      "\n",
      "At iterate 1308    f=  2.66675D-02    |proj g|=  1.94271D-04\n",
      "\n",
      "At iterate 1309    f=  2.66446D-02    |proj g|=  4.09215D-04\n",
      "\n",
      "At iterate 1310    f=  2.65689D-02    |proj g|=  3.07601D-04\n",
      "\n",
      "At iterate 1311    f=  2.63597D-02    |proj g|=  2.20956D-04\n",
      "\n",
      "At iterate 1312    f=  2.63035D-02    |proj g|=  9.20589D-04\n",
      "\n",
      "At iterate 1313    f=  2.61921D-02    |proj g|=  5.38334D-04\n",
      "\n",
      "At iterate 1314    f=  2.61303D-02    |proj g|=  3.21455D-04\n",
      "\n",
      "At iterate 1315    f=  2.60991D-02    |proj g|=  1.66972D-04\n",
      "\n",
      "At iterate 1316    f=  2.60772D-02    |proj g|=  2.15553D-04\n",
      "\n",
      "At iterate 1317    f=  2.60478D-02    |proj g|=  1.49539D-04\n",
      "\n",
      "At iterate 1318    f=  2.60096D-02    |proj g|=  5.51974D-04\n",
      "\n",
      "At iterate 1319    f=  2.59665D-02    |proj g|=  2.55615D-04\n",
      "\n",
      "At iterate 1320    f=  2.59452D-02    |proj g|=  1.50912D-04\n",
      "\n",
      "At iterate 1321    f=  2.59330D-02    |proj g|=  2.02891D-04\n",
      "\n",
      "At iterate 1322    f=  2.59141D-02    |proj g|=  2.48205D-04\n",
      "\n",
      "At iterate 1323    f=  2.59026D-02    |proj g|=  1.19077D-03\n",
      "\n",
      "At iterate 1324    f=  2.58318D-02    |proj g|=  5.71951D-04\n",
      "\n",
      "At iterate 1325    f=  2.57755D-02    |proj g|=  2.66959D-04\n",
      "\n",
      "At iterate 1326    f=  2.57222D-02    |proj g|=  3.62687D-04\n",
      "\n",
      "At iterate 1327    f=  2.56941D-02    |proj g|=  2.52142D-04\n",
      "\n",
      "At iterate 1328    f=  2.56525D-02    |proj g|=  5.14031D-04\n",
      "\n",
      "At iterate 1329    f=  2.56331D-02    |proj g|=  4.03478D-04\n",
      "\n",
      "At iterate 1330    f=  2.56017D-02    |proj g|=  3.42658D-04\n",
      "\n",
      "At iterate 1331    f=  2.55923D-02    |proj g|=  3.69041D-04\n",
      "\n",
      "At iterate 1332    f=  2.55589D-02    |proj g|=  5.06787D-04\n",
      "\n",
      "At iterate 1333    f=  2.54995D-02    |proj g|=  4.69067D-04\n",
      "\n",
      "At iterate 1334    f=  2.54302D-02    |proj g|=  5.43651D-04\n",
      "\n",
      "At iterate 1335    f=  2.52587D-02    |proj g|=  1.05394D-03\n",
      "\n",
      "At iterate 1336    f=  2.51440D-02    |proj g|=  3.81954D-04\n",
      "\n",
      "At iterate 1337    f=  2.50701D-02    |proj g|=  4.88679D-04\n",
      "\n",
      "At iterate 1338    f=  2.50420D-02    |proj g|=  1.42737D-04\n",
      "\n",
      "At iterate 1339    f=  2.50351D-02    |proj g|=  1.65534D-04\n",
      "\n",
      "At iterate 1340    f=  2.50216D-02    |proj g|=  9.27018D-05\n",
      "\n",
      "At iterate 1341    f=  2.50093D-02    |proj g|=  2.57743D-04\n",
      "\n",
      "At iterate 1342    f=  2.49943D-02    |proj g|=  1.32965D-04\n",
      "\n",
      "At iterate 1343    f=  2.49780D-02    |proj g|=  1.52505D-04\n",
      "\n",
      "At iterate 1344    f=  2.49534D-02    |proj g|=  1.68268D-04\n",
      "\n",
      "At iterate 1345    f=  2.49107D-02    |proj g|=  2.36907D-04\n",
      "\n",
      "At iterate 1346    f=  2.48386D-02    |proj g|=  2.66387D-04\n",
      "\n",
      "At iterate 1347    f=  2.48237D-02    |proj g|=  5.44911D-04\n",
      "\n",
      "At iterate 1348    f=  2.47428D-02    |proj g|=  3.68045D-04\n",
      "\n",
      "At iterate 1349    f=  2.47205D-02    |proj g|=  2.14027D-04\n",
      "\n",
      "At iterate 1350    f=  2.46946D-02    |proj g|=  1.38459D-04\n",
      "\n",
      "At iterate 1351    f=  2.46660D-02    |proj g|=  6.64441D-05\n",
      "\n",
      "At iterate 1352    f=  2.46558D-02    |proj g|=  4.01807D-04\n",
      "\n",
      "At iterate 1353    f=  2.46394D-02    |proj g|=  1.21568D-04\n",
      "\n",
      "At iterate 1354    f=  2.46338D-02    |proj g|=  1.08215D-04\n",
      "\n",
      "At iterate 1355    f=  2.46134D-02    |proj g|=  1.50792D-04\n",
      "\n",
      "At iterate 1356    f=  2.45636D-02    |proj g|=  1.78621D-04\n",
      "\n",
      "At iterate 1357    f=  2.44764D-02    |proj g|=  2.17161D-04\n",
      "\n",
      "At iterate 1358    f=  2.44650D-02    |proj g|=  3.62678D-04\n",
      "\n",
      "At iterate 1359    f=  2.43898D-02    |proj g|=  2.01597D-04\n",
      "\n",
      "At iterate 1360    f=  2.42540D-02    |proj g|=  5.02342D-04\n",
      "\n",
      "At iterate 1361    f=  2.42308D-02    |proj g|=  6.35535D-04\n",
      "\n",
      "At iterate 1362    f=  2.41979D-02    |proj g|=  4.50583D-04\n",
      "\n",
      "At iterate 1363    f=  2.40795D-02    |proj g|=  4.06096D-04\n",
      "\n",
      "At iterate 1364    f=  2.40026D-02    |proj g|=  5.21648D-04\n",
      "\n",
      "At iterate 1365    f=  2.39407D-02    |proj g|=  4.92189D-04\n",
      "\n",
      "At iterate 1366    f=  2.39019D-02    |proj g|=  6.74615D-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " This problem is unconstrained.\n",
      "\r",
      " 70%|███████   | 7/10 [00:01<00:00,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate 1367    f=  2.38132D-02    |proj g|=  5.77432D-04\n",
      "\n",
      "At iterate 1368    f=  2.36516D-02    |proj g|=  2.68329D-04\n",
      "\n",
      "At iterate 1369    f=  2.35770D-02    |proj g|=  3.41019D-04\n",
      "\n",
      "At iterate 1370    f=  2.32671D-02    |proj g|=  8.51278D-04\n",
      "\n",
      "At iterate 1371    f=  2.31552D-02    |proj g|=  4.54974D-04\n",
      "\n",
      "At iterate 1372    f=  2.31192D-02    |proj g|=  5.45409D-04\n",
      "\n",
      "At iterate 1373    f=  2.30490D-02    |proj g|=  5.84764D-04\n",
      "\n",
      "At iterate 1374    f=  2.29230D-02    |proj g|=  4.29634D-04\n",
      "\n",
      "At iterate 1375    f=  2.28884D-02    |proj g|=  8.78726D-04\n",
      "\n",
      "At iterate 1376    f=  2.28138D-02    |proj g|=  4.64587D-04\n",
      "\n",
      "At iterate 1377    f=  2.27539D-02    |proj g|=  4.08096D-04\n",
      "\n",
      "At iterate 1378    f=  2.26807D-02    |proj g|=  6.95442D-04\n",
      "\n",
      "At iterate 1379    f=  2.25206D-02    |proj g|=  1.03943D-03\n",
      "\n",
      "At iterate 1380    f=  2.22421D-02    |proj g|=  1.30407D-03\n",
      "\n",
      "At iterate 1381    f=  2.19258D-02    |proj g|=  8.90430D-04\n",
      "\n",
      "At iterate 1382    f=  2.17772D-02    |proj g|=  1.40045D-03\n",
      "\n",
      "At iterate 1383    f=  2.17649D-02    |proj g|=  6.62837D-04\n",
      "\n",
      "At iterate 1384    f=  2.16160D-02    |proj g|=  5.28608D-04\n",
      "\n",
      "At iterate 1385    f=  2.15645D-02    |proj g|=  5.63847D-04\n",
      "\n",
      "At iterate 1386    f=  2.13047D-02    |proj g|=  6.99764D-04\n",
      "\n",
      "At iterate 1387    f=  2.12239D-02    |proj g|=  1.18807D-03\n",
      "\n",
      "At iterate 1388    f=  2.11057D-02    |proj g|=  3.90732D-04\n",
      "\n",
      "At iterate 1389    f=  2.10564D-02    |proj g|=  4.60237D-04\n",
      "\n",
      "At iterate 1390    f=  2.10038D-02    |proj g|=  6.97104D-04\n",
      "\n",
      "At iterate 1391    f=  2.09021D-02    |proj g|=  6.09639D-04\n",
      "\n",
      "At iterate 1392    f=  2.08183D-02    |proj g|=  9.54859D-04\n",
      "\n",
      "At iterate 1393    f=  2.06746D-02    |proj g|=  5.86275D-04\n",
      "\n",
      "At iterate 1394    f=  2.05957D-02    |proj g|=  4.23012D-04\n",
      "\n",
      "At iterate 1395    f=  2.04759D-02    |proj g|=  6.53082D-04\n",
      "\n",
      "At iterate 1396    f=  2.03993D-02    |proj g|=  6.34067D-04\n",
      "\n",
      "At iterate 1397    f=  2.01402D-02    |proj g|=  2.97619D-04\n",
      "\n",
      "At iterate 1398    f=  1.99786D-02    |proj g|=  3.88256D-04\n",
      "\n",
      "At iterate 1399    f=  1.99045D-02    |proj g|=  6.13646D-04\n",
      "\n",
      "At iterate 1400    f=  1.97445D-02    |proj g|=  3.36511D-04\n",
      "\n",
      "At iterate 1401    f=  1.96597D-02    |proj g|=  3.66395D-04\n",
      "\n",
      "At iterate 1402    f=  1.95933D-02    |proj g|=  3.16840D-04\n",
      "\n",
      "At iterate 1403    f=  1.91412D-02    |proj g|=  7.80533D-04\n",
      "\n",
      "At iterate 1404    f=  1.84711D-02    |proj g|=  1.50158D-03\n",
      "\n",
      "At iterate 1405    f=  1.77352D-02    |proj g|=  1.67748D-03\n",
      "\n",
      "At iterate 1406    f=  1.76871D-02    |proj g|=  1.72749D-03\n",
      "\n",
      "At iterate 1407    f=  1.71543D-02    |proj g|=  8.60225D-04\n",
      "\n",
      "At iterate 1408    f=  1.69846D-02    |proj g|=  5.86249D-04\n",
      "\n",
      "At iterate 1409    f=  1.68909D-02    |proj g|=  4.70242D-04\n",
      "\n",
      "At iterate 1410    f=  1.66934D-02    |proj g|=  3.13535D-04\n",
      "\n",
      "At iterate 1411    f=  1.63945D-02    |proj g|=  4.41043D-04\n",
      "\n",
      "At iterate 1412    f=  1.63097D-02    |proj g|=  1.07094D-03\n",
      "\n",
      "At iterate 1413    f=  1.59681D-02    |proj g|=  8.62787D-04\n",
      "\n",
      "At iterate 1414    f=  1.57011D-02    |proj g|=  6.14228D-04\n",
      "\n",
      "At iterate 1415    f=  1.54146D-02    |proj g|=  2.16559D-04\n",
      "\n",
      "At iterate 1416    f=  1.52998D-02    |proj g|=  3.12593D-04\n",
      "\n",
      "At iterate 1417    f=  1.52520D-02    |proj g|=  8.69347D-04\n",
      "\n",
      "At iterate 1418    f=  1.51316D-02    |proj g|=  6.80177D-04\n",
      "\n",
      "At iterate 1419    f=  1.49950D-02    |proj g|=  5.45912D-04\n",
      "\n",
      "At iterate 1420    f=  1.48623D-02    |proj g|=  5.49588D-04\n",
      "\n",
      "At iterate 1421    f=  1.46350D-02    |proj g|=  6.65494D-04\n",
      "\n",
      "At iterate 1422    f=  1.43494D-02    |proj g|=  5.19165D-04\n",
      "\n",
      "At iterate 1423    f=  1.37311D-02    |proj g|=  1.02402D-03\n",
      "\n",
      "At iterate 1424    f=  1.33807D-02    |proj g|=  1.14394D-03\n",
      "\n",
      "At iterate 1425    f=  1.29852D-02    |proj g|=  1.09231D-03\n",
      "\n",
      "At iterate 1426    f=  1.24865D-02    |proj g|=  3.93100D-04\n",
      "\n",
      "At iterate 1427    f=  1.23125D-02    |proj g|=  3.42553D-04\n",
      "\n",
      "At iterate 1428    f=  1.21024D-02    |proj g|=  4.70467D-04\n",
      "\n",
      "At iterate 1429    f=  1.15890D-02    |proj g|=  3.61249D-04\n",
      "\n",
      "At iterate 1430    f=  1.13678D-02    |proj g|=  1.23910D-03\n",
      "\n",
      "At iterate 1431    f=  1.12864D-02    |proj g|=  7.46735D-04\n",
      "\n",
      "At iterate 1432    f=  1.11977D-02    |proj g|=  2.73149D-04\n",
      "\n",
      "At iterate 1433    f=  1.11714D-02    |proj g|=  4.26869D-04\n",
      "\n",
      "At iterate 1434    f=  1.11146D-02    |proj g|=  5.03507D-04\n",
      "\n",
      "At iterate 1435    f=  1.08312D-02    |proj g|=  4.60887D-04\n",
      "\n",
      "At iterate 1436    f=  1.03893D-02    |proj g|=  5.28138D-04\n",
      "\n",
      "At iterate 1437    f=  1.02185D-02    |proj g|=  5.63916D-04\n",
      "\n",
      "At iterate 1438    f=  9.97449D-03    |proj g|=  2.53282D-04\n",
      "\n",
      "At iterate 1439    f=  9.78662D-03    |proj g|=  2.58072D-04\n",
      "\n",
      "At iterate 1440    f=  9.58128D-03    |proj g|=  4.25994D-04\n",
      "\n",
      "At iterate 1441    f=  9.40245D-03    |proj g|=  1.12926D-03\n",
      "\n",
      "At iterate 1442    f=  9.13640D-03    |proj g|=  8.15040D-04\n",
      "\n",
      "At iterate 1443    f=  9.01051D-03    |proj g|=  5.07768D-04\n",
      "\n",
      "At iterate 1444    f=  8.87181D-03    |proj g|=  4.27757D-04\n",
      "\n",
      "At iterate 1445    f=  8.59645D-03    |proj g|=  4.30828D-04\n",
      "\n",
      "At iterate 1446    f=  8.07851D-03    |proj g|=  5.17166D-04\n",
      "\n",
      "At iterate 1447    f=  7.71575D-03    |proj g|=  2.78781D-04\n",
      "\n",
      "At iterate 1448    f=  7.36940D-03    |proj g|=  4.15717D-04\n",
      "\n",
      "At iterate 1449    f=  6.71519D-03    |proj g|=  6.17945D-04\n",
      "\n",
      "At iterate 1450    f=  6.39759D-03    |proj g|=  5.08470D-04\n",
      "\n",
      "At iterate 1451    f=  5.87160D-03    |proj g|=  3.56687D-04\n",
      "\n",
      "At iterate 1452    f=  5.60696D-03    |proj g|=  4.17758D-04\n",
      "\n",
      "At iterate 1453    f=  5.17387D-03    |proj g|=  2.92000D-04\n",
      "\n",
      "At iterate 1454    f=  5.00752D-03    |proj g|=  9.30339D-04\n",
      "\n",
      "At iterate 1455    f=  4.50875D-03    |proj g|=  2.83078D-04\n",
      "\n",
      "At iterate 1456    f=  4.41599D-03    |proj g|=  2.76059D-04\n",
      "\n",
      "At iterate 1457    f=  4.29958D-03    |proj g|=  1.74689D-04\n",
      "\n",
      "At iterate 1458    f=  4.14514D-03    |proj g|=  1.49621D-04\n",
      "\n",
      "At iterate 1459    f=  3.80676D-03    |proj g|=  1.31002D-04\n",
      "\n",
      "At iterate 1460    f=  3.64969D-03    |proj g|=  6.42480D-04\n",
      "\n",
      "At iterate 1461    f=  3.41941D-03    |proj g|=  3.09781D-04\n",
      "\n",
      "At iterate 1462    f=  3.29355D-03    |proj g|=  1.25754D-04\n",
      "\n",
      "At iterate 1463    f=  3.16682D-03    |proj g|=  2.43491D-04\n",
      "\n",
      "At iterate 1464    f=  3.10674D-03    |proj g|=  2.09570D-04\n",
      "\n",
      "At iterate 1465    f=  2.88515D-03    |proj g|=  1.68754D-04\n",
      "\n",
      "At iterate 1466    f=  2.63941D-03    |proj g|=  4.17046D-04\n",
      "\n",
      "At iterate 1467    f=  2.38820D-03    |proj g|=  4.83164D-04\n",
      "\n",
      "At iterate 1468    f=  2.31101D-03    |proj g|=  7.72104D-04\n",
      "\n",
      "At iterate 1469    f=  1.99310D-03    |proj g|=  6.16690D-04\n",
      "\n",
      "At iterate 1470    f=  1.51666D-03    |proj g|=  3.39208D-04\n",
      "\n",
      "At iterate 1471    f=  1.29720D-03    |proj g|=  3.76680D-04\n",
      "\n",
      "At iterate 1472    f=  1.24028D-03    |proj g|=  1.55805D-04\n",
      "\n",
      "At iterate 1473    f=  1.19122D-03    |proj g|=  1.44494D-04\n",
      "\n",
      "At iterate 1474    f=  1.10576D-03    |proj g|=  3.62636D-04\n",
      "\n",
      "At iterate 1475    f=  9.90171D-04    |proj g|=  2.09538D-04\n",
      "\n",
      "At iterate 1476    f=  9.18723D-04    |proj g|=  1.00831D-04\n",
      "\n",
      "At iterate 1477    f=  8.38345D-04    |proj g|=  8.32086D-05\n",
      "\n",
      "At iterate 1478    f=  6.96404D-04    |proj g|=  1.53473D-04\n",
      "\n",
      "At iterate 1479    f=  6.53461D-04    |proj g|=  2.78758D-04\n",
      "\n",
      "At iterate 1480    f=  4.92983D-04    |proj g|=  3.42178D-04\n",
      "\n",
      "At iterate 1481    f=  3.92493D-04    |proj g|=  9.53663D-05\n",
      "\n",
      "At iterate 1482    f=  3.07016D-04    |proj g|=  3.57845D-05\n",
      "\n",
      "At iterate 1483    f=  2.46790D-04    |proj g|=  2.38049D-05\n",
      "\n",
      "At iterate 1484    f=  2.22940D-04    |proj g|=  1.10995D-04\n",
      "\n",
      "At iterate 1485    f=  1.53420D-04    |proj g|=  2.27433D-05\n",
      "\n",
      "At iterate 1486    f=  1.15474D-04    |proj g|=  1.47929D-05\n",
      "\n",
      "At iterate 1487    f=  6.25472D-05    |proj g|=  1.93317D-05\n",
      "\n",
      "At iterate 1488    f=  3.36623D-05    |proj g|=  3.85006D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74   1488   1743      1     0     0   3.850D-06   3.366D-05\n",
      "  F =   3.3662329980512469E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  7.71800D-02\n",
      "\n",
      "At iterate    1    f=  6.81181D-01    |proj g|=  5.36121D-02\n",
      "\n",
      "At iterate    2    f=  5.59566D-01    |proj g|=  7.36729D-02\n",
      "\n",
      "At iterate    3    f=  3.63886D-01    |proj g|=  3.09281D-02\n",
      "\n",
      "At iterate    4    f=  3.01330D-01    |proj g|=  2.02867D-02\n",
      "\n",
      "At iterate    5    f=  2.32123D-01    |proj g|=  1.33279D-02\n",
      "\n",
      "At iterate    6    f=  1.57401D-01    |proj g|=  7.19897D-03\n",
      "\n",
      "At iterate    7    f=  8.28516D-02    |proj g|=  5.18887D-03\n",
      "\n",
      "At iterate    8    f=  4.71538D-02    |proj g|=  8.57547D-03\n",
      "\n",
      "At iterate    9    f=  2.64062D-02    |proj g|=  4.14522D-03\n",
      "\n",
      "At iterate   10    f=  1.53712D-02    |proj g|=  2.40897D-03\n",
      "\n",
      "At iterate   11    f=  7.52286D-03    |proj g|=  1.23964D-03\n",
      "\n",
      "At iterate   12    f=  4.77011D-03    |proj g|=  2.63163D-03\n",
      "\n",
      "At iterate   13    f=  1.48079D-03    |proj g|=  1.71544D-04\n",
      "\n",
      "At iterate   14    f=  9.72576D-04    |proj g|=  1.09487D-04\n",
      "\n",
      "At iterate   15    f=  4.38695D-04    |proj g|=  4.52844D-05\n",
      "\n",
      "At iterate   16    f=  2.27882D-04    |proj g|=  4.65379D-05\n",
      "\n",
      "At iterate   17    f=  9.05495D-05    |proj g|=  8.99216D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     17     21      1     0     0   8.992D-06   9.055D-05\n",
      "  F =   9.0549542945437785E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  5.78983D-02\n",
      "\n",
      "At iterate    1    f=  6.44068D-01    |proj g|=  1.38120D-01\n",
      "\n",
      "At iterate    2    f=  5.64761D-01    |proj g|=  7.59556D-02\n",
      "\n",
      "At iterate    3    f=  4.84433D-01    |proj g|=  6.46564D-02\n",
      "\n",
      "At iterate    4    f=  4.34383D-01    |proj g|=  7.60653D-02\n",
      "\n",
      "At iterate    5    f=  3.38023D-01    |proj g|=  5.02680D-02\n",
      "\n",
      "At iterate    6    f=  2.80825D-01    |proj g|=  1.96495D-02\n",
      "\n",
      "At iterate    7    f=  2.43277D-01    |proj g|=  1.17944D-02\n",
      "\n",
      "At iterate    8    f=  2.17537D-01    |proj g|=  7.59331D-03\n",
      "\n",
      "At iterate    9    f=  1.92704D-01    |proj g|=  6.91505D-03\n",
      "\n",
      "At iterate   10    f=  1.51101D-01    |proj g|=  1.75044D-02\n",
      "\n",
      "At iterate   11    f=  1.31683D-01    |proj g|=  6.82402D-03\n",
      "\n",
      "At iterate   12    f=  1.29351D-01    |proj g|=  1.00395D-02\n",
      "\n",
      "At iterate   13    f=  1.27576D-01    |proj g|=  8.38002D-03\n",
      "\n",
      "At iterate   14    f=  1.22173D-01    |proj g|=  5.12312D-03\n",
      "\n",
      "At iterate   15    f=  1.14859D-01    |proj g|=  2.87755D-03\n",
      "\n",
      "At iterate   16    f=  1.08477D-01    |proj g|=  3.54498D-03\n",
      "\n",
      "At iterate   17    f=  9.89022D-02    |proj g|=  3.77833D-03\n",
      "\n",
      "At iterate   18    f=  8.16935D-02    |proj g|=  3.81638D-03\n",
      "\n",
      "At iterate   19    f=  7.56276D-02    |proj g|=  1.18878D-02\n",
      "\n",
      "At iterate   20    f=  6.45595D-02    |proj g|=  3.16793D-03\n",
      "\n",
      "At iterate   21    f=  6.16867D-02    |proj g|=  1.93564D-03\n",
      "\n",
      "At iterate   22    f=  5.60058D-02    |proj g|=  3.18441D-03\n",
      "\n",
      "At iterate   23    f=  4.62529D-02    |proj g|=  6.02507D-03\n",
      "\n",
      "At iterate   24    f=  4.50744D-02    |proj g|=  6.34755D-03\n",
      "\n",
      "At iterate   25    f=  4.01262D-02    |proj g|=  4.53864D-03\n",
      "\n",
      "At iterate   26    f=  3.20743D-02    |proj g|=  3.17938D-03\n",
      "\n",
      "At iterate   27    f=  2.59040D-02    |proj g|=  2.23461D-03\n",
      "\n",
      "At iterate   28    f=  1.89266D-02    |proj g|=  1.93991D-03\n",
      "\n",
      "At iterate   29    f=  1.07777D-02    |proj g|=  8.94465D-04\n",
      "\n",
      "At iterate   30    f=  5.09453D-03    |proj g|=  1.22042D-03\n",
      "\n",
      "At iterate   31    f=  2.42025D-03    |proj g|=  3.92053D-04\n",
      "\n",
      "At iterate   32    f=  1.37682D-03    |proj g|=  1.57567D-04\n",
      "\n",
      "At iterate   33    f=  6.81694D-04    |proj g|=  7.95927D-05\n",
      "\n",
      "At iterate   34    f=  4.20917D-04    |proj g|=  1.59320D-04\n",
      "\n",
      "At iterate   35    f=  3.16773D-04    |proj g|=  2.43698D-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "100%|██████████| 10/10 [00:01<00:00,  5.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   36    f=  1.71088D-04    |proj g|=  9.58478D-05\n",
      "\n",
      "At iterate   37    f=  1.00856D-04    |proj g|=  4.27694D-05\n",
      "\n",
      "At iterate   38    f=  5.20257D-05    |proj g|=  1.67679D-05\n",
      "\n",
      "At iterate   39    f=  2.70176D-05    |proj g|=  6.69506D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     39     42      1     0     0   6.695D-06   2.702D-05\n",
      "  F =   2.7017600094213324E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.74205D-01\n",
      "\n",
      "At iterate    1    f=  6.33445D-01    |proj g|=  6.01594D-02\n",
      "\n",
      "At iterate    2    f=  5.46259D-01    |proj g|=  5.99139D-02\n",
      "\n",
      "At iterate    3    f=  3.58419D-01    |proj g|=  9.69215D-02\n",
      "\n",
      "At iterate    4    f=  2.69906D-01    |proj g|=  6.54063D-02\n",
      "\n",
      "At iterate    5    f=  2.00065D-01    |proj g|=  3.54794D-02\n",
      "\n",
      "At iterate    6    f=  1.56483D-01    |proj g|=  1.84257D-02\n",
      "\n",
      "At iterate    7    f=  1.21896D-01    |proj g|=  8.90098D-03\n",
      "\n",
      "At iterate    8    f=  9.15505D-02    |proj g|=  8.25807D-03\n",
      "\n",
      "At iterate    9    f=  6.29335D-02    |proj g|=  7.66284D-03\n",
      "\n",
      "At iterate   10    f=  3.32248D-02    |proj g|=  7.71830D-03\n",
      "\n",
      "At iterate   11    f=  1.65206D-02    |proj g|=  2.53041D-03\n",
      "\n",
      "At iterate   12    f=  1.08881D-02    |proj g|=  1.03271D-03\n",
      "\n",
      "At iterate   13    f=  6.40089D-03    |proj g|=  1.81014D-03\n",
      "\n",
      "At iterate   14    f=  3.53778D-03    |proj g|=  2.83184D-03\n",
      "\n",
      "At iterate   15    f=  2.55601D-03    |proj g|=  1.88011D-03\n",
      "\n",
      "At iterate   16    f=  1.48596D-03    |proj g|=  8.64291D-04\n",
      "\n",
      "At iterate   17    f=  8.97164D-04    |proj g|=  3.83792D-04\n",
      "\n",
      "At iterate   18    f=  5.04706D-04    |proj g|=  1.58859D-04\n",
      "\n",
      "At iterate   19    f=  2.74377D-04    |proj g|=  8.08123D-05\n",
      "\n",
      "At iterate   20    f=  1.45097D-04    |proj g|=  3.82196D-05\n",
      "\n",
      "At iterate   21    f=  7.58823D-05    |proj g|=  2.69625D-05\n",
      "\n",
      "At iterate   22    f=  3.97137D-05    |proj g|=  1.43117D-05\n",
      "\n",
      "At iterate   23    f=  2.94115D-05    |proj g|=  1.58027D-05\n",
      "\n",
      "At iterate   24    f=  9.72348D-06    |proj g|=  3.85241D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     24     29      1     0     0   3.852D-06   9.723D-06\n",
      "  F =   9.7234849399117114E-006\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  2.47181D-01\n",
      "\n",
      "At iterate    1    f=  5.65619D-01    |proj g|=  4.81331D-02\n",
      "\n",
      "At iterate    2    f=  5.47840D-01    |proj g|=  4.41768D-02\n",
      "\n",
      "At iterate    3    f=  4.33690D-01    |proj g|=  1.23273D-01\n",
      "\n",
      "At iterate    4    f=  3.68018D-01    |proj g|=  1.05855D-01\n",
      "\n",
      "At iterate    5    f=  2.79796D-01    |proj g|=  5.00323D-02\n",
      "\n",
      "At iterate    6    f=  2.40163D-01    |proj g|=  2.04329D-02\n",
      "\n",
      "At iterate    7    f=  2.16806D-01    |proj g|=  7.97392D-03\n",
      "\n",
      "At iterate    8    f=  2.00608D-01    |proj g|=  1.27387D-02\n",
      "\n",
      "At iterate    9    f=  1.78374D-01    |proj g|=  1.29683D-02\n",
      "\n",
      "At iterate   10    f=  9.84168D-02    |proj g|=  1.23131D-02\n",
      "\n",
      "At iterate   11    f=  8.21376D-02    |proj g|=  1.61180D-02\n",
      "\n",
      "At iterate   12    f=  6.12099D-02    |proj g|=  3.30863D-03\n",
      "\n",
      "At iterate   13    f=  5.10210D-02    |proj g|=  4.15829D-03\n",
      "\n",
      "At iterate   14    f=  4.94019D-02    |proj g|=  7.44855D-03\n",
      "\n",
      "At iterate   15    f=  4.20501D-02    |proj g|=  2.91397D-03\n",
      "\n",
      "At iterate   16    f=  3.83874D-02    |proj g|=  2.42695D-03\n",
      "\n",
      "At iterate   17    f=  3.47419D-02    |proj g|=  2.57023D-03\n",
      "\n",
      "At iterate   18    f=  2.97067D-02    |proj g|=  2.36063D-03\n",
      "\n",
      "At iterate   19    f=  1.80828D-02    |proj g|=  2.93732D-03\n",
      "\n",
      "At iterate   20    f=  1.13155D-02    |proj g|=  7.56464D-04\n",
      "\n",
      "At iterate   21    f=  7.26076D-03    |proj g|=  1.49047D-03\n",
      "\n",
      "At iterate   22    f=  2.78810D-03    |proj g|=  8.00926D-04\n",
      "\n",
      "At iterate   23    f=  1.41072D-03    |proj g|=  2.18022D-04\n",
      "\n",
      "At iterate   24    f=  8.54244D-04    |proj g|=  1.17036D-04\n",
      "\n",
      "At iterate   25    f=  4.55111D-04    |proj g|=  2.36649D-04\n",
      "\n",
      "At iterate   26    f=  2.44553D-04    |proj g|=  2.26310D-04\n",
      "\n",
      "At iterate   27    f=  8.97598D-05    |proj g|=  6.62625D-05\n",
      "\n",
      "At iterate   28    f=  5.98536D-05    |proj g|=  3.77137D-05\n",
      "\n",
      "At iterate   29    f=  3.24085D-05    |proj g|=  1.38707D-05\n",
      "\n",
      "At iterate   30    f=  1.93112D-05    |proj g|=  5.25311D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     30     34      1     0     0   5.253D-06   1.931D-05\n",
      "  F =   1.9311158954527789E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 5527.77it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 556.14it/s]\n",
      "100%|██████████| 13/13 [00:00<00:00, 5594.13it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 619.74it/s]\n",
      "x does not have column names.  The check that columns are provided in the same order when training and predicting will be skipped\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 (73,)\n",
      "# of CELL:13 / min size:12 / avg size:23.8 / max size:45 / # of singleton CELL:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of clf: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1/13 [00:00<00:01,  7.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: 0 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d55b12160>, dataframe=<capsule object NULL at 0x152d9b9d7a50>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7   \\\n",
      "0    0.360000  0.269729  1.0  0.097087  0.191029  0.0  0.0  1.000000   \n",
      "1    0.466667  0.145052  1.0  0.533981  0.206016  0.0  0.0  0.815789   \n",
      "2    0.946667  0.105553  1.0  0.456311  0.120211  0.0  0.0  0.947368   \n",
      "3    0.413333  0.193987  1.0  0.067961  0.127388  0.0  0.0  0.921053   \n",
      "4    0.186667  0.410438  1.0  0.116505  0.287071  0.0  0.0  0.921053   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...   \n",
      "305  0.786667  0.317161  1.0  0.077670  0.291398  0.0  0.0  1.000000   \n",
      "306  0.626667  0.009353  1.0  0.145631  0.142902  0.0  0.0  0.342105   \n",
      "307  0.493333  0.716576  1.0  0.126214  0.296887  0.0  0.0  1.000000   \n",
      "308  0.906667  0.009102  1.0  0.165049  0.317995  0.0  0.0  0.263158   \n",
      "309  0.400000  0.054196  1.0  0.747573  0.901953  0.0  0.0  0.921053   \n",
      "\n",
      "           8         9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.000000  1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    0.000000  0.794118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2    0.000000  0.941176  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.000000  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.000000  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "..        ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.000000  1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "306  0.777778  0.323529  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "307  0.000000  1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "308  0.111111  0.294118  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "309  0.000000  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
      "       1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
      "       1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0.,\n",
      "       0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
      "       0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
      "       1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "       0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
      "       0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
      "       0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0.,\n",
      "       1., 0., 1., 1.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n",
      "cluster: 1 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d55b12160>, dataframe=<capsule object NULL at 0x152d9b9d7a50>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7   \\\n",
      "0    0.360000  0.269729  1.0  0.097087  0.191029  0.0  0.0  1.000000   \n",
      "1    0.466667  0.145052  1.0  0.533981  0.206016  0.0  0.0  0.815789   \n",
      "2    0.946667  0.105553  1.0  0.456311  0.120211  0.0  0.0  0.947368   \n",
      "3    0.413333  0.193987  1.0  0.067961  0.127388  0.0  0.0  0.921053   \n",
      "4    0.186667  0.410438  1.0  0.116505  0.287071  0.0  0.0  0.921053   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...   \n",
      "305  0.786667  0.317161  1.0  0.077670  0.291398  0.0  0.0  1.000000   \n",
      "306  0.626667  0.009353  1.0  0.145631  0.142902  0.0  0.0  0.342105   \n",
      "307  0.493333  0.716576  1.0  0.126214  0.296887  0.0  0.0  1.000000   \n",
      "308  0.906667  0.009102  1.0  0.165049  0.317995  0.0  0.0  0.263158   \n",
      "309  0.400000  0.054196  1.0  0.747573  0.901953  0.0  0.0  0.921053   \n",
      "\n",
      "           8         9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.000000  1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    0.000000  0.794118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2    0.000000  0.941176  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.000000  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.000000  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "..        ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.000000  1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "306  0.777778  0.323529  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "307  0.000000  1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "308  0.111111  0.294118  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "309  0.000000  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
      "       1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
      "       1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0.,\n",
      "       0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
      "       0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
      "       1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "       0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
      "       0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
      "       0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0.,\n",
      "       1., 0., 1., 1.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 3/13 [00:00<00:01,  6.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: 2 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d55b12160>, dataframe=<capsule object NULL at 0x152d9b9d7a50>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7   \\\n",
      "0    0.360000  0.269729  1.0  0.097087  0.191029  0.0  0.0  1.000000   \n",
      "1    0.466667  0.145052  1.0  0.533981  0.206016  0.0  0.0  0.815789   \n",
      "2    0.946667  0.105553  1.0  0.456311  0.120211  0.0  0.0  0.947368   \n",
      "3    0.413333  0.193987  1.0  0.067961  0.127388  0.0  0.0  0.921053   \n",
      "4    0.186667  0.410438  1.0  0.116505  0.287071  0.0  0.0  0.921053   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...   \n",
      "305  0.786667  0.317161  1.0  0.077670  0.291398  0.0  0.0  1.000000   \n",
      "306  0.626667  0.009353  1.0  0.145631  0.142902  0.0  0.0  0.342105   \n",
      "307  0.493333  0.716576  1.0  0.126214  0.296887  0.0  0.0  1.000000   \n",
      "308  0.906667  0.009102  1.0  0.165049  0.317995  0.0  0.0  0.263158   \n",
      "309  0.400000  0.054196  1.0  0.747573  0.901953  0.0  0.0  0.921053   \n",
      "\n",
      "           8         9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.000000  1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    0.000000  0.794118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2    0.000000  0.941176  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.000000  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.000000  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "..        ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.000000  1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "306  0.777778  0.323529  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "307  0.000000  1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "308  0.111111  0.294118  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "309  0.000000  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
      "       1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
      "       1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0.,\n",
      "       0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
      "       0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
      "       1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "       0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
      "       0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
      "       0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0.,\n",
      "       1., 0., 1., 1.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n",
      "cluster: 3 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d55b12160>, dataframe=<capsule object NULL at 0x152d9b9d7a50>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7   \\\n",
      "0    0.360000  0.269729  1.0  0.097087  0.191029  0.0  0.0  1.000000   \n",
      "1    0.466667  0.145052  1.0  0.533981  0.206016  0.0  0.0  0.815789   \n",
      "2    0.946667  0.105553  1.0  0.456311  0.120211  0.0  0.0  0.947368   \n",
      "3    0.413333  0.193987  1.0  0.067961  0.127388  0.0  0.0  0.921053   \n",
      "4    0.186667  0.410438  1.0  0.116505  0.287071  0.0  0.0  0.921053   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...   \n",
      "305  0.786667  0.317161  1.0  0.077670  0.291398  0.0  0.0  1.000000   \n",
      "306  0.626667  0.009353  1.0  0.145631  0.142902  0.0  0.0  0.342105   \n",
      "307  0.493333  0.716576  1.0  0.126214  0.296887  0.0  0.0  1.000000   \n",
      "308  0.906667  0.009102  1.0  0.165049  0.317995  0.0  0.0  0.263158   \n",
      "309  0.400000  0.054196  1.0  0.747573  0.901953  0.0  0.0  0.921053   \n",
      "\n",
      "           8         9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.000000  1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    0.000000  0.794118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2    0.000000  0.941176  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.000000  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.000000  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "..        ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.000000  1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "306  0.777778  0.323529  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "307  0.000000  1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "308  0.111111  0.294118  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "309  0.000000  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
      "       1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
      "       1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0.,\n",
      "       0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
      "       0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
      "       1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "       0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
      "       0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
      "       0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0.,\n",
      "       1., 0., 1., 1.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|███       | 4/13 [00:00<00:01,  6.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cluster: 4 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d55b12160>, dataframe=<capsule object NULL at 0x152d9b9d7a50>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7   \\\n",
      "0    0.360000  0.269729  1.0  0.097087  0.191029  0.0  0.0  1.000000   \n",
      "1    0.466667  0.145052  1.0  0.533981  0.206016  0.0  0.0  0.815789   \n",
      "2    0.946667  0.105553  1.0  0.456311  0.120211  0.0  0.0  0.947368   \n",
      "3    0.413333  0.193987  1.0  0.067961  0.127388  0.0  0.0  0.921053   \n",
      "4    0.186667  0.410438  1.0  0.116505  0.287071  0.0  0.0  0.921053   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...   \n",
      "305  0.786667  0.317161  1.0  0.077670  0.291398  0.0  0.0  1.000000   \n",
      "306  0.626667  0.009353  1.0  0.145631  0.142902  0.0  0.0  0.342105   \n",
      "307  0.493333  0.716576  1.0  0.126214  0.296887  0.0  0.0  1.000000   \n",
      "308  0.906667  0.009102  1.0  0.165049  0.317995  0.0  0.0  0.263158   \n",
      "309  0.400000  0.054196  1.0  0.747573  0.901953  0.0  0.0  0.921053   \n",
      "\n",
      "           8         9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.000000  1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    0.000000  0.794118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2    0.000000  0.941176  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.000000  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.000000  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "..        ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.000000  1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "306  0.777778  0.323529  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "307  0.000000  1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "308  0.111111  0.294118  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "309  0.000000  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
      "       1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
      "       1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0.,\n",
      "       0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
      "       0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
      "       1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "       0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
      "       0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
      "       0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0.,\n",
      "       1., 0., 1., 1.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 6/13 [00:00<00:01,  6.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: 5 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d55b12160>, dataframe=<capsule object NULL at 0x152d9b9d7a50>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7   \\\n",
      "0    0.360000  0.269729  1.0  0.097087  0.191029  0.0  0.0  1.000000   \n",
      "1    0.466667  0.145052  1.0  0.533981  0.206016  0.0  0.0  0.815789   \n",
      "2    0.946667  0.105553  1.0  0.456311  0.120211  0.0  0.0  0.947368   \n",
      "3    0.413333  0.193987  1.0  0.067961  0.127388  0.0  0.0  0.921053   \n",
      "4    0.186667  0.410438  1.0  0.116505  0.287071  0.0  0.0  0.921053   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...   \n",
      "305  0.786667  0.317161  1.0  0.077670  0.291398  0.0  0.0  1.000000   \n",
      "306  0.626667  0.009353  1.0  0.145631  0.142902  0.0  0.0  0.342105   \n",
      "307  0.493333  0.716576  1.0  0.126214  0.296887  0.0  0.0  1.000000   \n",
      "308  0.906667  0.009102  1.0  0.165049  0.317995  0.0  0.0  0.263158   \n",
      "309  0.400000  0.054196  1.0  0.747573  0.901953  0.0  0.0  0.921053   \n",
      "\n",
      "           8         9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.000000  1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    0.000000  0.794118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2    0.000000  0.941176  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.000000  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.000000  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "..        ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.000000  1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "306  0.777778  0.323529  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "307  0.000000  1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "308  0.111111  0.294118  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "309  0.000000  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
      "       1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
      "       1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0.,\n",
      "       0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
      "       0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
      "       1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "       0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
      "       0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
      "       0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0.,\n",
      "       1., 0., 1., 1.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n",
      "cluster: 6 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d55b12160>, dataframe=<capsule object NULL at 0x152d9b9d7a50>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7   \\\n",
      "0    0.360000  0.269729  1.0  0.097087  0.191029  0.0  0.0  1.000000   \n",
      "1    0.466667  0.145052  1.0  0.533981  0.206016  0.0  0.0  0.815789   \n",
      "2    0.946667  0.105553  1.0  0.456311  0.120211  0.0  0.0  0.947368   \n",
      "3    0.413333  0.193987  1.0  0.067961  0.127388  0.0  0.0  0.921053   \n",
      "4    0.186667  0.410438  1.0  0.116505  0.287071  0.0  0.0  0.921053   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...   \n",
      "305  0.786667  0.317161  1.0  0.077670  0.291398  0.0  0.0  1.000000   \n",
      "306  0.626667  0.009353  1.0  0.145631  0.142902  0.0  0.0  0.342105   \n",
      "307  0.493333  0.716576  1.0  0.126214  0.296887  0.0  0.0  1.000000   \n",
      "308  0.906667  0.009102  1.0  0.165049  0.317995  0.0  0.0  0.263158   \n",
      "309  0.400000  0.054196  1.0  0.747573  0.901953  0.0  0.0  0.921053   \n",
      "\n",
      "           8         9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.000000  1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    0.000000  0.794118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2    0.000000  0.941176  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.000000  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.000000  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "..        ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.000000  1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "306  0.777778  0.323529  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "307  0.000000  1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "308  0.111111  0.294118  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "309  0.000000  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
      "       1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
      "       1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0.,\n",
      "       0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
      "       0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
      "       1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "       0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
      "       0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
      "       0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0.,\n",
      "       1., 0., 1., 1.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|█████▍    | 7/13 [00:01<00:00,  6.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cluster: 7 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d55b12160>, dataframe=<capsule object NULL at 0x152d9b9d7a50>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7   \\\n",
      "0    0.360000  0.269729  1.0  0.097087  0.191029  0.0  0.0  1.000000   \n",
      "1    0.466667  0.145052  1.0  0.533981  0.206016  0.0  0.0  0.815789   \n",
      "2    0.946667  0.105553  1.0  0.456311  0.120211  0.0  0.0  0.947368   \n",
      "3    0.413333  0.193987  1.0  0.067961  0.127388  0.0  0.0  0.921053   \n",
      "4    0.186667  0.410438  1.0  0.116505  0.287071  0.0  0.0  0.921053   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...   \n",
      "305  0.786667  0.317161  1.0  0.077670  0.291398  0.0  0.0  1.000000   \n",
      "306  0.626667  0.009353  1.0  0.145631  0.142902  0.0  0.0  0.342105   \n",
      "307  0.493333  0.716576  1.0  0.126214  0.296887  0.0  0.0  1.000000   \n",
      "308  0.906667  0.009102  1.0  0.165049  0.317995  0.0  0.0  0.263158   \n",
      "309  0.400000  0.054196  1.0  0.747573  0.901953  0.0  0.0  0.921053   \n",
      "\n",
      "           8         9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.000000  1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    0.000000  0.794118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2    0.000000  0.941176  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.000000  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.000000  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "..        ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.000000  1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "306  0.777778  0.323529  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "307  0.000000  1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "308  0.111111  0.294118  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "309  0.000000  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
      "       1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
      "       1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0.,\n",
      "       0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
      "       0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
      "       1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "       0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
      "       0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
      "       0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0.,\n",
      "       1., 0., 1., 1.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 9/13 [00:01<00:00,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: 8 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d55b12160>, dataframe=<capsule object NULL at 0x152d9b9d7a50>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7   \\\n",
      "0    0.360000  0.269729  1.0  0.097087  0.191029  0.0  0.0  1.000000   \n",
      "1    0.466667  0.145052  1.0  0.533981  0.206016  0.0  0.0  0.815789   \n",
      "2    0.946667  0.105553  1.0  0.456311  0.120211  0.0  0.0  0.947368   \n",
      "3    0.413333  0.193987  1.0  0.067961  0.127388  0.0  0.0  0.921053   \n",
      "4    0.186667  0.410438  1.0  0.116505  0.287071  0.0  0.0  0.921053   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...   \n",
      "305  0.786667  0.317161  1.0  0.077670  0.291398  0.0  0.0  1.000000   \n",
      "306  0.626667  0.009353  1.0  0.145631  0.142902  0.0  0.0  0.342105   \n",
      "307  0.493333  0.716576  1.0  0.126214  0.296887  0.0  0.0  1.000000   \n",
      "308  0.906667  0.009102  1.0  0.165049  0.317995  0.0  0.0  0.263158   \n",
      "309  0.400000  0.054196  1.0  0.747573  0.901953  0.0  0.0  0.921053   \n",
      "\n",
      "           8         9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.000000  1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    0.000000  0.794118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2    0.000000  0.941176  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.000000  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.000000  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "..        ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.000000  1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "306  0.777778  0.323529  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "307  0.000000  1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "308  0.111111  0.294118  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "309  0.000000  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
      "       1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
      "       1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0.,\n",
      "       0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
      "       0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
      "       1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "       0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
      "       0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
      "       0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0.,\n",
      "       1., 0., 1., 1.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n",
      "cluster: 9 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d55b12160>, dataframe=<capsule object NULL at 0x152d9b9d7a50>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7   \\\n",
      "0    0.360000  0.269729  1.0  0.097087  0.191029  0.0  0.0  1.000000   \n",
      "1    0.466667  0.145052  1.0  0.533981  0.206016  0.0  0.0  0.815789   \n",
      "2    0.946667  0.105553  1.0  0.456311  0.120211  0.0  0.0  0.947368   \n",
      "3    0.413333  0.193987  1.0  0.067961  0.127388  0.0  0.0  0.921053   \n",
      "4    0.186667  0.410438  1.0  0.116505  0.287071  0.0  0.0  0.921053   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...   \n",
      "305  0.786667  0.317161  1.0  0.077670  0.291398  0.0  0.0  1.000000   \n",
      "306  0.626667  0.009353  1.0  0.145631  0.142902  0.0  0.0  0.342105   \n",
      "307  0.493333  0.716576  1.0  0.126214  0.296887  0.0  0.0  1.000000   \n",
      "308  0.906667  0.009102  1.0  0.165049  0.317995  0.0  0.0  0.263158   \n",
      "309  0.400000  0.054196  1.0  0.747573  0.901953  0.0  0.0  0.921053   \n",
      "\n",
      "           8         9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.000000  1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    0.000000  0.794118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2    0.000000  0.941176  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.000000  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.000000  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "..        ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.000000  1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "306  0.777778  0.323529  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "307  0.000000  1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "308  0.111111  0.294118  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "309  0.000000  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
      "       1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
      "       1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0.,\n",
      "       0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
      "       0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
      "       1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "       0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
      "       0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
      "       0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0.,\n",
      "       1., 0., 1., 1.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 11/13 [00:01<00:00,  7.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: 10 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d55b12160>, dataframe=<capsule object NULL at 0x152d9b9d7a50>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7   \\\n",
      "0    0.360000  0.269729  1.0  0.097087  0.191029  0.0  0.0  1.000000   \n",
      "1    0.466667  0.145052  1.0  0.533981  0.206016  0.0  0.0  0.815789   \n",
      "2    0.946667  0.105553  1.0  0.456311  0.120211  0.0  0.0  0.947368   \n",
      "3    0.413333  0.193987  1.0  0.067961  0.127388  0.0  0.0  0.921053   \n",
      "4    0.186667  0.410438  1.0  0.116505  0.287071  0.0  0.0  0.921053   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...   \n",
      "305  0.786667  0.317161  1.0  0.077670  0.291398  0.0  0.0  1.000000   \n",
      "306  0.626667  0.009353  1.0  0.145631  0.142902  0.0  0.0  0.342105   \n",
      "307  0.493333  0.716576  1.0  0.126214  0.296887  0.0  0.0  1.000000   \n",
      "308  0.906667  0.009102  1.0  0.165049  0.317995  0.0  0.0  0.263158   \n",
      "309  0.400000  0.054196  1.0  0.747573  0.901953  0.0  0.0  0.921053   \n",
      "\n",
      "           8         9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.000000  1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    0.000000  0.794118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2    0.000000  0.941176  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.000000  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.000000  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "..        ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.000000  1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "306  0.777778  0.323529  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "307  0.000000  1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "308  0.111111  0.294118  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "309  0.000000  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
      "       1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
      "       1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0.,\n",
      "       0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
      "       0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
      "       1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "       0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
      "       0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
      "       0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0.,\n",
      "       1., 0., 1., 1.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n",
      "cluster: 11 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d55b12160>, dataframe=<capsule object NULL at 0x152d9b9d7a50>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7   \\\n",
      "0    0.360000  0.269729  1.0  0.097087  0.191029  0.0  0.0  1.000000   \n",
      "1    0.466667  0.145052  1.0  0.533981  0.206016  0.0  0.0  0.815789   \n",
      "2    0.946667  0.105553  1.0  0.456311  0.120211  0.0  0.0  0.947368   \n",
      "3    0.413333  0.193987  1.0  0.067961  0.127388  0.0  0.0  0.921053   \n",
      "4    0.186667  0.410438  1.0  0.116505  0.287071  0.0  0.0  0.921053   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...   \n",
      "305  0.786667  0.317161  1.0  0.077670  0.291398  0.0  0.0  1.000000   \n",
      "306  0.626667  0.009353  1.0  0.145631  0.142902  0.0  0.0  0.342105   \n",
      "307  0.493333  0.716576  1.0  0.126214  0.296887  0.0  0.0  1.000000   \n",
      "308  0.906667  0.009102  1.0  0.165049  0.317995  0.0  0.0  0.263158   \n",
      "309  0.400000  0.054196  1.0  0.747573  0.901953  0.0  0.0  0.921053   \n",
      "\n",
      "           8         9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.000000  1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    0.000000  0.794118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2    0.000000  0.941176  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.000000  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.000000  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "..        ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.000000  1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "306  0.777778  0.323529  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "307  0.000000  1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "308  0.111111  0.294118  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "309  0.000000  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
      "       1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
      "       1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0.,\n",
      "       0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
      "       0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
      "       1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "       0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
      "       0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
      "       0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0.,\n",
      "       1., 0., 1., 1.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00,  7.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: 12 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d55b12160>, dataframe=<capsule object NULL at 0x152d9b9d7a50>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7   \\\n",
      "0    0.360000  0.269729  1.0  0.097087  0.191029  0.0  0.0  1.000000   \n",
      "1    0.466667  0.145052  1.0  0.533981  0.206016  0.0  0.0  0.815789   \n",
      "2    0.946667  0.105553  1.0  0.456311  0.120211  0.0  0.0  0.947368   \n",
      "3    0.413333  0.193987  1.0  0.067961  0.127388  0.0  0.0  0.921053   \n",
      "4    0.186667  0.410438  1.0  0.116505  0.287071  0.0  0.0  0.921053   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...   \n",
      "305  0.786667  0.317161  1.0  0.077670  0.291398  0.0  0.0  1.000000   \n",
      "306  0.626667  0.009353  1.0  0.145631  0.142902  0.0  0.0  0.342105   \n",
      "307  0.493333  0.716576  1.0  0.126214  0.296887  0.0  0.0  1.000000   \n",
      "308  0.906667  0.009102  1.0  0.165049  0.317995  0.0  0.0  0.263158   \n",
      "309  0.400000  0.054196  1.0  0.747573  0.901953  0.0  0.0  0.921053   \n",
      "\n",
      "           8         9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.000000  1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    0.000000  0.794118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2    0.000000  0.941176  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.000000  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.000000  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "..        ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.000000  1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "306  0.777778  0.323529  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "307  0.000000  1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "308  0.111111  0.294118  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "309  0.000000  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
      "       1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
      "       1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0.,\n",
      "       0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
      "       0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
      "       1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "       0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
      "       0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
      "       0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0.,\n",
      "       1., 0., 1., 1.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13 [00:00<?, ?it/s] This problem is unconstrained.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.06087D-01\n",
      "\n",
      "At iterate    1    f=  6.70937D-01    |proj g|=  8.10024D-02\n",
      "\n",
      "At iterate    2    f=  5.54203D-01    |proj g|=  7.15390D-02\n",
      "\n",
      "At iterate    3    f=  3.60455D-01    |proj g|=  2.39990D-02\n",
      "\n",
      "At iterate    4    f=  3.02803D-01    |proj g|=  1.58781D-02\n",
      "\n",
      "At iterate    5    f=  1.87020D-01    |proj g|=  1.06703D-02\n",
      "\n",
      "At iterate    6    f=  1.24711D-01    |proj g|=  8.92098D-03\n",
      "\n",
      "At iterate    7    f=  7.50915D-02    |proj g|=  7.90755D-03\n",
      "\n",
      "At iterate    8    f=  5.20780D-02    |proj g|=  9.98156D-03\n",
      "\n",
      "At iterate    9    f=  3.11114D-02    |proj g|=  3.60707D-03\n",
      "\n",
      "At iterate   10    f=  2.07106D-02    |proj g|=  1.89933D-03\n",
      "\n",
      "At iterate   11    f=  1.26001D-02    |proj g|=  1.71151D-03\n",
      "\n",
      "At iterate   12    f=  1.22037D-02    |proj g|=  9.90122D-04\n",
      "\n",
      "At iterate   13    f=  7.69318D-03    |proj g|=  6.02349D-04\n",
      "\n",
      "At iterate   14    f=  4.01331D-03    |proj g|=  7.26590D-04\n",
      "\n",
      "At iterate   15    f=  1.92715D-03    |proj g|=  2.39794D-04\n",
      "\n",
      "At iterate   16    f=  1.22480D-03    |proj g|=  2.37027D-04\n",
      "\n",
      "At iterate   17    f=  5.63358D-04    |proj g|=  5.19005D-05\n",
      "\n",
      "At iterate   18    f=  3.36594D-04    |proj g|=  4.72641D-05\n",
      "\n",
      "At iterate   19    f=  1.56217D-04    |proj g|=  1.35919D-05\n",
      "\n",
      "At iterate   20    f=  7.98686D-05    |proj g|=  9.74272D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     20     25      1     0     0   9.743D-06   7.987D-05\n",
      "  F =   7.9868560396992701E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.61702D-01\n",
      "\n",
      "At iterate    1    f=  6.51341D-01    |proj g|=  3.48110D-02\n",
      "\n",
      "At iterate    2    f=  5.79382D-01    |proj g|=  2.84348D-02\n",
      "\n",
      "At iterate    3    f=  4.05124D-01    |proj g|=  3.75497D-02\n",
      "\n",
      "At iterate    4    f=  3.51241D-01    |proj g|=  2.70150D-02\n",
      "\n",
      "At iterate    5    f=  2.85425D-01    |proj g|=  1.68004D-02\n",
      "\n",
      "At iterate    6    f=  2.33079D-01    |proj g|=  8.89399D-03\n",
      "\n",
      "At iterate    7    f=  1.79062D-01    |proj g|=  3.87998D-03\n",
      "\n",
      "At iterate    8    f=  1.37678D-01    |proj g|=  5.75043D-03\n",
      "\n",
      "At iterate    9    f=  9.90042D-02    |proj g|=  3.63201D-03\n",
      "\n",
      "At iterate   10    f=  7.87427D-02    |proj g|=  7.98265D-03\n",
      "\n",
      "At iterate   11    f=  7.05650D-02    |proj g|=  6.76329D-03\n",
      "\n",
      "At iterate   12    f=  6.97345D-02    |proj g|=  3.46707D-03\n",
      "\n",
      "At iterate   13    f=  6.50046D-02    |proj g|=  3.06581D-03\n",
      "\n",
      "At iterate   14    f=  6.06381D-02    |proj g|=  7.28847D-03\n",
      "\n",
      "At iterate   15    f=  5.50898D-02    |proj g|=  4.18337D-03\n",
      "\n",
      "At iterate   16    f=  5.05136D-02    |proj g|=  1.70619D-03\n",
      "\n",
      "At iterate   17    f=  4.75516D-02    |proj g|=  2.73888D-03\n",
      "\n",
      "At iterate   18    f=  4.26294D-02    |proj g|=  2.19351D-03\n",
      "\n",
      "At iterate   19    f=  3.57343D-02    |proj g|=  1.22923D-03\n",
      "\n",
      "At iterate   20    f=  2.74794D-02    |proj g|=  1.42254D-03\n",
      "\n",
      "At iterate   21    f=  2.02325D-02    |proj g|=  2.81225D-03\n",
      "\n",
      "At iterate   22    f=  1.51714D-02    |proj g|=  1.57514D-03\n",
      "\n",
      "At iterate   23    f=  1.50079D-02    |proj g|=  5.38475D-03\n",
      "\n",
      "At iterate   24    f=  1.25422D-02    |proj g|=  2.30881D-03\n",
      "\n",
      "At iterate   25    f=  1.10673D-02    |proj g|=  9.30744D-04\n",
      "\n",
      "At iterate   26    f=  9.75091D-03    |proj g|=  2.16370D-03\n",
      "\n",
      "At iterate   27    f=  7.75380D-03    |proj g|=  1.87679D-03\n",
      "\n",
      "At iterate   28    f=  4.04767D-03    |proj g|=  2.66943D-03\n",
      "\n",
      "At iterate   29    f=  2.07711D-03    |proj g|=  1.38061D-03\n",
      "\n",
      "At iterate   30    f=  1.01151D-03    |proj g|=  6.61418D-04\n",
      "\n",
      "At iterate   31    f=  5.11028D-04    |proj g|=  3.24811D-04\n",
      "\n",
      "At iterate   32    f=  2.56688D-04    |proj g|=  1.57129D-04\n",
      "\n",
      "At iterate   33    f=  1.29745D-04    |proj g|=  7.58275D-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/discrete/discrete_model.py:1819: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " 31%|███       | 4/13 [00:00<00:00, 33.25it/s] This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   34    f=  6.55269D-05    |proj g|=  3.63042D-05\n",
      "\n",
      "At iterate   35    f=  3.30791D-05    |proj g|=  1.74969D-05\n",
      "\n",
      "At iterate   36    f=  1.66818D-05    |proj g|=  8.58706D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     36     41      1     0     0   8.587D-06   1.668D-05\n",
      "  F =   1.6681793865388156E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  8.45447D-02\n",
      "\n",
      "At iterate    1    f=  6.80708D-01    |proj g|=  6.40493D-02\n",
      "\n",
      "At iterate    2    f=  5.60215D-01    |proj g|=  6.66544D-02\n",
      "\n",
      "At iterate    3    f=  3.56395D-01    |proj g|=  2.56572D-02\n",
      "\n",
      "At iterate    4    f=  2.82744D-01    |proj g|=  1.90642D-02\n",
      "\n",
      "At iterate    5    f=  1.96222D-01    |proj g|=  1.25444D-02\n",
      "\n",
      "At iterate    6    f=  1.17630D-01    |proj g|=  7.64131D-03\n",
      "\n",
      "At iterate    7    f=  6.28409D-02    |proj g|=  4.86418D-03\n",
      "\n",
      "At iterate    8    f=  3.18229D-02    |proj g|=  5.32383D-03\n",
      "\n",
      "At iterate    9    f=  1.72301D-02    |proj g|=  1.24894D-03\n",
      "\n",
      "At iterate   10    f=  1.13331D-02    |proj g|=  1.19854D-03\n",
      "\n",
      "At iterate   11    f=  5.83612D-03    |proj g|=  3.88033D-04\n",
      "\n",
      "At iterate   12    f=  5.35104D-03    |proj g|=  2.37836D-03\n",
      "\n",
      "At iterate   13    f=  2.93311D-03    |proj g|=  8.32312D-04\n",
      "\n",
      "At iterate   14    f=  1.69780D-03    |proj g|=  3.97111D-04\n",
      "\n",
      "At iterate   15    f=  9.45091D-04    |proj g|=  1.84583D-04\n",
      "\n",
      "At iterate   16    f=  5.11195D-04    |proj g|=  8.38894D-05\n",
      "\n",
      "At iterate   17    f=  2.66170D-04    |proj g|=  3.87646D-05\n",
      "\n",
      "At iterate   18    f=  1.35932D-04    |proj g|=  2.01340D-05\n",
      "\n",
      "At iterate   19    f=  6.87076D-05    |proj g|=  1.03867D-05\n",
      "\n",
      "At iterate   20    f=  3.45804D-05    |proj g|=  5.76954D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     20     25      1     0     0   5.770D-06   3.458D-05\n",
      "  F =   3.4580445335075482E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  7.42485D-02\n",
      "\n",
      "At iterate    1    f=  6.82034D-01    |proj g|=  6.16061D-02\n",
      "\n",
      "At iterate    2    f=  6.05780D-01    |proj g|=  6.25534D-02\n",
      "\n",
      "At iterate    3    f=  3.02444D-01    |proj g|=  2.21441D-02\n",
      "\n",
      "At iterate    4    f=  2.32713D-01    |proj g|=  1.37435D-02\n",
      "\n",
      "At iterate    5    f=  1.49983D-01    |proj g|=  7.37510D-03\n",
      "\n",
      "At iterate    6    f=  1.05521D-01    |proj g|=  1.27784D-02\n",
      "\n",
      "At iterate    7    f=  7.39890D-02    |proj g|=  4.62359D-03\n",
      "\n",
      "At iterate    8    f=  5.56656D-02    |proj g|=  5.46748D-03\n",
      "\n",
      "At iterate    9    f=  3.73846D-02    |proj g|=  3.00273D-03\n",
      "\n",
      "At iterate   10    f=  2.77977D-02    |proj g|=  2.86764D-03\n",
      "\n",
      "At iterate   11    f=  1.55941D-02    |proj g|=  2.07446D-03\n",
      "\n",
      "At iterate   12    f=  1.40669D-02    |proj g|=  4.96340D-03\n",
      "\n",
      "At iterate   13    f=  9.83793D-03    |proj g|=  2.85875D-03\n",
      "\n",
      "At iterate   14    f=  6.07359D-03    |proj g|=  1.47224D-03\n",
      "\n",
      "At iterate   15    f=  3.52224D-03    |proj g|=  8.97220D-04\n",
      "\n",
      "At iterate   16    f=  1.66212D-03    |proj g|=  4.63631D-04\n",
      "\n",
      "At iterate   17    f=  8.45919D-04    |proj g|=  2.12681D-04\n",
      "\n",
      "At iterate   18    f=  4.25218D-04    |proj g|=  1.14028D-04\n",
      "\n",
      "At iterate   19    f=  2.14042D-04    |proj g|=  5.70510D-05\n",
      "\n",
      "At iterate   20    f=  1.07425D-04    |proj g|=  2.92788D-05\n",
      "\n",
      "At iterate   21    f=  5.38752D-05    |proj g|=  1.47554D-05\n",
      "\n",
      "At iterate   22    f=  2.69966D-05    |proj g|=  7.43283D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     22     26      1     0     0   7.433D-06   2.700D-05\n",
      "  F =   2.6996588296826581E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.48750D-01\n",
      "\n",
      "At iterate    1    f=  6.48183D-01    |proj g|=  4.21646D-02\n",
      "\n",
      "At iterate    2    f=  5.55280D-01    |proj g|=  3.55543D-02\n",
      "\n",
      "At iterate    3    f=  2.71903D-01    |proj g|=  3.11158D-02\n",
      "\n",
      "At iterate    4    f=  2.01917D-01    |proj g|=  2.27036D-02\n",
      "\n",
      "At iterate    5    f=  1.38264D-01    |proj g|=  1.29359D-02\n",
      "\n",
      "At iterate    6    f=  9.64348D-02    |proj g|=  5.90897D-03\n",
      "\n",
      "At iterate    7    f=  6.13298D-02    |proj g|=  3.11766D-03\n",
      "\n",
      "At iterate    8    f=  3.43867D-02    |proj g|=  4.77301D-03\n",
      "\n",
      "At iterate    9    f=  1.61530D-02    |proj g|=  2.27634D-03\n",
      "\n",
      "At iterate   10    f=  7.86964D-03    |proj g|=  1.35184D-03\n",
      "\n",
      "At iterate   11    f=  3.86722D-03    |proj g|=  4.51630D-04\n",
      "\n",
      "At iterate   12    f=  3.31370D-03    |proj g|=  1.83431D-03\n",
      "\n",
      "At iterate   13    f=  1.91779D-03    |proj g|=  8.42888D-04\n",
      "\n",
      "At iterate   14    f=  9.69549D-04    |proj g|=  3.77127D-04\n",
      "\n",
      "At iterate   15    f=  5.01933D-04    |proj g|=  1.81564D-04\n",
      "\n",
      "At iterate   16    f=  2.55323D-04    |proj g|=  8.72760D-05\n",
      "\n",
      "At iterate   17    f=  1.29863D-04    |proj g|=  4.20298D-05\n",
      "\n",
      "At iterate   18    f=  6.56138D-05    |proj g|=  1.98817D-05\n",
      "\n",
      "At iterate   19    f=  3.30739D-05    |proj g|=  9.23156D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     19     24      1     0     0   9.232D-06   3.307D-05\n",
      "  F =   3.3073870492182710E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  6.88137D-02\n",
      "\n",
      "At iterate    1    f=  6.74467D-01    |proj g|=  8.51636D-02\n",
      "\n",
      "At iterate    2    f=  5.41274D-01    |proj g|=  6.59950D-02\n",
      "\n",
      "At iterate    3    f=  3.32529D-01    |proj g|=  2.47095D-02\n",
      "\n",
      "At iterate    4    f=  2.73261D-01    |proj g|=  1.52121D-02\n",
      "\n",
      "At iterate    5    f=  1.86423D-01    |proj g|=  1.02455D-02\n",
      "\n",
      "At iterate    6    f=  1.16645D-01    |proj g|=  1.01128D-02\n",
      "\n",
      "At iterate    7    f=  6.80646D-02    |proj g|=  7.30925D-03\n",
      "\n",
      "At iterate    8    f=  3.91837D-02    |proj g|=  4.01139D-03\n",
      "\n",
      "At iterate    9    f=  1.95748D-02    |proj g|=  2.24234D-03\n",
      "\n",
      "At iterate   10    f=  1.05690D-02    |proj g|=  1.51276D-03\n",
      "\n",
      "At iterate   11    f=  5.15287D-03    |proj g|=  7.77030D-04\n",
      "\n",
      "At iterate   12    f=  4.78244D-03    |proj g|=  3.87759D-03\n",
      "\n",
      "At iterate   13    f=  1.51526D-03    |proj g|=  8.22856D-04\n",
      "\n",
      "At iterate   14    f=  9.63258D-04    |proj g|=  4.70328D-04\n",
      "\n",
      "At iterate   15    f=  4.52633D-04    |proj g|=  1.91345D-04\n",
      "\n",
      "At iterate   16    f=  2.38298D-04    |proj g|=  9.04408D-05\n",
      "\n",
      "At iterate   17    f=  1.19771D-04    |proj g|=  4.08345D-05\n",
      "\n",
      "At iterate   18    f=  6.11256D-05    |proj g|=  1.88270D-05\n",
      "\n",
      "At iterate   19    f=  3.09108D-05    |proj g|=  8.53558D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     19     22      1     0     0   8.536D-06   3.091D-05\n",
      "  F =   3.0910756380577963E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  7.05583D-02\n",
      "\n",
      "At iterate    1    f=  6.84920D-01    |proj g|=  6.18970D-02\n",
      "\n",
      "At iterate    2    f=  5.78216D-01    |proj g|=  8.01141D-02\n",
      "\n",
      "At iterate    3    f=  3.81236D-01    |proj g|=  3.69320D-02\n",
      "\n",
      "At iterate    4    f=  3.06412D-01    |proj g|=  1.87592D-02\n",
      "\n",
      "At iterate    5    f=  2.38636D-01    |proj g|=  1.11073D-02\n",
      "\n",
      "At iterate    6    f=  1.81038D-01    |proj g|=  7.98169D-03\n",
      "\n",
      "At iterate    7    f=  1.16044D-01    |proj g|=  6.08149D-03\n",
      "\n",
      "At iterate    8    f=  7.14027D-02    |proj g|=  5.34831D-03\n",
      "\n",
      "At iterate    9    f=  5.12117D-02    |proj g|=  3.27411D-03\n",
      "\n",
      "At iterate   10    f=  2.72868D-02    |proj g|=  2.44182D-03\n",
      "\n",
      "At iterate   11    f=  1.64364D-02    |proj g|=  1.45271D-03\n",
      "\n",
      "At iterate   12    f=  9.97641D-03    |proj g|=  4.53301D-03\n",
      "\n",
      "At iterate   13    f=  3.59548D-03    |proj g|=  8.56044D-04\n",
      "\n",
      "At iterate   14    f=  2.49388D-03    |proj g|=  5.32306D-04\n",
      "\n",
      "At iterate   15    f=  1.23478D-03    |proj g|=  2.31926D-04\n",
      "\n",
      "At iterate   16    f=  6.93285D-04    |proj g|=  1.25604D-04\n",
      "\n",
      "At iterate   17    f=  3.64134D-04    |proj g|=  6.25200D-05\n",
      "\n",
      "At iterate   18    f=  1.91907D-04    |proj g|=  3.50486D-05\n",
      "\n",
      "At iterate   19    f=  9.88680D-05    |proj g|=  1.80411D-05\n",
      "\n",
      "At iterate   20    f=  5.05700D-05    |proj g|=  9.70197D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     20     24      1     0     0   9.702D-06   5.057D-05\n",
      "  F =   5.0569987739632913E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " 62%|██████▏   | 8/13 [00:00<00:00, 33.34it/s] This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  2.18801D-01\n",
      "\n",
      "At iterate    1    f=  6.07633D-01    |proj g|=  5.72447D-02\n",
      "\n",
      "At iterate    2    f=  5.30711D-01    |proj g|=  6.46759D-02\n",
      "\n",
      "At iterate    3    f=  3.76220D-01    |proj g|=  9.64013D-02\n",
      "\n",
      "At iterate    4    f=  3.00534D-01    |proj g|=  6.21976D-02\n",
      "\n",
      "At iterate    5    f=  2.41391D-01    |proj g|=  2.89231D-02\n",
      "\n",
      "At iterate    6    f=  2.07624D-01    |proj g|=  1.40037D-02\n",
      "\n",
      "At iterate    7    f=  1.80862D-01    |proj g|=  1.28793D-02\n",
      "\n",
      "At iterate    8    f=  1.51146D-01    |proj g|=  1.75816D-02\n",
      "\n",
      "At iterate    9    f=  1.04484D-01    |proj g|=  1.31365D-02\n",
      "\n",
      "At iterate   10    f=  6.53272D-02    |proj g|=  3.49653D-03\n",
      "\n",
      "At iterate   11    f=  5.47138D-02    |proj g|=  1.94043D-03\n",
      "\n",
      "At iterate   12    f=  4.67676D-02    |proj g|=  6.56631D-03\n",
      "\n",
      "At iterate   13    f=  3.96256D-02    |proj g|=  2.73420D-03\n",
      "\n",
      "At iterate   14    f=  3.80314D-02    |proj g|=  2.62261D-03\n",
      "\n",
      "At iterate   15    f=  3.49557D-02    |proj g|=  2.28205D-03\n",
      "\n",
      "At iterate   16    f=  3.11284D-02    |proj g|=  2.54523D-03\n",
      "\n",
      "At iterate   17    f=  2.81271D-02    |proj g|=  3.16182D-03\n",
      "\n",
      "At iterate   18    f=  2.27399D-02    |proj g|=  1.77342D-03\n",
      "\n",
      "At iterate   19    f=  1.98524D-02    |proj g|=  1.87366D-03\n",
      "\n",
      "At iterate   20    f=  1.69463D-02    |proj g|=  2.73067D-03\n",
      "\n",
      "At iterate   21    f=  1.52269D-02    |proj g|=  1.68882D-03\n",
      "\n",
      "At iterate   22    f=  1.30073D-02    |proj g|=  1.11259D-03\n",
      "\n",
      "At iterate   23    f=  9.76408D-03    |proj g|=  1.45876D-03\n",
      "\n",
      "At iterate   24    f=  6.34306D-03    |proj g|=  1.58430D-03\n",
      "\n",
      "At iterate   25    f=  4.26515D-03    |proj g|=  1.03918D-03\n",
      "\n",
      "At iterate   26    f=  2.88271D-03    |proj g|=  5.73004D-04\n",
      "\n",
      "At iterate   27    f=  1.36600D-03    |proj g|=  2.47582D-04\n",
      "\n",
      "At iterate   28    f=  6.84093D-04    |proj g|=  1.39033D-04\n",
      "\n",
      "At iterate   29    f=  3.34237D-04    |proj g|=  7.35559D-05\n",
      "\n",
      "At iterate   30    f=  1.58760D-04    |proj g|=  3.62317D-05\n",
      "\n",
      "At iterate   31    f=  8.39188D-05    |proj g|=  1.98166D-05\n",
      "\n",
      "At iterate   32    f=  4.11406D-05    |proj g|=  1.31146D-05\n",
      "\n",
      "At iterate   33    f=  2.07109D-05    |proj g|=  5.63107D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     33     37      1     0     0   5.631D-06   2.071D-05\n",
      "  F =   2.0710919489856521E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  8.63427D-02\n",
      "\n",
      "At iterate    1    f=  6.76907D-01    |proj g|=  7.44974D-02\n",
      "\n",
      "At iterate    2    f=  5.87100D-01    |proj g|=  6.89099D-02\n",
      "\n",
      "At iterate    3    f=  3.54768D-01    |proj g|=  3.00407D-02\n",
      "\n",
      "At iterate    4    f=  2.82804D-01    |proj g|=  1.87088D-02\n",
      "\n",
      "At iterate    5    f=  2.01283D-01    |proj g|=  8.59231D-03\n",
      "\n",
      "At iterate    6    f=  1.42912D-01    |proj g|=  6.94873D-03\n",
      "\n",
      "At iterate    7    f=  1.15521D-01    |proj g|=  8.76860D-03\n",
      "\n",
      "At iterate    8    f=  8.48859D-02    |proj g|=  2.41792D-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " 92%|█████████▏| 12/13 [00:00<00:00, 31.20it/s] This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate    9    f=  7.68071D-02    |proj g|=  1.67855D-03\n",
      "\n",
      "At iterate   10    f=  7.03585D-02    |proj g|=  2.16187D-03\n",
      "\n",
      "At iterate   11    f=  6.60411D-02    |proj g|=  2.24709D-03\n",
      "\n",
      "At iterate   12    f=  6.59219D-02    |proj g|=  1.61282D-03\n",
      "\n",
      "At iterate   13    f=  6.28723D-02    |proj g|=  1.32102D-03\n",
      "\n",
      "At iterate   14    f=  6.00705D-02    |proj g|=  1.18953D-03\n",
      "\n",
      "At iterate   15    f=  5.52503D-02    |proj g|=  1.45220D-03\n",
      "\n",
      "At iterate   16    f=  5.21242D-02    |proj g|=  1.16768D-03\n",
      "\n",
      "At iterate   17    f=  5.09087D-02    |proj g|=  1.14373D-03\n",
      "\n",
      "At iterate   18    f=  4.57803D-02    |proj g|=  1.72952D-03\n",
      "\n",
      "At iterate   19    f=  4.11570D-02    |proj g|=  2.41150D-03\n",
      "\n",
      "At iterate   20    f=  3.92679D-02    |proj g|=  2.16417D-03\n",
      "\n",
      "At iterate   21    f=  3.62919D-02    |proj g|=  2.11309D-03\n",
      "\n",
      "At iterate   22    f=  3.53536D-02    |proj g|=  2.28034D-03\n",
      "\n",
      "At iterate   23    f=  3.51195D-02    |proj g|=  8.27035D-04\n",
      "\n",
      "At iterate   24    f=  3.43701D-02    |proj g|=  8.10206D-04\n",
      "\n",
      "At iterate   25    f=  3.35286D-02    |proj g|=  7.01510D-04\n",
      "\n",
      "At iterate   26    f=  3.00255D-02    |proj g|=  6.73912D-04\n",
      "\n",
      "At iterate   27    f=  2.16838D-02    |proj g|=  1.44944D-03\n",
      "\n",
      "At iterate   28    f=  1.09812D-02    |proj g|=  3.29980D-03\n",
      "\n",
      "At iterate   29    f=  5.91567D-03    |proj g|=  1.83737D-03\n",
      "\n",
      "At iterate   30    f=  3.81027D-03    |proj g|=  6.75922D-04\n",
      "\n",
      "At iterate   31    f=  2.99722D-03    |proj g|=  4.22950D-04\n",
      "\n",
      "At iterate   32    f=  1.68361D-03    |proj g|=  2.51617D-04\n",
      "\n",
      "At iterate   33    f=  9.38209D-04    |proj g|=  2.30421D-04\n",
      "\n",
      "At iterate   34    f=  8.90119D-04    |proj g|=  2.62484D-04\n",
      "\n",
      "At iterate   35    f=  4.59147D-04    |proj g|=  8.94902D-05\n",
      "\n",
      "At iterate   36    f=  2.47129D-04    |proj g|=  5.86425D-05\n",
      "\n",
      "At iterate   37    f=  1.19101D-04    |proj g|=  2.76806D-05\n",
      "\n",
      "At iterate   38    f=  6.00779D-05    |proj g|=  1.54443D-05\n",
      "\n",
      "At iterate   39    f=  2.97636D-05    |proj g|=  4.99899D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     39     46      1     0     0   4.999D-06   2.976D-05\n",
      "  F =   2.9763593632658952E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.23109D-01\n",
      "\n",
      "At iterate    1    f=  6.60675D-01    |proj g|=  5.66443D-02\n",
      "\n",
      "At iterate    2    f=  5.88933D-01    |proj g|=  4.78924D-02\n",
      "\n",
      "At iterate    3    f=  3.39794D-01    |proj g|=  2.97795D-02\n",
      "\n",
      "At iterate    4    f=  2.51526D-01    |proj g|=  2.11892D-02\n",
      "\n",
      "At iterate    5    f=  1.61009D-01    |proj g|=  1.20272D-02\n",
      "\n",
      "At iterate    6    f=  1.11469D-01    |proj g|=  5.43686D-03\n",
      "\n",
      "At iterate    7    f=  8.06713D-02    |proj g|=  2.58475D-03\n",
      "\n",
      "At iterate    8    f=  6.16067D-02    |proj g|=  2.96687D-03\n",
      "\n",
      "At iterate    9    f=  4.66585D-02    |proj g|=  6.32861D-03\n",
      "\n",
      "At iterate   10    f=  3.38792D-02    |proj g|=  2.53388D-03\n",
      "\n",
      "At iterate   11    f=  2.44405D-02    |proj g|=  2.33829D-03\n",
      "\n",
      "At iterate   12    f=  2.31942D-02    |proj g|=  3.78282D-03\n",
      "\n",
      "At iterate   13    f=  1.96757D-02    |proj g|=  1.68467D-03\n",
      "\n",
      "At iterate   14    f=  1.81409D-02    |proj g|=  2.28389D-03\n",
      "\n",
      "At iterate   15    f=  1.52793D-02    |proj g|=  2.35906D-03\n",
      "\n",
      "At iterate   16    f=  1.15647D-02    |proj g|=  1.90599D-03\n",
      "\n",
      "At iterate   17    f=  8.67004D-03    |proj g|=  2.12912D-03\n",
      "\n",
      "At iterate   18    f=  5.02410D-03    |proj g|=  1.43035D-03\n",
      "\n",
      "At iterate   19    f=  2.57484D-03    |proj g|=  5.98185D-04\n",
      "\n",
      "At iterate   20    f=  1.34208D-03    |proj g|=  2.09317D-04\n",
      "\n",
      "At iterate   21    f=  7.01908D-04    |proj g|=  8.01063D-05\n",
      "\n",
      "At iterate   22    f=  3.63347D-04    |proj g|=  3.20280D-05\n",
      "\n",
      "At iterate   23    f=  1.89379D-04    |proj g|=  4.66562D-05\n",
      "\n",
      "At iterate   24    f=  1.39616D-04    |proj g|=  8.30807D-05\n",
      "\n",
      "At iterate   25    f=  6.33980D-05    |proj g|=  2.10356D-05\n",
      "\n",
      "At iterate   26    f=  3.63404D-05    |proj g|=  1.00255D-05\n",
      "\n",
      "At iterate   27    f=  1.74876D-05    |proj g|=  3.88943D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     27     33      1     0     0   3.889D-06   1.749D-05\n",
      "  F =   1.7487571494737293E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  2.17827D-01\n",
      "\n",
      "At iterate    1    f=  6.10478D-01    |proj g|=  6.44744D-02\n",
      "\n",
      "At iterate    2    f=  5.08848D-01    |proj g|=  7.60188D-02\n",
      "\n",
      "At iterate    3    f=  3.31122D-01    |proj g|=  1.00217D-01\n",
      "\n",
      "At iterate    4    f=  2.47051D-01    |proj g|=  6.18657D-02\n",
      "\n",
      "At iterate    5    f=  1.82437D-01    |proj g|=  2.81440D-02\n",
      "\n",
      "At iterate    6    f=  1.45209D-01    |proj g|=  1.09189D-02\n",
      "\n",
      "At iterate    7    f=  1.16150D-01    |proj g|=  6.45477D-03\n",
      "\n",
      "At iterate    8    f=  8.76972D-02    |proj g|=  8.60720D-03\n",
      "\n",
      "At iterate    9    f=  4.65156D-02    |proj g|=  6.13418D-03\n",
      "\n",
      "At iterate   10    f=  1.58782D-02    |proj g|=  2.18720D-03\n",
      "\n",
      "At iterate   11    f=  8.57116D-03    |proj g|=  1.34059D-03\n",
      "\n",
      "At iterate   12    f=  7.17480D-03    |proj g|=  3.73703D-03\n",
      "\n",
      "At iterate   13    f=  3.27997D-03    |proj g|=  1.11351D-03\n",
      "\n",
      "At iterate   14    f=  1.88582D-03    |proj g|=  6.35136D-04\n",
      "\n",
      "At iterate   15    f=  9.44660D-04    |proj g|=  3.10800D-04\n",
      "\n",
      "At iterate   16    f=  4.85632D-04    |proj g|=  1.53910D-04\n",
      "\n",
      "At iterate   17    f=  2.44480D-04    |proj g|=  7.38931D-05\n",
      "\n",
      "At iterate   18    f=  1.23486D-04    |proj g|=  3.50443D-05\n",
      "\n",
      "At iterate   19    f=  6.21062D-05    |proj g|=  1.64490D-05\n",
      "\n",
      "At iterate   20    f=  3.12175D-05    |proj g|=  7.78916D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     20     25      1     0     0   7.789D-06   3.122D-05\n",
      "  F =   3.1217530976373556E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.85254D-01\n",
      "\n",
      "At iterate    1    f=  6.27224D-01    |proj g|=  4.15452D-02\n",
      "\n",
      "At iterate    2    f=  5.48359D-01    |proj g|=  3.84811D-02\n",
      "\n",
      "At iterate    3    f=  3.81871D-01    |proj g|=  5.84983D-02\n",
      "\n",
      "At iterate    4    f=  3.19016D-01    |proj g|=  3.79844D-02\n",
      "\n",
      "At iterate    5    f=  2.40744D-01    |proj g|=  1.28254D-02\n",
      "\n",
      "At iterate    6    f=  1.94867D-01    |proj g|=  7.73945D-03\n",
      "\n",
      "At iterate    7    f=  1.51430D-01    |proj g|=  8.91113D-03\n",
      "\n",
      "At iterate    8    f=  9.63117D-02    |proj g|=  8.07562D-03\n",
      "\n",
      "At iterate    9    f=  4.34939D-02    |proj g|=  1.01090D-02\n",
      "\n",
      "At iterate   10    f=  2.90836D-02    |proj g|=  5.93540D-03\n",
      "\n",
      "At iterate   11    f=  1.50328D-02    |proj g|=  2.38911D-03\n",
      "\n",
      "At iterate   12    f=  8.73630D-03    |proj g|=  3.46890D-03\n",
      "\n",
      "At iterate   13    f=  6.87655D-03    |proj g|=  2.93110D-03\n",
      "\n",
      "At iterate   14    f=  3.67561D-03    |proj g|=  6.51725D-04\n",
      "\n",
      "At iterate   15    f=  2.14703D-03    |proj g|=  3.74247D-04\n",
      "\n",
      "At iterate   16    f=  1.07716D-03    |proj g|=  1.77814D-04\n",
      "\n",
      "At iterate   17    f=  5.45543D-04    |proj g|=  8.82904D-05\n",
      "\n",
      "At iterate   18    f=  2.72347D-04    |proj g|=  3.59301D-05\n",
      "\n",
      "At iterate   19    f=  1.34684D-04    |proj g|=  3.51992D-05\n",
      "\n",
      "At iterate   20    f=  5.31939D-05    |proj g|=  6.09877D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     20     25      1     0     0   6.099D-06   5.319D-05\n",
      "  F =   5.3193857347958514E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.01011D-01\n",
      "\n",
      "At iterate    1    f=  6.72915D-01    |proj g|=  7.86626D-02\n",
      "\n",
      "At iterate    2    f=  5.26122D-01    |proj g|=  6.63707D-02\n",
      "\n",
      "At iterate    3    f=  2.95944D-01    |proj g|=  2.87511D-02\n",
      "\n",
      "At iterate    4    f=  2.23680D-01    |proj g|=  1.89622D-02\n",
      "\n",
      "At iterate    5    f=  1.38166D-01    |proj g|=  1.12865D-02\n",
      "\n",
      "At iterate    6    f=  7.54020D-02    |proj g|=  6.48640D-03\n",
      "\n",
      "At iterate    7    f=  3.74907D-02    |proj g|=  5.33308D-03\n",
      "\n",
      "At iterate    8    f=  2.02986D-02    |proj g|=  3.09694D-03\n",
      "\n",
      "At iterate    9    f=  1.01126D-02    |proj g|=  1.42882D-03\n",
      "\n",
      "At iterate   10    f=  5.23198D-03    |proj g|=  7.33955D-04\n",
      "\n",
      "At iterate   11    f=  2.64013D-03    |proj g|=  3.66247D-04\n",
      "\n",
      "At iterate   12    f=  1.35196D-03    |proj g|=  3.84070D-04\n",
      "\n",
      "At iterate   13    f=  1.28614D-03    |proj g|=  8.47596D-05\n",
      "\n",
      "At iterate   14    f=  6.49339D-04    |proj g|=  5.96452D-05\n",
      "\n",
      "At iterate   15    f=  3.29222D-04    |proj g|=  3.13291D-05\n",
      "\n",
      "At iterate   16    f=  1.65154D-04    |proj g|=  1.65289D-05\n",
      "\n",
      "At iterate   17    f=  8.35803D-05    |proj g|=  7.91197D-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 31.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     17     23      1     0     0   7.912D-06   8.358D-05\n",
      "  F =   8.3580334563969474E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 5199.39it/s]\n",
      "100%|██████████| 13/13 [00:00<00:00, 760.58it/s]\n",
      "100%|██████████| 13/13 [00:00<00:00, 5372.54it/s]\n",
      "100%|██████████| 13/13 [00:00<00:00, 872.57it/s]\n",
      "100%|██████████| 13/13 [00:00<00:00, 267.21it/s]\n",
      "/storage/work/eak5582/Research/generalized_mlm_2.py:404: ClusterWarning: scipy.cluster: The symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix\n",
      "  LocalModelsTree = linkage(self.dist_mat_avg, 'ward')\n",
      "  0%|          | 0/10 [00:00<?, ?it/s] This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  9.59057D-02\n",
      "\n",
      "At iterate    1    f=  6.73387D-01    |proj g|=  7.90447D-02\n",
      "\n",
      "At iterate    2    f=  5.68960D-01    |proj g|=  7.40979D-02\n",
      "\n",
      "At iterate    3    f=  3.64010D-01    |proj g|=  3.37607D-02\n",
      "\n",
      "At iterate    4    f=  3.00648D-01    |proj g|=  1.80422D-02\n",
      "\n",
      "At iterate    5    f=  2.47577D-01    |proj g|=  1.25172D-02\n",
      "\n",
      "At iterate    6    f=  2.02217D-01    |proj g|=  1.38270D-02\n",
      "\n",
      "At iterate    7    f=  1.73553D-01    |proj g|=  7.61602D-03\n",
      "\n",
      "At iterate    8    f=  1.38183D-01    |proj g|=  6.33942D-03\n",
      "\n",
      "At iterate    9    f=  1.15498D-01    |proj g|=  9.22744D-03\n",
      "\n",
      "At iterate   10    f=  9.71934D-02    |proj g|=  8.67306D-03\n",
      "\n",
      "At iterate   11    f=  6.81934D-02    |proj g|=  5.35610D-03\n",
      "\n",
      "At iterate   12    f=  6.55154D-02    |proj g|=  8.40144D-03\n",
      "\n",
      "At iterate   13    f=  5.86475D-02    |proj g|=  6.36222D-03\n",
      "\n",
      "At iterate   14    f=  4.35298D-02    |proj g|=  2.20665D-03\n",
      "\n",
      "At iterate   15    f=  3.73824D-02    |proj g|=  2.08941D-03\n",
      "\n",
      "At iterate   16    f=  2.54659D-02    |proj g|=  3.30444D-03\n",
      "\n",
      "At iterate   17    f=  1.33961D-02    |proj g|=  2.26633D-03\n",
      "\n",
      "At iterate   18    f=  9.08000D-03    |proj g|=  2.39299D-03\n",
      "\n",
      "At iterate   19    f=  4.41409D-03    |proj g|=  4.24607D-04\n",
      "\n",
      "At iterate   20    f=  2.97437D-03    |proj g|=  3.42488D-04\n",
      "\n",
      "At iterate   21    f=  1.25820D-03    |proj g|=  9.62200D-05\n",
      "\n",
      "At iterate   22    f=  6.36481D-04    |proj g|=  1.30111D-04\n",
      "\n",
      "At iterate   23    f=  5.91417D-04    |proj g|=  1.51191D-04\n",
      "\n",
      "At iterate   24    f=  3.79641D-04    |proj g|=  8.50672D-05\n",
      "\n",
      "At iterate   25    f=  1.89482D-04    |proj g|=  4.42222D-05\n",
      "\n",
      "At iterate   26    f=  9.76424D-05    |proj g|=  2.22820D-05\n",
      "\n",
      "At iterate   27    f=  4.87607D-05    |proj g|=  1.16583D-05\n",
      "\n",
      "At iterate   28    f=  2.44657D-05    |proj g|=  5.44537D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     28     35      1     0     0   5.445D-06   2.447D-05\n",
      "  F =   2.4465676619638963E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  8.45447D-02\n",
      "\n",
      "At iterate    1    f=  6.80708D-01    |proj g|=  6.40493D-02\n",
      "\n",
      "At iterate    2    f=  5.60215D-01    |proj g|=  6.66544D-02\n",
      "\n",
      "At iterate    3    f=  3.56395D-01    |proj g|=  2.56572D-02\n",
      "\n",
      "At iterate    4    f=  2.82744D-01    |proj g|=  1.90642D-02\n",
      "\n",
      "At iterate    5    f=  1.96222D-01    |proj g|=  1.25444D-02\n",
      "\n",
      "At iterate    6    f=  1.17630D-01    |proj g|=  7.64131D-03\n",
      "\n",
      "At iterate    7    f=  6.28409D-02    |proj g|=  4.86418D-03\n",
      "\n",
      "At iterate    8    f=  3.18229D-02    |proj g|=  5.32383D-03\n",
      "\n",
      "At iterate    9    f=  1.72301D-02    |proj g|=  1.24894D-03\n",
      "\n",
      "At iterate   10    f=  1.13331D-02    |proj g|=  1.19854D-03\n",
      "\n",
      "At iterate   11    f=  5.83612D-03    |proj g|=  3.88033D-04\n",
      "\n",
      "At iterate   12    f=  5.35104D-03    |proj g|=  2.37836D-03\n",
      "\n",
      "At iterate   13    f=  2.93311D-03    |proj g|=  8.32312D-04\n",
      "\n",
      "At iterate   14    f=  1.69780D-03    |proj g|=  3.97111D-04\n",
      "\n",
      "At iterate   15    f=  9.45091D-04    |proj g|=  1.84583D-04\n",
      "\n",
      "At iterate   16    f=  5.11195D-04    |proj g|=  8.38894D-05\n",
      "\n",
      "At iterate   17    f=  2.66170D-04    |proj g|=  3.87646D-05\n",
      "\n",
      "At iterate   18    f=  1.35932D-04    |proj g|=  2.01340D-05\n",
      "\n",
      "At iterate   19    f=  6.87076D-05    |proj g|=  1.03867D-05\n",
      "\n",
      "At iterate   20    f=  3.45804D-05    |proj g|=  5.76954D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     20     25      1     0     0   5.770D-06   3.458D-05\n",
      "  F =   3.4580445335075482E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  7.25871D-02\n",
      "\n",
      "At iterate    1    f=  6.52599D-01    |proj g|=  7.69417D-02\n",
      "\n",
      "At iterate    2    f=  6.26031D-01    |proj g|=  5.99832D-02\n",
      "\n",
      "At iterate    3    f=  5.88625D-01    |proj g|=  5.21208D-02\n",
      "\n",
      "At iterate    4    f=  4.03645D-01    |proj g|=  4.67015D-02\n",
      "\n",
      "At iterate    5    f=  3.19321D-01    |proj g|=  3.48260D-02\n",
      "\n",
      "At iterate    6    f=  2.70484D-01    |proj g|=  1.12198D-02\n",
      "\n",
      "At iterate    7    f=  2.40195D-01    |proj g|=  9.22187D-03\n",
      "\n",
      "At iterate    8    f=  2.18409D-01    |proj g|=  1.19612D-02\n",
      "\n",
      "At iterate    9    f=  1.95892D-01    |proj g|=  9.41253D-03\n",
      "\n",
      "At iterate   10    f=  1.70187D-01    |proj g|=  3.91495D-03\n",
      "\n",
      "At iterate   11    f=  1.56912D-01    |proj g|=  2.10400D-03\n",
      "\n",
      "At iterate   12    f=  1.50136D-01    |proj g|=  1.60827D-03\n",
      "\n",
      "At iterate   13    f=  1.49475D-01    |proj g|=  7.34881D-03\n",
      "\n",
      "At iterate   14    f=  1.44790D-01    |proj g|=  4.67389D-03\n",
      "\n",
      "At iterate   15    f=  1.41749D-01    |proj g|=  4.56425D-03\n",
      "\n",
      "At iterate   16    f=  1.38385D-01    |proj g|=  3.06485D-03\n",
      "\n",
      "At iterate   17    f=  1.35430D-01    |proj g|=  3.67172D-03\n",
      "\n",
      "At iterate   18    f=  1.31824D-01    |proj g|=  2.60722D-03\n",
      "\n",
      "At iterate   19    f=  1.29879D-01    |proj g|=  2.12774D-03\n",
      "\n",
      "At iterate   20    f=  1.26405D-01    |proj g|=  2.10729D-03\n",
      "\n",
      "At iterate   21    f=  1.24537D-01    |proj g|=  2.64278D-03\n",
      "\n",
      "At iterate   22    f=  1.23262D-01    |proj g|=  1.10515D-03\n",
      "\n",
      "At iterate   23    f=  1.22040D-01    |proj g|=  1.06895D-03\n",
      "\n",
      "At iterate   24    f=  1.21596D-01    |proj g|=  5.77967D-03\n",
      "\n",
      "At iterate   25    f=  1.20734D-01    |proj g|=  1.75174D-03\n",
      "\n",
      "At iterate   26    f=  1.20154D-01    |proj g|=  1.13986D-03\n",
      "\n",
      "At iterate   27    f=  1.19696D-01    |proj g|=  1.62327D-03\n",
      "\n",
      "At iterate   28    f=  1.18897D-01    |proj g|=  1.38457D-03\n",
      "\n",
      "At iterate   29    f=  1.18641D-01    |proj g|=  2.91459D-03\n",
      "\n",
      "At iterate   30    f=  1.17500D-01    |proj g|=  1.15620D-03\n",
      "\n",
      "At iterate   31    f=  1.16974D-01    |proj g|=  7.99694D-04\n",
      "\n",
      "At iterate   32    f=  1.16015D-01    |proj g|=  1.15479D-03\n",
      "\n",
      "At iterate   33    f=  1.15403D-01    |proj g|=  1.23461D-03\n",
      "\n",
      "At iterate   34    f=  1.14737D-01    |proj g|=  7.72367D-04\n",
      "\n",
      "At iterate   35    f=  1.14582D-01    |proj g|=  2.39382D-03\n",
      "\n",
      "At iterate   36    f=  1.14218D-01    |proj g|=  1.05422D-03\n",
      "\n",
      "At iterate   37    f=  1.14041D-01    |proj g|=  1.11898D-03\n",
      "\n",
      "At iterate   38    f=  1.13695D-01    |proj g|=  9.14654D-04\n",
      "\n",
      "At iterate   39    f=  1.13435D-01    |proj g|=  2.32203D-03\n",
      "\n",
      "At iterate   40    f=  1.13132D-01    |proj g|=  9.13528D-04\n",
      "\n",
      "At iterate   41    f=  1.13003D-01    |proj g|=  1.12707D-03\n",
      "\n",
      "At iterate   42    f=  1.12706D-01    |proj g|=  9.55939D-04\n",
      "\n",
      "At iterate   43    f=  1.12494D-01    |proj g|=  6.78453D-04\n",
      "\n",
      "At iterate   44    f=  1.12185D-01    |proj g|=  5.22702D-04\n",
      "\n",
      "At iterate   45    f=  1.11799D-01    |proj g|=  6.90571D-04\n",
      "\n",
      "At iterate   46    f=  1.11630D-01    |proj g|=  1.37090D-03\n",
      "\n",
      "At iterate   47    f=  1.11460D-01    |proj g|=  9.95315D-04\n",
      "\n",
      "At iterate   48    f=  1.11345D-01    |proj g|=  7.15360D-04\n",
      "\n",
      "At iterate   49    f=  1.11167D-01    |proj g|=  3.82090D-04\n",
      "\n",
      "At iterate   50    f=  1.11091D-01    |proj g|=  3.83600D-04\n",
      "\n",
      "At iterate   51    f=  1.11073D-01    |proj g|=  8.09738D-04\n",
      "\n",
      "At iterate   52    f=  1.11024D-01    |proj g|=  5.22643D-04\n",
      "\n",
      "At iterate   53    f=  1.10963D-01    |proj g|=  4.25991D-04\n",
      "\n",
      "At iterate   54    f=  1.10856D-01    |proj g|=  3.00218D-04\n",
      "\n",
      "At iterate   55    f=  1.10742D-01    |proj g|=  7.24781D-04\n",
      "\n",
      "At iterate   56    f=  1.10664D-01    |proj g|=  7.33812D-04\n",
      "\n",
      "At iterate   57    f=  1.10600D-01    |proj g|=  2.71126D-04\n",
      "\n",
      "At iterate   58    f=  1.10543D-01    |proj g|=  3.53842D-04\n",
      "\n",
      "At iterate   59    f=  1.10524D-01    |proj g|=  7.08165D-04\n",
      "\n",
      "At iterate   60    f=  1.10481D-01    |proj g|=  5.47542D-04\n",
      "\n",
      "At iterate   61    f=  1.10373D-01    |proj g|=  2.40104D-04\n",
      "\n",
      "At iterate   62    f=  1.10315D-01    |proj g|=  7.82872D-04\n",
      "\n",
      "At iterate   63    f=  1.10279D-01    |proj g|=  4.37224D-04\n",
      "\n",
      "At iterate   64    f=  1.10249D-01    |proj g|=  4.02044D-04\n",
      "\n",
      "At iterate   65    f=  1.10205D-01    |proj g|=  4.55193D-04\n",
      "\n",
      "At iterate   66    f=  1.10131D-01    |proj g|=  4.06292D-04\n",
      "\n",
      "At iterate   67    f=  1.10089D-01    |proj g|=  1.41482D-03\n",
      "\n",
      "At iterate   68    f=  1.09972D-01    |proj g|=  4.21401D-04\n",
      "\n",
      "At iterate   69    f=  1.09923D-01    |proj g|=  2.17076D-04\n",
      "\n",
      "At iterate   70    f=  1.09875D-01    |proj g|=  3.36360D-04\n",
      "\n",
      "At iterate   71    f=  1.09861D-01    |proj g|=  7.23912D-04\n",
      "\n",
      "At iterate   72    f=  1.09822D-01    |proj g|=  4.37004D-04\n",
      "\n",
      "At iterate   73    f=  1.09779D-01    |proj g|=  3.98995D-04\n",
      "\n",
      "At iterate   74    f=  1.09729D-01    |proj g|=  5.45937D-04\n",
      "\n",
      "At iterate   75    f=  1.09675D-01    |proj g|=  4.14955D-04\n",
      "\n",
      "At iterate   76    f=  1.09657D-01    |proj g|=  7.67906D-04\n",
      "\n",
      "At iterate   77    f=  1.09613D-01    |proj g|=  2.09805D-04\n",
      "\n",
      "At iterate   78    f=  1.09592D-01    |proj g|=  3.84067D-04\n",
      "\n",
      "At iterate   79    f=  1.09562D-01    |proj g|=  4.86276D-04\n",
      "\n",
      "At iterate   80    f=  1.09528D-01    |proj g|=  3.63673D-04\n",
      "\n",
      "At iterate   81    f=  1.09505D-01    |proj g|=  5.45176D-04\n",
      "\n",
      "At iterate   82    f=  1.09475D-01    |proj g|=  5.67817D-04\n",
      "\n",
      "At iterate   83    f=  1.09456D-01    |proj g|=  3.77511D-04\n",
      "\n",
      "At iterate   84    f=  1.09437D-01    |proj g|=  4.12084D-04\n",
      "\n",
      "At iterate   85    f=  1.09395D-01    |proj g|=  3.59253D-04\n",
      "\n",
      "At iterate   86    f=  1.09372D-01    |proj g|=  2.39753D-04\n",
      "\n",
      "At iterate   87    f=  1.09351D-01    |proj g|=  2.36826D-04\n",
      "\n",
      "At iterate   88    f=  1.09326D-01    |proj g|=  5.12480D-04\n",
      "\n",
      "At iterate   89    f=  1.09316D-01    |proj g|=  5.36838D-04\n",
      "\n",
      "At iterate   90    f=  1.09302D-01    |proj g|=  1.67122D-04\n",
      "\n",
      "At iterate   91    f=  1.09298D-01    |proj g|=  1.86460D-04\n",
      "\n",
      "At iterate   92    f=  1.09286D-01    |proj g|=  1.90882D-04\n",
      "\n",
      "At iterate   93    f=  1.09261D-01    |proj g|=  2.22728D-04\n",
      "\n",
      "At iterate   94    f=  1.09255D-01    |proj g|=  5.42473D-04\n",
      "\n",
      "At iterate   95    f=  1.09239D-01    |proj g|=  3.13977D-04\n",
      "\n",
      "At iterate   96    f=  1.09228D-01    |proj g|=  1.56924D-04\n",
      "\n",
      "At iterate   97    f=  1.09218D-01    |proj g|=  2.00472D-04\n",
      "\n",
      "At iterate   98    f=  1.09205D-01    |proj g|=  2.48927D-04\n",
      "\n",
      "At iterate   99    f=  1.09198D-01    |proj g|=  4.02956D-04\n",
      "\n",
      "At iterate  100    f=  1.09187D-01    |proj g|=  1.49994D-04\n",
      "\n",
      "At iterate  101    f=  1.09183D-01    |proj g|=  1.03713D-04\n",
      "\n",
      "At iterate  102    f=  1.09177D-01    |proj g|=  1.09381D-04\n",
      "\n",
      "At iterate  103    f=  1.09172D-01    |proj g|=  2.57512D-04\n",
      "\n",
      "At iterate  104    f=  1.09167D-01    |proj g|=  1.10791D-04\n",
      "\n",
      "At iterate  105    f=  1.09164D-01    |proj g|=  1.04335D-04\n",
      "\n",
      "At iterate  106    f=  1.09161D-01    |proj g|=  1.31482D-04\n",
      "\n",
      "At iterate  107    f=  1.09156D-01    |proj g|=  1.06454D-04\n",
      "\n",
      "At iterate  108    f=  1.09152D-01    |proj g|=  3.47948D-04\n",
      "\n",
      "At iterate  109    f=  1.09146D-01    |proj g|=  1.99941D-04\n",
      "\n",
      "At iterate  110    f=  1.09138D-01    |proj g|=  1.12218D-04\n",
      "\n",
      "At iterate  111    f=  1.09135D-01    |proj g|=  2.07358D-04\n",
      "\n",
      "At iterate  112    f=  1.09131D-01    |proj g|=  1.25853D-04\n",
      "\n",
      "At iterate  113    f=  1.09128D-01    |proj g|=  1.30039D-04\n",
      "\n",
      "At iterate  114    f=  1.09121D-01    |proj g|=  2.01182D-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate  115    f=  1.09116D-01    |proj g|=  1.55255D-04\n",
      "\n",
      "At iterate  116    f=  1.09112D-01    |proj g|=  1.27907D-04\n",
      "\n",
      "At iterate  117    f=  1.09110D-01    |proj g|=  7.90566D-05\n",
      "\n",
      "At iterate  118    f=  1.09104D-01    |proj g|=  1.38643D-04\n",
      "\n",
      "At iterate  119    f=  1.09098D-01    |proj g|=  1.63959D-04\n",
      "\n",
      "At iterate  120    f=  1.09097D-01    |proj g|=  3.27960D-04\n",
      "\n",
      "At iterate  121    f=  1.09090D-01    |proj g|=  1.90093D-04\n",
      "\n",
      "At iterate  122    f=  1.09086D-01    |proj g|=  9.91098D-05\n",
      "\n",
      "At iterate  123    f=  1.09083D-01    |proj g|=  1.14292D-04\n",
      "\n",
      "At iterate  124    f=  1.09079D-01    |proj g|=  1.85026D-04\n",
      "\n",
      "At iterate  125    f=  1.09076D-01    |proj g|=  2.27814D-04\n",
      "\n",
      "At iterate  126    f=  1.09073D-01    |proj g|=  1.23281D-04\n",
      "\n",
      "At iterate  127    f=  1.09073D-01    |proj g|=  8.26075D-05\n",
      "\n",
      "At iterate  128    f=  1.09071D-01    |proj g|=  9.33565D-05\n",
      "\n",
      "At iterate  129    f=  1.09068D-01    |proj g|=  1.50783D-04\n",
      "\n",
      "At iterate  130    f=  1.09064D-01    |proj g|=  1.48982D-04\n",
      "\n",
      "At iterate  131    f=  1.09058D-01    |proj g|=  1.77813D-04\n",
      "\n",
      "At iterate  132    f=  1.09058D-01    |proj g|=  8.49546D-05\n",
      "\n",
      "At iterate  133    f=  1.09054D-01    |proj g|=  4.63107D-05\n",
      "\n",
      "At iterate  134    f=  1.09052D-01    |proj g|=  1.27734D-04\n",
      "\n",
      "At iterate  135    f=  1.09051D-01    |proj g|=  5.87753D-05\n",
      "\n",
      "At iterate  136    f=  1.09050D-01    |proj g|=  8.06786D-05\n",
      "\n",
      "At iterate  137    f=  1.09049D-01    |proj g|=  6.38070D-05\n",
      "\n",
      "At iterate  138    f=  1.09048D-01    |proj g|=  1.16852D-04\n",
      "\n",
      "At iterate  139    f=  1.09046D-01    |proj g|=  5.62286D-05\n",
      "\n",
      "At iterate  140    f=  1.09044D-01    |proj g|=  1.06928D-04\n",
      "\n",
      "At iterate  141    f=  1.09042D-01    |proj g|=  7.50750D-05\n",
      "\n",
      "At iterate  142    f=  1.09041D-01    |proj g|=  6.95390D-05\n",
      "\n",
      "At iterate  143    f=  1.09041D-01    |proj g|=  1.36661D-04\n",
      "\n",
      "At iterate  144    f=  1.09040D-01    |proj g|=  1.11368D-04\n",
      "\n",
      "At iterate  145    f=  1.09038D-01    |proj g|=  5.39980D-05\n",
      "\n",
      "At iterate  146    f=  1.09037D-01    |proj g|=  6.77159D-05\n",
      "\n",
      "At iterate  147    f=  1.09035D-01    |proj g|=  1.69447D-04\n",
      "\n",
      "At iterate  148    f=  1.09034D-01    |proj g|=  8.64959D-05\n",
      "\n",
      "At iterate  149    f=  1.09032D-01    |proj g|=  5.46866D-05\n",
      "\n",
      "At iterate  150    f=  1.09031D-01    |proj g|=  5.64645D-05\n",
      "\n",
      "At iterate  151    f=  1.09030D-01    |proj g|=  7.29397D-05\n",
      "\n",
      "At iterate  152    f=  1.09028D-01    |proj g|=  5.30095D-05\n",
      "\n",
      "At iterate  153    f=  1.09025D-01    |proj g|=  5.83727D-05\n",
      "\n",
      "At iterate  154    f=  1.09023D-01    |proj g|=  1.14057D-04\n",
      "\n",
      "At iterate  155    f=  1.09022D-01    |proj g|=  1.22388D-04\n",
      "\n",
      "At iterate  156    f=  1.09022D-01    |proj g|=  7.10679D-05\n",
      "\n",
      "At iterate  157    f=  1.09020D-01    |proj g|=  4.37515D-05\n",
      "\n",
      "At iterate  158    f=  1.09019D-01    |proj g|=  4.87715D-05\n",
      "\n",
      "At iterate  159    f=  1.09017D-01    |proj g|=  1.17406D-04\n",
      "\n",
      "At iterate  160    f=  1.09015D-01    |proj g|=  1.51881D-04\n",
      "\n",
      "At iterate  161    f=  1.09014D-01    |proj g|=  5.48519D-05\n",
      "\n",
      "At iterate  162    f=  1.09013D-01    |proj g|=  3.40122D-05\n",
      "\n",
      "At iterate  163    f=  1.09012D-01    |proj g|=  4.32677D-05\n",
      "\n",
      "At iterate  164    f=  1.09011D-01    |proj g|=  8.52440D-05\n",
      "\n",
      "At iterate  165    f=  1.09010D-01    |proj g|=  5.12445D-05\n",
      "\n",
      "At iterate  166    f=  1.09009D-01    |proj g|=  7.11459D-05\n",
      "\n",
      "At iterate  167    f=  1.09009D-01    |proj g|=  7.45231D-05\n",
      "\n",
      "At iterate  168    f=  1.09009D-01    |proj g|=  4.28887D-05\n",
      "\n",
      "At iterate  169    f=  1.09008D-01    |proj g|=  5.66424D-05\n",
      "\n",
      "At iterate  170    f=  1.09007D-01    |proj g|=  8.50568D-05\n",
      "\n",
      "At iterate  171    f=  1.09005D-01    |proj g|=  9.17801D-05\n",
      "\n",
      "At iterate  172    f=  1.09003D-01    |proj g|=  7.34774D-05\n",
      "\n",
      "At iterate  173    f=  1.09003D-01    |proj g|=  8.97933D-05\n",
      "\n",
      "At iterate  174    f=  1.09002D-01    |proj g|=  3.83280D-05\n",
      "\n",
      "At iterate  175    f=  1.09002D-01    |proj g|=  5.15286D-05\n",
      "\n",
      "At iterate  176    f=  1.09001D-01    |proj g|=  7.36733D-05\n",
      "\n",
      "At iterate  177    f=  1.09001D-01    |proj g|=  8.35063D-05\n",
      "\n",
      "At iterate  178    f=  1.08999D-01    |proj g|=  7.07715D-05\n",
      "\n",
      "At iterate  179    f=  1.08999D-01    |proj g|=  1.42467D-04\n",
      "\n",
      "At iterate  180    f=  1.08998D-01    |proj g|=  8.66477D-05\n",
      "\n",
      "At iterate  181    f=  1.08997D-01    |proj g|=  3.66337D-05\n",
      "\n",
      "At iterate  182    f=  1.08997D-01    |proj g|=  3.63036D-05\n",
      "\n",
      "At iterate  183    f=  1.08997D-01    |proj g|=  3.15041D-05\n",
      "\n",
      "At iterate  184    f=  1.08996D-01    |proj g|=  6.00123D-05\n",
      "\n",
      "At iterate  185    f=  1.08995D-01    |proj g|=  7.61491D-05\n",
      "\n",
      "At iterate  186    f=  1.08995D-01    |proj g|=  4.93604D-05\n",
      "\n",
      "At iterate  187    f=  1.08994D-01    |proj g|=  4.10684D-05\n",
      "\n",
      "At iterate  188    f=  1.08994D-01    |proj g|=  5.45981D-05\n",
      "\n",
      "At iterate  189    f=  1.08993D-01    |proj g|=  4.78415D-05\n",
      "\n",
      "At iterate  190    f=  1.08992D-01    |proj g|=  4.30407D-05\n",
      "\n",
      "At iterate  191    f=  1.08992D-01    |proj g|=  8.70063D-05\n",
      "\n",
      "At iterate  192    f=  1.08992D-01    |proj g|=  4.92901D-05\n",
      "\n",
      "At iterate  193    f=  1.08991D-01    |proj g|=  3.00856D-05\n",
      "\n",
      "At iterate  194    f=  1.08991D-01    |proj g|=  2.55053D-05\n",
      "\n",
      "At iterate  195    f=  1.08991D-01    |proj g|=  6.74180D-05\n",
      "\n",
      "At iterate  196    f=  1.08990D-01    |proj g|=  3.63630D-05\n",
      "\n",
      "At iterate  197    f=  1.08990D-01    |proj g|=  4.41128D-05\n",
      "\n",
      "At iterate  198    f=  1.08989D-01    |proj g|=  5.73506D-05\n",
      "\n",
      "At iterate  199    f=  1.08989D-01    |proj g|=  4.77179D-05\n",
      "\n",
      "At iterate  200    f=  1.08988D-01    |proj g|=  6.05892D-05\n",
      "\n",
      "At iterate  201    f=  1.08988D-01    |proj g|=  2.65484D-05\n",
      "\n",
      "At iterate  202    f=  1.08987D-01    |proj g|=  6.11808D-05\n",
      "\n",
      "At iterate  203    f=  1.08987D-01    |proj g|=  4.49712D-05\n",
      "\n",
      "At iterate  204    f=  1.08987D-01    |proj g|=  7.73207D-05\n",
      "\n",
      "At iterate  205    f=  1.08987D-01    |proj g|=  3.12999D-05\n",
      "\n",
      "At iterate  206    f=  1.08987D-01    |proj g|=  1.99408D-05\n",
      "\n",
      "At iterate  207    f=  1.08987D-01    |proj g|=  3.54767D-05\n",
      "\n",
      "At iterate  208    f=  1.08986D-01    |proj g|=  5.12803D-05\n",
      "\n",
      "At iterate  209    f=  1.08986D-01    |proj g|=  6.18710D-05\n",
      "\n",
      "At iterate  210    f=  1.08986D-01    |proj g|=  9.43560D-05\n",
      "\n",
      "At iterate  211    f=  1.08986D-01    |proj g|=  5.66462D-05\n",
      "\n",
      "At iterate  212    f=  1.08985D-01    |proj g|=  3.30942D-05\n",
      "\n",
      "At iterate  213    f=  1.08985D-01    |proj g|=  2.49973D-05\n",
      "\n",
      "At iterate  214    f=  1.08985D-01    |proj g|=  2.48929D-05\n",
      "\n",
      "At iterate  215    f=  1.08985D-01    |proj g|=  8.05558D-05\n",
      "\n",
      "At iterate  216    f=  1.08985D-01    |proj g|=  6.49537D-05\n",
      "\n",
      "At iterate  217    f=  1.08984D-01    |proj g|=  5.42498D-05\n",
      "\n",
      "At iterate  218    f=  1.08984D-01    |proj g|=  2.17246D-05\n",
      "\n",
      "At iterate  219    f=  1.08984D-01    |proj g|=  1.52557D-05\n",
      "\n",
      "At iterate  220    f=  1.08984D-01    |proj g|=  2.57218D-05\n",
      "\n",
      "At iterate  221    f=  1.08984D-01    |proj g|=  3.38248D-05\n",
      "\n",
      "At iterate  222    f=  1.08983D-01    |proj g|=  2.85462D-05\n",
      "\n",
      "At iterate  223    f=  1.08983D-01    |proj g|=  1.61162D-05\n",
      "\n",
      "At iterate  224    f=  1.08982D-01    |proj g|=  3.01916D-05\n",
      "\n",
      "At iterate  225    f=  1.08982D-01    |proj g|=  1.60956D-05\n",
      "\n",
      "At iterate  226    f=  1.08982D-01    |proj g|=  5.97715D-05\n",
      "\n",
      "At iterate  227    f=  1.08982D-01    |proj g|=  4.33844D-05\n",
      "\n",
      "At iterate  228    f=  1.08982D-01    |proj g|=  1.86617D-05\n",
      "\n",
      "At iterate  229    f=  1.08982D-01    |proj g|=  2.11649D-05\n",
      "\n",
      "At iterate  230    f=  1.08981D-01    |proj g|=  7.80240D-05\n",
      "\n",
      "At iterate  231    f=  1.08981D-01    |proj g|=  3.15811D-05\n",
      "\n",
      "At iterate  232    f=  1.08981D-01    |proj g|=  2.28445D-05\n",
      "\n",
      "At iterate  233    f=  1.08981D-01    |proj g|=  4.20208D-05\n",
      "\n",
      "At iterate  234    f=  1.08981D-01    |proj g|=  3.95780D-05\n",
      "\n",
      "At iterate  235    f=  1.08980D-01    |proj g|=  5.16044D-05\n",
      "\n",
      "At iterate  236    f=  1.08980D-01    |proj g|=  2.52910D-05\n",
      "\n",
      "At iterate  237    f=  1.08979D-01    |proj g|=  4.57334D-05\n",
      "\n",
      "At iterate  238    f=  1.08979D-01    |proj g|=  4.26363D-05\n",
      "\n",
      "At iterate  239    f=  1.08979D-01    |proj g|=  5.45039D-05\n",
      "\n",
      "At iterate  240    f=  1.08979D-01    |proj g|=  4.12324D-05\n",
      "\n",
      "At iterate  241    f=  1.08978D-01    |proj g|=  2.26989D-05\n",
      "\n",
      "At iterate  242    f=  1.08978D-01    |proj g|=  3.43811D-05\n",
      "\n",
      "At iterate  243    f=  1.08978D-01    |proj g|=  2.90902D-05\n",
      "\n",
      "At iterate  244    f=  1.08977D-01    |proj g|=  7.46569D-05\n",
      "\n",
      "At iterate  245    f=  1.08977D-01    |proj g|=  3.94724D-05\n",
      "\n",
      "At iterate  246    f=  1.08976D-01    |proj g|=  2.80491D-05\n",
      "\n",
      "At iterate  247    f=  1.08976D-01    |proj g|=  2.95723D-05\n",
      "\n",
      "At iterate  248    f=  1.08975D-01    |proj g|=  4.68572D-05\n",
      "\n",
      "At iterate  249    f=  1.08975D-01    |proj g|=  9.27077D-05\n",
      "\n",
      "At iterate  250    f=  1.08975D-01    |proj g|=  1.15835D-04\n",
      "\n",
      "At iterate  251    f=  1.08974D-01    |proj g|=  4.33305D-05\n",
      "\n",
      "At iterate  252    f=  1.08974D-01    |proj g|=  3.40095D-05\n",
      "\n",
      "At iterate  253    f=  1.08974D-01    |proj g|=  4.90819D-05\n",
      "\n",
      "At iterate  254    f=  1.08973D-01    |proj g|=  4.74337D-05\n",
      "\n",
      "At iterate  255    f=  1.08973D-01    |proj g|=  3.48774D-05\n",
      "\n",
      "At iterate  256    f=  1.08972D-01    |proj g|=  3.27006D-05\n",
      "\n",
      "At iterate  257    f=  1.08972D-01    |proj g|=  2.81086D-05\n",
      "\n",
      "At iterate  258    f=  1.08971D-01    |proj g|=  4.81263D-05\n",
      "\n",
      "At iterate  259    f=  1.08970D-01    |proj g|=  4.59569D-05\n",
      "\n",
      "At iterate  260    f=  1.08969D-01    |proj g|=  5.01764D-05\n",
      "\n",
      "At iterate  261    f=  1.08969D-01    |proj g|=  5.48277D-05\n",
      "\n",
      "At iterate  262    f=  1.08968D-01    |proj g|=  3.66063D-05\n",
      "\n",
      "At iterate  263    f=  1.08968D-01    |proj g|=  2.41283D-05\n",
      "\n",
      "At iterate  264    f=  1.08968D-01    |proj g|=  2.23800D-05\n",
      "\n",
      "At iterate  265    f=  1.08968D-01    |proj g|=  5.97730D-05\n",
      "\n",
      "At iterate  266    f=  1.08967D-01    |proj g|=  4.09927D-05\n",
      "\n",
      "At iterate  267    f=  1.08967D-01    |proj g|=  2.28173D-05\n",
      "\n",
      "At iterate  268    f=  1.08967D-01    |proj g|=  2.19882D-05\n",
      "\n",
      "At iterate  269    f=  1.08966D-01    |proj g|=  4.40127D-05\n",
      "\n",
      "At iterate  270    f=  1.08966D-01    |proj g|=  5.85535D-05\n",
      "\n",
      "At iterate  271    f=  1.08965D-01    |proj g|=  2.22943D-05\n",
      "\n",
      "At iterate  272    f=  1.08965D-01    |proj g|=  3.56149D-05\n",
      "\n",
      "At iterate  273    f=  1.08965D-01    |proj g|=  1.65947D-05\n",
      "\n",
      "At iterate  274    f=  1.08965D-01    |proj g|=  5.89948D-05\n",
      "\n",
      "At iterate  275    f=  1.08965D-01    |proj g|=  2.48955D-05\n",
      "\n",
      "At iterate  276    f=  1.08965D-01    |proj g|=  4.15133D-05\n",
      "\n",
      "At iterate  277    f=  1.08964D-01    |proj g|=  4.02847D-05\n",
      "\n",
      "At iterate  278    f=  1.08964D-01    |proj g|=  9.31866D-05\n",
      "\n",
      "At iterate  279    f=  1.08964D-01    |proj g|=  5.35127D-05\n",
      "\n",
      "At iterate  280    f=  1.08964D-01    |proj g|=  2.57765D-05\n",
      "\n",
      "At iterate  281    f=  1.08963D-01    |proj g|=  4.41433D-05\n",
      "\n",
      "At iterate  282    f=  1.08963D-01    |proj g|=  4.87219D-05\n",
      "\n",
      "At iterate  283    f=  1.08963D-01    |proj g|=  8.10890D-05\n",
      "\n",
      "At iterate  284    f=  1.08962D-01    |proj g|=  3.34646D-05\n",
      "\n",
      "At iterate  285    f=  1.08962D-01    |proj g|=  1.81508D-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:00<00:01,  6.99it/s] This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " 60%|██████    | 6/10 [00:00<00:00, 12.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate  286    f=  1.08962D-01    |proj g|=  4.74446D-05\n",
      "\n",
      "At iterate  287    f=  1.08962D-01    |proj g|=  3.08334D-05\n",
      "\n",
      "At iterate  288    f=  1.08961D-01    |proj g|=  2.28948D-05\n",
      "\n",
      "At iterate  289    f=  1.08961D-01    |proj g|=  2.87506D-05\n",
      "\n",
      "At iterate  290    f=  1.08961D-01    |proj g|=  2.07328D-05\n",
      "\n",
      "At iterate  291    f=  1.08961D-01    |proj g|=  1.31032D-05\n",
      "\n",
      "At iterate  292    f=  1.08961D-01    |proj g|=  2.20988D-05\n",
      "\n",
      "At iterate  293    f=  1.08961D-01    |proj g|=  1.98620D-05\n",
      "\n",
      "At iterate  294    f=  1.08960D-01    |proj g|=  1.29202D-05\n",
      "\n",
      "At iterate  295    f=  1.08960D-01    |proj g|=  1.00656D-05\n",
      "\n",
      "At iterate  296    f=  1.08960D-01    |proj g|=  1.01491D-05\n",
      "\n",
      "At iterate  297    f=  1.08960D-01    |proj g|=  1.31822D-05\n",
      "\n",
      "At iterate  298    f=  1.08960D-01    |proj g|=  6.94040D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74    298    333      1     0     0   6.940D-06   1.090D-01\n",
      "  F =  0.10896021382998124     \n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  6.88137D-02\n",
      "\n",
      "At iterate    1    f=  6.74467D-01    |proj g|=  8.51636D-02\n",
      "\n",
      "At iterate    2    f=  5.41274D-01    |proj g|=  6.59950D-02\n",
      "\n",
      "At iterate    3    f=  3.32529D-01    |proj g|=  2.47095D-02\n",
      "\n",
      "At iterate    4    f=  2.73261D-01    |proj g|=  1.52121D-02\n",
      "\n",
      "At iterate    5    f=  1.86423D-01    |proj g|=  1.02455D-02\n",
      "\n",
      "At iterate    6    f=  1.16645D-01    |proj g|=  1.01128D-02\n",
      "\n",
      "At iterate    7    f=  6.80646D-02    |proj g|=  7.30925D-03\n",
      "\n",
      "At iterate    8    f=  3.91837D-02    |proj g|=  4.01139D-03\n",
      "\n",
      "At iterate    9    f=  1.95748D-02    |proj g|=  2.24234D-03\n",
      "\n",
      "At iterate   10    f=  1.05690D-02    |proj g|=  1.51276D-03\n",
      "\n",
      "At iterate   11    f=  5.15287D-03    |proj g|=  7.77030D-04\n",
      "\n",
      "At iterate   12    f=  4.78244D-03    |proj g|=  3.87759D-03\n",
      "\n",
      "At iterate   13    f=  1.51526D-03    |proj g|=  8.22856D-04\n",
      "\n",
      "At iterate   14    f=  9.63258D-04    |proj g|=  4.70328D-04\n",
      "\n",
      "At iterate   15    f=  4.52633D-04    |proj g|=  1.91345D-04\n",
      "\n",
      "At iterate   16    f=  2.38298D-04    |proj g|=  9.04408D-05\n",
      "\n",
      "At iterate   17    f=  1.19771D-04    |proj g|=  4.08345D-05\n",
      "\n",
      "At iterate   18    f=  6.11256D-05    |proj g|=  1.88270D-05\n",
      "\n",
      "At iterate   19    f=  3.09108D-05    |proj g|=  8.53558D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     19     22      1     0     0   8.536D-06   3.091D-05\n",
      "  F =   3.0910756380577963E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  8.63427D-02\n",
      "\n",
      "At iterate    1    f=  6.76907D-01    |proj g|=  7.44974D-02\n",
      "\n",
      "At iterate    2    f=  5.87100D-01    |proj g|=  6.89099D-02\n",
      "\n",
      "At iterate    3    f=  3.54768D-01    |proj g|=  3.00407D-02\n",
      "\n",
      "At iterate    4    f=  2.82804D-01    |proj g|=  1.87088D-02\n",
      "\n",
      "At iterate    5    f=  2.01283D-01    |proj g|=  8.59231D-03\n",
      "\n",
      "At iterate    6    f=  1.42912D-01    |proj g|=  6.94873D-03\n",
      "\n",
      "At iterate    7    f=  1.15521D-01    |proj g|=  8.76860D-03\n",
      "\n",
      "At iterate    8    f=  8.48859D-02    |proj g|=  2.41792D-03\n",
      "\n",
      "At iterate    9    f=  7.68071D-02    |proj g|=  1.67855D-03\n",
      "\n",
      "At iterate   10    f=  7.03585D-02    |proj g|=  2.16187D-03\n",
      "\n",
      "At iterate   11    f=  6.60411D-02    |proj g|=  2.24709D-03\n",
      "\n",
      "At iterate   12    f=  6.59219D-02    |proj g|=  1.61282D-03\n",
      "\n",
      "At iterate   13    f=  6.28723D-02    |proj g|=  1.32102D-03\n",
      "\n",
      "At iterate   14    f=  6.00705D-02    |proj g|=  1.18953D-03\n",
      "\n",
      "At iterate   15    f=  5.52503D-02    |proj g|=  1.45220D-03\n",
      "\n",
      "At iterate   16    f=  5.21242D-02    |proj g|=  1.16768D-03\n",
      "\n",
      "At iterate   17    f=  5.09087D-02    |proj g|=  1.14373D-03\n",
      "\n",
      "At iterate   18    f=  4.57803D-02    |proj g|=  1.72952D-03\n",
      "\n",
      "At iterate   19    f=  4.11570D-02    |proj g|=  2.41150D-03\n",
      "\n",
      "At iterate   20    f=  3.92679D-02    |proj g|=  2.16417D-03\n",
      "\n",
      "At iterate   21    f=  3.62919D-02    |proj g|=  2.11309D-03\n",
      "\n",
      "At iterate   22    f=  3.53536D-02    |proj g|=  2.28034D-03\n",
      "\n",
      "At iterate   23    f=  3.51195D-02    |proj g|=  8.27035D-04\n",
      "\n",
      "At iterate   24    f=  3.43701D-02    |proj g|=  8.10206D-04\n",
      "\n",
      "At iterate   25    f=  3.35286D-02    |proj g|=  7.01510D-04\n",
      "\n",
      "At iterate   26    f=  3.00255D-02    |proj g|=  6.73912D-04\n",
      "\n",
      "At iterate   27    f=  2.16838D-02    |proj g|=  1.44944D-03\n",
      "\n",
      "At iterate   28    f=  1.09812D-02    |proj g|=  3.29980D-03\n",
      "\n",
      "At iterate   29    f=  5.91567D-03    |proj g|=  1.83737D-03\n",
      "\n",
      "At iterate   30    f=  3.81027D-03    |proj g|=  6.75922D-04\n",
      "\n",
      "At iterate   31    f=  2.99722D-03    |proj g|=  4.22950D-04\n",
      "\n",
      "At iterate   32    f=  1.68361D-03    |proj g|=  2.51617D-04\n",
      "\n",
      "At iterate   33    f=  9.38209D-04    |proj g|=  2.30421D-04\n",
      "\n",
      "At iterate   34    f=  8.90119D-04    |proj g|=  2.62484D-04\n",
      "\n",
      "At iterate   35    f=  4.59147D-04    |proj g|=  8.94902D-05\n",
      "\n",
      "At iterate   36    f=  2.47129D-04    |proj g|=  5.86425D-05\n",
      "\n",
      "At iterate   37    f=  1.19101D-04    |proj g|=  2.76806D-05\n",
      "\n",
      "At iterate   38    f=  6.00779D-05    |proj g|=  1.54443D-05\n",
      "\n",
      "At iterate   39    f=  2.97636D-05    |proj g|=  4.99899D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     39     46      1     0     0   4.999D-06   2.976D-05\n",
      "  F =   2.9763593632658952E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  2.18801D-01\n",
      "\n",
      "At iterate    1    f=  6.07633D-01    |proj g|=  5.72447D-02\n",
      "\n",
      "At iterate    2    f=  5.30711D-01    |proj g|=  6.46759D-02\n",
      "\n",
      "At iterate    3    f=  3.76220D-01    |proj g|=  9.64013D-02\n",
      "\n",
      "At iterate    4    f=  3.00534D-01    |proj g|=  6.21976D-02\n",
      "\n",
      "At iterate    5    f=  2.41391D-01    |proj g|=  2.89231D-02\n",
      "\n",
      "At iterate    6    f=  2.07624D-01    |proj g|=  1.40037D-02\n",
      "\n",
      "At iterate    7    f=  1.80862D-01    |proj g|=  1.28793D-02\n",
      "\n",
      "At iterate    8    f=  1.51146D-01    |proj g|=  1.75816D-02\n",
      "\n",
      "At iterate    9    f=  1.04484D-01    |proj g|=  1.31365D-02\n",
      "\n",
      "At iterate   10    f=  6.53272D-02    |proj g|=  3.49653D-03\n",
      "\n",
      "At iterate   11    f=  5.47138D-02    |proj g|=  1.94043D-03\n",
      "\n",
      "At iterate   12    f=  4.67676D-02    |proj g|=  6.56631D-03\n",
      "\n",
      "At iterate   13    f=  3.96256D-02    |proj g|=  2.73420D-03\n",
      "\n",
      "At iterate   14    f=  3.80314D-02    |proj g|=  2.62261D-03\n",
      "\n",
      "At iterate   15    f=  3.49557D-02    |proj g|=  2.28205D-03\n",
      "\n",
      "At iterate   16    f=  3.11284D-02    |proj g|=  2.54523D-03\n",
      "\n",
      "At iterate   17    f=  2.81271D-02    |proj g|=  3.16182D-03\n",
      "\n",
      "At iterate   18    f=  2.27399D-02    |proj g|=  1.77342D-03\n",
      "\n",
      "At iterate   19    f=  1.98524D-02    |proj g|=  1.87366D-03\n",
      "\n",
      "At iterate   20    f=  1.69463D-02    |proj g|=  2.73067D-03\n",
      "\n",
      "At iterate   21    f=  1.52269D-02    |proj g|=  1.68882D-03\n",
      "\n",
      "At iterate   22    f=  1.30073D-02    |proj g|=  1.11259D-03\n",
      "\n",
      "At iterate   23    f=  9.76408D-03    |proj g|=  1.45876D-03\n",
      "\n",
      "At iterate   24    f=  6.34306D-03    |proj g|=  1.58430D-03\n",
      "\n",
      "At iterate   25    f=  4.26515D-03    |proj g|=  1.03918D-03\n",
      "\n",
      "At iterate   26    f=  2.88271D-03    |proj g|=  5.73004D-04\n",
      "\n",
      "At iterate   27    f=  1.36600D-03    |proj g|=  2.47582D-04\n",
      "\n",
      "At iterate   28    f=  6.84093D-04    |proj g|=  1.39033D-04\n",
      "\n",
      "At iterate   29    f=  3.34237D-04    |proj g|=  7.35559D-05\n",
      "\n",
      "At iterate   30    f=  1.58760D-04    |proj g|=  3.62317D-05\n",
      "\n",
      "At iterate   31    f=  8.39188D-05    |proj g|=  1.98166D-05\n",
      "\n",
      "At iterate   32    f=  4.11406D-05    |proj g|=  1.31146D-05\n",
      "\n",
      "At iterate   33    f=  2.07109D-05    |proj g|=  5.63107D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     33     37      1     0     0   5.631D-06   2.071D-05\n",
      "  F =   2.0710919489856521E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.85254D-01\n",
      "\n",
      "At iterate    1    f=  6.27224D-01    |proj g|=  4.15452D-02\n",
      "\n",
      "At iterate    2    f=  5.48359D-01    |proj g|=  3.84811D-02\n",
      "\n",
      "At iterate    3    f=  3.81871D-01    |proj g|=  5.84983D-02\n",
      "\n",
      "At iterate    4    f=  3.19016D-01    |proj g|=  3.79844D-02\n",
      "\n",
      "At iterate    5    f=  2.40744D-01    |proj g|=  1.28254D-02\n",
      "\n",
      "At iterate    6    f=  1.94867D-01    |proj g|=  7.73945D-03\n",
      "\n",
      "At iterate    7    f=  1.51430D-01    |proj g|=  8.91113D-03\n",
      "\n",
      "At iterate    8    f=  9.63117D-02    |proj g|=  8.07562D-03\n",
      "\n",
      "At iterate    9    f=  4.34939D-02    |proj g|=  1.01090D-02\n",
      "\n",
      "At iterate   10    f=  2.90836D-02    |proj g|=  5.93540D-03\n",
      "\n",
      "At iterate   11    f=  1.50328D-02    |proj g|=  2.38911D-03\n",
      "\n",
      "At iterate   12    f=  8.73630D-03    |proj g|=  3.46890D-03\n",
      "\n",
      "At iterate   13    f=  6.87655D-03    |proj g|=  2.93110D-03\n",
      "\n",
      "At iterate   14    f=  3.67561D-03    |proj g|=  6.51725D-04\n",
      "\n",
      "At iterate   15    f=  2.14703D-03    |proj g|=  3.74247D-04\n",
      "\n",
      "At iterate   16    f=  1.07716D-03    |proj g|=  1.77814D-04\n",
      "\n",
      "At iterate   17    f=  5.45543D-04    |proj g|=  8.82904D-05\n",
      "\n",
      "At iterate   18    f=  2.72347D-04    |proj g|=  3.59301D-05\n",
      "\n",
      "At iterate   19    f=  1.34684D-04    |proj g|=  3.51992D-05\n",
      "\n",
      "At iterate   20    f=  5.31939D-05    |proj g|=  6.09877D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     20     25      1     0     0   6.099D-06   5.319D-05\n",
      "  F =   5.3193857347958514E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.61702D-01\n",
      "\n",
      "At iterate    1    f=  6.51341D-01    |proj g|=  3.48110D-02\n",
      "\n",
      "At iterate    2    f=  5.79382D-01    |proj g|=  2.84348D-02\n",
      "\n",
      "At iterate    3    f=  4.05124D-01    |proj g|=  3.75497D-02\n",
      "\n",
      "At iterate    4    f=  3.51241D-01    |proj g|=  2.70150D-02\n",
      "\n",
      "At iterate    5    f=  2.85425D-01    |proj g|=  1.68004D-02\n",
      "\n",
      "At iterate    6    f=  2.33079D-01    |proj g|=  8.89399D-03\n",
      "\n",
      "At iterate    7    f=  1.79062D-01    |proj g|=  3.87998D-03\n",
      "\n",
      "At iterate    8    f=  1.37678D-01    |proj g|=  5.75043D-03\n",
      "\n",
      "At iterate    9    f=  9.90042D-02    |proj g|=  3.63201D-03\n",
      "\n",
      "At iterate   10    f=  7.87427D-02    |proj g|=  7.98265D-03\n",
      "\n",
      "At iterate   11    f=  7.05650D-02    |proj g|=  6.76329D-03\n",
      "\n",
      "At iterate   12    f=  6.97345D-02    |proj g|=  3.46707D-03\n",
      "\n",
      "At iterate   13    f=  6.50046D-02    |proj g|=  3.06581D-03\n",
      "\n",
      "At iterate   14    f=  6.06381D-02    |proj g|=  7.28847D-03\n",
      "\n",
      "At iterate   15    f=  5.50898D-02    |proj g|=  4.18337D-03\n",
      "\n",
      "At iterate   16    f=  5.05136D-02    |proj g|=  1.70619D-03\n",
      "\n",
      "At iterate   17    f=  4.75516D-02    |proj g|=  2.73888D-03\n",
      "\n",
      "At iterate   18    f=  4.26294D-02    |proj g|=  2.19351D-03\n",
      "\n",
      "At iterate   19    f=  3.57343D-02    |proj g|=  1.22923D-03\n",
      "\n",
      "At iterate   20    f=  2.74794D-02    |proj g|=  1.42254D-03\n",
      "\n",
      "At iterate   21    f=  2.02325D-02    |proj g|=  2.81225D-03\n",
      "\n",
      "At iterate   22    f=  1.51714D-02    |proj g|=  1.57514D-03\n",
      "\n",
      "At iterate   23    f=  1.50079D-02    |proj g|=  5.38475D-03\n",
      "\n",
      "At iterate   24    f=  1.25422D-02    |proj g|=  2.30881D-03\n",
      "\n",
      "At iterate   25    f=  1.10673D-02    |proj g|=  9.30744D-04\n",
      "\n",
      "At iterate   26    f=  9.75091D-03    |proj g|=  2.16370D-03\n",
      "\n",
      "At iterate   27    f=  7.75380D-03    |proj g|=  1.87679D-03\n",
      "\n",
      "At iterate   28    f=  4.04767D-03    |proj g|=  2.66943D-03\n",
      "\n",
      "At iterate   29    f=  2.07711D-03    |proj g|=  1.38061D-03\n",
      "\n",
      "At iterate   30    f=  1.01151D-03    |proj g|=  6.61418D-04\n",
      "\n",
      "At iterate   31    f=  5.11028D-04    |proj g|=  3.24811D-04\n",
      "\n",
      "At iterate   32    f=  2.56688D-04    |proj g|=  1.57129D-04\n",
      "\n",
      "At iterate   33    f=  1.29745D-04    |proj g|=  7.58275D-05\n",
      "\n",
      "At iterate   34    f=  6.55269D-05    |proj g|=  3.63042D-05\n",
      "\n",
      "At iterate   35    f=  3.30791D-05    |proj g|=  1.74969D-05\n",
      "\n",
      "At iterate   36    f=  1.66818D-05    |proj g|=  8.58706D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     36     41      1     0     0   8.587D-06   1.668D-05\n",
      "  F =   1.6681793865388156E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " 90%|█████████ | 9/10 [00:00<00:00, 17.07it/s] This problem is unconstrained.\n",
      "100%|██████████| 10/10 [00:00<00:00, 14.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  7.05583D-02\n",
      "\n",
      "At iterate    1    f=  6.84920D-01    |proj g|=  6.18970D-02\n",
      "\n",
      "At iterate    2    f=  5.78216D-01    |proj g|=  8.01141D-02\n",
      "\n",
      "At iterate    3    f=  3.81236D-01    |proj g|=  3.69320D-02\n",
      "\n",
      "At iterate    4    f=  3.06412D-01    |proj g|=  1.87592D-02\n",
      "\n",
      "At iterate    5    f=  2.38636D-01    |proj g|=  1.11073D-02\n",
      "\n",
      "At iterate    6    f=  1.81038D-01    |proj g|=  7.98169D-03\n",
      "\n",
      "At iterate    7    f=  1.16044D-01    |proj g|=  6.08149D-03\n",
      "\n",
      "At iterate    8    f=  7.14027D-02    |proj g|=  5.34831D-03\n",
      "\n",
      "At iterate    9    f=  5.12117D-02    |proj g|=  3.27411D-03\n",
      "\n",
      "At iterate   10    f=  2.72868D-02    |proj g|=  2.44182D-03\n",
      "\n",
      "At iterate   11    f=  1.64364D-02    |proj g|=  1.45271D-03\n",
      "\n",
      "At iterate   12    f=  9.97641D-03    |proj g|=  4.53301D-03\n",
      "\n",
      "At iterate   13    f=  3.59548D-03    |proj g|=  8.56044D-04\n",
      "\n",
      "At iterate   14    f=  2.49388D-03    |proj g|=  5.32306D-04\n",
      "\n",
      "At iterate   15    f=  1.23478D-03    |proj g|=  2.31926D-04\n",
      "\n",
      "At iterate   16    f=  6.93285D-04    |proj g|=  1.25604D-04\n",
      "\n",
      "At iterate   17    f=  3.64134D-04    |proj g|=  6.25200D-05\n",
      "\n",
      "At iterate   18    f=  1.91907D-04    |proj g|=  3.50486D-05\n",
      "\n",
      "At iterate   19    f=  9.88680D-05    |proj g|=  1.80411D-05\n",
      "\n",
      "At iterate   20    f=  5.05700D-05    |proj g|=  9.70197D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     20     24      1     0     0   9.702D-06   5.057D-05\n",
      "  F =   5.0569987739632913E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.48750D-01\n",
      "\n",
      "At iterate    1    f=  6.48183D-01    |proj g|=  4.21646D-02\n",
      "\n",
      "At iterate    2    f=  5.55280D-01    |proj g|=  3.55543D-02\n",
      "\n",
      "At iterate    3    f=  2.71903D-01    |proj g|=  3.11158D-02\n",
      "\n",
      "At iterate    4    f=  2.01917D-01    |proj g|=  2.27036D-02\n",
      "\n",
      "At iterate    5    f=  1.38264D-01    |proj g|=  1.29359D-02\n",
      "\n",
      "At iterate    6    f=  9.64348D-02    |proj g|=  5.90897D-03\n",
      "\n",
      "At iterate    7    f=  6.13298D-02    |proj g|=  3.11766D-03\n",
      "\n",
      "At iterate    8    f=  3.43867D-02    |proj g|=  4.77301D-03\n",
      "\n",
      "At iterate    9    f=  1.61530D-02    |proj g|=  2.27634D-03\n",
      "\n",
      "At iterate   10    f=  7.86964D-03    |proj g|=  1.35184D-03\n",
      "\n",
      "At iterate   11    f=  3.86722D-03    |proj g|=  4.51630D-04\n",
      "\n",
      "At iterate   12    f=  3.31370D-03    |proj g|=  1.83431D-03\n",
      "\n",
      "At iterate   13    f=  1.91779D-03    |proj g|=  8.42888D-04\n",
      "\n",
      "At iterate   14    f=  9.69549D-04    |proj g|=  3.77127D-04\n",
      "\n",
      "At iterate   15    f=  5.01933D-04    |proj g|=  1.81564D-04\n",
      "\n",
      "At iterate   16    f=  2.55323D-04    |proj g|=  8.72760D-05\n",
      "\n",
      "At iterate   17    f=  1.29863D-04    |proj g|=  4.20298D-05\n",
      "\n",
      "At iterate   18    f=  6.56138D-05    |proj g|=  1.98817D-05\n",
      "\n",
      "At iterate   19    f=  3.30739D-05    |proj g|=  9.23156D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     19     24      1     0     0   9.232D-06   3.307D-05\n",
      "  F =   3.3073870492182710E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 5531.14it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 542.74it/s]\n",
      "100%|██████████| 13/13 [00:00<00:00, 5305.11it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 614.39it/s]\n",
      "x does not have column names.  The check that columns are provided in the same order when training and predicting will be skipped\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 (73,)\n",
      "# of CELL:13 / min size:10 / avg size:23.8 / max size:46 / # of singleton CELL:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of clf: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1/13 [00:00<00:01,  7.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: 0 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9d3a9560>, dataframe=<capsule object NULL at 0x152d9c3781e0>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7    8   \\\n",
      "0    0.546667  0.491524  1.0  0.067961  0.781319  0.0  1.0  0.263158  0.0   \n",
      "1    0.506667  0.247766  1.0  0.825243  0.071979  0.0  0.0  0.236842  0.0   \n",
      "2    0.280000  0.075240  1.0  0.485437  0.663852  0.0  0.0  0.263158  0.0   \n",
      "3    1.000000  0.008768  1.0  0.990291  0.183852  0.0  0.0  0.184211  1.0   \n",
      "4    0.626667  0.104468  1.0  0.000000  0.575198  0.0  0.0  0.421053  0.0   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...  ...   \n",
      "305  0.120000  0.194572  1.0  0.077670  0.132559  0.0  0.0  0.921053  0.0   \n",
      "306  0.746667  0.015699  1.0  0.864078  0.418364  0.0  0.0  0.921053  0.0   \n",
      "307  0.853333  0.191148  1.0  0.097087  0.773720  0.0  0.0  1.000000  0.0   \n",
      "308  0.400000  0.237662  1.0  0.456311  0.101636  0.0  1.0  0.552632  0.0   \n",
      "309  0.800000  0.248685  1.0  0.097087  0.061214  0.0  0.0  0.921053  0.0   \n",
      "\n",
      "           9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.294118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1    0.264706  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2    0.294118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.235294  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.382353  ...  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  \n",
      "..        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "306  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "307  1.000000  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "308  0.500000  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
      "       1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0.,\n",
      "       0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
      "       0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
      "       1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1.,\n",
      "       1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "       1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "       0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
      "       1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
      "       1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "       1., 0., 0., 0.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n",
      "cluster: 1 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9d3a9560>, dataframe=<capsule object NULL at 0x152d9c3781e0>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7    8   \\\n",
      "0    0.546667  0.491524  1.0  0.067961  0.781319  0.0  1.0  0.263158  0.0   \n",
      "1    0.506667  0.247766  1.0  0.825243  0.071979  0.0  0.0  0.236842  0.0   \n",
      "2    0.280000  0.075240  1.0  0.485437  0.663852  0.0  0.0  0.263158  0.0   \n",
      "3    1.000000  0.008768  1.0  0.990291  0.183852  0.0  0.0  0.184211  1.0   \n",
      "4    0.626667  0.104468  1.0  0.000000  0.575198  0.0  0.0  0.421053  0.0   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...  ...   \n",
      "305  0.120000  0.194572  1.0  0.077670  0.132559  0.0  0.0  0.921053  0.0   \n",
      "306  0.746667  0.015699  1.0  0.864078  0.418364  0.0  0.0  0.921053  0.0   \n",
      "307  0.853333  0.191148  1.0  0.097087  0.773720  0.0  0.0  1.000000  0.0   \n",
      "308  0.400000  0.237662  1.0  0.456311  0.101636  0.0  1.0  0.552632  0.0   \n",
      "309  0.800000  0.248685  1.0  0.097087  0.061214  0.0  0.0  0.921053  0.0   \n",
      "\n",
      "           9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.294118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1    0.264706  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2    0.294118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.235294  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.382353  ...  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  \n",
      "..        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "306  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "307  1.000000  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "308  0.500000  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
      "       1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0.,\n",
      "       0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
      "       0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
      "       1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1.,\n",
      "       1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "       1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "       0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
      "       1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
      "       1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "       1., 0., 0., 0.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 3/13 [00:00<00:01,  7.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: 2 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9d3a9560>, dataframe=<capsule object NULL at 0x152d9c3781e0>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7    8   \\\n",
      "0    0.546667  0.491524  1.0  0.067961  0.781319  0.0  1.0  0.263158  0.0   \n",
      "1    0.506667  0.247766  1.0  0.825243  0.071979  0.0  0.0  0.236842  0.0   \n",
      "2    0.280000  0.075240  1.0  0.485437  0.663852  0.0  0.0  0.263158  0.0   \n",
      "3    1.000000  0.008768  1.0  0.990291  0.183852  0.0  0.0  0.184211  1.0   \n",
      "4    0.626667  0.104468  1.0  0.000000  0.575198  0.0  0.0  0.421053  0.0   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...  ...   \n",
      "305  0.120000  0.194572  1.0  0.077670  0.132559  0.0  0.0  0.921053  0.0   \n",
      "306  0.746667  0.015699  1.0  0.864078  0.418364  0.0  0.0  0.921053  0.0   \n",
      "307  0.853333  0.191148  1.0  0.097087  0.773720  0.0  0.0  1.000000  0.0   \n",
      "308  0.400000  0.237662  1.0  0.456311  0.101636  0.0  1.0  0.552632  0.0   \n",
      "309  0.800000  0.248685  1.0  0.097087  0.061214  0.0  0.0  0.921053  0.0   \n",
      "\n",
      "           9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.294118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1    0.264706  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2    0.294118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.235294  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.382353  ...  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  \n",
      "..        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "306  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "307  1.000000  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "308  0.500000  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
      "       1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0.,\n",
      "       0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
      "       0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
      "       1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1.,\n",
      "       1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "       1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "       0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
      "       1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
      "       1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "       1., 0., 0., 0.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n",
      "cluster: 3 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9d3a9560>, dataframe=<capsule object NULL at 0x152d9c3781e0>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7    8   \\\n",
      "0    0.546667  0.491524  1.0  0.067961  0.781319  0.0  1.0  0.263158  0.0   \n",
      "1    0.506667  0.247766  1.0  0.825243  0.071979  0.0  0.0  0.236842  0.0   \n",
      "2    0.280000  0.075240  1.0  0.485437  0.663852  0.0  0.0  0.263158  0.0   \n",
      "3    1.000000  0.008768  1.0  0.990291  0.183852  0.0  0.0  0.184211  1.0   \n",
      "4    0.626667  0.104468  1.0  0.000000  0.575198  0.0  0.0  0.421053  0.0   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...  ...   \n",
      "305  0.120000  0.194572  1.0  0.077670  0.132559  0.0  0.0  0.921053  0.0   \n",
      "306  0.746667  0.015699  1.0  0.864078  0.418364  0.0  0.0  0.921053  0.0   \n",
      "307  0.853333  0.191148  1.0  0.097087  0.773720  0.0  0.0  1.000000  0.0   \n",
      "308  0.400000  0.237662  1.0  0.456311  0.101636  0.0  1.0  0.552632  0.0   \n",
      "309  0.800000  0.248685  1.0  0.097087  0.061214  0.0  0.0  0.921053  0.0   \n",
      "\n",
      "           9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.294118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1    0.264706  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2    0.294118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.235294  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.382353  ...  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  \n",
      "..        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "306  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "307  1.000000  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "308  0.500000  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
      "       1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0.,\n",
      "       0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
      "       0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
      "       1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1.,\n",
      "       1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "       1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "       0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
      "       1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
      "       1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "       1., 0., 0., 0.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 5/13 [00:00<00:01,  7.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: 4 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9d3a9560>, dataframe=<capsule object NULL at 0x152d9c3781e0>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7    8   \\\n",
      "0    0.546667  0.491524  1.0  0.067961  0.781319  0.0  1.0  0.263158  0.0   \n",
      "1    0.506667  0.247766  1.0  0.825243  0.071979  0.0  0.0  0.236842  0.0   \n",
      "2    0.280000  0.075240  1.0  0.485437  0.663852  0.0  0.0  0.263158  0.0   \n",
      "3    1.000000  0.008768  1.0  0.990291  0.183852  0.0  0.0  0.184211  1.0   \n",
      "4    0.626667  0.104468  1.0  0.000000  0.575198  0.0  0.0  0.421053  0.0   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...  ...   \n",
      "305  0.120000  0.194572  1.0  0.077670  0.132559  0.0  0.0  0.921053  0.0   \n",
      "306  0.746667  0.015699  1.0  0.864078  0.418364  0.0  0.0  0.921053  0.0   \n",
      "307  0.853333  0.191148  1.0  0.097087  0.773720  0.0  0.0  1.000000  0.0   \n",
      "308  0.400000  0.237662  1.0  0.456311  0.101636  0.0  1.0  0.552632  0.0   \n",
      "309  0.800000  0.248685  1.0  0.097087  0.061214  0.0  0.0  0.921053  0.0   \n",
      "\n",
      "           9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.294118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1    0.264706  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2    0.294118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.235294  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.382353  ...  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  \n",
      "..        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "306  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "307  1.000000  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "308  0.500000  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
      "       1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0.,\n",
      "       0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
      "       0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
      "       1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1.,\n",
      "       1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "       1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "       0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
      "       1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
      "       1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "       1., 0., 0., 0.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n",
      "cluster: 5 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9d3a9560>, dataframe=<capsule object NULL at 0x152d9c3781e0>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7    8   \\\n",
      "0    0.546667  0.491524  1.0  0.067961  0.781319  0.0  1.0  0.263158  0.0   \n",
      "1    0.506667  0.247766  1.0  0.825243  0.071979  0.0  0.0  0.236842  0.0   \n",
      "2    0.280000  0.075240  1.0  0.485437  0.663852  0.0  0.0  0.263158  0.0   \n",
      "3    1.000000  0.008768  1.0  0.990291  0.183852  0.0  0.0  0.184211  1.0   \n",
      "4    0.626667  0.104468  1.0  0.000000  0.575198  0.0  0.0  0.421053  0.0   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...  ...   \n",
      "305  0.120000  0.194572  1.0  0.077670  0.132559  0.0  0.0  0.921053  0.0   \n",
      "306  0.746667  0.015699  1.0  0.864078  0.418364  0.0  0.0  0.921053  0.0   \n",
      "307  0.853333  0.191148  1.0  0.097087  0.773720  0.0  0.0  1.000000  0.0   \n",
      "308  0.400000  0.237662  1.0  0.456311  0.101636  0.0  1.0  0.552632  0.0   \n",
      "309  0.800000  0.248685  1.0  0.097087  0.061214  0.0  0.0  0.921053  0.0   \n",
      "\n",
      "           9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.294118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1    0.264706  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2    0.294118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.235294  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.382353  ...  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  \n",
      "..        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "306  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "307  1.000000  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "308  0.500000  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
      "       1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0.,\n",
      "       0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
      "       0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
      "       1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1.,\n",
      "       1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "       1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "       0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
      "       1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
      "       1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "       1., 0., 0., 0.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 7/13 [00:00<00:00,  7.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: 6 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9d3a9560>, dataframe=<capsule object NULL at 0x152d9c3781e0>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7    8   \\\n",
      "0    0.546667  0.491524  1.0  0.067961  0.781319  0.0  1.0  0.263158  0.0   \n",
      "1    0.506667  0.247766  1.0  0.825243  0.071979  0.0  0.0  0.236842  0.0   \n",
      "2    0.280000  0.075240  1.0  0.485437  0.663852  0.0  0.0  0.263158  0.0   \n",
      "3    1.000000  0.008768  1.0  0.990291  0.183852  0.0  0.0  0.184211  1.0   \n",
      "4    0.626667  0.104468  1.0  0.000000  0.575198  0.0  0.0  0.421053  0.0   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...  ...   \n",
      "305  0.120000  0.194572  1.0  0.077670  0.132559  0.0  0.0  0.921053  0.0   \n",
      "306  0.746667  0.015699  1.0  0.864078  0.418364  0.0  0.0  0.921053  0.0   \n",
      "307  0.853333  0.191148  1.0  0.097087  0.773720  0.0  0.0  1.000000  0.0   \n",
      "308  0.400000  0.237662  1.0  0.456311  0.101636  0.0  1.0  0.552632  0.0   \n",
      "309  0.800000  0.248685  1.0  0.097087  0.061214  0.0  0.0  0.921053  0.0   \n",
      "\n",
      "           9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.294118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1    0.264706  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2    0.294118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.235294  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.382353  ...  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  \n",
      "..        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "306  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "307  1.000000  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "308  0.500000  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
      "       1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0.,\n",
      "       0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
      "       0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
      "       1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1.,\n",
      "       1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "       1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "       0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
      "       1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
      "       1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "       1., 0., 0., 0.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n",
      "cluster: 7 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9d3a9560>, dataframe=<capsule object NULL at 0x152d9c3781e0>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7    8   \\\n",
      "0    0.546667  0.491524  1.0  0.067961  0.781319  0.0  1.0  0.263158  0.0   \n",
      "1    0.506667  0.247766  1.0  0.825243  0.071979  0.0  0.0  0.236842  0.0   \n",
      "2    0.280000  0.075240  1.0  0.485437  0.663852  0.0  0.0  0.263158  0.0   \n",
      "3    1.000000  0.008768  1.0  0.990291  0.183852  0.0  0.0  0.184211  1.0   \n",
      "4    0.626667  0.104468  1.0  0.000000  0.575198  0.0  0.0  0.421053  0.0   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...  ...   \n",
      "305  0.120000  0.194572  1.0  0.077670  0.132559  0.0  0.0  0.921053  0.0   \n",
      "306  0.746667  0.015699  1.0  0.864078  0.418364  0.0  0.0  0.921053  0.0   \n",
      "307  0.853333  0.191148  1.0  0.097087  0.773720  0.0  0.0  1.000000  0.0   \n",
      "308  0.400000  0.237662  1.0  0.456311  0.101636  0.0  1.0  0.552632  0.0   \n",
      "309  0.800000  0.248685  1.0  0.097087  0.061214  0.0  0.0  0.921053  0.0   \n",
      "\n",
      "           9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.294118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1    0.264706  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2    0.294118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.235294  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.382353  ...  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  \n",
      "..        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "306  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "307  1.000000  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "308  0.500000  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
      "       1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0.,\n",
      "       0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
      "       0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
      "       1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1.,\n",
      "       1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "       1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "       0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
      "       1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
      "       1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "       1., 0., 0., 0.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 9/13 [00:01<00:00,  7.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: 8 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9d3a9560>, dataframe=<capsule object NULL at 0x152d9c3781e0>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7    8   \\\n",
      "0    0.546667  0.491524  1.0  0.067961  0.781319  0.0  1.0  0.263158  0.0   \n",
      "1    0.506667  0.247766  1.0  0.825243  0.071979  0.0  0.0  0.236842  0.0   \n",
      "2    0.280000  0.075240  1.0  0.485437  0.663852  0.0  0.0  0.263158  0.0   \n",
      "3    1.000000  0.008768  1.0  0.990291  0.183852  0.0  0.0  0.184211  1.0   \n",
      "4    0.626667  0.104468  1.0  0.000000  0.575198  0.0  0.0  0.421053  0.0   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...  ...   \n",
      "305  0.120000  0.194572  1.0  0.077670  0.132559  0.0  0.0  0.921053  0.0   \n",
      "306  0.746667  0.015699  1.0  0.864078  0.418364  0.0  0.0  0.921053  0.0   \n",
      "307  0.853333  0.191148  1.0  0.097087  0.773720  0.0  0.0  1.000000  0.0   \n",
      "308  0.400000  0.237662  1.0  0.456311  0.101636  0.0  1.0  0.552632  0.0   \n",
      "309  0.800000  0.248685  1.0  0.097087  0.061214  0.0  0.0  0.921053  0.0   \n",
      "\n",
      "           9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.294118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1    0.264706  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2    0.294118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.235294  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.382353  ...  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  \n",
      "..        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "306  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "307  1.000000  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "308  0.500000  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
      "       1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0.,\n",
      "       0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
      "       0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
      "       1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1.,\n",
      "       1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "       1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "       0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
      "       1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
      "       1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "       1., 0., 0., 0.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n",
      "cluster: 9 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9d3a9560>, dataframe=<capsule object NULL at 0x152d9c3781e0>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7    8   \\\n",
      "0    0.546667  0.491524  1.0  0.067961  0.781319  0.0  1.0  0.263158  0.0   \n",
      "1    0.506667  0.247766  1.0  0.825243  0.071979  0.0  0.0  0.236842  0.0   \n",
      "2    0.280000  0.075240  1.0  0.485437  0.663852  0.0  0.0  0.263158  0.0   \n",
      "3    1.000000  0.008768  1.0  0.990291  0.183852  0.0  0.0  0.184211  1.0   \n",
      "4    0.626667  0.104468  1.0  0.000000  0.575198  0.0  0.0  0.421053  0.0   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...  ...   \n",
      "305  0.120000  0.194572  1.0  0.077670  0.132559  0.0  0.0  0.921053  0.0   \n",
      "306  0.746667  0.015699  1.0  0.864078  0.418364  0.0  0.0  0.921053  0.0   \n",
      "307  0.853333  0.191148  1.0  0.097087  0.773720  0.0  0.0  1.000000  0.0   \n",
      "308  0.400000  0.237662  1.0  0.456311  0.101636  0.0  1.0  0.552632  0.0   \n",
      "309  0.800000  0.248685  1.0  0.097087  0.061214  0.0  0.0  0.921053  0.0   \n",
      "\n",
      "           9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.294118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1    0.264706  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2    0.294118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.235294  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.382353  ...  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  \n",
      "..        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "306  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "307  1.000000  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "308  0.500000  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
      "       1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0.,\n",
      "       0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
      "       0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
      "       1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1.,\n",
      "       1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "       1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "       0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
      "       1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
      "       1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "       1., 0., 0., 0.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 11/13 [00:01<00:00,  7.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: 10 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9d3a9560>, dataframe=<capsule object NULL at 0x152d9c3781e0>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7    8   \\\n",
      "0    0.546667  0.491524  1.0  0.067961  0.781319  0.0  1.0  0.263158  0.0   \n",
      "1    0.506667  0.247766  1.0  0.825243  0.071979  0.0  0.0  0.236842  0.0   \n",
      "2    0.280000  0.075240  1.0  0.485437  0.663852  0.0  0.0  0.263158  0.0   \n",
      "3    1.000000  0.008768  1.0  0.990291  0.183852  0.0  0.0  0.184211  1.0   \n",
      "4    0.626667  0.104468  1.0  0.000000  0.575198  0.0  0.0  0.421053  0.0   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...  ...   \n",
      "305  0.120000  0.194572  1.0  0.077670  0.132559  0.0  0.0  0.921053  0.0   \n",
      "306  0.746667  0.015699  1.0  0.864078  0.418364  0.0  0.0  0.921053  0.0   \n",
      "307  0.853333  0.191148  1.0  0.097087  0.773720  0.0  0.0  1.000000  0.0   \n",
      "308  0.400000  0.237662  1.0  0.456311  0.101636  0.0  1.0  0.552632  0.0   \n",
      "309  0.800000  0.248685  1.0  0.097087  0.061214  0.0  0.0  0.921053  0.0   \n",
      "\n",
      "           9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.294118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1    0.264706  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2    0.294118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.235294  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.382353  ...  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  \n",
      "..        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "306  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "307  1.000000  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "308  0.500000  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
      "       1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0.,\n",
      "       0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
      "       0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
      "       1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1.,\n",
      "       1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "       1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "       0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
      "       1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
      "       1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "       1., 0., 0., 0.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n",
      "cluster: 11 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9d3a9560>, dataframe=<capsule object NULL at 0x152d9c3781e0>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7    8   \\\n",
      "0    0.546667  0.491524  1.0  0.067961  0.781319  0.0  1.0  0.263158  0.0   \n",
      "1    0.506667  0.247766  1.0  0.825243  0.071979  0.0  0.0  0.236842  0.0   \n",
      "2    0.280000  0.075240  1.0  0.485437  0.663852  0.0  0.0  0.263158  0.0   \n",
      "3    1.000000  0.008768  1.0  0.990291  0.183852  0.0  0.0  0.184211  1.0   \n",
      "4    0.626667  0.104468  1.0  0.000000  0.575198  0.0  0.0  0.421053  0.0   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...  ...   \n",
      "305  0.120000  0.194572  1.0  0.077670  0.132559  0.0  0.0  0.921053  0.0   \n",
      "306  0.746667  0.015699  1.0  0.864078  0.418364  0.0  0.0  0.921053  0.0   \n",
      "307  0.853333  0.191148  1.0  0.097087  0.773720  0.0  0.0  1.000000  0.0   \n",
      "308  0.400000  0.237662  1.0  0.456311  0.101636  0.0  1.0  0.552632  0.0   \n",
      "309  0.800000  0.248685  1.0  0.097087  0.061214  0.0  0.0  0.921053  0.0   \n",
      "\n",
      "           9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.294118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1    0.264706  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2    0.294118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.235294  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.382353  ...  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  \n",
      "..        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "306  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "307  1.000000  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "308  0.500000  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
      "       1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0.,\n",
      "       0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
      "       0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
      "       1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1.,\n",
      "       1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "       1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "       0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
      "       1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
      "       1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "       1., 0., 0., 0.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00,  7.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: 12 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9d3a9560>, dataframe=<capsule object NULL at 0x152d9c3781e0>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7    8   \\\n",
      "0    0.546667  0.491524  1.0  0.067961  0.781319  0.0  1.0  0.263158  0.0   \n",
      "1    0.506667  0.247766  1.0  0.825243  0.071979  0.0  0.0  0.236842  0.0   \n",
      "2    0.280000  0.075240  1.0  0.485437  0.663852  0.0  0.0  0.263158  0.0   \n",
      "3    1.000000  0.008768  1.0  0.990291  0.183852  0.0  0.0  0.184211  1.0   \n",
      "4    0.626667  0.104468  1.0  0.000000  0.575198  0.0  0.0  0.421053  0.0   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...  ...   \n",
      "305  0.120000  0.194572  1.0  0.077670  0.132559  0.0  0.0  0.921053  0.0   \n",
      "306  0.746667  0.015699  1.0  0.864078  0.418364  0.0  0.0  0.921053  0.0   \n",
      "307  0.853333  0.191148  1.0  0.097087  0.773720  0.0  0.0  1.000000  0.0   \n",
      "308  0.400000  0.237662  1.0  0.456311  0.101636  0.0  1.0  0.552632  0.0   \n",
      "309  0.800000  0.248685  1.0  0.097087  0.061214  0.0  0.0  0.921053  0.0   \n",
      "\n",
      "           9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.294118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1    0.264706  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2    0.294118  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.235294  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.382353  ...  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  \n",
      "..        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "306  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "307  1.000000  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "308  0.500000  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
      "       1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0.,\n",
      "       0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
      "       0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1.,\n",
      "       1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "       1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1.,\n",
      "       1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
      "       1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "       0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
      "       1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
      "       1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "       1., 0., 0., 0.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13 [00:00<?, ?it/s] This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  4.76344D-02\n",
      "\n",
      "At iterate    1    f=  6.86887D-01    |proj g|=  4.90764D-02\n",
      "\n",
      "At iterate    2    f=  5.92635D-01    |proj g|=  7.41949D-02\n",
      "\n",
      "At iterate    3    f=  4.45178D-01    |proj g|=  3.66769D-02\n",
      "\n",
      "At iterate    4    f=  3.96383D-01    |proj g|=  2.58546D-02\n",
      "\n",
      "At iterate    5    f=  3.52365D-01    |proj g|=  1.78500D-02\n",
      "\n",
      "At iterate    6    f=  3.13573D-01    |proj g|=  1.80467D-02\n",
      "\n",
      "At iterate    7    f=  2.42832D-01    |proj g|=  1.14937D-02\n",
      "\n",
      "At iterate    8    f=  1.66609D-01    |proj g|=  1.32331D-02\n",
      "\n",
      "At iterate    9    f=  1.23660D-01    |proj g|=  7.97705D-03\n",
      "\n",
      "At iterate   10    f=  9.01199D-02    |proj g|=  2.19822D-03\n",
      "\n",
      "At iterate   11    f=  7.03397D-02    |proj g|=  5.06287D-03\n",
      "\n",
      "At iterate   12    f=  6.59922D-02    |proj g|=  1.23912D-02\n",
      "\n",
      "At iterate   13    f=  5.49529D-02    |proj g|=  4.09265D-03\n",
      "\n",
      "At iterate   14    f=  4.95120D-02    |proj g|=  2.91516D-03\n",
      "\n",
      "At iterate   15    f=  4.40673D-02    |proj g|=  3.61740D-03\n",
      "\n",
      "At iterate   16    f=  3.56581D-02    |proj g|=  3.63199D-03\n",
      "\n",
      "At iterate   17    f=  2.39285D-02    |proj g|=  2.34099D-03\n",
      "\n",
      "At iterate   18    f=  8.73234D-03    |proj g|=  1.88211D-03\n",
      "\n",
      "At iterate   19    f=  4.05869D-03    |proj g|=  9.14756D-04\n",
      "\n",
      "At iterate   20    f=  1.85691D-03    |proj g|=  8.72128D-04\n",
      "\n",
      "At iterate   21    f=  1.37324D-03    |proj g|=  8.36354D-04\n",
      "\n",
      "At iterate   22    f=  7.31728D-04    |proj g|=  2.97819D-04\n",
      "\n",
      "At iterate   23    f=  4.36781D-04    |proj g|=  1.12026D-04\n",
      "\n",
      "At iterate   24    f=  2.45167D-04    |proj g|=  4.41781D-05\n",
      "\n",
      "At iterate   25    f=  1.41674D-04    |proj g|=  7.27415D-05\n",
      "\n",
      "At iterate   26    f=  5.23684D-05    |proj g|=  1.51644D-05\n",
      "\n",
      "At iterate   27    f=  3.36063D-05    |proj g|=  1.14692D-05\n",
      "\n",
      "At iterate   28    f=  1.55758D-05    |proj g|=  6.30105D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     28     33      1     0     0   6.301D-06   1.558D-05\n",
      "  F =   1.5575779194516725E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  2.78005D-01\n",
      "\n",
      "At iterate    1    f=  5.52844D-01    |proj g|=  2.93757D-02\n",
      "\n",
      "At iterate    2    f=  5.33443D-01    |proj g|=  3.57351D-02\n",
      "\n",
      "At iterate    3    f=  4.14572D-01    |proj g|=  1.35337D-01\n",
      "\n",
      "At iterate    4    f=  3.47983D-01    |proj g|=  1.09861D-01\n",
      "\n",
      "At iterate    5    f=  2.57476D-01    |proj g|=  4.96926D-02\n",
      "\n",
      "At iterate    6    f=  2.16337D-01    |proj g|=  2.18168D-02\n",
      "\n",
      "At iterate    7    f=  1.88038D-01    |proj g|=  1.30817D-02\n",
      "\n",
      "At iterate    8    f=  1.64056D-01    |proj g|=  1.39508D-02\n",
      "\n",
      "At iterate    9    f=  1.28962D-01    |proj g|=  1.38189D-02\n",
      "\n",
      "At iterate   10    f=  6.08769D-02    |proj g|=  3.62486D-03\n",
      "\n",
      "At iterate   11    f=  3.62414D-02    |proj g|=  5.01406D-03\n",
      "\n",
      "At iterate   12    f=  1.91755D-02    |proj g|=  2.30092D-03\n",
      "\n",
      "At iterate   13    f=  1.01277D-02    |proj g|=  5.61331D-03\n",
      "\n",
      "At iterate   14    f=  3.04851D-03    |proj g|=  1.90672D-03\n",
      "\n",
      "At iterate   15    f=  1.97919D-03    |proj g|=  9.99643D-04\n",
      "\n",
      "At iterate   16    f=  1.03274D-03    |proj g|=  3.59437D-04\n",
      "\n",
      "At iterate   17    f=  5.78346D-04    |proj g|=  1.29129D-04\n",
      "\n",
      "At iterate   18    f=  3.08977D-04    |proj g|=  4.01298D-05\n",
      "\n",
      "At iterate   19    f=  1.64673D-04    |proj g|=  2.66768D-05\n",
      "\n",
      "At iterate   20    f=  8.59863D-05    |proj g|=  1.67349D-05\n",
      "\n",
      "At iterate   21    f=  4.77826D-05    |proj g|=  2.45677D-05\n",
      "\n",
      "At iterate   22    f=  2.14293D-05    |proj g|=  9.85743D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     22     25      1     0     0   9.857D-06   2.143D-05\n",
      "  F =   2.1429338218305343E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  2.31718D-01\n",
      "\n",
      "At iterate    1    f=  5.84794D-01    |proj g|=  4.14755D-02\n",
      "\n",
      "At iterate    2    f=  5.62912D-01    |proj g|=  4.00366D-02\n",
      "\n",
      "At iterate    3    f=  4.21136D-01    |proj g|=  1.25857D-01\n",
      "\n",
      "At iterate    4    f=  3.48915D-01    |proj g|=  1.00257D-01\n",
      "\n",
      "At iterate    5    f=  2.71695D-01    |proj g|=  4.85258D-02\n",
      "\n",
      "At iterate    6    f=  2.38849D-01    |proj g|=  2.15002D-02\n",
      "\n",
      "At iterate    7    f=  2.19631D-01    |proj g|=  1.74626D-02\n",
      "\n",
      "At iterate    8    f=  2.05102D-01    |proj g|=  1.99803D-02\n",
      "\n",
      "At iterate    9    f=  1.83802D-01    |proj g|=  2.00583D-02\n",
      "\n",
      "At iterate   10    f=  1.36264D-01    |proj g|=  1.34606D-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/discrete/discrete_model.py:1819: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " 23%|██▎       | 3/13 [00:00<00:00, 28.76it/s] This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   11    f=  9.86543D-02    |proj g|=  3.74566D-03\n",
      "\n",
      "At iterate   12    f=  8.87158D-02    |proj g|=  4.35396D-03\n",
      "\n",
      "At iterate   13    f=  8.64380D-02    |proj g|=  9.97296D-03\n",
      "\n",
      "At iterate   14    f=  7.84940D-02    |proj g|=  4.82423D-03\n",
      "\n",
      "At iterate   15    f=  7.27542D-02    |proj g|=  3.90231D-03\n",
      "\n",
      "At iterate   16    f=  6.22310D-02    |proj g|=  3.96031D-03\n",
      "\n",
      "At iterate   17    f=  4.92238D-02    |proj g|=  5.85414D-03\n",
      "\n",
      "At iterate   18    f=  4.49216D-02    |proj g|=  3.05698D-03\n",
      "\n",
      "At iterate   19    f=  4.37672D-02    |proj g|=  1.96230D-03\n",
      "\n",
      "At iterate   20    f=  4.27462D-02    |proj g|=  1.53739D-03\n",
      "\n",
      "At iterate   21    f=  3.98732D-02    |proj g|=  1.10382D-03\n",
      "\n",
      "At iterate   22    f=  3.67862D-02    |proj g|=  1.69493D-03\n",
      "\n",
      "At iterate   23    f=  3.35367D-02    |proj g|=  2.01251D-03\n",
      "\n",
      "At iterate   24    f=  3.31543D-02    |proj g|=  1.77045D-03\n",
      "\n",
      "At iterate   25    f=  3.11355D-02    |proj g|=  1.29874D-03\n",
      "\n",
      "At iterate   26    f=  3.01451D-02    |proj g|=  4.51254D-04\n",
      "\n",
      "At iterate   27    f=  2.95923D-02    |proj g|=  3.89950D-04\n",
      "\n",
      "At iterate   28    f=  2.84412D-02    |proj g|=  6.03559D-04\n",
      "\n",
      "At iterate   29    f=  2.52693D-02    |proj g|=  9.11159D-04\n",
      "\n",
      "At iterate   30    f=  2.45238D-02    |proj g|=  1.15803D-03\n",
      "\n",
      "At iterate   31    f=  1.59145D-02    |proj g|=  7.96483D-04\n",
      "\n",
      "At iterate   32    f=  6.30487D-03    |proj g|=  1.69257D-03\n",
      "\n",
      "At iterate   33    f=  2.42215D-03    |proj g|=  5.32920D-04\n",
      "\n",
      "At iterate   34    f=  1.29996D-03    |proj g|=  2.08307D-04\n",
      "\n",
      "At iterate   35    f=  7.44180D-04    |proj g|=  5.10586D-04\n",
      "\n",
      "At iterate   36    f=  1.33346D-04    |proj g|=  9.26307D-05\n",
      "\n",
      "At iterate   37    f=  9.79667D-05    |proj g|=  6.28216D-05\n",
      "\n",
      "At iterate   38    f=  4.63162D-05    |proj g|=  2.28037D-05\n",
      "\n",
      "At iterate   39    f=  2.62531D-05    |proj g|=  9.54396D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     39     46      1     0     0   9.544D-06   2.625D-05\n",
      "  F =   2.6253060790781528E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.10229D-01\n",
      "\n",
      "At iterate    1    f=  6.65477D-01    |proj g|=  7.37513D-02\n",
      "\n",
      "At iterate    2    f=  5.28059D-01    |proj g|=  6.00808D-02\n",
      "\n",
      "At iterate    3    f=  2.61622D-01    |proj g|=  2.65916D-02\n",
      "\n",
      "At iterate    4    f=  1.86148D-01    |proj g|=  1.76048D-02\n",
      "\n",
      "At iterate    5    f=  1.12993D-01    |proj g|=  6.57905D-03\n",
      "\n",
      "At iterate    6    f=  7.68261D-02    |proj g|=  5.26752D-03\n",
      "\n",
      "At iterate    7    f=  4.68668D-02    |proj g|=  2.40623D-03\n",
      "\n",
      "At iterate    8    f=  3.11337D-02    |proj g|=  2.33816D-03\n",
      "\n",
      "At iterate    9    f=  1.85164D-02    |proj g|=  1.15562D-03\n",
      "\n",
      "At iterate   10    f=  1.04385D-02    |proj g|=  2.57667D-03\n",
      "\n",
      "At iterate   11    f=  6.03010D-03    |proj g|=  1.70563D-03\n",
      "\n",
      "At iterate   12    f=  4.98843D-03    |proj g|=  1.50080D-03\n",
      "\n",
      "At iterate   13    f=  2.39747D-03    |proj g|=  3.05484D-04\n",
      "\n",
      "At iterate   14    f=  1.33376D-03    |proj g|=  1.60987D-04\n",
      "\n",
      "At iterate   15    f=  6.75799D-04    |proj g|=  7.73078D-05\n",
      "\n",
      "At iterate   16    f=  3.49236D-04    |proj g|=  3.82341D-05\n",
      "\n",
      "At iterate   17    f=  1.75748D-04    |proj g|=  1.83025D-05\n",
      "\n",
      "At iterate   18    f=  8.84417D-05    |proj g|=  8.43452D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     18     23      1     0     0   8.435D-06   8.844D-05\n",
      "  F =   8.8441669038366079E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.26638D-01\n",
      "\n",
      "At iterate    1    f=  6.63171D-01    |proj g|=  4.68828D-02\n",
      "\n",
      "At iterate    2    f=  5.72116D-01    |proj g|=  4.09843D-02\n",
      "\n",
      "At iterate    3    f=  3.36324D-01    |proj g|=  2.40404D-02\n",
      "\n",
      "At iterate    4    f=  2.80325D-01    |proj g|=  1.94948D-02\n",
      "\n",
      "At iterate    5    f=  1.85790D-01    |proj g|=  9.22411D-03\n",
      "\n",
      "At iterate    6    f=  1.33126D-01    |proj g|=  8.03396D-03\n",
      "\n",
      "At iterate    7    f=  8.68411D-02    |proj g|=  7.23437D-03\n",
      "\n",
      "At iterate    8    f=  4.93433D-02    |proj g|=  3.63941D-03\n",
      "\n",
      "At iterate    9    f=  3.41939D-02    |proj g|=  3.14149D-03\n",
      "\n",
      "At iterate   10    f=  1.41481D-02    |proj g|=  1.09936D-03\n",
      "\n",
      "At iterate   11    f=  7.30467D-03    |proj g|=  4.04758D-04\n",
      "\n",
      "At iterate   12    f=  6.53038D-03    |proj g|=  5.65285D-03\n",
      "\n",
      "At iterate   13    f=  1.82880D-03    |proj g|=  9.78176D-04\n",
      "\n",
      "At iterate   14    f=  1.23665D-03    |proj g|=  5.58543D-04\n",
      "\n",
      "At iterate   15    f=  6.03369D-04    |proj g|=  1.85203D-04\n",
      "\n",
      "At iterate   16    f=  3.34126D-04    |proj g|=  7.74671D-05\n",
      "\n",
      "At iterate   17    f=  1.73966D-04    |proj g|=  3.14158D-05\n",
      "\n",
      "At iterate   18    f=  9.00981D-05    |proj g|=  1.33619D-05\n",
      "\n",
      "At iterate   19    f=  4.51422D-05    |proj g|=  6.64994D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     19     23      1     0     0   6.650D-06   4.514D-05\n",
      "  F =   4.5142234558747644E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  7.62139D-02\n",
      "\n",
      "At iterate    1    f=  6.80652D-01    |proj g|=  6.04703D-02\n",
      "\n",
      "At iterate    2    f=  5.63140D-01    |proj g|=  7.51281D-02\n",
      "\n",
      "At iterate    3    f=  3.52690D-01    |proj g|=  3.39167D-02\n",
      "\n",
      "At iterate    4    f=  2.72471D-01    |proj g|=  1.66844D-02\n",
      "\n",
      "At iterate    5    f=  1.92922D-01    |proj g|=  9.06949D-03\n",
      "\n",
      "At iterate    6    f=  1.22165D-01    |proj g|=  8.40900D-03\n",
      "\n",
      "At iterate    7    f=  6.85864D-02    |proj g|=  5.47741D-03\n",
      "\n",
      "At iterate    8    f=  3.35024D-02    |proj g|=  3.71584D-03\n",
      "\n",
      "At iterate    9    f=  2.19885D-02    |proj g|=  2.82462D-03\n",
      "\n",
      "At iterate   10    f=  1.02984D-02    |proj g|=  2.12294D-03\n",
      "\n",
      "At iterate   11    f=  5.00869D-03    |proj g|=  7.28870D-04\n",
      "\n",
      "At iterate   12    f=  3.35463D-03    |proj g|=  1.79339D-03\n",
      "\n",
      "At iterate   13    f=  9.15022D-04    |proj g|=  2.44006D-04\n",
      "\n",
      "At iterate   14    f=  6.21834D-04    |proj g|=  1.35720D-04\n",
      "\n",
      "At iterate   15    f=  2.92644D-04    |proj g|=  4.39374D-05\n",
      "\n",
      "At iterate   16    f=  1.57837D-04    |proj g|=  1.77250D-05\n",
      "\n",
      "At iterate   17    f=  7.92785D-05    |proj g|=  8.11277D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     17     21      1     0     0   8.113D-06   7.928D-05\n",
      "  F =   7.9278475389391937E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  5.99866D-02\n",
      "\n",
      "At iterate    1    f=  6.77421D-01    |proj g|=  8.37362D-02\n",
      "\n",
      "At iterate    2    f=  5.67879D-01    |proj g|=  6.29967D-02\n",
      "\n",
      "At iterate    3    f=  3.82661D-01    |proj g|=  2.40192D-02\n",
      "\n",
      "At iterate    4    f=  3.26393D-01    |proj g|=  1.74156D-02\n",
      "\n",
      "At iterate    5    f=  2.44381D-01    |proj g|=  9.49313D-03\n",
      "\n",
      "At iterate    6    f=  1.36513D-01    |proj g|=  1.94961D-02\n",
      "\n",
      "At iterate    7    f=  8.03560D-02    |proj g|=  9.10256D-03\n",
      "\n",
      "At iterate    8    f=  4.90992D-02    |proj g|=  4.21305D-03\n",
      "\n",
      "At iterate    9    f=  2.58858D-02    |proj g|=  2.75021D-03\n",
      "\n",
      "At iterate   10    f=  1.27090D-02    |proj g|=  2.77572D-03\n",
      "\n",
      "At iterate   11    f=  5.59929D-03    |proj g|=  1.69953D-03\n",
      "\n",
      "At iterate   12    f=  5.53102D-03    |proj g|=  2.54900D-03\n",
      "\n",
      "At iterate   13    f=  3.28009D-03    |proj g|=  6.20367D-04\n",
      "\n",
      "At iterate   14    f=  1.89794D-03    |proj g|=  2.37091D-04\n",
      "\n",
      "At iterate   15    f=  9.96672D-04    |proj g|=  1.53370D-04\n",
      "\n",
      "At iterate   16    f=  5.05110D-04    |proj g|=  7.70323D-05\n",
      "\n",
      "At iterate   17    f=  2.57451D-04    |proj g|=  5.61690D-05\n",
      "\n",
      "At iterate   18    f=  1.17286D-04    |proj g|=  2.79540D-05\n",
      "\n",
      "At iterate   19    f=  5.65760D-05    |proj g|=  1.32334D-05\n",
      "\n",
      "At iterate   20    f=  3.22548D-05    |proj g|=  7.12933D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     20     24      1     0     0   7.129D-06   3.225D-05\n",
      "  F =   3.2254754897977624E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n",
      "\r",
      " 62%|██████▏   | 8/13 [00:00<00:00, 35.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.32731D-01\n",
      "\n",
      "At iterate    1    f=  6.65800D-01    |proj g|=  6.99601D-02\n",
      "\n",
      "At iterate    2    f=  5.34905D-01    |proj g|=  5.22665D-02\n",
      "\n",
      "At iterate    3    f=  3.12013D-01    |proj g|=  2.12127D-02\n",
      "\n",
      "At iterate    4    f=  2.48640D-01    |proj g|=  1.14890D-02\n",
      "\n",
      "At iterate    5    f=  1.59249D-01    |proj g|=  1.26572D-02\n",
      "\n",
      "At iterate    6    f=  1.16491D-01    |proj g|=  6.66827D-03\n",
      "\n",
      "At iterate    7    f=  6.85774D-02    |proj g|=  3.35160D-03\n",
      "\n",
      "At iterate    8    f=  4.02619D-02    |proj g|=  2.45510D-03\n",
      "\n",
      "At iterate    9    f=  2.37013D-02    |proj g|=  1.11736D-03\n",
      "\n",
      "At iterate   10    f=  1.42449D-02    |proj g|=  2.71769D-03\n",
      "\n",
      "At iterate   11    f=  9.42159D-03    |proj g|=  2.00387D-03\n",
      "\n",
      "At iterate   12    f=  8.72794D-03    |proj g|=  1.42824D-03\n",
      "\n",
      "At iterate   13    f=  4.87489D-03    |proj g|=  4.81500D-04\n",
      "\n",
      "At iterate   14    f=  2.80581D-03    |proj g|=  3.16799D-04\n",
      "\n",
      "At iterate   15    f=  1.31884D-03    |proj g|=  1.22386D-04\n",
      "\n",
      "At iterate   16    f=  6.34172D-04    |proj g|=  1.90472D-04\n",
      "\n",
      "At iterate   17    f=  3.96269D-04    |proj g|=  1.02787D-04\n",
      "\n",
      "At iterate   18    f=  1.91516D-04    |proj g|=  3.99884D-05\n",
      "\n",
      "At iterate   19    f=  1.02416D-04    |proj g|=  1.74680D-05\n",
      "\n",
      "At iterate   20    f=  5.16885D-05    |proj g|=  7.69954D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     20     25      1     0     0   7.700D-06   5.169D-05\n",
      "  F =   5.1688520714829644E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.55818D-01\n",
      "\n",
      "At iterate    1    f=  6.44294D-01    |proj g|=  6.28805D-02\n",
      "\n",
      "At iterate    2    f=  5.32451D-01    |proj g|=  5.03990D-02\n",
      "\n",
      "At iterate    3    f=  2.97765D-01    |proj g|=  3.08142D-02\n",
      "\n",
      "At iterate    4    f=  2.28420D-01    |proj g|=  1.95285D-02\n",
      "\n",
      "At iterate    5    f=  1.58034D-01    |proj g|=  1.10833D-02\n",
      "\n",
      "At iterate    6    f=  1.04034D-01    |proj g|=  8.84720D-03\n",
      "\n",
      "At iterate    7    f=  5.25646D-02    |proj g|=  6.95161D-03\n",
      "\n",
      "At iterate    8    f=  3.47305D-02    |proj g|=  1.01079D-02\n",
      "\n",
      "At iterate    9    f=  1.59002D-02    |proj g|=  5.09283D-03\n",
      "\n",
      "At iterate   10    f=  1.03385D-02    |proj g|=  3.66769D-03\n",
      "\n",
      "At iterate   11    f=  5.71107D-03    |proj g|=  2.22342D-03\n",
      "\n",
      "At iterate   12    f=  4.86046D-03    |proj g|=  1.22556D-03\n",
      "\n",
      "At iterate   13    f=  2.59567D-03    |proj g|=  3.32059D-04\n",
      "\n",
      "At iterate   14    f=  1.50534D-03    |proj g|=  2.74393D-04\n",
      "\n",
      "At iterate   15    f=  1.37810D-03    |proj g|=  3.99574D-04\n",
      "\n",
      "At iterate   16    f=  4.23027D-04    |proj g|=  8.62329D-05\n",
      "\n",
      "At iterate   17    f=  2.82976D-04    |proj g|=  5.54312D-05\n",
      "\n",
      "At iterate   18    f=  1.31266D-04    |proj g|=  2.56338D-05\n",
      "\n",
      "At iterate   19    f=  6.96106D-05    |proj g|=  1.32887D-05\n",
      "\n",
      "At iterate   20    f=  3.47854D-05    |proj g|=  6.58570D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     20     25      1     0     0   6.586D-06   3.479D-05\n",
      "  F =   3.4785358362069613E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.28070D-01\n",
      "\n",
      "At iterate    1    f=  6.66021D-01    |proj g|=  5.33326D-02\n",
      "\n",
      "At iterate    2    f=  5.52363D-01    |proj g|=  4.73146D-02\n",
      "\n",
      "At iterate    3    f=  4.04086D-01    |proj g|=  2.51718D-02\n",
      "\n",
      "At iterate    4    f=  3.51390D-01    |proj g|=  1.95167D-02\n",
      "\n",
      "At iterate    5    f=  2.40221D-01    |proj g|=  1.10018D-02\n",
      "\n",
      "At iterate    6    f=  1.54210D-01    |proj g|=  9.41888D-03\n",
      "\n",
      "At iterate    7    f=  1.04831D-01    |proj g|=  4.04719D-03\n",
      "\n",
      "At iterate    8    f=  8.57655D-02    |proj g|=  2.30740D-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " 92%|█████████▏| 12/13 [00:00<00:00, 34.88it/s] This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "100%|██████████| 13/13 [00:00<00:00, 33.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate    9    f=  6.55236D-02    |proj g|=  2.50576D-03\n",
      "\n",
      "At iterate   10    f=  5.20969D-02    |proj g|=  1.94395D-03\n",
      "\n",
      "At iterate   11    f=  4.16092D-02    |proj g|=  1.85007D-03\n",
      "\n",
      "At iterate   12    f=  4.08014D-02    |proj g|=  4.14288D-03\n",
      "\n",
      "At iterate   13    f=  3.25313D-02    |proj g|=  2.48266D-03\n",
      "\n",
      "At iterate   14    f=  2.59813D-02    |proj g|=  2.10663D-03\n",
      "\n",
      "At iterate   15    f=  2.03393D-02    |proj g|=  1.64880D-03\n",
      "\n",
      "At iterate   16    f=  1.38536D-02    |proj g|=  1.34099D-03\n",
      "\n",
      "At iterate   17    f=  5.29603D-03    |proj g|=  1.15172D-03\n",
      "\n",
      "At iterate   18    f=  2.84092D-03    |proj g|=  1.19976D-03\n",
      "\n",
      "At iterate   19    f=  1.86625D-03    |proj g|=  6.74265D-04\n",
      "\n",
      "At iterate   20    f=  1.25860D-03    |proj g|=  2.74311D-04\n",
      "\n",
      "At iterate   21    f=  6.70343D-04    |proj g|=  7.46851D-05\n",
      "\n",
      "At iterate   22    f=  3.89037D-04    |proj g|=  5.03591D-05\n",
      "\n",
      "At iterate   23    f=  3.52957D-04    |proj g|=  1.21837D-04\n",
      "\n",
      "At iterate   24    f=  1.76349D-04    |proj g|=  3.39075D-05\n",
      "\n",
      "At iterate   25    f=  9.98859D-05    |proj g|=  1.80326D-05\n",
      "\n",
      "At iterate   26    f=  4.68317D-05    |proj g|=  7.96534D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     26     33      1     0     0   7.965D-06   4.683D-05\n",
      "  F =   4.6831718281613840E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  5.74951D-02\n",
      "\n",
      "At iterate    1    f=  5.95381D-01    |proj g|=  5.29514D-02\n",
      "\n",
      "At iterate    2    f=  5.31873D-01    |proj g|=  1.36853D-01\n",
      "\n",
      "At iterate    3    f=  4.16778D-01    |proj g|=  2.74852D-02\n",
      "\n",
      "At iterate    4    f=  3.89869D-01    |proj g|=  1.81201D-02\n",
      "\n",
      "At iterate    5    f=  3.68900D-01    |proj g|=  1.57999D-02\n",
      "\n",
      "At iterate    6    f=  3.14085D-01    |proj g|=  1.45193D-02\n",
      "\n",
      "At iterate    7    f=  1.84781D-01    |proj g|=  1.69244D-02\n",
      "\n",
      "At iterate    8    f=  1.34256D-01    |proj g|=  5.93776D-03\n",
      "\n",
      "At iterate    9    f=  1.05885D-01    |proj g|=  7.56977D-03\n",
      "\n",
      "At iterate   10    f=  7.86469D-02    |proj g|=  8.79649D-03\n",
      "\n",
      "At iterate   11    f=  4.28931D-02    |proj g|=  3.72602D-03\n",
      "\n",
      "At iterate   12    f=  3.02261D-02    |proj g|=  2.93931D-03\n",
      "\n",
      "At iterate   13    f=  1.46418D-02    |proj g|=  2.04937D-03\n",
      "\n",
      "At iterate   14    f=  7.95049D-03    |proj g|=  2.36546D-03\n",
      "\n",
      "At iterate   15    f=  6.06669D-03    |proj g|=  2.93743D-03\n",
      "\n",
      "At iterate   16    f=  3.44982D-03    |proj g|=  8.96230D-04\n",
      "\n",
      "At iterate   17    f=  1.97866D-03    |proj g|=  3.14195D-04\n",
      "\n",
      "At iterate   18    f=  1.06342D-03    |proj g|=  1.47013D-04\n",
      "\n",
      "At iterate   19    f=  5.45030D-04    |proj g|=  6.70056D-05\n",
      "\n",
      "At iterate   20    f=  2.75511D-04    |proj g|=  3.52171D-05\n",
      "\n",
      "At iterate   21    f=  1.38577D-04    |proj g|=  2.04281D-05\n",
      "\n",
      "At iterate   22    f=  7.10123D-05    |proj g|=  1.20176D-05\n",
      "\n",
      "At iterate   23    f=  3.41206D-05    |proj g|=  8.79281D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     23     26      1     0     0   8.793D-06   3.412D-05\n",
      "  F =   3.4120583370980034E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.88833D-01\n",
      "\n",
      "At iterate    1    f=  6.47589D-01    |proj g|=  5.88420D-02\n",
      "\n",
      "At iterate    2    f=  5.30649D-01    |proj g|=  4.83947D-02\n",
      "\n",
      "At iterate    3    f=  2.70673D-01    |proj g|=  4.49347D-02\n",
      "\n",
      "At iterate    4    f=  1.88721D-01    |proj g|=  2.99908D-02\n",
      "\n",
      "At iterate    5    f=  1.08500D-01    |proj g|=  1.45879D-02\n",
      "\n",
      "At iterate    6    f=  6.10401D-02    |proj g|=  7.48123D-03\n",
      "\n",
      "At iterate    7    f=  3.13252D-02    |proj g|=  3.86210D-03\n",
      "\n",
      "At iterate    8    f=  1.57227D-02    |proj g|=  2.11316D-03\n",
      "\n",
      "At iterate    9    f=  7.85961D-03    |proj g|=  1.10723D-03\n",
      "\n",
      "At iterate   10    f=  3.90120D-03    |proj g|=  6.14053D-04\n",
      "\n",
      "At iterate   11    f=  1.91133D-03    |proj g|=  3.05130D-04\n",
      "\n",
      "At iterate   12    f=  9.71203D-04    |proj g|=  2.78401D-04\n",
      "\n",
      "At iterate   13    f=  9.33178D-04    |proj g|=  6.47955D-05\n",
      "\n",
      "At iterate   14    f=  4.79407D-04    |proj g|=  5.17889D-05\n",
      "\n",
      "At iterate   15    f=  2.39185D-04    |proj g|=  2.61557D-05\n",
      "\n",
      "At iterate   16    f=  1.19829D-04    |proj g|=  1.32415D-05\n",
      "\n",
      "At iterate   17    f=  5.54666D-05    |proj g|=  6.43911D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     17     23      1     0     0   6.439D-06   5.547D-05\n",
      "  F =   5.5466568885936399E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  8.91689D-02\n",
      "\n",
      "At iterate    1    f=  6.76993D-01    |proj g|=  5.29472D-02\n",
      "\n",
      "At iterate    2    f=  5.87638D-01    |proj g|=  5.53797D-02\n",
      "\n",
      "At iterate    3    f=  3.56485D-01    |proj g|=  2.44498D-02\n",
      "\n",
      "At iterate    4    f=  2.91771D-01    |proj g|=  1.87664D-02\n",
      "\n",
      "At iterate    5    f=  1.89809D-01    |proj g|=  1.23326D-02\n",
      "\n",
      "At iterate    6    f=  1.35318D-01    |proj g|=  4.62308D-03\n",
      "\n",
      "At iterate    7    f=  9.80768D-02    |proj g|=  4.80735D-03\n",
      "\n",
      "At iterate    8    f=  6.63165D-02    |proj g|=  2.22378D-03\n",
      "\n",
      "At iterate    9    f=  4.49254D-02    |proj g|=  5.19316D-03\n",
      "\n",
      "At iterate   10    f=  3.30595D-02    |proj g|=  5.33523D-03\n",
      "\n",
      "At iterate   11    f=  2.89647D-02    |proj g|=  2.74487D-03\n",
      "\n",
      "At iterate   12    f=  2.55018D-02    |proj g|=  2.48792D-03\n",
      "\n",
      "At iterate   13    f=  2.50916D-02    |proj g|=  2.74190D-03\n",
      "\n",
      "At iterate   14    f=  2.38117D-02    |proj g|=  2.67022D-03\n",
      "\n",
      "At iterate   15    f=  2.13445D-02    |proj g|=  2.52354D-03\n",
      "\n",
      "At iterate   16    f=  1.62794D-02    |proj g|=  1.70099D-03\n",
      "\n",
      "At iterate   17    f=  9.05400D-03    |proj g|=  9.36350D-04\n",
      "\n",
      "At iterate   18    f=  4.21115D-03    |proj g|=  7.17351D-04\n",
      "\n",
      "At iterate   19    f=  2.02434D-03    |proj g|=  3.25320D-04\n",
      "\n",
      "At iterate   20    f=  1.10451D-03    |proj g|=  1.76638D-04\n",
      "\n",
      "At iterate   21    f=  5.19599D-04    |proj g|=  9.01696D-05\n",
      "\n",
      "At iterate   22    f=  2.65619D-04    |proj g|=  5.76800D-05\n",
      "\n",
      "At iterate   23    f=  1.23044D-04    |proj g|=  4.95990D-05\n",
      "\n",
      "At iterate   24    f=  9.14174D-05    |proj g|=  4.28788D-05\n",
      "\n",
      "At iterate   25    f=  5.62548D-05    |proj g|=  1.86022D-05\n",
      "\n",
      "At iterate   26    f=  2.89888D-05    |proj g|=  9.88080D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     26     32      1     0     0   9.881D-06   2.899D-05\n",
      "  F =   2.8988826182414912E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 5537.32it/s]\n",
      "100%|██████████| 13/13 [00:00<00:00, 762.61it/s]\n",
      "100%|██████████| 13/13 [00:00<00:00, 5435.20it/s]\n",
      "100%|██████████| 13/13 [00:00<00:00, 868.37it/s]\n",
      "100%|██████████| 13/13 [00:00<00:00, 305.96it/s]\n",
      "/storage/work/eak5582/Research/generalized_mlm_2.py:404: ClusterWarning: scipy.cluster: The symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix\n",
      "  LocalModelsTree = linkage(self.dist_mat_avg, 'ward')\n",
      "  0%|          | 0/10 [00:00<?, ?it/s] This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " 30%|███       | 3/10 [00:00<00:00, 26.76it/s] This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  7.62139D-02\n",
      "\n",
      "At iterate    1    f=  6.80652D-01    |proj g|=  6.04703D-02\n",
      "\n",
      "At iterate    2    f=  5.63140D-01    |proj g|=  7.51281D-02\n",
      "\n",
      "At iterate    3    f=  3.52690D-01    |proj g|=  3.39167D-02\n",
      "\n",
      "At iterate    4    f=  2.72471D-01    |proj g|=  1.66844D-02\n",
      "\n",
      "At iterate    5    f=  1.92922D-01    |proj g|=  9.06949D-03\n",
      "\n",
      "At iterate    6    f=  1.22165D-01    |proj g|=  8.40900D-03\n",
      "\n",
      "At iterate    7    f=  6.85864D-02    |proj g|=  5.47741D-03\n",
      "\n",
      "At iterate    8    f=  3.35024D-02    |proj g|=  3.71584D-03\n",
      "\n",
      "At iterate    9    f=  2.19885D-02    |proj g|=  2.82462D-03\n",
      "\n",
      "At iterate   10    f=  1.02984D-02    |proj g|=  2.12294D-03\n",
      "\n",
      "At iterate   11    f=  5.00869D-03    |proj g|=  7.28870D-04\n",
      "\n",
      "At iterate   12    f=  3.35463D-03    |proj g|=  1.79339D-03\n",
      "\n",
      "At iterate   13    f=  9.15022D-04    |proj g|=  2.44006D-04\n",
      "\n",
      "At iterate   14    f=  6.21834D-04    |proj g|=  1.35720D-04\n",
      "\n",
      "At iterate   15    f=  2.92644D-04    |proj g|=  4.39374D-05\n",
      "\n",
      "At iterate   16    f=  1.57837D-04    |proj g|=  1.77250D-05\n",
      "\n",
      "At iterate   17    f=  7.92785D-05    |proj g|=  8.11277D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     17     21      1     0     0   8.113D-06   7.928D-05\n",
      "  F =   7.9278475389391937E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  8.91689D-02\n",
      "\n",
      "At iterate    1    f=  6.76993D-01    |proj g|=  5.29472D-02\n",
      "\n",
      "At iterate    2    f=  5.87638D-01    |proj g|=  5.53797D-02\n",
      "\n",
      "At iterate    3    f=  3.56485D-01    |proj g|=  2.44498D-02\n",
      "\n",
      "At iterate    4    f=  2.91771D-01    |proj g|=  1.87664D-02\n",
      "\n",
      "At iterate    5    f=  1.89809D-01    |proj g|=  1.23326D-02\n",
      "\n",
      "At iterate    6    f=  1.35318D-01    |proj g|=  4.62308D-03\n",
      "\n",
      "At iterate    7    f=  9.80768D-02    |proj g|=  4.80735D-03\n",
      "\n",
      "At iterate    8    f=  6.63165D-02    |proj g|=  2.22378D-03\n",
      "\n",
      "At iterate    9    f=  4.49254D-02    |proj g|=  5.19316D-03\n",
      "\n",
      "At iterate   10    f=  3.30595D-02    |proj g|=  5.33523D-03\n",
      "\n",
      "At iterate   11    f=  2.89647D-02    |proj g|=  2.74487D-03\n",
      "\n",
      "At iterate   12    f=  2.55018D-02    |proj g|=  2.48792D-03\n",
      "\n",
      "At iterate   13    f=  2.50916D-02    |proj g|=  2.74190D-03\n",
      "\n",
      "At iterate   14    f=  2.38117D-02    |proj g|=  2.67022D-03\n",
      "\n",
      "At iterate   15    f=  2.13445D-02    |proj g|=  2.52354D-03\n",
      "\n",
      "At iterate   16    f=  1.62794D-02    |proj g|=  1.70099D-03\n",
      "\n",
      "At iterate   17    f=  9.05400D-03    |proj g|=  9.36350D-04\n",
      "\n",
      "At iterate   18    f=  4.21115D-03    |proj g|=  7.17351D-04\n",
      "\n",
      "At iterate   19    f=  2.02434D-03    |proj g|=  3.25320D-04\n",
      "\n",
      "At iterate   20    f=  1.10451D-03    |proj g|=  1.76638D-04\n",
      "\n",
      "At iterate   21    f=  5.19599D-04    |proj g|=  9.01696D-05\n",
      "\n",
      "At iterate   22    f=  2.65619D-04    |proj g|=  5.76800D-05\n",
      "\n",
      "At iterate   23    f=  1.23044D-04    |proj g|=  4.95990D-05\n",
      "\n",
      "At iterate   24    f=  9.14174D-05    |proj g|=  4.28788D-05\n",
      "\n",
      "At iterate   25    f=  5.62548D-05    |proj g|=  1.86022D-05\n",
      "\n",
      "At iterate   26    f=  2.89888D-05    |proj g|=  9.88080D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     26     32      1     0     0   9.881D-06   2.899D-05\n",
      "  F =   2.8988826182414912E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  2.10186D-01\n",
      "\n",
      "At iterate    1    f=  6.20497D-01    |proj g|=  5.12672D-02\n",
      "\n",
      "At iterate    2    f=  5.48033D-01    |proj g|=  6.01318D-02\n",
      "\n",
      "At iterate    3    f=  4.39825D-01    |proj g|=  6.67592D-02\n",
      "\n",
      "At iterate    4    f=  3.46271D-01    |proj g|=  6.52046D-02\n",
      "\n",
      "At iterate    5    f=  2.95361D-01    |proj g|=  2.92925D-02\n",
      "\n",
      "At iterate    6    f=  2.60121D-01    |proj g|=  9.06619D-03\n",
      "\n",
      "At iterate    7    f=  2.42189D-01    |proj g|=  1.49029D-02\n",
      "\n",
      "At iterate    8    f=  2.28217D-01    |proj g|=  1.96904D-02\n",
      "\n",
      "At iterate    9    f=  2.05696D-01    |proj g|=  1.90242D-02\n",
      "\n",
      "At iterate   10    f=  1.64393D-01    |proj g|=  5.75306D-03\n",
      "\n",
      "At iterate   11    f=  1.58274D-01    |proj g|=  6.11246D-03\n",
      "\n",
      "At iterate   12    f=  1.54670D-01    |proj g|=  1.34375D-02\n",
      "\n",
      "At iterate   13    f=  1.49221D-01    |proj g|=  5.60069D-03\n",
      "\n",
      "At iterate   14    f=  1.44895D-01    |proj g|=  2.68810D-03\n",
      "\n",
      "At iterate   15    f=  1.38678D-01    |proj g|=  3.69766D-03\n",
      "\n",
      "At iterate   16    f=  1.28734D-01    |proj g|=  4.19199D-03\n",
      "\n",
      "At iterate   17    f=  1.15299D-01    |proj g|=  6.50893D-03\n",
      "\n",
      "At iterate   18    f=  1.08194D-01    |proj g|=  4.44295D-03\n",
      "\n",
      "At iterate   19    f=  1.03291D-01    |proj g|=  2.68493D-03\n",
      "\n",
      "At iterate   20    f=  9.48541D-02    |proj g|=  1.57572D-03\n",
      "\n",
      "At iterate   21    f=  8.40003D-02    |proj g|=  2.45056D-03\n",
      "\n",
      "At iterate   22    f=  7.23254D-02    |proj g|=  3.05490D-03\n",
      "\n",
      "At iterate   23    f=  7.03969D-02    |proj g|=  1.30090D-02\n",
      "\n",
      "At iterate   24    f=  6.36394D-02    |proj g|=  8.12065D-03\n",
      "\n",
      "At iterate   25    f=  5.79356D-02    |proj g|=  3.41989D-03\n",
      "\n",
      "At iterate   26    f=  5.34654D-02    |proj g|=  1.97845D-03\n",
      "\n",
      "At iterate   27    f=  4.94302D-02    |proj g|=  2.39525D-03\n",
      "\n",
      "At iterate   28    f=  4.29857D-02    |proj g|=  2.74545D-03\n",
      "\n",
      "At iterate   29    f=  3.34168D-02    |proj g|=  2.97496D-03\n",
      "\n",
      "At iterate   30    f=  2.10794D-02    |proj g|=  1.34915D-03\n",
      "\n",
      "At iterate   31    f=  1.85238D-02    |proj g|=  2.48837D-03\n",
      "\n",
      "At iterate   32    f=  1.68441D-02    |proj g|=  2.14383D-03\n",
      "\n",
      "At iterate   33    f=  1.50876D-02    |proj g|=  1.08710D-03\n",
      "\n",
      "At iterate   34    f=  1.38938D-02    |proj g|=  8.75300D-04\n",
      "\n",
      "At iterate   35    f=  1.29137D-02    |proj g|=  7.83833D-04\n",
      "\n",
      "At iterate   36    f=  1.20389D-02    |proj g|=  2.42399D-03\n",
      "\n",
      "At iterate   37    f=  1.09993D-02    |proj g|=  1.64076D-03\n",
      "\n",
      "At iterate   38    f=  8.42496D-03    |proj g|=  8.67000D-04\n",
      "\n",
      "At iterate   39    f=  4.85796D-03    |proj g|=  7.53619D-04\n",
      "\n",
      "At iterate   40    f=  1.89764D-03    |proj g|=  6.28321D-04\n",
      "\n",
      "At iterate   41    f=  1.05388D-03    |proj g|=  4.44881D-04\n",
      "\n",
      "At iterate   42    f=  5.97456D-04    |proj g|=  1.95384D-04\n",
      "\n",
      "At iterate   43    f=  3.03500D-04    |proj g|=  7.29400D-05\n",
      "\n",
      "At iterate   44    f=  1.66902D-04    |proj g|=  2.99720D-05\n",
      "\n",
      "At iterate   45    f=  8.64469D-05    |proj g|=  1.15862D-05\n",
      "\n",
      "At iterate   46    f=  4.26600D-05    |proj g|=  6.54658D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     46     53      1     0     0   6.547D-06   4.266D-05\n",
      "  F =   4.2659981937557997E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.10229D-01\n",
      "\n",
      "At iterate    1    f=  6.65477D-01    |proj g|=  7.37513D-02\n",
      "\n",
      "At iterate    2    f=  5.28059D-01    |proj g|=  6.00808D-02\n",
      "\n",
      "At iterate    3    f=  2.61622D-01    |proj g|=  2.65916D-02\n",
      "\n",
      "At iterate    4    f=  1.86148D-01    |proj g|=  1.76048D-02\n",
      "\n",
      "At iterate    5    f=  1.12993D-01    |proj g|=  6.57905D-03\n",
      "\n",
      "At iterate    6    f=  7.68261D-02    |proj g|=  5.26752D-03\n",
      "\n",
      "At iterate    7    f=  4.68668D-02    |proj g|=  2.40623D-03\n",
      "\n",
      "At iterate    8    f=  3.11337D-02    |proj g|=  2.33816D-03\n",
      "\n",
      "At iterate    9    f=  1.85164D-02    |proj g|=  1.15562D-03\n",
      "\n",
      "At iterate   10    f=  1.04385D-02    |proj g|=  2.57667D-03\n",
      "\n",
      "At iterate   11    f=  6.03010D-03    |proj g|=  1.70563D-03\n",
      "\n",
      "At iterate   12    f=  4.98843D-03    |proj g|=  1.50080D-03\n",
      "\n",
      "At iterate   13    f=  2.39747D-03    |proj g|=  3.05484D-04\n",
      "\n",
      "At iterate   14    f=  1.33376D-03    |proj g|=  1.60987D-04\n",
      "\n",
      "At iterate   15    f=  6.75799D-04    |proj g|=  7.73078D-05\n",
      "\n",
      "At iterate   16    f=  3.49236D-04    |proj g|=  3.82341D-05\n",
      "\n",
      "At iterate   17    f=  1.75748D-04    |proj g|=  1.83025D-05\n",
      "\n",
      "At iterate   18    f=  8.84417D-05    |proj g|=  8.43452D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     18     23      1     0     0   8.435D-06   8.844D-05\n",
      "  F =   8.8441669038366079E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.26638D-01\n",
      "\n",
      "At iterate    1    f=  6.63171D-01    |proj g|=  4.68828D-02\n",
      "\n",
      "At iterate    2    f=  5.72116D-01    |proj g|=  4.09843D-02\n",
      "\n",
      "At iterate    3    f=  3.36324D-01    |proj g|=  2.40404D-02\n",
      "\n",
      "At iterate    4    f=  2.80325D-01    |proj g|=  1.94948D-02\n",
      "\n",
      "At iterate    5    f=  1.85790D-01    |proj g|=  9.22411D-03\n",
      "\n",
      "At iterate    6    f=  1.33126D-01    |proj g|=  8.03396D-03\n",
      "\n",
      "At iterate    7    f=  8.68411D-02    |proj g|=  7.23437D-03\n",
      "\n",
      "At iterate    8    f=  4.93433D-02    |proj g|=  3.63941D-03\n",
      "\n",
      "At iterate    9    f=  3.41939D-02    |proj g|=  3.14149D-03\n",
      "\n",
      "At iterate   10    f=  1.41481D-02    |proj g|=  1.09936D-03\n",
      "\n",
      "At iterate   11    f=  7.30467D-03    |proj g|=  4.04758D-04\n",
      "\n",
      "At iterate   12    f=  6.53038D-03    |proj g|=  5.65285D-03\n",
      "\n",
      "At iterate   13    f=  1.82880D-03    |proj g|=  9.78176D-04\n",
      "\n",
      "At iterate   14    f=  1.23665D-03    |proj g|=  5.58543D-04\n",
      "\n",
      "At iterate   15    f=  6.03369D-04    |proj g|=  1.85203D-04\n",
      "\n",
      "At iterate   16    f=  3.34126D-04    |proj g|=  7.74671D-05\n",
      "\n",
      "At iterate   17    f=  1.73966D-04    |proj g|=  3.14158D-05\n",
      "\n",
      "At iterate   18    f=  9.00981D-05    |proj g|=  1.33619D-05\n",
      "\n",
      "At iterate   19    f=  4.51422D-05    |proj g|=  6.64994D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     19     23      1     0     0   6.650D-06   4.514D-05\n",
      "  F =   4.5142234558747644E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.32731D-01\n",
      "\n",
      "At iterate    1    f=  6.65800D-01    |proj g|=  6.99601D-02\n",
      "\n",
      "At iterate    2    f=  5.34905D-01    |proj g|=  5.22665D-02\n",
      "\n",
      "At iterate    3    f=  3.12013D-01    |proj g|=  2.12127D-02\n",
      "\n",
      "At iterate    4    f=  2.48640D-01    |proj g|=  1.14890D-02\n",
      "\n",
      "At iterate    5    f=  1.59249D-01    |proj g|=  1.26572D-02\n",
      "\n",
      "At iterate    6    f=  1.16491D-01    |proj g|=  6.66827D-03\n",
      "\n",
      "At iterate    7    f=  6.85774D-02    |proj g|=  3.35160D-03\n",
      "\n",
      "At iterate    8    f=  4.02619D-02    |proj g|=  2.45510D-03\n",
      "\n",
      "At iterate    9    f=  2.37013D-02    |proj g|=  1.11736D-03\n",
      "\n",
      "At iterate   10    f=  1.42449D-02    |proj g|=  2.71769D-03\n",
      "\n",
      "At iterate   11    f=  9.42159D-03    |proj g|=  2.00387D-03\n",
      "\n",
      "At iterate   12    f=  8.72794D-03    |proj g|=  1.42824D-03\n",
      "\n",
      "At iterate   13    f=  4.87489D-03    |proj g|=  4.81500D-04\n",
      "\n",
      "At iterate   14    f=  2.80581D-03    |proj g|=  3.16799D-04\n",
      "\n",
      "At iterate   15    f=  1.31884D-03    |proj g|=  1.22386D-04\n",
      "\n",
      "At iterate   16    f=  6.34172D-04    |proj g|=  1.90472D-04\n",
      "\n",
      "At iterate   17    f=  3.96269D-04    |proj g|=  1.02787D-04\n",
      "\n",
      "At iterate   18    f=  1.91516D-04    |proj g|=  3.99884D-05\n",
      "\n",
      "At iterate   19    f=  1.02416D-04    |proj g|=  1.74680D-05\n",
      "\n",
      "At iterate   20    f=  5.16885D-05    |proj g|=  7.69954D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     20     25      1     0     0   7.700D-06   5.169D-05\n",
      "  F =   5.1688520714829644E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.28070D-01\n",
      "\n",
      "At iterate    1    f=  6.66021D-01    |proj g|=  5.33326D-02\n",
      "\n",
      "At iterate    2    f=  5.52363D-01    |proj g|=  4.73146D-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " 70%|███████   | 7/10 [00:00<00:00, 30.06it/s] This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " This problem is unconstrained.\n",
      "100%|██████████| 10/10 [00:00<00:00, 26.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate    3    f=  4.04086D-01    |proj g|=  2.51718D-02\n",
      "\n",
      "At iterate    4    f=  3.51390D-01    |proj g|=  1.95167D-02\n",
      "\n",
      "At iterate    5    f=  2.40221D-01    |proj g|=  1.10018D-02\n",
      "\n",
      "At iterate    6    f=  1.54210D-01    |proj g|=  9.41888D-03\n",
      "\n",
      "At iterate    7    f=  1.04831D-01    |proj g|=  4.04719D-03\n",
      "\n",
      "At iterate    8    f=  8.57655D-02    |proj g|=  2.30740D-03\n",
      "\n",
      "At iterate    9    f=  6.55236D-02    |proj g|=  2.50576D-03\n",
      "\n",
      "At iterate   10    f=  5.20969D-02    |proj g|=  1.94395D-03\n",
      "\n",
      "At iterate   11    f=  4.16092D-02    |proj g|=  1.85007D-03\n",
      "\n",
      "At iterate   12    f=  4.08014D-02    |proj g|=  4.14288D-03\n",
      "\n",
      "At iterate   13    f=  3.25313D-02    |proj g|=  2.48266D-03\n",
      "\n",
      "At iterate   14    f=  2.59813D-02    |proj g|=  2.10663D-03\n",
      "\n",
      "At iterate   15    f=  2.03393D-02    |proj g|=  1.64880D-03\n",
      "\n",
      "At iterate   16    f=  1.38536D-02    |proj g|=  1.34099D-03\n",
      "\n",
      "At iterate   17    f=  5.29603D-03    |proj g|=  1.15172D-03\n",
      "\n",
      "At iterate   18    f=  2.84092D-03    |proj g|=  1.19976D-03\n",
      "\n",
      "At iterate   19    f=  1.86625D-03    |proj g|=  6.74265D-04\n",
      "\n",
      "At iterate   20    f=  1.25860D-03    |proj g|=  2.74311D-04\n",
      "\n",
      "At iterate   21    f=  6.70343D-04    |proj g|=  7.46851D-05\n",
      "\n",
      "At iterate   22    f=  3.89037D-04    |proj g|=  5.03591D-05\n",
      "\n",
      "At iterate   23    f=  3.52957D-04    |proj g|=  1.21837D-04\n",
      "\n",
      "At iterate   24    f=  1.76349D-04    |proj g|=  3.39075D-05\n",
      "\n",
      "At iterate   25    f=  9.98859D-05    |proj g|=  1.80326D-05\n",
      "\n",
      "At iterate   26    f=  4.68317D-05    |proj g|=  7.96534D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     26     33      1     0     0   7.965D-06   4.683D-05\n",
      "  F =   4.6831718281613840E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  9.40194D-02\n",
      "\n",
      "At iterate    1    f=  6.75069D-01    |proj g|=  5.49472D-02\n",
      "\n",
      "At iterate    2    f=  6.53713D-01    |proj g|=  5.47106D-02\n",
      "\n",
      "At iterate    3    f=  5.22914D-01    |proj g|=  5.03331D-02\n",
      "\n",
      "At iterate    4    f=  4.06056D-01    |proj g|=  3.58878D-02\n",
      "\n",
      "At iterate    5    f=  3.49707D-01    |proj g|=  2.07675D-02\n",
      "\n",
      "At iterate    6    f=  3.18851D-01    |proj g|=  1.39885D-02\n",
      "\n",
      "At iterate    7    f=  2.95766D-01    |proj g|=  1.16579D-02\n",
      "\n",
      "At iterate    8    f=  2.63107D-01    |proj g|=  1.22516D-02\n",
      "\n",
      "At iterate    9    f=  1.94078D-01    |proj g|=  6.09690D-03\n",
      "\n",
      "At iterate   10    f=  1.70766D-01    |proj g|=  4.27577D-03\n",
      "\n",
      "At iterate   11    f=  1.53007D-01    |proj g|=  2.39620D-03\n",
      "\n",
      "At iterate   12    f=  1.52353D-01    |proj g|=  9.30865D-03\n",
      "\n",
      "At iterate   13    f=  1.44802D-01    |proj g|=  6.16259D-03\n",
      "\n",
      "At iterate   14    f=  1.38573D-01    |proj g|=  3.31492D-03\n",
      "\n",
      "At iterate   15    f=  1.32372D-01    |proj g|=  1.74727D-03\n",
      "\n",
      "At iterate   16    f=  1.27472D-01    |proj g|=  2.71894D-03\n",
      "\n",
      "At iterate   17    f=  1.19850D-01    |proj g|=  3.05700D-03\n",
      "\n",
      "At iterate   18    f=  1.13279D-01    |proj g|=  1.04207D-02\n",
      "\n",
      "At iterate   19    f=  1.04779D-01    |proj g|=  2.24252D-03\n",
      "\n",
      "At iterate   20    f=  1.01202D-01    |proj g|=  1.97535D-03\n",
      "\n",
      "At iterate   21    f=  9.37793D-02    |proj g|=  5.00341D-03\n",
      "\n",
      "At iterate   22    f=  8.82992D-02    |proj g|=  4.90116D-03\n",
      "\n",
      "At iterate   23    f=  8.05229D-02    |proj g|=  1.56064D-02\n",
      "\n",
      "At iterate   24    f=  7.36453D-02    |proj g|=  6.51154D-03\n",
      "\n",
      "At iterate   25    f=  7.09661D-02    |proj g|=  2.37293D-03\n",
      "\n",
      "At iterate   26    f=  6.96395D-02    |proj g|=  2.63774D-03\n",
      "\n",
      "At iterate   27    f=  6.60738D-02    |proj g|=  3.33642D-03\n",
      "\n",
      "At iterate   28    f=  5.29366D-02    |proj g|=  5.18030D-03\n",
      "\n",
      "At iterate   29    f=  4.02589D-02    |proj g|=  3.43751D-03\n",
      "\n",
      "At iterate   30    f=  2.72568D-02    |proj g|=  9.60398D-03\n",
      "\n",
      "At iterate   31    f=  1.94309D-02    |proj g|=  5.05872D-03\n",
      "\n",
      "At iterate   32    f=  1.35961D-02    |proj g|=  2.00976D-03\n",
      "\n",
      "At iterate   33    f=  1.02560D-02    |proj g|=  9.60265D-04\n",
      "\n",
      "At iterate   34    f=  6.49764D-03    |proj g|=  1.49012D-03\n",
      "\n",
      "At iterate   35    f=  4.38120D-03    |proj g|=  2.32227D-03\n",
      "\n",
      "At iterate   36    f=  2.83801D-03    |proj g|=  1.00837D-03\n",
      "\n",
      "At iterate   37    f=  1.81224D-03    |proj g|=  3.73451D-04\n",
      "\n",
      "At iterate   38    f=  1.26601D-03    |proj g|=  3.63063D-04\n",
      "\n",
      "At iterate   39    f=  8.08862D-04    |proj g|=  3.25591D-04\n",
      "\n",
      "At iterate   40    f=  3.77913D-04    |proj g|=  1.49275D-04\n",
      "\n",
      "At iterate   41    f=  1.98004D-04    |proj g|=  6.81224D-05\n",
      "\n",
      "At iterate   42    f=  9.64421D-05    |proj g|=  4.11364D-05\n",
      "\n",
      "At iterate   43    f=  7.54747D-05    |proj g|=  2.57792D-05\n",
      "\n",
      "At iterate   44    f=  3.72728D-05    |proj g|=  1.13828D-05\n",
      "\n",
      "At iterate   45    f=  2.02978D-05    |proj g|=  5.95888D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     45     50      1     0     0   5.959D-06   2.030D-05\n",
      "  F =   2.0297766828424444E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.72883D-01\n",
      "\n",
      "At iterate    1    f=  6.38838D-01    |proj g|=  6.37374D-02\n",
      "\n",
      "At iterate    2    f=  6.26548D-01    |proj g|=  5.52945D-02\n",
      "\n",
      "At iterate    3    f=  5.50857D-01    |proj g|=  3.93297D-02\n",
      "\n",
      "At iterate    4    f=  5.20256D-01    |proj g|=  4.24808D-02\n",
      "\n",
      "At iterate    5    f=  4.08459D-01    |proj g|=  3.69083D-02\n",
      "\n",
      "At iterate    6    f=  3.16232D-01    |proj g|=  1.49110D-02\n",
      "\n",
      "At iterate    7    f=  2.63320D-01    |proj g|=  1.04345D-02\n",
      "\n",
      "At iterate    8    f=  2.24332D-01    |proj g|=  7.22998D-03\n",
      "\n",
      "At iterate    9    f=  1.82188D-01    |proj g|=  4.34324D-03\n",
      "\n",
      "At iterate   10    f=  1.47096D-01    |proj g|=  5.42691D-03\n",
      "\n",
      "At iterate   11    f=  1.17585D-01    |proj g|=  4.54485D-03\n",
      "\n",
      "At iterate   12    f=  1.16709D-01    |proj g|=  5.81707D-03\n",
      "\n",
      "At iterate   13    f=  1.04298D-01    |proj g|=  5.25635D-03\n",
      "\n",
      "At iterate   14    f=  8.67632D-02    |proj g|=  7.50099D-03\n",
      "\n",
      "At iterate   15    f=  7.32462D-02    |proj g|=  8.89381D-03\n",
      "\n",
      "At iterate   16    f=  6.02085D-02    |proj g|=  4.82618D-03\n",
      "\n",
      "At iterate   17    f=  4.62613D-02    |proj g|=  2.75074D-03\n",
      "\n",
      "At iterate   18    f=  2.99590D-02    |proj g|=  3.14097D-03\n",
      "\n",
      "At iterate   19    f=  1.60611D-02    |proj g|=  2.08830D-03\n",
      "\n",
      "At iterate   20    f=  7.96263D-03    |proj g|=  9.01102D-04\n",
      "\n",
      "At iterate   21    f=  3.83021D-03    |proj g|=  6.45439D-04\n",
      "\n",
      "At iterate   22    f=  1.95171D-03    |proj g|=  2.78742D-04\n",
      "\n",
      "At iterate   23    f=  1.12391D-03    |proj g|=  6.79805D-04\n",
      "\n",
      "At iterate   24    f=  3.08369D-04    |proj g|=  2.01450D-04\n",
      "\n",
      "At iterate   25    f=  2.03275D-04    |proj g|=  1.07925D-04\n",
      "\n",
      "At iterate   26    f=  1.03665D-04    |proj g|=  3.50767D-05\n",
      "\n",
      "At iterate   27    f=  5.79133D-05    |proj g|=  1.07111D-05\n",
      "\n",
      "At iterate   28    f=  3.09129D-05    |proj g|=  3.27261D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     28     31      1     0     0   3.273D-06   3.091D-05\n",
      "  F =   3.0912857271608191E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  5.74951D-02\n",
      "\n",
      "At iterate    1    f=  5.95381D-01    |proj g|=  5.29514D-02\n",
      "\n",
      "At iterate    2    f=  5.31873D-01    |proj g|=  1.36853D-01\n",
      "\n",
      "At iterate    3    f=  4.16778D-01    |proj g|=  2.74852D-02\n",
      "\n",
      "At iterate    4    f=  3.89869D-01    |proj g|=  1.81201D-02\n",
      "\n",
      "At iterate    5    f=  3.68900D-01    |proj g|=  1.57999D-02\n",
      "\n",
      "At iterate    6    f=  3.14085D-01    |proj g|=  1.45193D-02\n",
      "\n",
      "At iterate    7    f=  1.84781D-01    |proj g|=  1.69244D-02\n",
      "\n",
      "At iterate    8    f=  1.34256D-01    |proj g|=  5.93776D-03\n",
      "\n",
      "At iterate    9    f=  1.05885D-01    |proj g|=  7.56977D-03\n",
      "\n",
      "At iterate   10    f=  7.86469D-02    |proj g|=  8.79649D-03\n",
      "\n",
      "At iterate   11    f=  4.28931D-02    |proj g|=  3.72602D-03\n",
      "\n",
      "At iterate   12    f=  3.02261D-02    |proj g|=  2.93931D-03\n",
      "\n",
      "At iterate   13    f=  1.46418D-02    |proj g|=  2.04937D-03\n",
      "\n",
      "At iterate   14    f=  7.95049D-03    |proj g|=  2.36546D-03\n",
      "\n",
      "At iterate   15    f=  6.06669D-03    |proj g|=  2.93743D-03\n",
      "\n",
      "At iterate   16    f=  3.44982D-03    |proj g|=  8.96230D-04\n",
      "\n",
      "At iterate   17    f=  1.97866D-03    |proj g|=  3.14195D-04\n",
      "\n",
      "At iterate   18    f=  1.06342D-03    |proj g|=  1.47013D-04\n",
      "\n",
      "At iterate   19    f=  5.45030D-04    |proj g|=  6.70056D-05\n",
      "\n",
      "At iterate   20    f=  2.75511D-04    |proj g|=  3.52171D-05\n",
      "\n",
      "At iterate   21    f=  1.38577D-04    |proj g|=  2.04281D-05\n",
      "\n",
      "At iterate   22    f=  7.10123D-05    |proj g|=  1.20176D-05\n",
      "\n",
      "At iterate   23    f=  3.41206D-05    |proj g|=  8.79281D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     23     26      1     0     0   8.793D-06   3.412D-05\n",
      "  F =   3.4120583370980034E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 5802.49it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 550.07it/s]\n",
      "100%|██████████| 13/13 [00:00<00:00, 5803.10it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 612.11it/s]\n",
      "x does not have column names.  The check that columns are provided in the same order when training and predicting will be skipped\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 (73,)\n",
      "# of CELL:13 / min size:8 / avg size:23.8 / max size:37 / # of singleton CELL:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of clf: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1/13 [00:00<00:01,  7.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: 0 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9c378b40>, dataframe=<capsule object NULL at 0x152d55cafcf0>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7    8   \\\n",
      "0    0.440000  0.007933  1.0  0.844660  0.000211  0.0  0.0  0.921053  0.0   \n",
      "1    0.680000  0.313069  1.0  0.262136  0.234934  0.0  0.0  0.921053  0.0   \n",
      "2    0.573333  0.835073  1.0  0.262136  0.183852  0.0  1.0  0.500000  0.0   \n",
      "3    0.413333  0.285261  1.0  0.689320  0.734354  0.0  0.0  0.657895  0.0   \n",
      "4    0.346667  0.235491  1.0  0.359223  0.015409  0.0  0.0  0.921053  0.0   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...  ...   \n",
      "305  0.320000  0.120334  1.0  0.776699  0.994301  1.0  0.0  0.657895  0.0   \n",
      "306  0.680000  0.009937  1.0  0.048544  0.187230  0.0  0.0  0.368421  0.0   \n",
      "307  0.546667  0.033820  1.0  0.941748  0.290554  0.0  0.0  0.973684  0.0   \n",
      "308  0.640000  0.028643  1.0  0.203883  0.162216  0.0  0.0  0.947368  0.0   \n",
      "309  0.546667  0.066388  1.0  0.359223  0.370976  0.0  0.0  0.105263  0.0   \n",
      "\n",
      "           9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "2    0.470588  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.617647  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "..        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.617647  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "306  0.352941  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "307  0.970588  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "308  0.941176  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.147059  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
      "       1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0.,\n",
      "       0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0.,\n",
      "       0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
      "       0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
      "       1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "       1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
      "       1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1.,\n",
      "       0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
      "       1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 1., 0.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n",
      "cluster: 1 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9c378b40>, dataframe=<capsule object NULL at 0x152d55cafcf0>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7    8   \\\n",
      "0    0.440000  0.007933  1.0  0.844660  0.000211  0.0  0.0  0.921053  0.0   \n",
      "1    0.680000  0.313069  1.0  0.262136  0.234934  0.0  0.0  0.921053  0.0   \n",
      "2    0.573333  0.835073  1.0  0.262136  0.183852  0.0  1.0  0.500000  0.0   \n",
      "3    0.413333  0.285261  1.0  0.689320  0.734354  0.0  0.0  0.657895  0.0   \n",
      "4    0.346667  0.235491  1.0  0.359223  0.015409  0.0  0.0  0.921053  0.0   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...  ...   \n",
      "305  0.320000  0.120334  1.0  0.776699  0.994301  1.0  0.0  0.657895  0.0   \n",
      "306  0.680000  0.009937  1.0  0.048544  0.187230  0.0  0.0  0.368421  0.0   \n",
      "307  0.546667  0.033820  1.0  0.941748  0.290554  0.0  0.0  0.973684  0.0   \n",
      "308  0.640000  0.028643  1.0  0.203883  0.162216  0.0  0.0  0.947368  0.0   \n",
      "309  0.546667  0.066388  1.0  0.359223  0.370976  0.0  0.0  0.105263  0.0   \n",
      "\n",
      "           9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "2    0.470588  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.617647  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "..        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.617647  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "306  0.352941  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "307  0.970588  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "308  0.941176  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.147059  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
      "       1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0.,\n",
      "       0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0.,\n",
      "       0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
      "       0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
      "       1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "       1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
      "       1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1.,\n",
      "       0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
      "       1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 1., 0.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 3/13 [00:00<00:01,  6.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: 2 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9c378b40>, dataframe=<capsule object NULL at 0x152d55cafcf0>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7    8   \\\n",
      "0    0.440000  0.007933  1.0  0.844660  0.000211  0.0  0.0  0.921053  0.0   \n",
      "1    0.680000  0.313069  1.0  0.262136  0.234934  0.0  0.0  0.921053  0.0   \n",
      "2    0.573333  0.835073  1.0  0.262136  0.183852  0.0  1.0  0.500000  0.0   \n",
      "3    0.413333  0.285261  1.0  0.689320  0.734354  0.0  0.0  0.657895  0.0   \n",
      "4    0.346667  0.235491  1.0  0.359223  0.015409  0.0  0.0  0.921053  0.0   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...  ...   \n",
      "305  0.320000  0.120334  1.0  0.776699  0.994301  1.0  0.0  0.657895  0.0   \n",
      "306  0.680000  0.009937  1.0  0.048544  0.187230  0.0  0.0  0.368421  0.0   \n",
      "307  0.546667  0.033820  1.0  0.941748  0.290554  0.0  0.0  0.973684  0.0   \n",
      "308  0.640000  0.028643  1.0  0.203883  0.162216  0.0  0.0  0.947368  0.0   \n",
      "309  0.546667  0.066388  1.0  0.359223  0.370976  0.0  0.0  0.105263  0.0   \n",
      "\n",
      "           9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "2    0.470588  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.617647  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "..        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.617647  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "306  0.352941  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "307  0.970588  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "308  0.941176  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.147059  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
      "       1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0.,\n",
      "       0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0.,\n",
      "       0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
      "       0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
      "       1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "       1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
      "       1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1.,\n",
      "       0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
      "       1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 1., 0.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n",
      "cluster: 3 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9c378b40>, dataframe=<capsule object NULL at 0x152d55cafcf0>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7    8   \\\n",
      "0    0.440000  0.007933  1.0  0.844660  0.000211  0.0  0.0  0.921053  0.0   \n",
      "1    0.680000  0.313069  1.0  0.262136  0.234934  0.0  0.0  0.921053  0.0   \n",
      "2    0.573333  0.835073  1.0  0.262136  0.183852  0.0  1.0  0.500000  0.0   \n",
      "3    0.413333  0.285261  1.0  0.689320  0.734354  0.0  0.0  0.657895  0.0   \n",
      "4    0.346667  0.235491  1.0  0.359223  0.015409  0.0  0.0  0.921053  0.0   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...  ...   \n",
      "305  0.320000  0.120334  1.0  0.776699  0.994301  1.0  0.0  0.657895  0.0   \n",
      "306  0.680000  0.009937  1.0  0.048544  0.187230  0.0  0.0  0.368421  0.0   \n",
      "307  0.546667  0.033820  1.0  0.941748  0.290554  0.0  0.0  0.973684  0.0   \n",
      "308  0.640000  0.028643  1.0  0.203883  0.162216  0.0  0.0  0.947368  0.0   \n",
      "309  0.546667  0.066388  1.0  0.359223  0.370976  0.0  0.0  0.105263  0.0   \n",
      "\n",
      "           9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "2    0.470588  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.617647  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "..        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.617647  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "306  0.352941  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "307  0.970588  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "308  0.941176  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.147059  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
      "       1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0.,\n",
      "       0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0.,\n",
      "       0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
      "       0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
      "       1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "       1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
      "       1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1.,\n",
      "       0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
      "       1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 1., 0.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|███       | 4/13 [00:00<00:01,  6.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cluster: 4 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9c378b40>, dataframe=<capsule object NULL at 0x152d55cafcf0>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7    8   \\\n",
      "0    0.440000  0.007933  1.0  0.844660  0.000211  0.0  0.0  0.921053  0.0   \n",
      "1    0.680000  0.313069  1.0  0.262136  0.234934  0.0  0.0  0.921053  0.0   \n",
      "2    0.573333  0.835073  1.0  0.262136  0.183852  0.0  1.0  0.500000  0.0   \n",
      "3    0.413333  0.285261  1.0  0.689320  0.734354  0.0  0.0  0.657895  0.0   \n",
      "4    0.346667  0.235491  1.0  0.359223  0.015409  0.0  0.0  0.921053  0.0   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...  ...   \n",
      "305  0.320000  0.120334  1.0  0.776699  0.994301  1.0  0.0  0.657895  0.0   \n",
      "306  0.680000  0.009937  1.0  0.048544  0.187230  0.0  0.0  0.368421  0.0   \n",
      "307  0.546667  0.033820  1.0  0.941748  0.290554  0.0  0.0  0.973684  0.0   \n",
      "308  0.640000  0.028643  1.0  0.203883  0.162216  0.0  0.0  0.947368  0.0   \n",
      "309  0.546667  0.066388  1.0  0.359223  0.370976  0.0  0.0  0.105263  0.0   \n",
      "\n",
      "           9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "2    0.470588  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.617647  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "..        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.617647  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "306  0.352941  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "307  0.970588  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "308  0.941176  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.147059  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
      "       1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0.,\n",
      "       0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0.,\n",
      "       0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
      "       0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
      "       1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "       1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
      "       1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1.,\n",
      "       0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
      "       1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 1., 0.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 6/13 [00:00<00:00,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: 5 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9c378b40>, dataframe=<capsule object NULL at 0x152d55cafcf0>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7    8   \\\n",
      "0    0.440000  0.007933  1.0  0.844660  0.000211  0.0  0.0  0.921053  0.0   \n",
      "1    0.680000  0.313069  1.0  0.262136  0.234934  0.0  0.0  0.921053  0.0   \n",
      "2    0.573333  0.835073  1.0  0.262136  0.183852  0.0  1.0  0.500000  0.0   \n",
      "3    0.413333  0.285261  1.0  0.689320  0.734354  0.0  0.0  0.657895  0.0   \n",
      "4    0.346667  0.235491  1.0  0.359223  0.015409  0.0  0.0  0.921053  0.0   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...  ...   \n",
      "305  0.320000  0.120334  1.0  0.776699  0.994301  1.0  0.0  0.657895  0.0   \n",
      "306  0.680000  0.009937  1.0  0.048544  0.187230  0.0  0.0  0.368421  0.0   \n",
      "307  0.546667  0.033820  1.0  0.941748  0.290554  0.0  0.0  0.973684  0.0   \n",
      "308  0.640000  0.028643  1.0  0.203883  0.162216  0.0  0.0  0.947368  0.0   \n",
      "309  0.546667  0.066388  1.0  0.359223  0.370976  0.0  0.0  0.105263  0.0   \n",
      "\n",
      "           9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "2    0.470588  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.617647  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "..        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.617647  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "306  0.352941  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "307  0.970588  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "308  0.941176  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.147059  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
      "       1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0.,\n",
      "       0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0.,\n",
      "       0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
      "       0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
      "       1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "       1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
      "       1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1.,\n",
      "       0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
      "       1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 1., 0.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n",
      "cluster: 6 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9c378b40>, dataframe=<capsule object NULL at 0x152d55cafcf0>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7    8   \\\n",
      "0    0.440000  0.007933  1.0  0.844660  0.000211  0.0  0.0  0.921053  0.0   \n",
      "1    0.680000  0.313069  1.0  0.262136  0.234934  0.0  0.0  0.921053  0.0   \n",
      "2    0.573333  0.835073  1.0  0.262136  0.183852  0.0  1.0  0.500000  0.0   \n",
      "3    0.413333  0.285261  1.0  0.689320  0.734354  0.0  0.0  0.657895  0.0   \n",
      "4    0.346667  0.235491  1.0  0.359223  0.015409  0.0  0.0  0.921053  0.0   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...  ...   \n",
      "305  0.320000  0.120334  1.0  0.776699  0.994301  1.0  0.0  0.657895  0.0   \n",
      "306  0.680000  0.009937  1.0  0.048544  0.187230  0.0  0.0  0.368421  0.0   \n",
      "307  0.546667  0.033820  1.0  0.941748  0.290554  0.0  0.0  0.973684  0.0   \n",
      "308  0.640000  0.028643  1.0  0.203883  0.162216  0.0  0.0  0.947368  0.0   \n",
      "309  0.546667  0.066388  1.0  0.359223  0.370976  0.0  0.0  0.105263  0.0   \n",
      "\n",
      "           9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "2    0.470588  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.617647  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "..        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.617647  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "306  0.352941  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "307  0.970588  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "308  0.941176  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.147059  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
      "       1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0.,\n",
      "       0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0.,\n",
      "       0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
      "       0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
      "       1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "       1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
      "       1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1.,\n",
      "       0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
      "       1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 1., 0.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 8/13 [00:01<00:00,  6.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: 7 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9c378b40>, dataframe=<capsule object NULL at 0x152d55cafcf0>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7    8   \\\n",
      "0    0.440000  0.007933  1.0  0.844660  0.000211  0.0  0.0  0.921053  0.0   \n",
      "1    0.680000  0.313069  1.0  0.262136  0.234934  0.0  0.0  0.921053  0.0   \n",
      "2    0.573333  0.835073  1.0  0.262136  0.183852  0.0  1.0  0.500000  0.0   \n",
      "3    0.413333  0.285261  1.0  0.689320  0.734354  0.0  0.0  0.657895  0.0   \n",
      "4    0.346667  0.235491  1.0  0.359223  0.015409  0.0  0.0  0.921053  0.0   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...  ...   \n",
      "305  0.320000  0.120334  1.0  0.776699  0.994301  1.0  0.0  0.657895  0.0   \n",
      "306  0.680000  0.009937  1.0  0.048544  0.187230  0.0  0.0  0.368421  0.0   \n",
      "307  0.546667  0.033820  1.0  0.941748  0.290554  0.0  0.0  0.973684  0.0   \n",
      "308  0.640000  0.028643  1.0  0.203883  0.162216  0.0  0.0  0.947368  0.0   \n",
      "309  0.546667  0.066388  1.0  0.359223  0.370976  0.0  0.0  0.105263  0.0   \n",
      "\n",
      "           9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "2    0.470588  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.617647  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "..        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.617647  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "306  0.352941  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "307  0.970588  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "308  0.941176  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.147059  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
      "       1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0.,\n",
      "       0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0.,\n",
      "       0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
      "       0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
      "       1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "       1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
      "       1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1.,\n",
      "       0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
      "       1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 1., 0.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n",
      "cluster: 8 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9c378b40>, dataframe=<capsule object NULL at 0x152d55cafcf0>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7    8   \\\n",
      "0    0.440000  0.007933  1.0  0.844660  0.000211  0.0  0.0  0.921053  0.0   \n",
      "1    0.680000  0.313069  1.0  0.262136  0.234934  0.0  0.0  0.921053  0.0   \n",
      "2    0.573333  0.835073  1.0  0.262136  0.183852  0.0  1.0  0.500000  0.0   \n",
      "3    0.413333  0.285261  1.0  0.689320  0.734354  0.0  0.0  0.657895  0.0   \n",
      "4    0.346667  0.235491  1.0  0.359223  0.015409  0.0  0.0  0.921053  0.0   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...  ...   \n",
      "305  0.320000  0.120334  1.0  0.776699  0.994301  1.0  0.0  0.657895  0.0   \n",
      "306  0.680000  0.009937  1.0  0.048544  0.187230  0.0  0.0  0.368421  0.0   \n",
      "307  0.546667  0.033820  1.0  0.941748  0.290554  0.0  0.0  0.973684  0.0   \n",
      "308  0.640000  0.028643  1.0  0.203883  0.162216  0.0  0.0  0.947368  0.0   \n",
      "309  0.546667  0.066388  1.0  0.359223  0.370976  0.0  0.0  0.105263  0.0   \n",
      "\n",
      "           9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "2    0.470588  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.617647  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "..        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.617647  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "306  0.352941  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "307  0.970588  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "308  0.941176  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.147059  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
      "       1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0.,\n",
      "       0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0.,\n",
      "       0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
      "       0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
      "       1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "       1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
      "       1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1.,\n",
      "       0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
      "       1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 1., 0.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 10/13 [00:01<00:00,  6.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: 9 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9c378b40>, dataframe=<capsule object NULL at 0x152d55cafcf0>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7    8   \\\n",
      "0    0.440000  0.007933  1.0  0.844660  0.000211  0.0  0.0  0.921053  0.0   \n",
      "1    0.680000  0.313069  1.0  0.262136  0.234934  0.0  0.0  0.921053  0.0   \n",
      "2    0.573333  0.835073  1.0  0.262136  0.183852  0.0  1.0  0.500000  0.0   \n",
      "3    0.413333  0.285261  1.0  0.689320  0.734354  0.0  0.0  0.657895  0.0   \n",
      "4    0.346667  0.235491  1.0  0.359223  0.015409  0.0  0.0  0.921053  0.0   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...  ...   \n",
      "305  0.320000  0.120334  1.0  0.776699  0.994301  1.0  0.0  0.657895  0.0   \n",
      "306  0.680000  0.009937  1.0  0.048544  0.187230  0.0  0.0  0.368421  0.0   \n",
      "307  0.546667  0.033820  1.0  0.941748  0.290554  0.0  0.0  0.973684  0.0   \n",
      "308  0.640000  0.028643  1.0  0.203883  0.162216  0.0  0.0  0.947368  0.0   \n",
      "309  0.546667  0.066388  1.0  0.359223  0.370976  0.0  0.0  0.105263  0.0   \n",
      "\n",
      "           9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "2    0.470588  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.617647  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "..        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.617647  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "306  0.352941  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "307  0.970588  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "308  0.941176  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.147059  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
      "       1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0.,\n",
      "       0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0.,\n",
      "       0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
      "       0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
      "       1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "       1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
      "       1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1.,\n",
      "       0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
      "       1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 1., 0.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n",
      "cluster: 10 model: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▍ | 11/13 [00:01<00:00,  6.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9c378b40>, dataframe=<capsule object NULL at 0x152d55cafcf0>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7    8   \\\n",
      "0    0.440000  0.007933  1.0  0.844660  0.000211  0.0  0.0  0.921053  0.0   \n",
      "1    0.680000  0.313069  1.0  0.262136  0.234934  0.0  0.0  0.921053  0.0   \n",
      "2    0.573333  0.835073  1.0  0.262136  0.183852  0.0  1.0  0.500000  0.0   \n",
      "3    0.413333  0.285261  1.0  0.689320  0.734354  0.0  0.0  0.657895  0.0   \n",
      "4    0.346667  0.235491  1.0  0.359223  0.015409  0.0  0.0  0.921053  0.0   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...  ...   \n",
      "305  0.320000  0.120334  1.0  0.776699  0.994301  1.0  0.0  0.657895  0.0   \n",
      "306  0.680000  0.009937  1.0  0.048544  0.187230  0.0  0.0  0.368421  0.0   \n",
      "307  0.546667  0.033820  1.0  0.941748  0.290554  0.0  0.0  0.973684  0.0   \n",
      "308  0.640000  0.028643  1.0  0.203883  0.162216  0.0  0.0  0.947368  0.0   \n",
      "309  0.546667  0.066388  1.0  0.359223  0.370976  0.0  0.0  0.105263  0.0   \n",
      "\n",
      "           9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "2    0.470588  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.617647  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "..        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.617647  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "306  0.352941  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "307  0.970588  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "308  0.941176  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.147059  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
      "       1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0.,\n",
      "       0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0.,\n",
      "       0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
      "       0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
      "       1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "       1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
      "       1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1.,\n",
      "       0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
      "       1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 1., 0.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n",
      "cluster: 11 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9c378b40>, dataframe=<capsule object NULL at 0x152d55cafcf0>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7    8   \\\n",
      "0    0.440000  0.007933  1.0  0.844660  0.000211  0.0  0.0  0.921053  0.0   \n",
      "1    0.680000  0.313069  1.0  0.262136  0.234934  0.0  0.0  0.921053  0.0   \n",
      "2    0.573333  0.835073  1.0  0.262136  0.183852  0.0  1.0  0.500000  0.0   \n",
      "3    0.413333  0.285261  1.0  0.689320  0.734354  0.0  0.0  0.657895  0.0   \n",
      "4    0.346667  0.235491  1.0  0.359223  0.015409  0.0  0.0  0.921053  0.0   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...  ...   \n",
      "305  0.320000  0.120334  1.0  0.776699  0.994301  1.0  0.0  0.657895  0.0   \n",
      "306  0.680000  0.009937  1.0  0.048544  0.187230  0.0  0.0  0.368421  0.0   \n",
      "307  0.546667  0.033820  1.0  0.941748  0.290554  0.0  0.0  0.973684  0.0   \n",
      "308  0.640000  0.028643  1.0  0.203883  0.162216  0.0  0.0  0.947368  0.0   \n",
      "309  0.546667  0.066388  1.0  0.359223  0.370976  0.0  0.0  0.105263  0.0   \n",
      "\n",
      "           9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "2    0.470588  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.617647  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "..        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.617647  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "306  0.352941  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "307  0.970588  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "308  0.941176  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.147059  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
      "       1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0.,\n",
      "       0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0.,\n",
      "       0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
      "       0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
      "       1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "       1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
      "       1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1.,\n",
      "       0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
      "       1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 1., 0.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00,  6.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: 12 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9c378b40>, dataframe=<capsule object NULL at 0x152d55cafcf0>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7    8   \\\n",
      "0    0.440000  0.007933  1.0  0.844660  0.000211  0.0  0.0  0.921053  0.0   \n",
      "1    0.680000  0.313069  1.0  0.262136  0.234934  0.0  0.0  0.921053  0.0   \n",
      "2    0.573333  0.835073  1.0  0.262136  0.183852  0.0  1.0  0.500000  0.0   \n",
      "3    0.413333  0.285261  1.0  0.689320  0.734354  0.0  0.0  0.657895  0.0   \n",
      "4    0.346667  0.235491  1.0  0.359223  0.015409  0.0  0.0  0.921053  0.0   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...  ...   \n",
      "305  0.320000  0.120334  1.0  0.776699  0.994301  1.0  0.0  0.657895  0.0   \n",
      "306  0.680000  0.009937  1.0  0.048544  0.187230  0.0  0.0  0.368421  0.0   \n",
      "307  0.546667  0.033820  1.0  0.941748  0.290554  0.0  0.0  0.973684  0.0   \n",
      "308  0.640000  0.028643  1.0  0.203883  0.162216  0.0  0.0  0.947368  0.0   \n",
      "309  0.546667  0.066388  1.0  0.359223  0.370976  0.0  0.0  0.105263  0.0   \n",
      "\n",
      "           9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "2    0.470588  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.617647  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "..        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.617647  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "306  0.352941  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "307  0.970588  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "308  0.941176  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.147059  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
      "       1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0.,\n",
      "       0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0.,\n",
      "       0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
      "       0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
      "       1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
      "       0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "       1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
      "       1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1.,\n",
      "       0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
      "       1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 1., 0.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13 [00:00<?, ?it/s] This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.05175D-01\n",
      "\n",
      "At iterate    1    f=  6.70225D-01    |proj g|=  4.98568D-02\n",
      "\n",
      "At iterate    2    f=  5.71966D-01    |proj g|=  4.70821D-02\n",
      "\n",
      "At iterate    3    f=  3.79314D-01    |proj g|=  2.06836D-02\n",
      "\n",
      "At iterate    4    f=  2.92132D-01    |proj g|=  1.65169D-02\n",
      "\n",
      "At iterate    5    f=  1.58596D-01    |proj g|=  9.54870D-03\n",
      "\n",
      "At iterate    6    f=  9.79834D-02    |proj g|=  4.21703D-03\n",
      "\n",
      "At iterate    7    f=  6.03905D-02    |proj g|=  7.23907D-03\n",
      "\n",
      "At iterate    8    f=  4.25446D-02    |proj g|=  4.50375D-03\n",
      "\n",
      "At iterate    9    f=  2.65121D-02    |proj g|=  3.04734D-03\n",
      "\n",
      "At iterate   10    f=  1.55671D-02    |proj g|=  2.88700D-03\n",
      "\n",
      "At iterate   11    f=  1.29292D-02    |proj g|=  4.02961D-03\n",
      "\n",
      "At iterate   12    f=  1.17262D-02    |proj g|=  8.28587D-03\n",
      "\n",
      "At iterate   13    f=  5.84939D-03    |proj g|=  1.65540D-03\n",
      "\n",
      "At iterate   14    f=  4.61863D-03    |proj g|=  9.44830D-04\n",
      "\n",
      "At iterate   15    f=  3.02966D-03    |proj g|=  1.15200D-03\n",
      "\n",
      "At iterate   16    f=  1.73760D-03    |proj g|=  7.88546D-04\n",
      "\n",
      "At iterate   17    f=  8.29992D-04    |proj g|=  1.52368D-04\n",
      "\n",
      "At iterate   18    f=  4.99755D-04    |proj g|=  1.08363D-04\n",
      "\n",
      "At iterate   19    f=  2.30806D-04    |proj g|=  4.28775D-05\n",
      "\n",
      "At iterate   20    f=  1.21739D-04    |proj g|=  2.16830D-05\n",
      "\n",
      "At iterate   21    f=  6.01377D-05    |proj g|=  1.35568D-05\n",
      "\n",
      "At iterate   22    f=  3.12685D-05    |proj g|=  6.28702D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     22     26      1     0     0   6.287D-06   3.127D-05\n",
      "  F =   3.1268522328296676E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  2.04371D-01\n",
      "\n",
      "At iterate    1    f=  6.12827D-01    |proj g|=  5.59407D-02\n",
      "\n",
      "At iterate    2    f=  5.31511D-01    |proj g|=  7.46755D-02\n",
      "\n",
      "At iterate    3    f=  3.85963D-01    |proj g|=  1.06872D-01\n",
      "\n",
      "At iterate    4    f=  2.89287D-01    |proj g|=  6.59646D-02\n",
      "\n",
      "At iterate    5    f=  2.22845D-01    |proj g|=  3.13515D-02\n",
      "\n",
      "At iterate    6    f=  1.82016D-01    |proj g|=  1.11654D-02\n",
      "\n",
      "At iterate    7    f=  1.53625D-01    |proj g|=  6.68976D-03\n",
      "\n",
      "At iterate    8    f=  1.29552D-01    |proj g|=  1.07382D-02\n",
      "\n",
      "At iterate    9    f=  9.60826D-02    |proj g|=  1.03284D-02\n",
      "\n",
      "At iterate   10    f=  5.00359D-02    |proj g|=  4.11150D-03\n",
      "\n",
      "At iterate   11    f=  3.57364D-02    |proj g|=  1.79608D-03\n",
      "\n",
      "At iterate   12    f=  2.47642D-02    |proj g|=  1.59381D-03\n",
      "\n",
      "At iterate   13    f=  2.33243D-02    |proj g|=  6.75022D-03\n",
      "\n",
      "At iterate   14    f=  1.29959D-02    |proj g|=  2.84134D-03\n",
      "\n",
      "At iterate   15    f=  6.41986D-03    |proj g|=  1.16919D-03\n",
      "\n",
      "At iterate   16    f=  3.02294D-03    |proj g|=  3.79933D-04\n",
      "\n",
      "At iterate   17    f=  1.53654D-03    |proj g|=  1.97229D-04\n",
      "\n",
      "At iterate   18    f=  7.57185D-04    |proj g|=  1.19139D-04\n",
      "\n",
      "At iterate   19    f=  3.88687D-04    |proj g|=  4.20689D-05\n",
      "\n",
      "At iterate   20    f=  1.47473D-04    |proj g|=  5.10751D-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " This problem is unconstrained.\n",
      " 31%|███       | 4/13 [00:00<00:00, 30.10it/s] This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   21    f=  8.79557D-05    |proj g|=  2.21104D-05\n",
      "\n",
      "At iterate   22    f=  4.78217D-05    |proj g|=  9.55788D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     22     27      1     0     0   9.558D-06   4.782D-05\n",
      "  F =   4.7821703414963512E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.30212D-01\n",
      "\n",
      "At iterate    1    f=  6.60880D-01    |proj g|=  6.60794D-02\n",
      "\n",
      "At iterate    2    f=  5.88762D-01    |proj g|=  5.65362D-02\n",
      "\n",
      "At iterate    3    f=  3.31215D-01    |proj g|=  3.34004D-02\n",
      "\n",
      "At iterate    4    f=  2.67397D-01    |proj g|=  2.38281D-02\n",
      "\n",
      "At iterate    5    f=  1.99683D-01    |proj g|=  1.61221D-02\n",
      "\n",
      "At iterate    6    f=  1.46198D-01    |proj g|=  1.20732D-02\n",
      "\n",
      "At iterate    7    f=  9.88785D-02    |proj g|=  9.77101D-03\n",
      "\n",
      "At iterate    8    f=  7.55449D-02    |proj g|=  1.27023D-02\n",
      "\n",
      "At iterate    9    f=  5.73338D-02    |proj g|=  6.73586D-03\n",
      "\n",
      "At iterate   10    f=  4.35497D-02    |proj g|=  3.43418D-03\n",
      "\n",
      "At iterate   11    f=  3.61040D-02    |proj g|=  3.79059D-03\n",
      "\n",
      "At iterate   12    f=  1.95930D-02    |proj g|=  7.25565D-03\n",
      "\n",
      "At iterate   13    f=  1.57819D-02    |proj g|=  3.78230D-03\n",
      "\n",
      "At iterate   14    f=  1.19224D-02    |proj g|=  1.60691D-03\n",
      "\n",
      "At iterate   15    f=  8.30394D-03    |proj g|=  1.81575D-03\n",
      "\n",
      "At iterate   16    f=  4.30544D-03    |proj g|=  7.70740D-04\n",
      "\n",
      "At iterate   17    f=  2.18552D-03    |proj g|=  5.63187D-04\n",
      "\n",
      "At iterate   18    f=  1.11907D-03    |proj g|=  2.65519D-04\n",
      "\n",
      "At iterate   19    f=  5.69221D-04    |proj g|=  1.18939D-04\n",
      "\n",
      "At iterate   20    f=  2.92938D-04    |proj g|=  5.03326D-05\n",
      "\n",
      "At iterate   21    f=  1.36456D-04    |proj g|=  4.84933D-05\n",
      "\n",
      "At iterate   22    f=  3.77873D-05    |proj g|=  9.45395D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     22     27      1     0     0   9.454D-06   3.779D-05\n",
      "  F =   3.7787338466975221E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  2.33759D-01\n",
      "\n",
      "At iterate    1    f=  5.82826D-01    |proj g|=  4.43110D-02\n",
      "\n",
      "At iterate    2    f=  4.88259D-01    |proj g|=  7.63365D-02\n",
      "\n",
      "At iterate    3    f=  3.34285D-01    |proj g|=  1.07921D-01\n",
      "\n",
      "At iterate    4    f=  2.38913D-01    |proj g|=  6.81664D-02\n",
      "\n",
      "At iterate    5    f=  1.71567D-01    |proj g|=  3.39335D-02\n",
      "\n",
      "At iterate    6    f=  1.31580D-01    |proj g|=  1.52027D-02\n",
      "\n",
      "At iterate    7    f=  1.01317D-01    |proj g|=  9.54116D-03\n",
      "\n",
      "At iterate    8    f=  7.04613D-02    |proj g|=  7.92812D-03\n",
      "\n",
      "At iterate    9    f=  2.96654D-02    |proj g|=  3.10279D-03\n",
      "\n",
      "At iterate   10    f=  1.35221D-02    |proj g|=  1.48454D-03\n",
      "\n",
      "At iterate   11    f=  6.53352D-03    |proj g|=  7.39364D-04\n",
      "\n",
      "At iterate   12    f=  3.18914D-03    |proj g|=  4.14531D-04\n",
      "\n",
      "At iterate   13    f=  2.61504D-03    |proj g|=  1.48092D-03\n",
      "\n",
      "At iterate   14    f=  1.37617D-03    |proj g|=  4.91695D-04\n",
      "\n",
      "At iterate   15    f=  7.40655D-04    |proj g|=  2.12174D-04\n",
      "\n",
      "At iterate   16    f=  3.74542D-04    |proj g|=  1.13418D-04\n",
      "\n",
      "At iterate   17    f=  1.90019D-04    |proj g|=  5.85931D-05\n",
      "\n",
      "At iterate   18    f=  9.53839D-05    |proj g|=  2.93273D-05\n",
      "\n",
      "At iterate   19    f=  4.79070D-05    |proj g|=  1.45687D-05\n",
      "\n",
      "At iterate   20    f=  2.40151D-05    |proj g|=  7.21386D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     20     25      1     0     0   7.214D-06   2.402D-05\n",
      "  F =   2.4015117238792479E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  2.24636D-01\n",
      "\n",
      "At iterate    1    f=  6.00225D-01    |proj g|=  4.66774D-02\n",
      "\n",
      "At iterate    2    f=  5.24014D-01    |proj g|=  7.14379D-02\n",
      "\n",
      "At iterate    3    f=  3.88049D-01    |proj g|=  9.66917D-02\n",
      "\n",
      "At iterate    4    f=  3.15813D-01    |proj g|=  6.00884D-02\n",
      "\n",
      "At iterate    5    f=  2.59342D-01    |proj g|=  2.46184D-02\n",
      "\n",
      "At iterate    6    f=  2.28693D-01    |proj g|=  1.07452D-02\n",
      "\n",
      "At iterate    7    f=  2.04859D-01    |proj g|=  1.72683D-02\n",
      "\n",
      "At iterate    8    f=  1.74711D-01    |proj g|=  2.22670D-02\n",
      "\n",
      "At iterate    9    f=  1.08415D-01    |proj g|=  1.61839D-02\n",
      "\n",
      "At iterate   10    f=  5.64686D-02    |proj g|=  7.24511D-03\n",
      "\n",
      "At iterate   11    f=  3.32612D-02    |proj g|=  3.64370D-03\n",
      "\n",
      "At iterate   12    f=  1.61674D-02    |proj g|=  4.13385D-03\n",
      "\n",
      "At iterate   13    f=  1.28630D-02    |proj g|=  7.31605D-03\n",
      "\n",
      "At iterate   14    f=  7.36156D-03    |proj g|=  1.74027D-03\n",
      "\n",
      "At iterate   15    f=  4.76650D-03    |proj g|=  7.32087D-04\n",
      "\n",
      "At iterate   16    f=  2.68832D-03    |proj g|=  6.96354D-04\n",
      "\n",
      "At iterate   17    f=  1.38677D-03    |proj g|=  3.97451D-04\n",
      "\n",
      "At iterate   18    f=  6.99581D-04    |proj g|=  2.12805D-04\n",
      "\n",
      "At iterate   19    f=  3.54403D-04    |proj g|=  1.13791D-04\n",
      "\n",
      "At iterate   20    f=  1.78827D-04    |proj g|=  5.98252D-05\n",
      "\n",
      "At iterate   21    f=  9.02159D-05    |proj g|=  3.13127D-05\n",
      "\n",
      "At iterate   22    f=  4.54482D-05    |proj g|=  1.62930D-05\n",
      "\n",
      "At iterate   23    f=  2.28770D-05    |proj g|=  8.48899D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     23     27      1     0     0   8.489D-06   2.288D-05\n",
      "  F =   2.2877048579269650E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.49903D-01\n",
      "\n",
      "At iterate    1    f=  6.49331D-01    |proj g|=  5.53754D-02\n",
      "\n",
      "At iterate    2    f=  5.64556D-01    |proj g|=  4.59956D-02\n",
      "\n",
      "At iterate    3    f=  3.30700D-01    |proj g|=  2.84876D-02\n",
      "\n",
      "At iterate    4    f=  2.58223D-01    |proj g|=  1.65225D-02\n",
      "\n",
      "At iterate    5    f=  1.91131D-01    |proj g|=  5.87077D-03\n",
      "\n",
      "At iterate    6    f=  1.50502D-01    |proj g|=  3.09199D-03\n",
      "\n",
      "At iterate    7    f=  1.13844D-01    |proj g|=  7.06550D-03\n",
      "\n",
      "At iterate    8    f=  9.04384D-02    |proj g|=  6.56998D-03\n",
      "\n",
      "At iterate    9    f=  7.54882D-02    |proj g|=  8.68629D-03\n",
      "\n",
      "At iterate   10    f=  6.48237D-02    |proj g|=  3.70884D-03\n",
      "\n",
      "At iterate   11    f=  5.75206D-02    |proj g|=  3.18868D-03\n",
      "\n",
      "At iterate   12    f=  5.58865D-02    |proj g|=  5.42927D-03\n",
      "\n",
      "At iterate   13    f=  5.15485D-02    |proj g|=  2.12588D-03\n",
      "\n",
      "At iterate   14    f=  4.86922D-02    |proj g|=  2.39262D-03\n",
      "\n",
      "At iterate   15    f=  4.56694D-02    |proj g|=  3.32608D-03\n",
      "\n",
      "At iterate   16    f=  3.94919D-02    |proj g|=  3.26822D-03\n",
      "\n",
      "At iterate   17    f=  3.43420D-02    |proj g|=  4.83233D-03\n",
      "\n",
      "At iterate   18    f=  3.02645D-02    |proj g|=  2.23494D-03\n",
      "\n",
      "At iterate   19    f=  2.93642D-02    |proj g|=  1.52494D-03\n",
      "\n",
      "At iterate   20    f=  2.85894D-02    |proj g|=  8.04344D-04\n",
      "\n",
      "At iterate   21    f=  2.78595D-02    |proj g|=  1.34120D-03\n",
      "\n",
      "At iterate   22    f=  2.70665D-02    |proj g|=  1.64321D-03\n",
      "\n",
      "At iterate   23    f=  2.61852D-02    |proj g|=  4.35931D-03\n",
      "\n",
      "At iterate   24    f=  2.44410D-02    |proj g|=  5.61784D-04\n",
      "\n",
      "At iterate   25    f=  2.40436D-02    |proj g|=  8.09783D-04\n",
      "\n",
      "At iterate   26    f=  2.37474D-02    |proj g|=  1.09293D-03\n",
      "\n",
      "At iterate   27    f=  2.31815D-02    |proj g|=  1.03956D-03\n",
      "\n",
      "At iterate   28    f=  2.11082D-02    |proj g|=  1.83374D-03\n",
      "\n",
      "At iterate   29    f=  1.93554D-02    |proj g|=  3.48782D-03\n",
      "\n",
      "At iterate   30    f=  1.15599D-02    |proj g|=  1.67425D-03\n",
      "\n",
      "At iterate   31    f=  6.07585D-03    |proj g|=  6.74447D-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/discrete/discrete_model.py:1819: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   32    f=  4.06903D-03    |proj g|=  1.04650D-03\n",
      "\n",
      "At iterate   33    f=  1.39651D-03    |proj g|=  3.11691D-04\n",
      "\n",
      "At iterate   34    f=  8.85241D-04    |proj g|=  1.26088D-04\n",
      "\n",
      "At iterate   35    f=  4.14456D-04    |proj g|=  1.11720D-04\n",
      "\n",
      "At iterate   36    f=  4.03335D-04    |proj g|=  7.80048D-05\n",
      "\n",
      "At iterate   37    f=  2.38563D-04    |proj g|=  4.34897D-05\n",
      "\n",
      "At iterate   38    f=  1.10506D-04    |proj g|=  1.49334D-05\n",
      "\n",
      "At iterate   39    f=  5.63082D-05    |proj g|=  1.04928D-05\n",
      "\n",
      "At iterate   40    f=  2.58002D-05    |proj g|=  3.15869D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     40     47      1     0     0   3.159D-06   2.580D-05\n",
      "  F =   2.5800157547481172E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  6.98719D-02\n",
      "\n",
      "At iterate    1    f=  6.81439D-01    |proj g|=  9.71679D-02\n",
      "\n",
      "At iterate    2    f=  6.19318D-01    |proj g|=  9.26410D-02\n",
      "\n",
      "At iterate    3    f=  3.22480D-01    |proj g|=  2.80405D-02\n",
      "\n",
      "At iterate    4    f=  2.55315D-01    |proj g|=  1.76357D-02\n",
      "\n",
      "At iterate    5    f=  1.57544D-01    |proj g|=  1.63021D-02\n",
      "\n",
      "At iterate    6    f=  1.12820D-01    |proj g|=  1.10243D-02\n",
      "\n",
      "At iterate    7    f=  6.81537D-02    |proj g|=  6.93387D-03\n",
      "\n",
      "At iterate    8    f=  4.21270D-02    |proj g|=  2.89166D-03\n",
      "\n",
      "At iterate    9    f=  2.93075D-02    |proj g|=  2.29198D-03\n",
      "\n",
      "At iterate   10    f=  1.57917D-02    |proj g|=  2.87438D-03\n",
      "\n",
      "At iterate   11    f=  7.77745D-03    |proj g|=  1.21059D-03\n",
      "\n",
      "At iterate   12    f=  4.47007D-03    |proj g|=  2.79554D-03\n",
      "\n",
      "At iterate   13    f=  1.13408D-03    |proj g|=  2.20022D-04\n",
      "\n",
      "At iterate   14    f=  7.97245D-04    |proj g|=  1.26264D-04\n",
      "\n",
      "At iterate   15    f=  3.57911D-04    |proj g|=  4.42887D-05\n",
      "\n",
      "At iterate   16    f=  1.89961D-04    |proj g|=  2.28568D-05\n",
      "\n",
      "At iterate   17    f=  9.35144D-05    |proj g|=  1.06449D-05\n",
      "\n",
      "At iterate   18    f=  4.71861D-05    |proj g|=  6.10102D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     18     21      1     0     0   6.101D-06   4.719D-05\n",
      "  F =   4.7186141743937353E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.59555D-01\n",
      "\n",
      "At iterate    1    f=  6.41365D-01    |proj g|=  6.16368D-02\n",
      "\n",
      "At iterate    2    f=  5.32552D-01    |proj g|=  4.67671D-02\n",
      "\n",
      "At iterate    3    f=  3.11921D-01    |proj g|=  4.35991D-02\n",
      "\n",
      "At iterate    4    f=  2.34599D-01    |proj g|=  2.81208D-02\n",
      "\n",
      "At iterate    5    f=  1.55710D-01    |proj g|=  1.41311D-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " 62%|██████▏   | 8/13 [00:00<00:00, 29.36it/s] This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " 85%|████████▍ | 11/13 [00:00<00:00, 29.56it/s] This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate    6    f=  1.06679D-01    |proj g|=  7.44090D-03\n",
      "\n",
      "At iterate    7    f=  6.27239D-02    |proj g|=  3.67490D-03\n",
      "\n",
      "At iterate    8    f=  3.14359D-02    |proj g|=  2.96408D-03\n",
      "\n",
      "At iterate    9    f=  2.43962D-02    |proj g|=  4.05643D-03\n",
      "\n",
      "At iterate   10    f=  7.64558D-03    |proj g|=  8.38775D-04\n",
      "\n",
      "At iterate   11    f=  4.67934D-03    |proj g|=  5.81349D-04\n",
      "\n",
      "At iterate   12    f=  4.31167D-03    |proj g|=  1.94958D-03\n",
      "\n",
      "At iterate   13    f=  1.95265D-03    |proj g|=  5.23159D-04\n",
      "\n",
      "At iterate   14    f=  1.09413D-03    |proj g|=  2.77354D-04\n",
      "\n",
      "At iterate   15    f=  5.48306D-04    |proj g|=  1.36360D-04\n",
      "\n",
      "At iterate   16    f=  2.83269D-04    |proj g|=  6.99181D-05\n",
      "\n",
      "At iterate   17    f=  1.43652D-04    |proj g|=  3.54506D-05\n",
      "\n",
      "At iterate   18    f=  7.30252D-05    |proj g|=  1.82730D-05\n",
      "\n",
      "At iterate   19    f=  3.69577D-05    |proj g|=  9.46736D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     19     24      1     0     0   9.467D-06   3.696D-05\n",
      "  F =   3.6957679691730421E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.90634D-01\n",
      "\n",
      "At iterate    1    f=  6.21542D-01    |proj g|=  6.72563D-02\n",
      "\n",
      "At iterate    2    f=  5.19053D-01    |proj g|=  4.58916D-02\n",
      "\n",
      "At iterate    3    f=  3.32701D-01    |proj g|=  5.02680D-02\n",
      "\n",
      "At iterate    4    f=  2.46352D-01    |proj g|=  3.29069D-02\n",
      "\n",
      "At iterate    5    f=  1.61352D-01    |proj g|=  1.63755D-02\n",
      "\n",
      "At iterate    6    f=  1.09566D-01    |proj g|=  9.66730D-03\n",
      "\n",
      "At iterate    7    f=  6.88623D-02    |proj g|=  5.20913D-03\n",
      "\n",
      "At iterate    8    f=  4.06188D-02    |proj g|=  2.10002D-03\n",
      "\n",
      "At iterate    9    f=  2.29983D-02    |proj g|=  3.08092D-03\n",
      "\n",
      "At iterate   10    f=  1.51541D-02    |proj g|=  4.59217D-03\n",
      "\n",
      "At iterate   11    f=  8.32566D-03    |proj g|=  2.01709D-03\n",
      "\n",
      "At iterate   12    f=  4.60184D-03    |proj g|=  4.38762D-04\n",
      "\n",
      "At iterate   13    f=  3.00116D-03    |proj g|=  6.59205D-04\n",
      "\n",
      "At iterate   14    f=  2.99413D-03    |proj g|=  2.81536D-03\n",
      "\n",
      "At iterate   15    f=  1.16360D-03    |proj g|=  1.03626D-03\n",
      "\n",
      "At iterate   16    f=  7.53080D-04    |proj g|=  6.04568D-04\n",
      "\n",
      "At iterate   17    f=  4.41537D-04    |proj g|=  2.62370D-04\n",
      "\n",
      "At iterate   18    f=  2.88954D-04    |proj g|=  1.08768D-04\n",
      "\n",
      "At iterate   19    f=  1.81376D-04    |proj g|=  5.73707D-05\n",
      "\n",
      "At iterate   20    f=  1.06681D-04    |proj g|=  3.86853D-05\n",
      "\n",
      "At iterate   21    f=  5.18352D-05    |proj g|=  1.61076D-05\n",
      "\n",
      "At iterate   22    f=  2.67362D-05    |proj g|=  1.29131D-05\n",
      "\n",
      "At iterate   23    f=  1.98075D-05    |proj g|=  5.47351D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     23     28      1     0     0   5.474D-06   1.981D-05\n",
      "  F =   1.9807512267270204E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.76066D-01\n",
      "\n",
      "At iterate    1    f=  6.36820D-01    |proj g|=  4.60556D-02\n",
      "\n",
      "At iterate    2    f=  5.61766D-01    |proj g|=  4.24249D-02\n",
      "\n",
      "At iterate    3    f=  3.80872D-01    |proj g|=  7.09783D-02\n",
      "\n",
      "At iterate    4    f=  3.16396D-01    |proj g|=  4.67025D-02\n",
      "\n",
      "At iterate    5    f=  2.51155D-01    |proj g|=  1.95277D-02\n",
      "\n",
      "At iterate    6    f=  2.10355D-01    |proj g|=  8.32676D-03\n",
      "\n",
      "At iterate    7    f=  1.71684D-01    |proj g|=  1.07797D-02\n",
      "\n",
      "At iterate    8    f=  1.22221D-01    |proj g|=  6.94047D-03\n",
      "\n",
      "At iterate    9    f=  6.24137D-02    |proj g|=  8.99388D-03\n",
      "\n",
      "At iterate   10    f=  3.41633D-02    |proj g|=  3.72332D-03\n",
      "\n",
      "At iterate   11    f=  1.81473D-02    |proj g|=  2.37753D-03\n",
      "\n",
      "At iterate   12    f=  1.55453D-02    |proj g|=  6.51426D-03\n",
      "\n",
      "At iterate   13    f=  7.52948D-03    |proj g|=  2.03496D-03\n",
      "\n",
      "At iterate   14    f=  4.22496D-03    |proj g|=  1.10373D-03\n",
      "\n",
      "At iterate   15    f=  2.13009D-03    |proj g|=  5.13853D-04\n",
      "\n",
      "At iterate   16    f=  1.10673D-03    |proj g|=  2.48099D-04\n",
      "\n",
      "At iterate   17    f=  5.65747D-04    |proj g|=  1.18008D-04\n",
      "\n",
      "At iterate   18    f=  2.90682D-04    |proj g|=  5.64053D-05\n",
      "\n",
      "At iterate   19    f=  1.49077D-04    |proj g|=  2.66762D-05\n",
      "\n",
      "At iterate   20    f=  7.65165D-05    |proj g|=  1.24910D-05\n",
      "\n",
      "At iterate   21    f=  3.92269D-05    |proj g|=  6.04045D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     21     26      1     0     0   6.040D-06   3.923D-05\n",
      "  F =   3.9226945092584345E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.44468D-01\n",
      "\n",
      "At iterate    1    f=  6.58504D-01    |proj g|=  5.66807D-02\n",
      "\n",
      "At iterate    2    f=  5.88002D-01    |proj g|=  5.29245D-02\n",
      "\n",
      "At iterate    3    f=  3.91827D-01    |proj g|=  3.74780D-02\n",
      "\n",
      "At iterate    4    f=  3.28685D-01    |proj g|=  2.54965D-02\n",
      "\n",
      "At iterate    5    f=  2.59219D-01    |proj g|=  1.13768D-02\n",
      "\n",
      "At iterate    6    f=  2.20640D-01    |proj g|=  6.58543D-03\n",
      "\n",
      "At iterate    7    f=  1.84397D-01    |proj g|=  4.96285D-03\n",
      "\n",
      "At iterate    8    f=  1.36201D-01    |proj g|=  3.69430D-03\n",
      "\n",
      "At iterate    9    f=  9.15981D-02    |proj g|=  6.22548D-03\n",
      "\n",
      "At iterate   10    f=  6.46381D-02    |proj g|=  7.21896D-03\n",
      "\n",
      "At iterate   11    f=  3.47189D-02    |proj g|=  7.40225D-03\n",
      "\n",
      "At iterate   12    f=  3.28566D-02    |proj g|=  5.20443D-03\n",
      "\n",
      "At iterate   13    f=  2.61602D-02    |proj g|=  4.04125D-03\n",
      "\n",
      "At iterate   14    f=  1.79157D-02    |proj g|=  2.74629D-03\n",
      "\n",
      "At iterate   15    f=  8.80447D-03    |proj g|=  1.35874D-03\n",
      "\n",
      "At iterate   16    f=  4.57584D-03    |proj g|=  7.40163D-04\n",
      "\n",
      "At iterate   17    f=  2.26537D-03    |proj g|=  3.39955D-04\n",
      "\n",
      "At iterate   18    f=  1.08125D-03    |proj g|=  1.63624D-04\n",
      "\n",
      "At iterate   19    f=  4.82488D-04    |proj g|=  8.99989D-05\n",
      "\n",
      "At iterate   20    f=  2.68014D-04    |proj g|=  4.48432D-05\n",
      "\n",
      "At iterate   21    f=  1.27799D-04    |proj g|=  2.18739D-05\n",
      "\n",
      "At iterate   22    f=  6.44876D-05    |proj g|=  1.03652D-05\n",
      "\n",
      "At iterate   23    f=  4.67856D-05    |proj g|=  3.66076D-05\n",
      "\n",
      "At iterate   24    f=  1.25059D-05    |proj g|=  4.20941D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     24     29      1     0     0   4.209D-06   1.251D-05\n",
      "  F =   1.2505888873153833E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  6.54271D-02\n",
      "\n",
      "At iterate    1    f=  5.88653D-01    |proj g|=  5.69113D-02\n",
      "\n",
      "At iterate    2    f=  5.01573D-01    |proj g|=  1.49756D-01\n",
      "\n",
      "At iterate    3    f=  3.77840D-01    |proj g|=  3.19563D-02\n",
      "\n",
      "At iterate    4    f=  3.49038D-01    |proj g|=  2.01319D-02\n",
      "\n",
      "At iterate    5    f=  3.27384D-01    |proj g|=  1.61247D-02\n",
      "\n",
      "At iterate    6    f=  2.85481D-01    |proj g|=  1.73326D-02\n",
      "\n",
      "At iterate    7    f=  1.90743D-01    |proj g|=  1.14692D-02\n",
      "\n",
      "At iterate    8    f=  1.25265D-01    |proj g|=  4.91774D-03\n",
      "\n",
      "At iterate    9    f=  8.43132D-02    |proj g|=  6.00854D-03\n",
      "\n",
      "At iterate   10    f=  5.65151D-02    |proj g|=  6.35473D-03\n",
      "\n",
      "At iterate   11    f=  3.38115D-02    |proj g|=  7.18579D-03\n",
      "\n",
      "At iterate   12    f=  2.40533D-02    |proj g|=  4.88516D-03\n",
      "\n",
      "At iterate   13    f=  1.26500D-02    |proj g|=  2.45450D-03\n",
      "\n",
      "At iterate   14    f=  8.09049D-03    |proj g|=  1.10813D-03\n",
      "\n",
      "At iterate   15    f=  4.51569D-03    |proj g|=  1.87651D-03\n",
      "\n",
      "At iterate   16    f=  2.70031D-03    |proj g|=  1.01706D-03\n",
      "\n",
      "At iterate   17    f=  1.45800D-03    |proj g|=  5.52368D-04\n",
      "\n",
      "At iterate   18    f=  7.71078D-04    |proj g|=  2.83124D-04\n",
      "\n",
      "At iterate   19    f=  3.97849D-04    |proj g|=  1.39030D-04\n",
      "\n",
      "At iterate   20    f=  2.03056D-04    |proj g|=  6.71392D-05\n",
      "\n",
      "At iterate   21    f=  1.02917D-04    |proj g|=  3.23117D-05\n",
      "\n",
      "At iterate   22    f=  5.18865D-05    |proj g|=  1.62276D-05\n",
      "\n",
      "At iterate   23    f=  2.61113D-05    |proj g|=  7.62923D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     23     25      1     0     0   7.629D-06   2.611D-05\n",
      "  F =   2.6111285509570865E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " This problem is unconstrained.\n",
      "100%|██████████| 13/13 [00:00<00:00, 30.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.97719D-01\n",
      "\n",
      "At iterate    1    f=  6.19738D-01    |proj g|=  6.69481D-02\n",
      "\n",
      "At iterate    2    f=  5.12143D-01    |proj g|=  4.82379D-02\n",
      "\n",
      "At iterate    3    f=  3.07135D-01    |proj g|=  7.33019D-02\n",
      "\n",
      "At iterate    4    f=  2.24499D-01    |proj g|=  4.65218D-02\n",
      "\n",
      "At iterate    5    f=  1.44272D-01    |proj g|=  2.01908D-02\n",
      "\n",
      "At iterate    6    f=  9.70302D-02    |proj g|=  7.83652D-03\n",
      "\n",
      "At iterate    7    f=  5.99182D-02    |proj g|=  3.51686D-03\n",
      "\n",
      "At iterate    8    f=  2.90204D-02    |proj g|=  2.72692D-03\n",
      "\n",
      "At iterate    9    f=  1.30400D-02    |proj g|=  1.32887D-03\n",
      "\n",
      "At iterate   10    f=  6.54421D-03    |proj g|=  7.22988D-04\n",
      "\n",
      "At iterate   11    f=  3.44085D-03    |proj g|=  5.04911D-04\n",
      "\n",
      "At iterate   12    f=  3.26920D-03    |proj g|=  9.82191D-04\n",
      "\n",
      "At iterate   13    f=  1.57596D-03    |proj g|=  2.14543D-04\n",
      "\n",
      "At iterate   14    f=  8.27583D-04    |proj g|=  9.78503D-05\n",
      "\n",
      "At iterate   15    f=  4.10446D-04    |proj g|=  4.78322D-05\n",
      "\n",
      "At iterate   16    f=  2.06868D-04    |proj g|=  2.25183D-05\n",
      "\n",
      "At iterate   17    f=  1.03267D-04    |proj g|=  1.33302D-05\n",
      "\n",
      "At iterate   18    f=  5.11201D-05    |proj g|=  9.19278D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     18     23      1     0     0   9.193D-06   5.112D-05\n",
      "  F =   5.1120065023503283E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 13/13 [00:00<00:00, 5716.11it/s]\n",
      "100%|██████████| 13/13 [00:00<00:00, 784.56it/s]\n",
      "100%|██████████| 13/13 [00:00<00:00, 5780.95it/s]\n",
      "100%|██████████| 13/13 [00:00<00:00, 654.55it/s]\n",
      "100%|██████████| 13/13 [00:00<00:00, 208.81it/s]\n",
      "/storage/work/eak5582/Research/generalized_mlm_2.py:404: ClusterWarning: scipy.cluster: The symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix\n",
      "  LocalModelsTree = linkage(self.dist_mat_avg, 'ward')\n",
      "  0%|          | 0/10 [00:00<?, ?it/s] This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " 10%|█         | 1/10 [00:00<00:01,  5.65it/s] This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  8.70918D-02\n",
      "\n",
      "At iterate    1    f=  6.76194D-01    |proj g|=  7.96962D-02\n",
      "\n",
      "At iterate    2    f=  5.91975D-01    |proj g|=  7.19390D-02\n",
      "\n",
      "At iterate    3    f=  4.49311D-01    |proj g|=  5.64701D-02\n",
      "\n",
      "At iterate    4    f=  3.24620D-01    |proj g|=  2.92878D-02\n",
      "\n",
      "At iterate    5    f=  2.71973D-01    |proj g|=  1.22290D-02\n",
      "\n",
      "At iterate    6    f=  2.35408D-01    |proj g|=  9.77649D-03\n",
      "\n",
      "At iterate    7    f=  2.13140D-01    |proj g|=  1.23025D-02\n",
      "\n",
      "At iterate    8    f=  1.82260D-01    |proj g|=  7.21056D-03\n",
      "\n",
      "At iterate    9    f=  1.53383D-01    |proj g|=  3.36313D-03\n",
      "\n",
      "At iterate   10    f=  1.26202D-01    |proj g|=  3.32496D-03\n",
      "\n",
      "At iterate   11    f=  1.00963D-01    |proj g|=  7.61335D-03\n",
      "\n",
      "At iterate   12    f=  9.93608D-02    |proj g|=  6.97022D-03\n",
      "\n",
      "At iterate   13    f=  9.38478D-02    |proj g|=  4.55297D-03\n",
      "\n",
      "At iterate   14    f=  8.49469D-02    |proj g|=  1.81764D-03\n",
      "\n",
      "At iterate   15    f=  7.77798D-02    |proj g|=  3.04945D-03\n",
      "\n",
      "At iterate   16    f=  7.06091D-02    |proj g|=  4.24852D-03\n",
      "\n",
      "At iterate   17    f=  6.37193D-02    |proj g|=  2.21021D-03\n",
      "\n",
      "At iterate   18    f=  5.68907D-02    |proj g|=  2.05863D-03\n",
      "\n",
      "At iterate   19    f=  4.88519D-02    |proj g|=  2.20091D-03\n",
      "\n",
      "At iterate   20    f=  4.38648D-02    |proj g|=  1.63637D-03\n",
      "\n",
      "At iterate   21    f=  3.99185D-02    |proj g|=  1.06117D-03\n",
      "\n",
      "At iterate   22    f=  3.88635D-02    |proj g|=  1.04982D-03\n",
      "\n",
      "At iterate   23    f=  3.83259D-02    |proj g|=  2.59151D-03\n",
      "\n",
      "At iterate   24    f=  3.70373D-02    |proj g|=  1.00060D-03\n",
      "\n",
      "At iterate   25    f=  3.64686D-02    |proj g|=  8.50062D-04\n",
      "\n",
      "At iterate   26    f=  3.61257D-02    |proj g|=  6.93821D-04\n",
      "\n",
      "At iterate   27    f=  3.51399D-02    |proj g|=  5.58568D-04\n",
      "\n",
      "At iterate   28    f=  3.37423D-02    |proj g|=  8.80095D-04\n",
      "\n",
      "At iterate   29    f=  3.27001D-02    |proj g|=  5.22595D-04\n",
      "\n",
      "At iterate   30    f=  3.17099D-02    |proj g|=  6.01997D-04\n",
      "\n",
      "At iterate   31    f=  3.13229D-02    |proj g|=  4.46818D-04\n",
      "\n",
      "At iterate   32    f=  3.06722D-02    |proj g|=  3.68414D-04\n",
      "\n",
      "At iterate   33    f=  3.00283D-02    |proj g|=  5.89074D-04\n",
      "\n",
      "At iterate   34    f=  2.93954D-02    |proj g|=  8.72285D-04\n",
      "\n",
      "At iterate   35    f=  2.89620D-02    |proj g|=  2.36174D-03\n",
      "\n",
      "At iterate   36    f=  2.84009D-02    |proj g|=  7.13564D-04\n",
      "\n",
      "At iterate   37    f=  2.80451D-02    |proj g|=  1.03761D-03\n",
      "\n",
      "At iterate   38    f=  2.74538D-02    |proj g|=  9.56490D-04\n",
      "\n",
      "At iterate   39    f=  2.64956D-02    |proj g|=  4.19657D-04\n",
      "\n",
      "At iterate   40    f=  2.57521D-02    |proj g|=  7.01980D-04\n",
      "\n",
      "At iterate   41    f=  2.49680D-02    |proj g|=  1.01982D-03\n",
      "\n",
      "At iterate   42    f=  2.41268D-02    |proj g|=  5.28021D-04\n",
      "\n",
      "At iterate   43    f=  2.36457D-02    |proj g|=  6.51078D-04\n",
      "\n",
      "At iterate   44    f=  2.32569D-02    |proj g|=  7.68954D-04\n",
      "\n",
      "At iterate   45    f=  2.24861D-02    |proj g|=  5.91499D-04\n",
      "\n",
      "At iterate   46    f=  2.15741D-02    |proj g|=  7.45781D-04\n",
      "\n",
      "At iterate   47    f=  2.14799D-02    |proj g|=  6.12226D-04\n",
      "\n",
      "At iterate   48    f=  2.12331D-02    |proj g|=  6.62321D-04\n",
      "\n",
      "At iterate   49    f=  2.06060D-02    |proj g|=  3.91349D-04\n",
      "\n",
      "At iterate   50    f=  1.99356D-02    |proj g|=  3.54762D-04\n",
      "\n",
      "At iterate   51    f=  1.95140D-02    |proj g|=  4.60236D-04\n",
      "\n",
      "At iterate   52    f=  1.89385D-02    |proj g|=  4.80217D-04\n",
      "\n",
      "At iterate   53    f=  1.84639D-02    |proj g|=  4.27785D-04\n",
      "\n",
      "At iterate   54    f=  1.80554D-02    |proj g|=  5.71947D-04\n",
      "\n",
      "At iterate   55    f=  1.76405D-02    |proj g|=  5.68773D-04\n",
      "\n",
      "At iterate   56    f=  1.73632D-02    |proj g|=  2.79078D-04\n",
      "\n",
      "At iterate   57    f=  1.68754D-02    |proj g|=  3.05408D-04\n",
      "\n",
      "At iterate   58    f=  1.68032D-02    |proj g|=  9.88284D-04\n",
      "\n",
      "At iterate   59    f=  1.62753D-02    |proj g|=  8.51706D-04\n",
      "\n",
      "At iterate   60    f=  1.53953D-02    |proj g|=  2.36749D-03\n",
      "\n",
      "At iterate   61    f=  1.45689D-02    |proj g|=  9.77159D-04\n",
      "\n",
      "At iterate   62    f=  1.40407D-02    |proj g|=  6.84306D-04\n",
      "\n",
      "At iterate   63    f=  1.32965D-02    |proj g|=  8.33294D-04\n",
      "\n",
      "At iterate   64    f=  1.23439D-02    |proj g|=  7.31129D-04\n",
      "\n",
      "At iterate   65    f=  1.09619D-02    |proj g|=  8.72016D-04\n",
      "\n",
      "At iterate   66    f=  9.82138D-03    |proj g|=  6.30877D-04\n",
      "\n",
      "At iterate   67    f=  9.30147D-03    |proj g|=  1.54609D-03\n",
      "\n",
      "At iterate   68    f=  8.78591D-03    |proj g|=  6.27559D-04\n",
      "\n",
      "At iterate   69    f=  8.50902D-03    |proj g|=  4.62158D-04\n",
      "\n",
      "At iterate   70    f=  8.36586D-03    |proj g|=  5.64469D-04\n",
      "\n",
      "At iterate   71    f=  7.75666D-03    |proj g|=  4.17078D-04\n",
      "\n",
      "At iterate   72    f=  7.67759D-03    |proj g|=  8.37178D-04\n",
      "\n",
      "At iterate   73    f=  7.46484D-03    |proj g|=  4.69158D-04\n",
      "\n",
      "At iterate   74    f=  7.33601D-03    |proj g|=  4.11280D-04\n",
      "\n",
      "At iterate   75    f=  7.20027D-03    |proj g|=  5.18966D-04\n",
      "\n",
      "At iterate   76    f=  7.05790D-03    |proj g|=  4.04155D-04\n",
      "\n",
      "At iterate   77    f=  6.70234D-03    |proj g|=  4.13877D-04\n",
      "\n",
      "At iterate   78    f=  6.20117D-03    |proj g|=  6.03862D-04\n",
      "\n",
      "At iterate   79    f=  6.10167D-03    |proj g|=  5.14766D-04\n",
      "\n",
      "At iterate   80    f=  5.72094D-03    |proj g|=  4.84965D-04\n",
      "\n",
      "At iterate   81    f=  5.40934D-03    |proj g|=  5.94614D-04\n",
      "\n",
      "At iterate   82    f=  4.95812D-03    |proj g|=  7.48402D-04\n",
      "\n",
      "At iterate   83    f=  4.84506D-03    |proj g|=  2.09849D-04\n",
      "\n",
      "At iterate   84    f=  4.64724D-03    |proj g|=  2.39437D-04\n",
      "\n",
      "At iterate   85    f=  4.52272D-03    |proj g|=  2.44072D-04\n",
      "\n",
      "At iterate   86    f=  4.38423D-03    |proj g|=  5.88771D-04\n",
      "\n",
      "At iterate   87    f=  4.14985D-03    |proj g|=  3.54486D-04\n",
      "\n",
      "At iterate   88    f=  3.76865D-03    |proj g|=  3.69248D-04\n",
      "\n",
      "At iterate   89    f=  3.59665D-03    |proj g|=  3.58510D-04\n",
      "\n",
      "At iterate   90    f=  3.10272D-03    |proj g|=  1.95827D-04\n",
      "\n",
      "At iterate   91    f=  2.71120D-03    |proj g|=  1.37382D-04\n",
      "\n",
      "At iterate   92    f=  2.16905D-03    |proj g|=  3.46294D-04\n",
      "\n",
      "At iterate   93    f=  1.68415D-03    |proj g|=  4.37137D-04\n",
      "\n",
      "At iterate   94    f=  1.43624D-03    |proj g|=  2.14284D-04\n",
      "\n",
      "At iterate   95    f=  1.24781D-03    |proj g|=  1.26397D-04\n",
      "\n",
      "At iterate   96    f=  9.50695D-04    |proj g|=  5.49062D-05\n",
      "\n",
      "At iterate   97    f=  9.24053D-04    |proj g|=  2.04033D-04\n",
      "\n",
      "At iterate   98    f=  8.42677D-04    |proj g|=  1.61555D-04\n",
      "\n",
      "At iterate   99    f=  7.43709D-04    |proj g|=  8.36693D-05\n",
      "\n",
      "At iterate  100    f=  6.89578D-04    |proj g|=  5.77646D-05\n",
      "\n",
      "At iterate  101    f=  6.31806D-04    |proj g|=  1.02518D-04\n",
      "\n",
      "At iterate  102    f=  5.47362D-04    |proj g|=  1.13593D-04\n",
      "\n",
      "At iterate  103    f=  4.48799D-04    |proj g|=  7.38094D-05\n",
      "\n",
      "At iterate  104    f=  3.76768D-04    |proj g|=  3.80510D-05\n",
      "\n",
      "At iterate  105    f=  2.89431D-04    |proj g|=  4.62060D-05\n",
      "\n",
      "At iterate  106    f=  2.08335D-04    |proj g|=  2.65311D-05\n",
      "\n",
      "At iterate  107    f=  1.66882D-04    |proj g|=  1.87058D-05\n",
      "\n",
      "At iterate  108    f=  1.29382D-04    |proj g|=  1.81694D-05\n",
      "\n",
      "At iterate  109    f=  1.03951D-04    |proj g|=  5.65934D-05\n",
      "\n",
      "At iterate  110    f=  8.44954D-05    |proj g|=  3.48638D-05\n",
      "\n",
      "At iterate  111    f=  6.10302D-05    |proj g|=  1.18802D-05\n",
      "\n",
      "At iterate  112    f=  4.70983D-05    |proj g|=  1.46369D-05\n",
      "\n",
      "At iterate  113    f=  2.59391D-05    |proj g|=  7.93377D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74    113    131      1     0     0   7.934D-06   2.594D-05\n",
      "  F =   2.5939064082198312E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.05175D-01\n",
      "\n",
      "At iterate    1    f=  6.70225D-01    |proj g|=  4.98568D-02\n",
      "\n",
      "At iterate    2    f=  5.71966D-01    |proj g|=  4.70821D-02\n",
      "\n",
      "At iterate    3    f=  3.79314D-01    |proj g|=  2.06836D-02\n",
      "\n",
      "At iterate    4    f=  2.92132D-01    |proj g|=  1.65169D-02\n",
      "\n",
      "At iterate    5    f=  1.58596D-01    |proj g|=  9.54870D-03\n",
      "\n",
      "At iterate    6    f=  9.79834D-02    |proj g|=  4.21703D-03\n",
      "\n",
      "At iterate    7    f=  6.03905D-02    |proj g|=  7.23907D-03\n",
      "\n",
      "At iterate    8    f=  4.25446D-02    |proj g|=  4.50375D-03\n",
      "\n",
      "At iterate    9    f=  2.65121D-02    |proj g|=  3.04734D-03\n",
      "\n",
      "At iterate   10    f=  1.55671D-02    |proj g|=  2.88700D-03\n",
      "\n",
      "At iterate   11    f=  1.29292D-02    |proj g|=  4.02961D-03\n",
      "\n",
      "At iterate   12    f=  1.17262D-02    |proj g|=  8.28587D-03\n",
      "\n",
      "At iterate   13    f=  5.84939D-03    |proj g|=  1.65540D-03\n",
      "\n",
      "At iterate   14    f=  4.61863D-03    |proj g|=  9.44830D-04\n",
      "\n",
      "At iterate   15    f=  3.02966D-03    |proj g|=  1.15200D-03\n",
      "\n",
      "At iterate   16    f=  1.73760D-03    |proj g|=  7.88546D-04\n",
      "\n",
      "At iterate   17    f=  8.29992D-04    |proj g|=  1.52368D-04\n",
      "\n",
      "At iterate   18    f=  4.99755D-04    |proj g|=  1.08363D-04\n",
      "\n",
      "At iterate   19    f=  2.30806D-04    |proj g|=  4.28775D-05\n",
      "\n",
      "At iterate   20    f=  1.21739D-04    |proj g|=  2.16830D-05\n",
      "\n",
      "At iterate   21    f=  6.01377D-05    |proj g|=  1.35568D-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " 40%|████      | 4/10 [00:00<00:00, 14.11it/s] This problem is unconstrained.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   22    f=  3.12685D-05    |proj g|=  6.28702D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     22     26      1     0     0   6.287D-06   3.127D-05\n",
      "  F =   3.1268522328296676E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.90634D-01\n",
      "\n",
      "At iterate    1    f=  6.21542D-01    |proj g|=  6.72563D-02\n",
      "\n",
      "At iterate    2    f=  5.19053D-01    |proj g|=  4.58916D-02\n",
      "\n",
      "At iterate    3    f=  3.32701D-01    |proj g|=  5.02680D-02\n",
      "\n",
      "At iterate    4    f=  2.46352D-01    |proj g|=  3.29069D-02\n",
      "\n",
      "At iterate    5    f=  1.61352D-01    |proj g|=  1.63755D-02\n",
      "\n",
      "At iterate    6    f=  1.09566D-01    |proj g|=  9.66730D-03\n",
      "\n",
      "At iterate    7    f=  6.88623D-02    |proj g|=  5.20913D-03\n",
      "\n",
      "At iterate    8    f=  4.06188D-02    |proj g|=  2.10002D-03\n",
      "\n",
      "At iterate    9    f=  2.29983D-02    |proj g|=  3.08092D-03\n",
      "\n",
      "At iterate   10    f=  1.51541D-02    |proj g|=  4.59217D-03\n",
      "\n",
      "At iterate   11    f=  8.32566D-03    |proj g|=  2.01709D-03\n",
      "\n",
      "At iterate   12    f=  4.60184D-03    |proj g|=  4.38762D-04\n",
      "\n",
      "At iterate   13    f=  3.00116D-03    |proj g|=  6.59205D-04\n",
      "\n",
      "At iterate   14    f=  2.99413D-03    |proj g|=  2.81536D-03\n",
      "\n",
      "At iterate   15    f=  1.16360D-03    |proj g|=  1.03626D-03\n",
      "\n",
      "At iterate   16    f=  7.53080D-04    |proj g|=  6.04568D-04\n",
      "\n",
      "At iterate   17    f=  4.41537D-04    |proj g|=  2.62370D-04\n",
      "\n",
      "At iterate   18    f=  2.88954D-04    |proj g|=  1.08768D-04\n",
      "\n",
      "At iterate   19    f=  1.81376D-04    |proj g|=  5.73707D-05\n",
      "\n",
      "At iterate   20    f=  1.06681D-04    |proj g|=  3.86853D-05\n",
      "\n",
      "At iterate   21    f=  5.18352D-05    |proj g|=  1.61076D-05\n",
      "\n",
      "At iterate   22    f=  2.67362D-05    |proj g|=  1.29131D-05\n",
      "\n",
      "At iterate   23    f=  1.98075D-05    |proj g|=  5.47351D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     23     28      1     0     0   5.474D-06   1.981D-05\n",
      "  F =   1.9807512267270204E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.49903D-01\n",
      "\n",
      "At iterate    1    f=  6.49331D-01    |proj g|=  5.53754D-02\n",
      "\n",
      "At iterate    2    f=  5.64556D-01    |proj g|=  4.59956D-02\n",
      "\n",
      "At iterate    3    f=  3.30700D-01    |proj g|=  2.84876D-02\n",
      "\n",
      "At iterate    4    f=  2.58223D-01    |proj g|=  1.65225D-02\n",
      "\n",
      "At iterate    5    f=  1.91131D-01    |proj g|=  5.87077D-03\n",
      "\n",
      "At iterate    6    f=  1.50502D-01    |proj g|=  3.09199D-03\n",
      "\n",
      "At iterate    7    f=  1.13844D-01    |proj g|=  7.06550D-03\n",
      "\n",
      "At iterate    8    f=  9.04384D-02    |proj g|=  6.56998D-03\n",
      "\n",
      "At iterate    9    f=  7.54882D-02    |proj g|=  8.68629D-03\n",
      "\n",
      "At iterate   10    f=  6.48237D-02    |proj g|=  3.70884D-03\n",
      "\n",
      "At iterate   11    f=  5.75206D-02    |proj g|=  3.18868D-03\n",
      "\n",
      "At iterate   12    f=  5.58865D-02    |proj g|=  5.42927D-03\n",
      "\n",
      "At iterate   13    f=  5.15485D-02    |proj g|=  2.12588D-03\n",
      "\n",
      "At iterate   14    f=  4.86922D-02    |proj g|=  2.39262D-03\n",
      "\n",
      "At iterate   15    f=  4.56694D-02    |proj g|=  3.32608D-03\n",
      "\n",
      "At iterate   16    f=  3.94919D-02    |proj g|=  3.26822D-03\n",
      "\n",
      "At iterate   17    f=  3.43420D-02    |proj g|=  4.83233D-03\n",
      "\n",
      "At iterate   18    f=  3.02645D-02    |proj g|=  2.23494D-03\n",
      "\n",
      "At iterate   19    f=  2.93642D-02    |proj g|=  1.52494D-03\n",
      "\n",
      "At iterate   20    f=  2.85894D-02    |proj g|=  8.04344D-04\n",
      "\n",
      "At iterate   21    f=  2.78595D-02    |proj g|=  1.34120D-03\n",
      "\n",
      "At iterate   22    f=  2.70665D-02    |proj g|=  1.64321D-03\n",
      "\n",
      "At iterate   23    f=  2.61852D-02    |proj g|=  4.35931D-03\n",
      "\n",
      "At iterate   24    f=  2.44410D-02    |proj g|=  5.61784D-04\n",
      "\n",
      "At iterate   25    f=  2.40436D-02    |proj g|=  8.09783D-04\n",
      "\n",
      "At iterate   26    f=  2.37474D-02    |proj g|=  1.09293D-03\n",
      "\n",
      "At iterate   27    f=  2.31815D-02    |proj g|=  1.03956D-03\n",
      "\n",
      "At iterate   28    f=  2.11082D-02    |proj g|=  1.83374D-03\n",
      "\n",
      "At iterate   29    f=  1.93554D-02    |proj g|=  3.48782D-03\n",
      "\n",
      "At iterate   30    f=  1.15599D-02    |proj g|=  1.67425D-03\n",
      "\n",
      "At iterate   31    f=  6.07585D-03    |proj g|=  6.74447D-04\n",
      "\n",
      "At iterate   32    f=  4.06903D-03    |proj g|=  1.04650D-03\n",
      "\n",
      "At iterate   33    f=  1.39651D-03    |proj g|=  3.11691D-04\n",
      "\n",
      "At iterate   34    f=  8.85241D-04    |proj g|=  1.26088D-04\n",
      "\n",
      "At iterate   35    f=  4.14456D-04    |proj g|=  1.11720D-04\n",
      "\n",
      "At iterate   36    f=  4.03335D-04    |proj g|=  7.80048D-05\n",
      "\n",
      "At iterate   37    f=  2.38563D-04    |proj g|=  4.34897D-05\n",
      "\n",
      "At iterate   38    f=  1.10506D-04    |proj g|=  1.49334D-05\n",
      "\n",
      "At iterate   39    f=  5.63082D-05    |proj g|=  1.04928D-05\n",
      "\n",
      "At iterate   40    f=  2.58002D-05    |proj g|=  3.15869D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     40     47      1     0     0   3.159D-06   2.580D-05\n",
      "  F =   2.5800157547481172E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.97719D-01\n",
      "\n",
      "At iterate    1    f=  6.19738D-01    |proj g|=  6.69481D-02\n",
      "\n",
      "At iterate    2    f=  5.12143D-01    |proj g|=  4.82379D-02\n",
      "\n",
      "At iterate    3    f=  3.07135D-01    |proj g|=  7.33019D-02\n",
      "\n",
      "At iterate    4    f=  2.24499D-01    |proj g|=  4.65218D-02\n",
      "\n",
      "At iterate    5    f=  1.44272D-01    |proj g|=  2.01908D-02\n",
      "\n",
      "At iterate    6    f=  9.70302D-02    |proj g|=  7.83652D-03\n",
      "\n",
      "At iterate    7    f=  5.99182D-02    |proj g|=  3.51686D-03\n",
      "\n",
      "At iterate    8    f=  2.90204D-02    |proj g|=  2.72692D-03\n",
      "\n",
      "At iterate    9    f=  1.30400D-02    |proj g|=  1.32887D-03\n",
      "\n",
      "At iterate   10    f=  6.54421D-03    |proj g|=  7.22988D-04\n",
      "\n",
      "At iterate   11    f=  3.44085D-03    |proj g|=  5.04911D-04\n",
      "\n",
      "At iterate   12    f=  3.26920D-03    |proj g|=  9.82191D-04\n",
      "\n",
      "At iterate   13    f=  1.57596D-03    |proj g|=  2.14543D-04\n",
      "\n",
      "At iterate   14    f=  8.27583D-04    |proj g|=  9.78503D-05\n",
      "\n",
      "At iterate   15    f=  4.10446D-04    |proj g|=  4.78322D-05\n",
      "\n",
      "At iterate   16    f=  2.06868D-04    |proj g|=  2.25183D-05\n",
      "\n",
      "At iterate   17    f=  1.03267D-04    |proj g|=  1.33302D-05\n",
      "\n",
      "At iterate   18    f=  5.11201D-05    |proj g|=  9.19278D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     18     23      1     0     0   9.193D-06   5.112D-05\n",
      "  F =   5.1120065023503283E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  9.47345D-02\n",
      "\n",
      "At iterate    1    f=  6.74147D-01    |proj g|=  5.07429D-02\n",
      "\n",
      "At iterate    2    f=  6.40237D-01    |proj g|=  4.79465D-02\n",
      "\n",
      "At iterate    3    f=  5.61157D-01    |proj g|=  4.14223D-02\n",
      "\n",
      "At iterate    4    f=  4.47336D-01    |proj g|=  4.18541D-02\n",
      "\n",
      "At iterate    5    f=  3.16544D-01    |proj g|=  2.59763D-02\n",
      "\n",
      "At iterate    6    f=  2.71549D-01    |proj g|=  1.43711D-02\n",
      "\n",
      "At iterate    7    f=  2.36057D-01    |proj g|=  1.66231D-02\n",
      "\n",
      "At iterate    8    f=  2.10717D-01    |proj g|=  1.56223D-02\n",
      "\n",
      "At iterate    9    f=  1.77810D-01    |proj g|=  9.01173D-03\n",
      "\n",
      "At iterate   10    f=  1.46539D-01    |proj g|=  4.32298D-03\n",
      "\n",
      "At iterate   11    f=  1.26178D-01    |proj g|=  2.68198D-03\n",
      "\n",
      "At iterate   12    f=  1.23810D-01    |proj g|=  1.28511D-02\n",
      "\n",
      "At iterate   13    f=  1.12775D-01    |proj g|=  4.02751D-03\n",
      "\n",
      "At iterate   14    f=  1.07869D-01    |proj g|=  3.46684D-03\n",
      "\n",
      "At iterate   15    f=  1.01357D-01    |proj g|=  3.40882D-03\n",
      "\n",
      "At iterate   16    f=  8.64612D-02    |proj g|=  4.33535D-03\n",
      "\n",
      "At iterate   17    f=  7.99140D-02    |proj g|=  9.20147D-03\n",
      "\n",
      "At iterate   18    f=  6.92616D-02    |proj g|=  3.90267D-03\n",
      "\n",
      "At iterate   19    f=  6.22660D-02    |proj g|=  1.96182D-03\n",
      "\n",
      "At iterate   20    f=  5.52256D-02    |proj g|=  2.21932D-03\n",
      "\n",
      "At iterate   21    f=  4.67682D-02    |proj g|=  3.45570D-03\n",
      "\n",
      "At iterate   22    f=  2.92526D-02    |proj g|=  3.53922D-03\n",
      "\n",
      "At iterate   23    f=  2.84086D-02    |proj g|=  4.31957D-03\n",
      "\n",
      "At iterate   24    f=  2.41814D-02    |proj g|=  6.89623D-03\n",
      "\n",
      "At iterate   25    f=  1.40264D-02    |proj g|=  1.03131D-03\n",
      "\n",
      "At iterate   26    f=  1.13609D-02    |proj g|=  9.20549D-04\n",
      "\n",
      "At iterate   27    f=  3.66451D-03    |proj g|=  9.42295D-04\n",
      "\n",
      "At iterate   28    f=  1.78873D-03    |proj g|=  4.55900D-04\n",
      "\n",
      "At iterate   29    f=  1.20478D-03    |proj g|=  2.83005D-04\n",
      "\n",
      "At iterate   30    f=  5.47338D-04    |proj g|=  1.23173D-04\n",
      "\n",
      "At iterate   31    f=  2.89953D-04    |proj g|=  6.46064D-05\n",
      "\n",
      "At iterate   32    f=  1.44445D-04    |proj g|=  3.14999D-05\n",
      "\n",
      "At iterate   33    f=  7.32596D-05    |proj g|=  1.59608D-05\n",
      "\n",
      "At iterate   34    f=  3.75345D-05    |proj g|=  1.47114D-05\n",
      "\n",
      "At iterate   35    f=  3.45345D-05    |proj g|=  1.68459D-05\n",
      "\n",
      "At iterate   36    f=  1.47961D-05    |proj g|=  2.44488D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     36     43      1     0     0   2.445D-06   1.480D-05\n",
      "  F =   1.4796129572076284E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " This problem is unconstrained.\n",
      " 70%|███████   | 7/10 [00:00<00:00, 18.20it/s] This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " This problem is unconstrained.\n",
      " 90%|█████████ | 9/10 [00:00<00:00, 17.46it/s] This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "100%|██████████| 10/10 [00:00<00:00, 16.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.59555D-01\n",
      "\n",
      "At iterate    1    f=  6.41365D-01    |proj g|=  6.16368D-02\n",
      "\n",
      "At iterate    2    f=  5.32552D-01    |proj g|=  4.67671D-02\n",
      "\n",
      "At iterate    3    f=  3.11921D-01    |proj g|=  4.35991D-02\n",
      "\n",
      "At iterate    4    f=  2.34599D-01    |proj g|=  2.81208D-02\n",
      "\n",
      "At iterate    5    f=  1.55710D-01    |proj g|=  1.41311D-02\n",
      "\n",
      "At iterate    6    f=  1.06679D-01    |proj g|=  7.44090D-03\n",
      "\n",
      "At iterate    7    f=  6.27239D-02    |proj g|=  3.67490D-03\n",
      "\n",
      "At iterate    8    f=  3.14359D-02    |proj g|=  2.96408D-03\n",
      "\n",
      "At iterate    9    f=  2.43962D-02    |proj g|=  4.05643D-03\n",
      "\n",
      "At iterate   10    f=  7.64558D-03    |proj g|=  8.38775D-04\n",
      "\n",
      "At iterate   11    f=  4.67934D-03    |proj g|=  5.81349D-04\n",
      "\n",
      "At iterate   12    f=  4.31167D-03    |proj g|=  1.94958D-03\n",
      "\n",
      "At iterate   13    f=  1.95265D-03    |proj g|=  5.23159D-04\n",
      "\n",
      "At iterate   14    f=  1.09413D-03    |proj g|=  2.77354D-04\n",
      "\n",
      "At iterate   15    f=  5.48306D-04    |proj g|=  1.36360D-04\n",
      "\n",
      "At iterate   16    f=  2.83269D-04    |proj g|=  6.99181D-05\n",
      "\n",
      "At iterate   17    f=  1.43652D-04    |proj g|=  3.54506D-05\n",
      "\n",
      "At iterate   18    f=  7.30252D-05    |proj g|=  1.82730D-05\n",
      "\n",
      "At iterate   19    f=  3.69577D-05    |proj g|=  9.46736D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     19     24      1     0     0   9.467D-06   3.696D-05\n",
      "  F =   3.6957679691730421E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  1.61835D-01\n",
      "\n",
      "At iterate    1    f=  6.40041D-01    |proj g|=  5.50016D-02\n",
      "\n",
      "At iterate    2    f=  5.80672D-01    |proj g|=  5.61345D-02\n",
      "\n",
      "At iterate    3    f=  4.68412D-01    |proj g|=  7.39455D-02\n",
      "\n",
      "At iterate    4    f=  3.81771D-01    |proj g|=  5.49393D-02\n",
      "\n",
      "At iterate    5    f=  2.94442D-01    |proj g|=  2.41982D-02\n",
      "\n",
      "At iterate    6    f=  2.47904D-01    |proj g|=  1.03890D-02\n",
      "\n",
      "At iterate    7    f=  2.20759D-01    |proj g|=  4.77131D-03\n",
      "\n",
      "At iterate    8    f=  2.03186D-01    |proj g|=  5.68269D-03\n",
      "\n",
      "At iterate    9    f=  1.86145D-01    |proj g|=  6.07639D-03\n",
      "\n",
      "At iterate   10    f=  1.57142D-01    |proj g|=  3.21465D-03\n",
      "\n",
      "At iterate   11    f=  1.44714D-01    |proj g|=  5.60057D-03\n",
      "\n",
      "At iterate   12    f=  1.43871D-01    |proj g|=  4.72747D-03\n",
      "\n",
      "At iterate   13    f=  1.40817D-01    |proj g|=  2.68911D-03\n",
      "\n",
      "At iterate   14    f=  1.37067D-01    |proj g|=  2.86722D-03\n",
      "\n",
      "At iterate   15    f=  1.29366D-01    |proj g|=  5.08199D-03\n",
      "\n",
      "At iterate   16    f=  1.18945D-01    |proj g|=  2.52858D-03\n",
      "\n",
      "At iterate   17    f=  1.13839D-01    |proj g|=  7.27185D-03\n",
      "\n",
      "At iterate   18    f=  1.06078D-01    |proj g|=  1.59142D-03\n",
      "\n",
      "At iterate   19    f=  1.00510D-01    |proj g|=  1.78103D-03\n",
      "\n",
      "At iterate   20    f=  9.23870D-02    |proj g|=  2.81110D-03\n",
      "\n",
      "At iterate   21    f=  7.82051D-02    |proj g|=  4.81731D-03\n",
      "\n",
      "At iterate   22    f=  6.90064D-02    |proj g|=  3.65851D-03\n",
      "\n",
      "At iterate   23    f=  6.73265D-02    |proj g|=  6.61898D-03\n",
      "\n",
      "At iterate   24    f=  6.35217D-02    |proj g|=  3.85404D-03\n",
      "\n",
      "At iterate   25    f=  6.07196D-02    |proj g|=  1.57034D-03\n",
      "\n",
      "At iterate   26    f=  5.93615D-02    |proj g|=  1.43530D-03\n",
      "\n",
      "At iterate   27    f=  5.81649D-02    |proj g|=  1.56549D-03\n",
      "\n",
      "At iterate   28    f=  5.63988D-02    |proj g|=  1.07986D-03\n",
      "\n",
      "At iterate   29    f=  5.32179D-02    |proj g|=  1.70844D-03\n",
      "\n",
      "At iterate   30    f=  5.12540D-02    |proj g|=  1.27017D-03\n",
      "\n",
      "At iterate   31    f=  4.92806D-02    |proj g|=  8.66101D-04\n",
      "\n",
      "At iterate   32    f=  4.52424D-02    |proj g|=  1.04797D-03\n",
      "\n",
      "At iterate   33    f=  4.07830D-02    |proj g|=  8.66853D-04\n",
      "\n",
      "At iterate   34    f=  3.76106D-02    |proj g|=  1.99470D-03\n",
      "\n",
      "At iterate   35    f=  3.71777D-02    |proj g|=  2.70843D-03\n",
      "\n",
      "At iterate   36    f=  3.58371D-02    |proj g|=  1.48152D-03\n",
      "\n",
      "At iterate   37    f=  3.48407D-02    |proj g|=  1.21021D-03\n",
      "\n",
      "At iterate   38    f=  2.87632D-02    |proj g|=  1.30822D-03\n",
      "\n",
      "At iterate   39    f=  2.28752D-02    |proj g|=  1.59891D-03\n",
      "\n",
      "At iterate   40    f=  1.78843D-02    |proj g|=  3.96188D-03\n",
      "\n",
      "At iterate   41    f=  1.36927D-02    |proj g|=  1.54740D-03\n",
      "\n",
      "At iterate   42    f=  1.26531D-02    |proj g|=  9.10026D-04\n",
      "\n",
      "At iterate   43    f=  1.13124D-02    |proj g|=  9.83763D-04\n",
      "\n",
      "At iterate   44    f=  1.02013D-02    |proj g|=  1.12864D-03\n",
      "\n",
      "At iterate   45    f=  6.79278D-03    |proj g|=  1.33588D-03\n",
      "\n",
      "At iterate   46    f=  6.66303D-03    |proj g|=  6.57826D-04\n",
      "\n",
      "At iterate   47    f=  4.88793D-03    |proj g|=  5.17637D-04\n",
      "\n",
      "At iterate   48    f=  2.98456D-03    |proj g|=  1.36150D-03\n",
      "\n",
      "At iterate   49    f=  1.51406D-03    |proj g|=  2.15581D-04\n",
      "\n",
      "At iterate   50    f=  1.13836D-03    |proj g|=  1.96825D-04\n",
      "\n",
      "At iterate   51    f=  6.02901D-04    |proj g|=  1.12608D-04\n",
      "\n",
      "At iterate   52    f=  3.04725D-04    |proj g|=  3.68399D-05\n",
      "\n",
      "At iterate   53    f=  1.54603D-04    |proj g|=  3.97090D-05\n",
      "\n",
      "At iterate   54    f=  7.63174D-05    |proj g|=  1.40367D-05\n",
      "\n",
      "At iterate   55    f=  4.36400D-05    |proj g|=  6.02534D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     55     64      1     0     0   6.025D-06   4.364D-05\n",
      "  F =   4.3639986971044748E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  2.33759D-01\n",
      "\n",
      "At iterate    1    f=  5.82826D-01    |proj g|=  4.43110D-02\n",
      "\n",
      "At iterate    2    f=  4.88259D-01    |proj g|=  7.63365D-02\n",
      "\n",
      "At iterate    3    f=  3.34285D-01    |proj g|=  1.07921D-01\n",
      "\n",
      "At iterate    4    f=  2.38913D-01    |proj g|=  6.81664D-02\n",
      "\n",
      "At iterate    5    f=  1.71567D-01    |proj g|=  3.39335D-02\n",
      "\n",
      "At iterate    6    f=  1.31580D-01    |proj g|=  1.52027D-02\n",
      "\n",
      "At iterate    7    f=  1.01317D-01    |proj g|=  9.54116D-03\n",
      "\n",
      "At iterate    8    f=  7.04613D-02    |proj g|=  7.92812D-03\n",
      "\n",
      "At iterate    9    f=  2.96654D-02    |proj g|=  3.10279D-03\n",
      "\n",
      "At iterate   10    f=  1.35221D-02    |proj g|=  1.48454D-03\n",
      "\n",
      "At iterate   11    f=  6.53352D-03    |proj g|=  7.39364D-04\n",
      "\n",
      "At iterate   12    f=  3.18914D-03    |proj g|=  4.14531D-04\n",
      "\n",
      "At iterate   13    f=  2.61504D-03    |proj g|=  1.48092D-03\n",
      "\n",
      "At iterate   14    f=  1.37617D-03    |proj g|=  4.91695D-04\n",
      "\n",
      "At iterate   15    f=  7.40655D-04    |proj g|=  2.12174D-04\n",
      "\n",
      "At iterate   16    f=  3.74542D-04    |proj g|=  1.13418D-04\n",
      "\n",
      "At iterate   17    f=  1.90019D-04    |proj g|=  5.85931D-05\n",
      "\n",
      "At iterate   18    f=  9.53839D-05    |proj g|=  2.93273D-05\n",
      "\n",
      "At iterate   19    f=  4.79070D-05    |proj g|=  1.45687D-05\n",
      "\n",
      "At iterate   20    f=  2.40151D-05    |proj g|=  7.21386D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     20     25      1     0     0   7.214D-06   2.402D-05\n",
      "  F =   2.4015117238792479E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  2.24636D-01\n",
      "\n",
      "At iterate    1    f=  6.00225D-01    |proj g|=  4.66774D-02\n",
      "\n",
      "At iterate    2    f=  5.24014D-01    |proj g|=  7.14379D-02\n",
      "\n",
      "At iterate    3    f=  3.88049D-01    |proj g|=  9.66917D-02\n",
      "\n",
      "At iterate    4    f=  3.15813D-01    |proj g|=  6.00884D-02\n",
      "\n",
      "At iterate    5    f=  2.59342D-01    |proj g|=  2.46184D-02\n",
      "\n",
      "At iterate    6    f=  2.28693D-01    |proj g|=  1.07452D-02\n",
      "\n",
      "At iterate    7    f=  2.04859D-01    |proj g|=  1.72683D-02\n",
      "\n",
      "At iterate    8    f=  1.74711D-01    |proj g|=  2.22670D-02\n",
      "\n",
      "At iterate    9    f=  1.08415D-01    |proj g|=  1.61839D-02\n",
      "\n",
      "At iterate   10    f=  5.64686D-02    |proj g|=  7.24511D-03\n",
      "\n",
      "At iterate   11    f=  3.32612D-02    |proj g|=  3.64370D-03\n",
      "\n",
      "At iterate   12    f=  1.61674D-02    |proj g|=  4.13385D-03\n",
      "\n",
      "At iterate   13    f=  1.28630D-02    |proj g|=  7.31605D-03\n",
      "\n",
      "At iterate   14    f=  7.36156D-03    |proj g|=  1.74027D-03\n",
      "\n",
      "At iterate   15    f=  4.76650D-03    |proj g|=  7.32087D-04\n",
      "\n",
      "At iterate   16    f=  2.68832D-03    |proj g|=  6.96354D-04\n",
      "\n",
      "At iterate   17    f=  1.38677D-03    |proj g|=  3.97451D-04\n",
      "\n",
      "At iterate   18    f=  6.99581D-04    |proj g|=  2.12805D-04\n",
      "\n",
      "At iterate   19    f=  3.54403D-04    |proj g|=  1.13791D-04\n",
      "\n",
      "At iterate   20    f=  1.78827D-04    |proj g|=  5.98252D-05\n",
      "\n",
      "At iterate   21    f=  9.02159D-05    |proj g|=  3.13127D-05\n",
      "\n",
      "At iterate   22    f=  4.54482D-05    |proj g|=  1.62930D-05\n",
      "\n",
      "At iterate   23    f=  2.28770D-05    |proj g|=  8.48899D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     23     27      1     0     0   8.489D-06   2.288D-05\n",
      "  F =   2.2877048579269650E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 13/13 [00:00<00:00, 5563.87it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 544.35it/s]\n",
      "100%|██████████| 13/13 [00:00<00:00, 5769.94it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 625.52it/s]\n",
      "x does not have column names.  The check that columns are provided in the same order when training and predicting will be skipped\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 (73,)\n",
      "# of CELL:13 / min size:14 / avg size:23.8 / max size:39 / # of singleton CELL:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of clf: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1/13 [00:00<00:01,  9.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: 0 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9c378480>, dataframe=<capsule object NULL at 0x152e101c0150>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7   \\\n",
      "0    0.480000  1.000000  1.0  0.126214  0.440844  0.0  0.0  0.421053   \n",
      "1    0.893333  0.102714  1.0  0.262136  0.152929  0.0  0.0  0.868421   \n",
      "2    0.146667  0.593820  1.0  0.854369  0.519156  0.0  0.0  0.921053   \n",
      "3    0.560000  0.578539  1.0  0.854369  0.118522  0.0  0.0  0.789474   \n",
      "4    0.506667  0.012860  1.0  0.932039  0.000633  0.0  0.0  0.421053   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...   \n",
      "305  0.840000  0.066221  1.0  0.067961  0.179208  0.0  0.0  0.921053   \n",
      "306  0.613333  0.062547  1.0  0.077670  0.244222  0.0  0.0  0.947368   \n",
      "307  0.680000  0.287015  1.0  0.271845  0.357150  0.0  0.0  0.000000   \n",
      "308  0.506667  0.263967  1.0  0.067961  0.324538  0.0  0.0  0.973684   \n",
      "309  0.480000  0.101628  1.0  0.349515  0.000000  0.0  0.0  0.947368   \n",
      "\n",
      "           8         9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.000000  0.382353  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    0.000000  0.852941  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2    0.000000  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.000000  0.764706  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.000000  0.382353  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "..        ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.000000  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "306  0.000000  0.941176  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "307  0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "308  0.000000  0.970588  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.111111  0.941176  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "       1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
      "       1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
      "       1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
      "       0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "       0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "       0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0.,\n",
      "       0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
      "       0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
      "       1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
      "       1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0.,\n",
      "       0., 0., 1., 1.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n",
      "cluster: 1 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9c378480>, dataframe=<capsule object NULL at 0x152e101c0150>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7   \\\n",
      "0    0.480000  1.000000  1.0  0.126214  0.440844  0.0  0.0  0.421053   \n",
      "1    0.893333  0.102714  1.0  0.262136  0.152929  0.0  0.0  0.868421   \n",
      "2    0.146667  0.593820  1.0  0.854369  0.519156  0.0  0.0  0.921053   \n",
      "3    0.560000  0.578539  1.0  0.854369  0.118522  0.0  0.0  0.789474   \n",
      "4    0.506667  0.012860  1.0  0.932039  0.000633  0.0  0.0  0.421053   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...   \n",
      "305  0.840000  0.066221  1.0  0.067961  0.179208  0.0  0.0  0.921053   \n",
      "306  0.613333  0.062547  1.0  0.077670  0.244222  0.0  0.0  0.947368   \n",
      "307  0.680000  0.287015  1.0  0.271845  0.357150  0.0  0.0  0.000000   \n",
      "308  0.506667  0.263967  1.0  0.067961  0.324538  0.0  0.0  0.973684   \n",
      "309  0.480000  0.101628  1.0  0.349515  0.000000  0.0  0.0  0.947368   \n",
      "\n",
      "           8         9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.000000  0.382353  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    0.000000  0.852941  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2    0.000000  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.000000  0.764706  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.000000  0.382353  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "..        ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.000000  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "306  0.000000  0.941176  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "307  0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "308  0.000000  0.970588  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.111111  0.941176  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "       1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
      "       1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
      "       1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
      "       0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "       0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "       0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0.,\n",
      "       0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
      "       0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
      "       1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
      "       1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0.,\n",
      "       0., 0., 1., 1.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 3/13 [00:00<00:01,  7.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: 2 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9c378480>, dataframe=<capsule object NULL at 0x152e101c0150>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7   \\\n",
      "0    0.480000  1.000000  1.0  0.126214  0.440844  0.0  0.0  0.421053   \n",
      "1    0.893333  0.102714  1.0  0.262136  0.152929  0.0  0.0  0.868421   \n",
      "2    0.146667  0.593820  1.0  0.854369  0.519156  0.0  0.0  0.921053   \n",
      "3    0.560000  0.578539  1.0  0.854369  0.118522  0.0  0.0  0.789474   \n",
      "4    0.506667  0.012860  1.0  0.932039  0.000633  0.0  0.0  0.421053   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...   \n",
      "305  0.840000  0.066221  1.0  0.067961  0.179208  0.0  0.0  0.921053   \n",
      "306  0.613333  0.062547  1.0  0.077670  0.244222  0.0  0.0  0.947368   \n",
      "307  0.680000  0.287015  1.0  0.271845  0.357150  0.0  0.0  0.000000   \n",
      "308  0.506667  0.263967  1.0  0.067961  0.324538  0.0  0.0  0.973684   \n",
      "309  0.480000  0.101628  1.0  0.349515  0.000000  0.0  0.0  0.947368   \n",
      "\n",
      "           8         9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.000000  0.382353  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    0.000000  0.852941  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2    0.000000  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.000000  0.764706  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.000000  0.382353  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "..        ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.000000  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "306  0.000000  0.941176  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "307  0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "308  0.000000  0.970588  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.111111  0.941176  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "       1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
      "       1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
      "       1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
      "       0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "       0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "       0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0.,\n",
      "       0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
      "       0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
      "       1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
      "       1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0.,\n",
      "       0., 0., 1., 1.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n",
      "cluster: 3 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9c378480>, dataframe=<capsule object NULL at 0x152e101c0150>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7   \\\n",
      "0    0.480000  1.000000  1.0  0.126214  0.440844  0.0  0.0  0.421053   \n",
      "1    0.893333  0.102714  1.0  0.262136  0.152929  0.0  0.0  0.868421   \n",
      "2    0.146667  0.593820  1.0  0.854369  0.519156  0.0  0.0  0.921053   \n",
      "3    0.560000  0.578539  1.0  0.854369  0.118522  0.0  0.0  0.789474   \n",
      "4    0.506667  0.012860  1.0  0.932039  0.000633  0.0  0.0  0.421053   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...   \n",
      "305  0.840000  0.066221  1.0  0.067961  0.179208  0.0  0.0  0.921053   \n",
      "306  0.613333  0.062547  1.0  0.077670  0.244222  0.0  0.0  0.947368   \n",
      "307  0.680000  0.287015  1.0  0.271845  0.357150  0.0  0.0  0.000000   \n",
      "308  0.506667  0.263967  1.0  0.067961  0.324538  0.0  0.0  0.973684   \n",
      "309  0.480000  0.101628  1.0  0.349515  0.000000  0.0  0.0  0.947368   \n",
      "\n",
      "           8         9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.000000  0.382353  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    0.000000  0.852941  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2    0.000000  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.000000  0.764706  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.000000  0.382353  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "..        ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.000000  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "306  0.000000  0.941176  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "307  0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "308  0.000000  0.970588  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.111111  0.941176  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "       1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
      "       1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
      "       1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
      "       0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "       0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "       0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0.,\n",
      "       0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
      "       0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
      "       1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
      "       1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0.,\n",
      "       0., 0., 1., 1.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 5/13 [00:00<00:01,  7.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: 4 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9c378480>, dataframe=<capsule object NULL at 0x152e101c0150>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7   \\\n",
      "0    0.480000  1.000000  1.0  0.126214  0.440844  0.0  0.0  0.421053   \n",
      "1    0.893333  0.102714  1.0  0.262136  0.152929  0.0  0.0  0.868421   \n",
      "2    0.146667  0.593820  1.0  0.854369  0.519156  0.0  0.0  0.921053   \n",
      "3    0.560000  0.578539  1.0  0.854369  0.118522  0.0  0.0  0.789474   \n",
      "4    0.506667  0.012860  1.0  0.932039  0.000633  0.0  0.0  0.421053   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...   \n",
      "305  0.840000  0.066221  1.0  0.067961  0.179208  0.0  0.0  0.921053   \n",
      "306  0.613333  0.062547  1.0  0.077670  0.244222  0.0  0.0  0.947368   \n",
      "307  0.680000  0.287015  1.0  0.271845  0.357150  0.0  0.0  0.000000   \n",
      "308  0.506667  0.263967  1.0  0.067961  0.324538  0.0  0.0  0.973684   \n",
      "309  0.480000  0.101628  1.0  0.349515  0.000000  0.0  0.0  0.947368   \n",
      "\n",
      "           8         9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.000000  0.382353  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    0.000000  0.852941  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2    0.000000  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.000000  0.764706  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.000000  0.382353  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "..        ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.000000  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "306  0.000000  0.941176  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "307  0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "308  0.000000  0.970588  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.111111  0.941176  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "       1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
      "       1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
      "       1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
      "       0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "       0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "       0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0.,\n",
      "       0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
      "       0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
      "       1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
      "       1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0.,\n",
      "       0., 0., 1., 1.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n",
      "cluster: 5 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9c378480>, dataframe=<capsule object NULL at 0x152e101c0150>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7   \\\n",
      "0    0.480000  1.000000  1.0  0.126214  0.440844  0.0  0.0  0.421053   \n",
      "1    0.893333  0.102714  1.0  0.262136  0.152929  0.0  0.0  0.868421   \n",
      "2    0.146667  0.593820  1.0  0.854369  0.519156  0.0  0.0  0.921053   \n",
      "3    0.560000  0.578539  1.0  0.854369  0.118522  0.0  0.0  0.789474   \n",
      "4    0.506667  0.012860  1.0  0.932039  0.000633  0.0  0.0  0.421053   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...   \n",
      "305  0.840000  0.066221  1.0  0.067961  0.179208  0.0  0.0  0.921053   \n",
      "306  0.613333  0.062547  1.0  0.077670  0.244222  0.0  0.0  0.947368   \n",
      "307  0.680000  0.287015  1.0  0.271845  0.357150  0.0  0.0  0.000000   \n",
      "308  0.506667  0.263967  1.0  0.067961  0.324538  0.0  0.0  0.973684   \n",
      "309  0.480000  0.101628  1.0  0.349515  0.000000  0.0  0.0  0.947368   \n",
      "\n",
      "           8         9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.000000  0.382353  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    0.000000  0.852941  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2    0.000000  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.000000  0.764706  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.000000  0.382353  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "..        ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.000000  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "306  0.000000  0.941176  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "307  0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "308  0.000000  0.970588  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.111111  0.941176  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "       1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
      "       1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
      "       1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
      "       0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "       0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "       0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0.,\n",
      "       0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
      "       0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
      "       1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
      "       1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0.,\n",
      "       0., 0., 1., 1.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 7/13 [00:00<00:00,  7.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: 6 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9c378480>, dataframe=<capsule object NULL at 0x152e101c0150>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7   \\\n",
      "0    0.480000  1.000000  1.0  0.126214  0.440844  0.0  0.0  0.421053   \n",
      "1    0.893333  0.102714  1.0  0.262136  0.152929  0.0  0.0  0.868421   \n",
      "2    0.146667  0.593820  1.0  0.854369  0.519156  0.0  0.0  0.921053   \n",
      "3    0.560000  0.578539  1.0  0.854369  0.118522  0.0  0.0  0.789474   \n",
      "4    0.506667  0.012860  1.0  0.932039  0.000633  0.0  0.0  0.421053   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...   \n",
      "305  0.840000  0.066221  1.0  0.067961  0.179208  0.0  0.0  0.921053   \n",
      "306  0.613333  0.062547  1.0  0.077670  0.244222  0.0  0.0  0.947368   \n",
      "307  0.680000  0.287015  1.0  0.271845  0.357150  0.0  0.0  0.000000   \n",
      "308  0.506667  0.263967  1.0  0.067961  0.324538  0.0  0.0  0.973684   \n",
      "309  0.480000  0.101628  1.0  0.349515  0.000000  0.0  0.0  0.947368   \n",
      "\n",
      "           8         9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.000000  0.382353  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    0.000000  0.852941  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2    0.000000  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.000000  0.764706  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.000000  0.382353  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "..        ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.000000  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "306  0.000000  0.941176  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "307  0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "308  0.000000  0.970588  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.111111  0.941176  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "       1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
      "       1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
      "       1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
      "       0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "       0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "       0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0.,\n",
      "       0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
      "       0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
      "       1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
      "       1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0.,\n",
      "       0., 0., 1., 1.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n",
      "cluster: 7 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9c378480>, dataframe=<capsule object NULL at 0x152e101c0150>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7   \\\n",
      "0    0.480000  1.000000  1.0  0.126214  0.440844  0.0  0.0  0.421053   \n",
      "1    0.893333  0.102714  1.0  0.262136  0.152929  0.0  0.0  0.868421   \n",
      "2    0.146667  0.593820  1.0  0.854369  0.519156  0.0  0.0  0.921053   \n",
      "3    0.560000  0.578539  1.0  0.854369  0.118522  0.0  0.0  0.789474   \n",
      "4    0.506667  0.012860  1.0  0.932039  0.000633  0.0  0.0  0.421053   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...   \n",
      "305  0.840000  0.066221  1.0  0.067961  0.179208  0.0  0.0  0.921053   \n",
      "306  0.613333  0.062547  1.0  0.077670  0.244222  0.0  0.0  0.947368   \n",
      "307  0.680000  0.287015  1.0  0.271845  0.357150  0.0  0.0  0.000000   \n",
      "308  0.506667  0.263967  1.0  0.067961  0.324538  0.0  0.0  0.973684   \n",
      "309  0.480000  0.101628  1.0  0.349515  0.000000  0.0  0.0  0.947368   \n",
      "\n",
      "           8         9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.000000  0.382353  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    0.000000  0.852941  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2    0.000000  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.000000  0.764706  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.000000  0.382353  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "..        ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.000000  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "306  0.000000  0.941176  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "307  0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "308  0.000000  0.970588  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.111111  0.941176  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "       1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
      "       1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
      "       1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
      "       0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "       0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "       0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0.,\n",
      "       0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
      "       0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
      "       1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
      "       1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0.,\n",
      "       0., 0., 1., 1.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 9/13 [00:01<00:00,  7.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: 8 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9c378480>, dataframe=<capsule object NULL at 0x152e101c0150>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7   \\\n",
      "0    0.480000  1.000000  1.0  0.126214  0.440844  0.0  0.0  0.421053   \n",
      "1    0.893333  0.102714  1.0  0.262136  0.152929  0.0  0.0  0.868421   \n",
      "2    0.146667  0.593820  1.0  0.854369  0.519156  0.0  0.0  0.921053   \n",
      "3    0.560000  0.578539  1.0  0.854369  0.118522  0.0  0.0  0.789474   \n",
      "4    0.506667  0.012860  1.0  0.932039  0.000633  0.0  0.0  0.421053   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...   \n",
      "305  0.840000  0.066221  1.0  0.067961  0.179208  0.0  0.0  0.921053   \n",
      "306  0.613333  0.062547  1.0  0.077670  0.244222  0.0  0.0  0.947368   \n",
      "307  0.680000  0.287015  1.0  0.271845  0.357150  0.0  0.0  0.000000   \n",
      "308  0.506667  0.263967  1.0  0.067961  0.324538  0.0  0.0  0.973684   \n",
      "309  0.480000  0.101628  1.0  0.349515  0.000000  0.0  0.0  0.947368   \n",
      "\n",
      "           8         9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.000000  0.382353  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    0.000000  0.852941  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2    0.000000  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.000000  0.764706  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.000000  0.382353  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "..        ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.000000  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "306  0.000000  0.941176  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "307  0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "308  0.000000  0.970588  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.111111  0.941176  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "       1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
      "       1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
      "       1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
      "       0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "       0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "       0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0.,\n",
      "       0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
      "       0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
      "       1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
      "       1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0.,\n",
      "       0., 0., 1., 1.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n",
      "cluster: 9 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9c378480>, dataframe=<capsule object NULL at 0x152e101c0150>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7   \\\n",
      "0    0.480000  1.000000  1.0  0.126214  0.440844  0.0  0.0  0.421053   \n",
      "1    0.893333  0.102714  1.0  0.262136  0.152929  0.0  0.0  0.868421   \n",
      "2    0.146667  0.593820  1.0  0.854369  0.519156  0.0  0.0  0.921053   \n",
      "3    0.560000  0.578539  1.0  0.854369  0.118522  0.0  0.0  0.789474   \n",
      "4    0.506667  0.012860  1.0  0.932039  0.000633  0.0  0.0  0.421053   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...   \n",
      "305  0.840000  0.066221  1.0  0.067961  0.179208  0.0  0.0  0.921053   \n",
      "306  0.613333  0.062547  1.0  0.077670  0.244222  0.0  0.0  0.947368   \n",
      "307  0.680000  0.287015  1.0  0.271845  0.357150  0.0  0.0  0.000000   \n",
      "308  0.506667  0.263967  1.0  0.067961  0.324538  0.0  0.0  0.973684   \n",
      "309  0.480000  0.101628  1.0  0.349515  0.000000  0.0  0.0  0.947368   \n",
      "\n",
      "           8         9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.000000  0.382353  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    0.000000  0.852941  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2    0.000000  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.000000  0.764706  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.000000  0.382353  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "..        ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.000000  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "306  0.000000  0.941176  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "307  0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "308  0.000000  0.970588  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.111111  0.941176  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "       1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
      "       1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
      "       1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
      "       0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "       0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "       0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0.,\n",
      "       0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
      "       0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
      "       1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
      "       1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0.,\n",
      "       0., 0., 1., 1.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 11/13 [00:01<00:00,  7.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: 10 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9c378480>, dataframe=<capsule object NULL at 0x152e101c0150>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7   \\\n",
      "0    0.480000  1.000000  1.0  0.126214  0.440844  0.0  0.0  0.421053   \n",
      "1    0.893333  0.102714  1.0  0.262136  0.152929  0.0  0.0  0.868421   \n",
      "2    0.146667  0.593820  1.0  0.854369  0.519156  0.0  0.0  0.921053   \n",
      "3    0.560000  0.578539  1.0  0.854369  0.118522  0.0  0.0  0.789474   \n",
      "4    0.506667  0.012860  1.0  0.932039  0.000633  0.0  0.0  0.421053   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...   \n",
      "305  0.840000  0.066221  1.0  0.067961  0.179208  0.0  0.0  0.921053   \n",
      "306  0.613333  0.062547  1.0  0.077670  0.244222  0.0  0.0  0.947368   \n",
      "307  0.680000  0.287015  1.0  0.271845  0.357150  0.0  0.0  0.000000   \n",
      "308  0.506667  0.263967  1.0  0.067961  0.324538  0.0  0.0  0.973684   \n",
      "309  0.480000  0.101628  1.0  0.349515  0.000000  0.0  0.0  0.947368   \n",
      "\n",
      "           8         9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.000000  0.382353  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    0.000000  0.852941  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2    0.000000  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.000000  0.764706  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.000000  0.382353  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "..        ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.000000  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "306  0.000000  0.941176  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "307  0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "308  0.000000  0.970588  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.111111  0.941176  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "       1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
      "       1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
      "       1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
      "       0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "       0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "       0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0.,\n",
      "       0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
      "       0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
      "       1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
      "       1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0.,\n",
      "       0., 0., 1., 1.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n",
      "cluster: 11 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9c378480>, dataframe=<capsule object NULL at 0x152e101c0150>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7   \\\n",
      "0    0.480000  1.000000  1.0  0.126214  0.440844  0.0  0.0  0.421053   \n",
      "1    0.893333  0.102714  1.0  0.262136  0.152929  0.0  0.0  0.868421   \n",
      "2    0.146667  0.593820  1.0  0.854369  0.519156  0.0  0.0  0.921053   \n",
      "3    0.560000  0.578539  1.0  0.854369  0.118522  0.0  0.0  0.789474   \n",
      "4    0.506667  0.012860  1.0  0.932039  0.000633  0.0  0.0  0.421053   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...   \n",
      "305  0.840000  0.066221  1.0  0.067961  0.179208  0.0  0.0  0.921053   \n",
      "306  0.613333  0.062547  1.0  0.077670  0.244222  0.0  0.0  0.947368   \n",
      "307  0.680000  0.287015  1.0  0.271845  0.357150  0.0  0.0  0.000000   \n",
      "308  0.506667  0.263967  1.0  0.067961  0.324538  0.0  0.0  0.973684   \n",
      "309  0.480000  0.101628  1.0  0.349515  0.000000  0.0  0.0  0.947368   \n",
      "\n",
      "           8         9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.000000  0.382353  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    0.000000  0.852941  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2    0.000000  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.000000  0.764706  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.000000  0.382353  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "..        ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.000000  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "306  0.000000  0.941176  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "307  0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "308  0.000000  0.970588  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.111111  0.941176  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "       1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
      "       1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
      "       1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
      "       0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "       0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "       0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0.,\n",
      "       0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
      "       0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
      "       1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
      "       1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0.,\n",
      "       0., 0., 1., 1.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00,  7.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: 12 model: RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=95, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x152d9c378480>, dataframe=<capsule object NULL at 0x152e101c0150>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7   \\\n",
      "0    0.480000  1.000000  1.0  0.126214  0.440844  0.0  0.0  0.421053   \n",
      "1    0.893333  0.102714  1.0  0.262136  0.152929  0.0  0.0  0.868421   \n",
      "2    0.146667  0.593820  1.0  0.854369  0.519156  0.0  0.0  0.921053   \n",
      "3    0.560000  0.578539  1.0  0.854369  0.118522  0.0  0.0  0.789474   \n",
      "4    0.506667  0.012860  1.0  0.932039  0.000633  0.0  0.0  0.421053   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...   \n",
      "305  0.840000  0.066221  1.0  0.067961  0.179208  0.0  0.0  0.921053   \n",
      "306  0.613333  0.062547  1.0  0.077670  0.244222  0.0  0.0  0.947368   \n",
      "307  0.680000  0.287015  1.0  0.271845  0.357150  0.0  0.0  0.000000   \n",
      "308  0.506667  0.263967  1.0  0.067961  0.324538  0.0  0.0  0.973684   \n",
      "309  0.480000  0.101628  1.0  0.349515  0.000000  0.0  0.0  0.947368   \n",
      "\n",
      "           8         9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.000000  0.382353  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "1    0.000000  0.852941  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2    0.000000  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.000000  0.764706  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.000000  0.382353  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "..        ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.000000  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "306  0.000000  0.941176  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
      "307  0.000000  0.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "308  0.000000  0.970588  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.111111  0.941176  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1.,\n",
      "       0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "       1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "       1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
      "       1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
      "       1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
      "       0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "       0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "       1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "       0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "       0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0.,\n",
      "       0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
      "       1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "       1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
      "       0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
      "       1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
      "       1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0.,\n",
      "       0., 0., 1., 1.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13 [00:00<?, ?it/s] This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/discrete/discrete_model.py:1819: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  2.32930D-01\n",
      "\n",
      "At iterate    1    f=  6.01411D-01    |proj g|=  4.52282D-02\n",
      "\n",
      "At iterate    2    f=  5.82667D-01    |proj g|=  4.38019D-02\n",
      "\n",
      "At iterate    3    f=  4.32816D-01    |proj g|=  1.29740D-01\n",
      "\n",
      "At iterate    4    f=  3.62037D-01    |proj g|=  1.01478D-01\n",
      "\n",
      "At iterate    5    f=  2.82111D-01    |proj g|=  5.00174D-02\n",
      "\n",
      "At iterate    6    f=  2.43187D-01    |proj g|=  2.18067D-02\n",
      "\n",
      "At iterate    7    f=  2.17662D-01    |proj g|=  1.10347D-02\n",
      "\n",
      "At iterate    8    f=  1.97712D-01    |proj g|=  1.29499D-02\n",
      "\n",
      "At iterate    9    f=  1.71869D-01    |proj g|=  1.19862D-02\n",
      "\n",
      "At iterate   10    f=  1.23320D-01    |proj g|=  3.95035D-03\n",
      "\n",
      "At iterate   11    f=  1.04059D-01    |proj g|=  3.34595D-03\n",
      "\n",
      "At iterate   12    f=  8.78687D-02    |proj g|=  2.97422D-02\n",
      "\n",
      "At iterate   13    f=  7.26592D-02    |proj g|=  9.20539D-03\n",
      "\n",
      "At iterate   14    f=  6.79768D-02    |proj g|=  4.73159D-03\n",
      "\n",
      "At iterate   15    f=  6.49289D-02    |proj g|=  3.94929D-03\n",
      "\n",
      "At iterate   16    f=  6.17070D-02    |proj g|=  3.24949D-03\n",
      "\n",
      "At iterate   17    f=  5.31454D-02    |proj g|=  3.37898D-03\n",
      "\n",
      "At iterate   18    f=  4.94764D-02    |proj g|=  4.14670D-03\n",
      "\n",
      "At iterate   19    f=  4.42447D-02    |proj g|=  3.57130D-03\n",
      "\n",
      "At iterate   20    f=  3.50273D-02    |proj g|=  1.89001D-03\n",
      "\n",
      "At iterate   21    f=  3.48018D-02    |proj g|=  4.37173D-03\n",
      "\n",
      "At iterate   22    f=  3.31984D-02    |proj g|=  1.13647D-03\n",
      "\n",
      "At iterate   23    f=  3.30533D-02    |proj g|=  4.51694D-04\n",
      "\n",
      "At iterate   24    f=  3.30140D-02    |proj g|=  5.84602D-04\n",
      "\n",
      "At iterate   25    f=  3.28889D-02    |proj g|=  7.34477D-04\n",
      "\n",
      "At iterate   26    f=  3.26473D-02    |proj g|=  9.30684D-04\n",
      "\n",
      "At iterate   27    f=  3.24533D-02    |proj g|=  1.05148D-03\n",
      "\n",
      "At iterate   28    f=  3.16577D-02    |proj g|=  1.34477D-03\n",
      "\n",
      "At iterate   29    f=  2.98253D-02    |proj g|=  1.38710D-03\n",
      "\n",
      "At iterate   30    f=  2.83330D-02    |proj g|=  1.27893D-03\n",
      "\n",
      "At iterate   31    f=  2.64482D-02    |proj g|=  9.44658D-04\n",
      "\n",
      "At iterate   32    f=  2.22847D-02    |proj g|=  1.18577D-03\n",
      "\n",
      "At iterate   33    f=  2.07818D-02    |proj g|=  1.24110D-03\n",
      "\n",
      "At iterate   34    f=  1.86319D-02    |proj g|=  4.02664D-03\n",
      "\n",
      "At iterate   35    f=  1.69193D-02    |proj g|=  4.52797D-03\n",
      "\n",
      "At iterate   36    f=  1.06579D-02    |proj g|=  1.62370D-03\n",
      "\n",
      "At iterate   37    f=  7.32632D-03    |proj g|=  7.53254D-04\n",
      "\n",
      "At iterate   38    f=  3.46055D-03    |proj g|=  1.78780D-03\n",
      "\n",
      "At iterate   39    f=  2.06152D-03    |proj g|=  1.82622D-03\n",
      "\n",
      "At iterate   40    f=  5.75653D-04    |proj g|=  5.31555D-04\n",
      "\n",
      "At iterate   41    f=  3.76205D-04    |proj g|=  3.41051D-04\n",
      "\n",
      "At iterate   42    f=  2.05558D-04    |proj g|=  1.75943D-04\n",
      "\n",
      "At iterate   43    f=  1.27920D-04    |proj g|=  1.02122D-04\n",
      "\n",
      "At iterate   44    f=  7.95901D-05    |proj g|=  5.78739D-05\n",
      "\n",
      "At iterate   45    f=  4.95020D-05    |proj g|=  3.08870D-05\n",
      "\n",
      "At iterate   46    f=  2.93499D-05    |proj g|=  1.36453D-05\n",
      "\n",
      "At iterate   47    f=  1.67438D-05    |proj g|=  3.85571D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     47     53      1     0     0   3.856D-06   1.674D-05\n",
      "  F =   1.6743830296933965E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  2.61218D-01\n",
      "\n",
      "At iterate    1    f=  5.58867D-01    |proj g|=  5.04352D-02\n",
      "\n",
      "At iterate    2    f=  5.38408D-01    |proj g|=  4.59172D-02\n",
      "\n",
      "At iterate    3    f=  3.93676D-01    |proj g|=  1.37716D-01\n",
      "\n",
      "At iterate    4    f=  3.14900D-01    |proj g|=  1.11536D-01\n",
      "\n",
      "At iterate    5    f=  2.05549D-01    |proj g|=  5.28165D-02\n",
      "\n",
      "At iterate    6    f=  1.53704D-01    |proj g|=  2.44144D-02\n",
      "\n",
      "At iterate    7    f=  1.17367D-01    |proj g|=  1.00217D-02\n",
      "\n",
      "At iterate    8    f=  9.01431D-02    |proj g|=  7.69907D-03\n",
      "\n",
      "At iterate    9    f=  5.92146D-02    |proj g|=  7.34815D-03\n",
      "\n",
      "At iterate   10    f=  2.77634D-02    |proj g|=  5.26603D-03\n",
      "\n",
      "At iterate   11    f=  1.06812D-02    |proj g|=  1.39562D-03\n",
      "\n",
      "At iterate   12    f=  6.58662D-03    |proj g|=  5.60088D-04\n",
      "\n",
      "At iterate   13    f=  3.36124D-03    |proj g|=  1.49720D-03\n",
      "\n",
      "At iterate   14    f=  2.01786D-03    |proj g|=  1.82036D-03\n",
      "\n",
      "At iterate   15    f=  8.97096D-04    |proj g|=  6.84076D-04\n",
      "\n",
      "At iterate   16    f=  5.39063D-04    |proj g|=  3.30010D-04\n",
      "\n",
      "At iterate   17    f=  3.04786D-04    |proj g|=  1.20062D-04\n",
      "\n",
      "At iterate   18    f=  1.80404D-04    |proj g|=  4.58875D-05\n",
      "\n",
      "At iterate   19    f=  1.04901D-04    |proj g|=  1.92363D-05\n",
      "\n",
      "At iterate   20    f=  6.00267D-05    |proj g|=  2.06870D-05\n",
      "\n",
      "At iterate   21    f=  3.18323D-05    |proj g|=  1.36234D-05\n",
      "\n",
      "At iterate   22    f=  1.65657D-05    |proj g|=  8.55930D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     22     25      1     0     0   8.559D-06   1.657D-05\n",
      "  F =   1.6565683368680238E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " 23%|██▎       | 3/13 [00:00<00:00, 29.56it/s] This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  6.32124D-02\n",
      "\n",
      "At iterate    1    f=  5.86708D-01    |proj g|=  5.30041D-02\n",
      "\n",
      "At iterate    2    f=  5.63534D-01    |proj g|=  1.18547D-01\n",
      "\n",
      "At iterate    3    f=  3.84076D-01    |proj g|=  3.48204D-02\n",
      "\n",
      "At iterate    4    f=  3.37361D-01    |proj g|=  2.63665D-02\n",
      "\n",
      "At iterate    5    f=  2.34567D-01    |proj g|=  1.32827D-02\n",
      "\n",
      "At iterate    6    f=  1.69685D-01    |proj g|=  8.10127D-03\n",
      "\n",
      "At iterate    7    f=  1.04510D-01    |proj g|=  5.82897D-03\n",
      "\n",
      "At iterate    8    f=  7.00509D-02    |proj g|=  1.11864D-02\n",
      "\n",
      "At iterate    9    f=  4.45280D-02    |proj g|=  6.87529D-03\n",
      "\n",
      "At iterate   10    f=  3.00423D-02    |proj g|=  5.12490D-03\n",
      "\n",
      "At iterate   11    f=  1.88814D-02    |proj g|=  3.82550D-03\n",
      "\n",
      "At iterate   12    f=  1.13739D-02    |proj g|=  2.20282D-03\n",
      "\n",
      "At iterate   13    f=  8.20227D-03    |proj g|=  3.78958D-03\n",
      "\n",
      "At iterate   14    f=  4.19374D-03    |proj g|=  9.89485D-04\n",
      "\n",
      "At iterate   15    f=  2.78282D-03    |proj g|=  5.59823D-04\n",
      "\n",
      "At iterate   16    f=  1.49276D-03    |proj g|=  2.60133D-04\n",
      "\n",
      "At iterate   17    f=  7.94988D-04    |proj g|=  1.15752D-04\n",
      "\n",
      "At iterate   18    f=  4.03566D-04    |proj g|=  5.12786D-05\n",
      "\n",
      "At iterate   19    f=  2.05107D-04    |proj g|=  3.03725D-05\n",
      "\n",
      "At iterate   20    f=  1.03199D-04    |proj g|=  1.72211D-05\n",
      "\n",
      "At iterate   21    f=  5.18293D-05    |proj g|=  9.33146D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     21     23      1     0     0   9.331D-06   5.183D-05\n",
      "  F =   5.1829312640979296E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  8.94113D-02\n",
      "\n",
      "At iterate    1    f=  6.81585D-01    |proj g|=  7.69933D-02\n",
      "\n",
      "At iterate    2    f=  5.47358D-01    |proj g|=  6.89228D-02\n",
      "\n",
      "At iterate    3    f=  3.51706D-01    |proj g|=  2.53225D-02\n",
      "\n",
      "At iterate    4    f=  2.97066D-01    |proj g|=  1.51499D-02\n",
      "\n",
      "At iterate    5    f=  2.44031D-01    |proj g|=  1.00281D-02\n",
      "\n",
      "At iterate    6    f=  1.78323D-01    |proj g|=  7.01357D-03\n",
      "\n",
      "At iterate    7    f=  9.05793D-02    |proj g|=  5.87015D-03\n",
      "\n",
      "At iterate    8    f=  4.78432D-02    |proj g|=  3.55413D-03\n",
      "\n",
      "At iterate    9    f=  3.22370D-02    |proj g|=  1.54291D-03\n",
      "\n",
      "At iterate   10    f=  2.14873D-02    |proj g|=  9.66647D-04\n",
      "\n",
      "At iterate   11    f=  1.32198D-02    |proj g|=  2.47466D-03\n",
      "\n",
      "At iterate   12    f=  1.19433D-02    |proj g|=  3.20019D-03\n",
      "\n",
      "At iterate   13    f=  8.87872D-03    |proj g|=  1.28231D-03\n",
      "\n",
      "At iterate   14    f=  5.71569D-03    |proj g|=  6.20049D-04\n",
      "\n",
      "At iterate   15    f=  3.66719D-03    |proj g|=  3.42417D-04\n",
      "\n",
      "At iterate   16    f=  2.20077D-03    |proj g|=  1.97906D-04\n",
      "\n",
      "At iterate   17    f=  1.21930D-03    |proj g|=  1.05812D-04\n",
      "\n",
      "At iterate   18    f=  6.29624D-04    |proj g|=  6.19771D-05\n",
      "\n",
      "At iterate   19    f=  3.19742D-04    |proj g|=  3.32414D-05\n",
      "\n",
      "At iterate   20    f=  1.71351D-04    |proj g|=  3.26895D-05\n",
      "\n",
      "At iterate   21    f=  1.51286D-04    |proj g|=  4.85284D-05\n",
      "\n",
      "At iterate   22    f=  7.30424D-05    |proj g|=  1.15125D-05\n",
      "\n",
      "At iterate   23    f=  3.82289D-05    |proj g|=  4.54680D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     23     29      1     0     0   4.547D-06   3.823D-05\n",
      "  F =   3.8228914617515541E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  2.06185D-01\n",
      "\n",
      "At iterate    1    f=  6.13554D-01    |proj g|=  5.41759D-02\n",
      "\n",
      "At iterate    2    f=  5.48645D-01    |proj g|=  6.47427D-02\n",
      "\n",
      "At iterate    3    f=  4.15785D-01    |proj g|=  1.00470D-01\n",
      "\n",
      "At iterate    4    f=  3.27444D-01    |proj g|=  6.12688D-02\n",
      "\n",
      "At iterate    5    f=  2.71471D-01    |proj g|=  2.61910D-02\n",
      "\n",
      "At iterate    6    f=  2.43088D-01    |proj g|=  1.05498D-02\n",
      "\n",
      "At iterate    7    f=  2.25873D-01    |proj g|=  1.20476D-02\n",
      "\n",
      "At iterate    8    f=  2.08810D-01    |proj g|=  1.68294D-02\n",
      "\n",
      "At iterate    9    f=  1.71936D-01    |proj g|=  1.35376D-02\n",
      "\n",
      "At iterate   10    f=  1.12469D-01    |proj g|=  5.77846D-03\n",
      "\n",
      "At iterate   11    f=  8.86666D-02    |proj g|=  4.59098D-03\n",
      "\n",
      "At iterate   12    f=  8.25913D-02    |proj g|=  1.52716D-02\n",
      "\n",
      "At iterate   13    f=  7.10869D-02    |proj g|=  8.95080D-03\n",
      "\n",
      "At iterate   14    f=  6.00359D-02    |proj g|=  6.81698D-03\n",
      "\n",
      "At iterate   15    f=  4.35244D-02    |proj g|=  5.06324D-03\n",
      "\n",
      "At iterate   16    f=  2.40075D-02    |proj g|=  9.20172D-03\n",
      "\n",
      "At iterate   17    f=  1.38755D-02    |proj g|=  3.14843D-03\n",
      "\n",
      "At iterate   18    f=  8.74013D-03    |proj g|=  1.09754D-03\n",
      "\n",
      "At iterate   19    f=  5.67772D-03    |proj g|=  8.37310D-04\n",
      "\n",
      "At iterate   20    f=  3.20438D-03    |proj g|=  6.81896D-04\n",
      "\n",
      "At iterate   21    f=  1.65186D-03    |proj g|=  4.74951D-04\n",
      "\n",
      "At iterate   22    f=  9.16570D-04    |proj g|=  2.47222D-04\n",
      "\n",
      "At iterate   23    f=  4.59150D-04    |proj g|=  2.23955D-04\n",
      "\n",
      "At iterate   24    f=  4.34962D-04    |proj g|=  1.68018D-04\n",
      "\n",
      "At iterate   25    f=  2.26458D-04    |proj g|=  6.61120D-05\n",
      "\n",
      "At iterate   26    f=  1.14029D-04    |proj g|=  3.23049D-05\n",
      "\n",
      "At iterate   27    f=  5.75398D-05    |proj g|=  1.62972D-05\n",
      "\n",
      "At iterate   28    f=  2.88734D-05    |proj g|=  8.18019D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     28     34      1     0     0   8.180D-06   2.887D-05\n",
      "  F =   2.8873421324094533E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  5.37147D-02\n",
      "\n",
      "At iterate    1    f=  6.84019D-01    |proj g|=  6.30143D-02\n",
      "\n",
      "At iterate    2    f=  5.65762D-01    |proj g|=  7.82856D-02\n",
      "\n",
      "At iterate    3    f=  3.66992D-01    |proj g|=  3.15596D-02\n",
      "\n",
      "At iterate    4    f=  3.06013D-01    |proj g|=  1.55084D-02\n",
      "\n",
      "At iterate    5    f=  2.57568D-01    |proj g|=  1.20598D-02\n",
      "\n",
      "At iterate    6    f=  2.17262D-01    |proj g|=  1.05948D-02\n",
      "\n",
      "At iterate    7    f=  1.50780D-01    |proj g|=  1.58795D-02\n",
      "\n",
      "At iterate    8    f=  1.30864D-01    |proj g|=  9.59380D-03\n",
      "\n",
      "At iterate    9    f=  1.08425D-01    |proj g|=  4.54336D-03\n",
      "\n",
      "At iterate   10    f=  9.62991D-02    |proj g|=  4.91583D-03\n",
      "\n",
      "At iterate   11    f=  8.50360D-02    |proj g|=  7.73385D-03\n",
      "\n",
      "At iterate   12    f=  8.10864D-02    |proj g|=  7.04699D-03\n",
      "\n",
      "At iterate   13    f=  7.16095D-02    |proj g|=  4.73044D-03\n",
      "\n",
      "At iterate   14    f=  6.49900D-02    |proj g|=  5.44456D-03\n",
      "\n",
      "At iterate   15    f=  3.82032D-02    |proj g|=  4.91532D-03\n",
      "\n",
      "At iterate   16    f=  2.08029D-02    |proj g|=  2.99376D-03\n",
      "\n",
      "At iterate   17    f=  1.55579D-02    |proj g|=  1.91719D-03\n",
      "\n",
      "At iterate   18    f=  1.01426D-02    |proj g|=  1.15953D-03\n",
      "\n",
      "At iterate   19    f=  6.71834D-03    |proj g|=  9.87367D-04\n",
      "\n",
      "At iterate   20    f=  3.42871D-03    |proj g|=  7.78236D-04\n",
      "\n",
      "At iterate   21    f=  2.32131D-03    |proj g|=  1.33553D-03\n",
      "\n",
      "At iterate   22    f=  1.22895D-03    |proj g|=  5.47342D-04\n",
      "\n",
      "At iterate   23    f=  6.33522D-04    |proj g|=  2.37512D-04\n",
      "\n",
      "At iterate   24    f=  3.18539D-04    |proj g|=  1.02850D-04\n",
      "\n",
      "At iterate   25    f=  1.61162D-04    |proj g|=  4.52454D-05\n",
      "\n",
      "At iterate   26    f=  8.12725D-05    |proj g|=  1.93095D-05\n",
      "\n",
      "At iterate   27    f=  4.08380D-05    |proj g|=  9.14977D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     27     32      1     0     0   9.150D-06   4.084D-05\n",
      "  F =   4.0838022497719912E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  7.36396D-02\n",
      "\n",
      "At iterate    1    f=  6.80665D-01    |proj g|=  6.52790D-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n",
      " 54%|█████▍    | 7/13 [00:00<00:00, 31.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate    2    f=  5.41310D-01    |proj g|=  7.67767D-02\n",
      "\n",
      "At iterate    3    f=  3.78736D-01    |proj g|=  2.36344D-02\n",
      "\n",
      "At iterate    4    f=  3.33064D-01    |proj g|=  1.29375D-02\n",
      "\n",
      "At iterate    5    f=  2.82620D-01    |proj g|=  9.60754D-03\n",
      "\n",
      "At iterate    6    f=  1.73353D-01    |proj g|=  9.42553D-03\n",
      "\n",
      "At iterate    7    f=  9.60984D-02    |proj g|=  3.43667D-03\n",
      "\n",
      "At iterate    8    f=  6.21613D-02    |proj g|=  3.04859D-03\n",
      "\n",
      "At iterate    9    f=  3.83797D-02    |proj g|=  4.62457D-03\n",
      "\n",
      "At iterate   10    f=  1.76837D-02    |proj g|=  1.52216D-03\n",
      "\n",
      "At iterate   11    f=  1.13216D-02    |proj g|=  5.84170D-04\n",
      "\n",
      "At iterate   12    f=  1.07246D-02    |proj g|=  3.85305D-03\n",
      "\n",
      "At iterate   13    f=  6.57586D-03    |proj g|=  1.67749D-03\n",
      "\n",
      "At iterate   14    f=  4.42063D-03    |proj g|=  1.11055D-03\n",
      "\n",
      "At iterate   15    f=  2.68155D-03    |proj g|=  6.24481D-04\n",
      "\n",
      "At iterate   16    f=  1.54624D-03    |proj g|=  3.07733D-04\n",
      "\n",
      "At iterate   17    f=  8.56676D-04    |proj g|=  1.32189D-04\n",
      "\n",
      "At iterate   18    f=  4.56324D-04    |proj g|=  5.66912D-05\n",
      "\n",
      "At iterate   19    f=  2.35938D-04    |proj g|=  3.34179D-05\n",
      "\n",
      "At iterate   20    f=  1.19478D-04    |proj g|=  1.79585D-05\n",
      "\n",
      "At iterate   21    f=  5.99100D-05    |proj g|=  9.19867D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     21     26      1     0     0   9.199D-06   5.991D-05\n",
      "  F =   5.9909991054302479E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  2.21176D-01\n",
      "\n",
      "At iterate    1    f=  6.04290D-01    |proj g|=  4.71823D-02\n",
      "\n",
      "At iterate    2    f=  5.20924D-01    |proj g|=  7.32478D-02\n",
      "\n",
      "At iterate    3    f=  3.70201D-01    |proj g|=  1.02958D-01\n",
      "\n",
      "At iterate    4    f=  2.92798D-01    |proj g|=  6.41876D-02\n",
      "\n",
      "At iterate    5    f=  2.38489D-01    |proj g|=  2.99365D-02\n",
      "\n",
      "At iterate    6    f=  2.08893D-01    |proj g|=  1.21615D-02\n",
      "\n",
      "At iterate    7    f=  1.85230D-01    |proj g|=  9.88460D-03\n",
      "\n",
      "At iterate    8    f=  1.57970D-01    |proj g|=  1.33339D-02\n",
      "\n",
      "At iterate    9    f=  1.06547D-01    |proj g|=  8.36796D-03\n",
      "\n",
      "At iterate   10    f=  5.13753D-02    |proj g|=  1.14962D-02\n",
      "\n",
      "At iterate   11    f=  2.92494D-02    |proj g|=  8.82240D-03\n",
      "\n",
      "At iterate   12    f=  2.55219D-02    |proj g|=  6.00863D-03\n",
      "\n",
      "At iterate   13    f=  1.73622D-02    |proj g|=  3.30134D-03\n",
      "\n",
      "At iterate   14    f=  1.02960D-02    |proj g|=  1.97583D-03\n",
      "\n",
      "At iterate   15    f=  4.99077D-03    |proj g|=  9.85961D-04\n",
      "\n",
      "At iterate   16    f=  2.52444D-03    |proj g|=  4.85817D-04\n",
      "\n",
      "At iterate   17    f=  1.25538D-03    |proj g|=  2.23859D-04\n",
      "\n",
      "At iterate   18    f=  6.29946D-04    |proj g|=  1.14190D-04\n",
      "\n",
      "At iterate   19    f=  3.15184D-04    |proj g|=  4.74609D-05\n",
      "\n",
      "At iterate   20    f=  1.57432D-04    |proj g|=  3.37823D-05\n",
      "\n",
      "At iterate   21    f=  1.45856D-04    |proj g|=  6.93259D-05\n",
      "\n",
      "At iterate   22    f=  4.60281D-05    |proj g|=  8.80496D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     22     27      1     0     0   8.805D-06   4.603D-05\n",
      "  F =   4.6028059255053962E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  2.97262D-01\n",
      "\n",
      "At iterate    1    f=  5.20266D-01    |proj g|=  3.95779D-02\n",
      "\n",
      "At iterate    2    f=  5.01909D-01    |proj g|=  3.93375D-02\n",
      "\n",
      "At iterate    3    f=  3.85888D-01    |proj g|=  1.33718D-01\n",
      "\n",
      "At iterate    4    f=  3.20038D-01    |proj g|=  1.10968D-01\n",
      "\n",
      "At iterate    5    f=  2.19116D-01    |proj g|=  5.06002D-02\n",
      "\n",
      "At iterate    6    f=  1.75150D-01    |proj g|=  2.64502D-02\n",
      "\n",
      "At iterate    7    f=  1.43057D-01    |proj g|=  1.27905D-02\n",
      "\n",
      "At iterate    8    f=  1.17342D-01    |proj g|=  8.56932D-03\n",
      "\n",
      "At iterate    9    f=  8.17104D-02    |proj g|=  7.25375D-03\n",
      "\n",
      "At iterate   10    f=  3.73699D-02    |proj g|=  2.91076D-03\n",
      "\n",
      "At iterate   11    f=  1.89002D-02    |proj g|=  1.80136D-03\n",
      "\n",
      "At iterate   12    f=  8.78285D-03    |proj g|=  2.27365D-03\n",
      "\n",
      "At iterate   13    f=  4.14763D-03    |proj g|=  4.50058D-04\n",
      "\n",
      "At iterate   14    f=  1.88847D-03    |proj g|=  8.77019D-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " This problem is unconstrained.\n",
      " 85%|████████▍ | 11/13 [00:00<00:00, 32.09it/s] This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      "100%|██████████| 13/13 [00:00<00:00, 32.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   15    f=  9.44751D-04    |proj g|=  2.50505D-04\n",
      "\n",
      "At iterate   16    f=  5.14959D-04    |proj g|=  9.36130D-05\n",
      "\n",
      "At iterate   17    f=  2.58822D-04    |proj g|=  4.41050D-05\n",
      "\n",
      "At iterate   18    f=  1.30319D-04    |proj g|=  3.04372D-05\n",
      "\n",
      "At iterate   19    f=  7.18911D-05    |proj g|=  2.45847D-05\n",
      "\n",
      "At iterate   20    f=  4.28817D-05    |proj g|=  1.40574D-05\n",
      "\n",
      "At iterate   21    f=  2.01283D-05    |proj g|=  6.36586D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     21     24      1     0     0   6.366D-06   2.013D-05\n",
      "  F =   2.0128289768847820E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  2.37355D-01\n",
      "\n",
      "At iterate    1    f=  5.99703D-01    |proj g|=  3.69949D-02\n",
      "\n",
      "At iterate    2    f=  5.81358D-01    |proj g|=  4.39845D-02\n",
      "\n",
      "At iterate    3    f=  4.43344D-01    |proj g|=  1.26704D-01\n",
      "\n",
      "At iterate    4    f=  3.74519D-01    |proj g|=  9.83913D-02\n",
      "\n",
      "At iterate    5    f=  2.93611D-01    |proj g|=  4.69263D-02\n",
      "\n",
      "At iterate    6    f=  2.54659D-01    |proj g|=  1.73023D-02\n",
      "\n",
      "At iterate    7    f=  2.31206D-01    |proj g|=  1.00673D-02\n",
      "\n",
      "At iterate    8    f=  2.12603D-01    |proj g|=  1.60198D-02\n",
      "\n",
      "At iterate    9    f=  1.84999D-01    |proj g|=  2.05103D-02\n",
      "\n",
      "At iterate   10    f=  1.27653D-01    |proj g|=  1.71370D-02\n",
      "\n",
      "At iterate   11    f=  6.60911D-02    |proj g|=  5.06182D-03\n",
      "\n",
      "At iterate   12    f=  4.92981D-02    |proj g|=  3.31497D-03\n",
      "\n",
      "At iterate   13    f=  3.82690D-02    |proj g|=  2.02826D-02\n",
      "\n",
      "At iterate   14    f=  2.33358D-02    |proj g|=  5.92354D-03\n",
      "\n",
      "At iterate   15    f=  1.99778D-02    |proj g|=  4.45771D-03\n",
      "\n",
      "At iterate   16    f=  1.59515D-02    |proj g|=  2.55095D-03\n",
      "\n",
      "At iterate   17    f=  1.32391D-02    |proj g|=  1.58350D-03\n",
      "\n",
      "At iterate   18    f=  1.09561D-02    |proj g|=  1.10627D-03\n",
      "\n",
      "At iterate   19    f=  8.38909D-03    |proj g|=  8.83221D-04\n",
      "\n",
      "At iterate   20    f=  5.02304D-03    |proj g|=  7.73038D-04\n",
      "\n",
      "At iterate   21    f=  2.19717D-03    |proj g|=  2.93772D-04\n",
      "\n",
      "At iterate   22    f=  1.02458D-03    |proj g|=  2.53585D-04\n",
      "\n",
      "At iterate   23    f=  5.20539D-04    |proj g|=  1.34467D-04\n",
      "\n",
      "At iterate   24    f=  2.57361D-04    |proj g|=  4.26509D-05\n",
      "\n",
      "At iterate   25    f=  1.44077D-04    |proj g|=  1.26897D-04\n",
      "\n",
      "At iterate   26    f=  5.20333D-05    |proj g|=  3.75936D-05\n",
      "\n",
      "At iterate   27    f=  3.13143D-05    |proj g|=  2.00751D-05\n",
      "\n",
      "At iterate   28    f=  1.53959D-05    |proj g|=  8.15040D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     28     31      1     0     0   8.150D-06   1.540D-05\n",
      "  F =   1.5395873870856503E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  7.92006D-02\n",
      "\n",
      "At iterate    1    f=  6.79451D-01    |proj g|=  5.28862D-02\n",
      "\n",
      "At iterate    2    f=  5.85359D-01    |proj g|=  6.18547D-02\n",
      "\n",
      "At iterate    3    f=  3.54846D-01    |proj g|=  1.88689D-02\n",
      "\n",
      "At iterate    4    f=  2.83205D-01    |proj g|=  1.33289D-02\n",
      "\n",
      "At iterate    5    f=  2.14554D-01    |proj g|=  8.37610D-03\n",
      "\n",
      "At iterate    6    f=  1.45645D-01    |proj g|=  5.26756D-03\n",
      "\n",
      "At iterate    7    f=  9.79827D-02    |proj g|=  4.69842D-03\n",
      "\n",
      "At iterate    8    f=  6.31949D-02    |proj g|=  9.45861D-03\n",
      "\n",
      "At iterate    9    f=  4.36667D-02    |proj g|=  4.85444D-03\n",
      "\n",
      "At iterate   10    f=  2.85563D-02    |proj g|=  1.77897D-03\n",
      "\n",
      "At iterate   11    f=  1.82370D-02    |proj g|=  8.28358D-04\n",
      "\n",
      "At iterate   12    f=  1.33897D-02    |proj g|=  7.87166D-03\n",
      "\n",
      "At iterate   13    f=  4.52980D-03    |proj g|=  1.02364D-03\n",
      "\n",
      "At iterate   14    f=  2.96646D-03    |proj g|=  4.72078D-04\n",
      "\n",
      "At iterate   15    f=  1.45240D-03    |proj g|=  1.50880D-04\n",
      "\n",
      "At iterate   16    f=  7.78367D-04    |proj g|=  1.02072D-04\n",
      "\n",
      "At iterate   17    f=  3.90131D-04    |proj g|=  5.67876D-05\n",
      "\n",
      "At iterate   18    f=  1.97344D-04    |proj g|=  3.31730D-05\n",
      "\n",
      "At iterate   19    f=  1.02598D-04    |proj g|=  2.30342D-05\n",
      "\n",
      "At iterate   20    f=  4.79421D-05    |proj g|=  6.40847D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     20     24      1     0     0   6.408D-06   4.794D-05\n",
      "  F =   4.7942078844509375E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  8.02318D-02\n",
      "\n",
      "At iterate    1    f=  6.71725D-01    |proj g|=  8.74793D-02\n",
      "\n",
      "At iterate    2    f=  5.21887D-01    |proj g|=  6.17431D-02\n",
      "\n",
      "At iterate    3    f=  3.20201D-01    |proj g|=  2.31499D-02\n",
      "\n",
      "At iterate    4    f=  2.50842D-01    |proj g|=  1.80295D-02\n",
      "\n",
      "At iterate    5    f=  1.73067D-01    |proj g|=  1.46990D-02\n",
      "\n",
      "At iterate    6    f=  1.02701D-01    |proj g|=  9.66273D-03\n",
      "\n",
      "At iterate    7    f=  6.32659D-02    |proj g|=  1.04712D-02\n",
      "\n",
      "At iterate    8    f=  3.27672D-02    |proj g|=  2.63159D-03\n",
      "\n",
      "At iterate    9    f=  2.04363D-02    |proj g|=  1.13154D-03\n",
      "\n",
      "At iterate   10    f=  1.00825D-02    |proj g|=  5.18161D-04\n",
      "\n",
      "At iterate   11    f=  5.28941D-03    |proj g|=  1.12571D-03\n",
      "\n",
      "At iterate   12    f=  4.82213D-03    |proj g|=  1.89972D-03\n",
      "\n",
      "At iterate   13    f=  2.96077D-03    |proj g|=  6.86983D-04\n",
      "\n",
      "At iterate   14    f=  1.55003D-03    |proj g|=  3.70626D-04\n",
      "\n",
      "At iterate   15    f=  7.72264D-04    |proj g|=  1.84557D-04\n",
      "\n",
      "At iterate   16    f=  3.88587D-04    |proj g|=  9.41979D-05\n",
      "\n",
      "At iterate   17    f=  1.94972D-04    |proj g|=  4.74008D-05\n",
      "\n",
      "At iterate   18    f=  9.79036D-05    |proj g|=  2.39283D-05\n",
      "\n",
      "At iterate   19    f=  4.91144D-05    |proj g|=  1.18849D-05\n",
      "\n",
      "At iterate   20    f=  2.46289D-05    |proj g|=  6.03343D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     20     24      1     0     0   6.033D-06   2.463D-05\n",
      "  F =   2.4628926940867957E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  8.39315D-02\n",
      "\n",
      "At iterate    1    f=  6.80194D-01    |proj g|=  7.38680D-02\n",
      "\n",
      "At iterate    2    f=  4.96232D-01    |proj g|=  7.57266D-02\n",
      "\n",
      "At iterate    3    f=  2.85576D-01    |proj g|=  2.75139D-02\n",
      "\n",
      "At iterate    4    f=  2.19953D-01    |proj g|=  1.94497D-02\n",
      "\n",
      "At iterate    5    f=  1.57580D-01    |proj g|=  1.14362D-02\n",
      "\n",
      "At iterate    6    f=  9.40223D-02    |proj g|=  8.36509D-03\n",
      "\n",
      "At iterate    7    f=  6.33170D-02    |proj g|=  6.94157D-03\n",
      "\n",
      "At iterate    8    f=  3.62019D-02    |proj g|=  7.08690D-03\n",
      "\n",
      "At iterate    9    f=  1.39102D-02    |proj g|=  1.43018D-03\n",
      "\n",
      "At iterate   10    f=  8.98895D-03    |proj g|=  8.92098D-04\n",
      "\n",
      "At iterate   11    f=  4.31514D-03    |proj g|=  3.45087D-04\n",
      "\n",
      "At iterate   12    f=  4.07828D-03    |proj g|=  1.77551D-03\n",
      "\n",
      "At iterate   13    f=  2.12772D-03    |proj g|=  6.42938D-04\n",
      "\n",
      "At iterate   14    f=  1.15286D-03    |proj g|=  3.20616D-04\n",
      "\n",
      "At iterate   15    f=  6.05798D-04    |proj g|=  1.50547D-04\n",
      "\n",
      "At iterate   16    f=  3.22275D-04    |proj g|=  7.17645D-05\n",
      "\n",
      "At iterate   17    f=  1.70430D-04    |proj g|=  3.40336D-05\n",
      "\n",
      "At iterate   18    f=  8.98040D-05    |proj g|=  1.63762D-05\n",
      "\n",
      "At iterate   19    f=  4.69336D-05    |proj g|=  7.85974D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     19     24      1     0     0   7.860D-06   4.693D-05\n",
      "  F =   4.6933634823404408E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 13/13 [00:00<00:00, 5685.71it/s]\n",
      "100%|██████████| 13/13 [00:00<00:00, 772.78it/s]\n",
      "100%|██████████| 13/13 [00:00<00:00, 5769.94it/s]\n",
      "100%|██████████| 13/13 [00:00<00:00, 875.96it/s]\n",
      "100%|██████████| 13/13 [00:00<00:00, 271.04it/s]\n",
      "/storage/work/eak5582/Research/generalized_mlm_2.py:404: ClusterWarning: scipy.cluster: The symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix\n",
      "  LocalModelsTree = linkage(self.dist_mat_avg, 'ward')\n",
      "  0%|          | 0/10 [00:00<?, ?it/s] This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " 30%|███       | 3/10 [00:00<00:00, 28.70it/s] This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  2.32930D-01\n",
      "\n",
      "At iterate    1    f=  6.01411D-01    |proj g|=  4.52282D-02\n",
      "\n",
      "At iterate    2    f=  5.82667D-01    |proj g|=  4.38019D-02\n",
      "\n",
      "At iterate    3    f=  4.32816D-01    |proj g|=  1.29740D-01\n",
      "\n",
      "At iterate    4    f=  3.62037D-01    |proj g|=  1.01478D-01\n",
      "\n",
      "At iterate    5    f=  2.82111D-01    |proj g|=  5.00174D-02\n",
      "\n",
      "At iterate    6    f=  2.43187D-01    |proj g|=  2.18067D-02\n",
      "\n",
      "At iterate    7    f=  2.17662D-01    |proj g|=  1.10347D-02\n",
      "\n",
      "At iterate    8    f=  1.97712D-01    |proj g|=  1.29499D-02\n",
      "\n",
      "At iterate    9    f=  1.71869D-01    |proj g|=  1.19862D-02\n",
      "\n",
      "At iterate   10    f=  1.23320D-01    |proj g|=  3.95035D-03\n",
      "\n",
      "At iterate   11    f=  1.04059D-01    |proj g|=  3.34595D-03\n",
      "\n",
      "At iterate   12    f=  8.78687D-02    |proj g|=  2.97422D-02\n",
      "\n",
      "At iterate   13    f=  7.26592D-02    |proj g|=  9.20539D-03\n",
      "\n",
      "At iterate   14    f=  6.79768D-02    |proj g|=  4.73159D-03\n",
      "\n",
      "At iterate   15    f=  6.49289D-02    |proj g|=  3.94929D-03\n",
      "\n",
      "At iterate   16    f=  6.17070D-02    |proj g|=  3.24949D-03\n",
      "\n",
      "At iterate   17    f=  5.31454D-02    |proj g|=  3.37898D-03\n",
      "\n",
      "At iterate   18    f=  4.94764D-02    |proj g|=  4.14670D-03\n",
      "\n",
      "At iterate   19    f=  4.42447D-02    |proj g|=  3.57130D-03\n",
      "\n",
      "At iterate   20    f=  3.50273D-02    |proj g|=  1.89001D-03\n",
      "\n",
      "At iterate   21    f=  3.48018D-02    |proj g|=  4.37173D-03\n",
      "\n",
      "At iterate   22    f=  3.31984D-02    |proj g|=  1.13647D-03\n",
      "\n",
      "At iterate   23    f=  3.30533D-02    |proj g|=  4.51694D-04\n",
      "\n",
      "At iterate   24    f=  3.30140D-02    |proj g|=  5.84602D-04\n",
      "\n",
      "At iterate   25    f=  3.28889D-02    |proj g|=  7.34477D-04\n",
      "\n",
      "At iterate   26    f=  3.26473D-02    |proj g|=  9.30684D-04\n",
      "\n",
      "At iterate   27    f=  3.24533D-02    |proj g|=  1.05148D-03\n",
      "\n",
      "At iterate   28    f=  3.16577D-02    |proj g|=  1.34477D-03\n",
      "\n",
      "At iterate   29    f=  2.98253D-02    |proj g|=  1.38710D-03\n",
      "\n",
      "At iterate   30    f=  2.83330D-02    |proj g|=  1.27893D-03\n",
      "\n",
      "At iterate   31    f=  2.64482D-02    |proj g|=  9.44658D-04\n",
      "\n",
      "At iterate   32    f=  2.22847D-02    |proj g|=  1.18577D-03\n",
      "\n",
      "At iterate   33    f=  2.07818D-02    |proj g|=  1.24110D-03\n",
      "\n",
      "At iterate   34    f=  1.86319D-02    |proj g|=  4.02664D-03\n",
      "\n",
      "At iterate   35    f=  1.69193D-02    |proj g|=  4.52797D-03\n",
      "\n",
      "At iterate   36    f=  1.06579D-02    |proj g|=  1.62370D-03\n",
      "\n",
      "At iterate   37    f=  7.32632D-03    |proj g|=  7.53254D-04\n",
      "\n",
      "At iterate   38    f=  3.46055D-03    |proj g|=  1.78780D-03\n",
      "\n",
      "At iterate   39    f=  2.06152D-03    |proj g|=  1.82622D-03\n",
      "\n",
      "At iterate   40    f=  5.75653D-04    |proj g|=  5.31555D-04\n",
      "\n",
      "At iterate   41    f=  3.76205D-04    |proj g|=  3.41051D-04\n",
      "\n",
      "At iterate   42    f=  2.05558D-04    |proj g|=  1.75943D-04\n",
      "\n",
      "At iterate   43    f=  1.27920D-04    |proj g|=  1.02122D-04\n",
      "\n",
      "At iterate   44    f=  7.95901D-05    |proj g|=  5.78739D-05\n",
      "\n",
      "At iterate   45    f=  4.95020D-05    |proj g|=  3.08870D-05\n",
      "\n",
      "At iterate   46    f=  2.93499D-05    |proj g|=  1.36453D-05\n",
      "\n",
      "At iterate   47    f=  1.67438D-05    |proj g|=  3.85571D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     47     53      1     0     0   3.856D-06   1.674D-05\n",
      "  F =   1.6743830296933965E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  7.36396D-02\n",
      "\n",
      "At iterate    1    f=  6.80665D-01    |proj g|=  6.52790D-02\n",
      "\n",
      "At iterate    2    f=  5.41310D-01    |proj g|=  7.67767D-02\n",
      "\n",
      "At iterate    3    f=  3.78736D-01    |proj g|=  2.36344D-02\n",
      "\n",
      "At iterate    4    f=  3.33064D-01    |proj g|=  1.29375D-02\n",
      "\n",
      "At iterate    5    f=  2.82620D-01    |proj g|=  9.60754D-03\n",
      "\n",
      "At iterate    6    f=  1.73353D-01    |proj g|=  9.42553D-03\n",
      "\n",
      "At iterate    7    f=  9.60984D-02    |proj g|=  3.43667D-03\n",
      "\n",
      "At iterate    8    f=  6.21613D-02    |proj g|=  3.04859D-03\n",
      "\n",
      "At iterate    9    f=  3.83797D-02    |proj g|=  4.62457D-03\n",
      "\n",
      "At iterate   10    f=  1.76837D-02    |proj g|=  1.52216D-03\n",
      "\n",
      "At iterate   11    f=  1.13216D-02    |proj g|=  5.84170D-04\n",
      "\n",
      "At iterate   12    f=  1.07246D-02    |proj g|=  3.85305D-03\n",
      "\n",
      "At iterate   13    f=  6.57586D-03    |proj g|=  1.67749D-03\n",
      "\n",
      "At iterate   14    f=  4.42063D-03    |proj g|=  1.11055D-03\n",
      "\n",
      "At iterate   15    f=  2.68155D-03    |proj g|=  6.24481D-04\n",
      "\n",
      "At iterate   16    f=  1.54624D-03    |proj g|=  3.07733D-04\n",
      "\n",
      "At iterate   17    f=  8.56676D-04    |proj g|=  1.32189D-04\n",
      "\n",
      "At iterate   18    f=  4.56324D-04    |proj g|=  5.66912D-05\n",
      "\n",
      "At iterate   19    f=  2.35938D-04    |proj g|=  3.34179D-05\n",
      "\n",
      "At iterate   20    f=  1.19478D-04    |proj g|=  1.79585D-05\n",
      "\n",
      "At iterate   21    f=  5.99100D-05    |proj g|=  9.19867D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     21     26      1     0     0   9.199D-06   5.991D-05\n",
      "  F =   5.9909991054302479E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  2.21176D-01\n",
      "\n",
      "At iterate    1    f=  6.04290D-01    |proj g|=  4.71823D-02\n",
      "\n",
      "At iterate    2    f=  5.20924D-01    |proj g|=  7.32478D-02\n",
      "\n",
      "At iterate    3    f=  3.70201D-01    |proj g|=  1.02958D-01\n",
      "\n",
      "At iterate    4    f=  2.92798D-01    |proj g|=  6.41876D-02\n",
      "\n",
      "At iterate    5    f=  2.38489D-01    |proj g|=  2.99365D-02\n",
      "\n",
      "At iterate    6    f=  2.08893D-01    |proj g|=  1.21615D-02\n",
      "\n",
      "At iterate    7    f=  1.85230D-01    |proj g|=  9.88460D-03\n",
      "\n",
      "At iterate    8    f=  1.57970D-01    |proj g|=  1.33339D-02\n",
      "\n",
      "At iterate    9    f=  1.06547D-01    |proj g|=  8.36796D-03\n",
      "\n",
      "At iterate   10    f=  5.13753D-02    |proj g|=  1.14962D-02\n",
      "\n",
      "At iterate   11    f=  2.92494D-02    |proj g|=  8.82240D-03\n",
      "\n",
      "At iterate   12    f=  2.55219D-02    |proj g|=  6.00863D-03\n",
      "\n",
      "At iterate   13    f=  1.73622D-02    |proj g|=  3.30134D-03\n",
      "\n",
      "At iterate   14    f=  1.02960D-02    |proj g|=  1.97583D-03\n",
      "\n",
      "At iterate   15    f=  4.99077D-03    |proj g|=  9.85961D-04\n",
      "\n",
      "At iterate   16    f=  2.52444D-03    |proj g|=  4.85817D-04\n",
      "\n",
      "At iterate   17    f=  1.25538D-03    |proj g|=  2.23859D-04\n",
      "\n",
      "At iterate   18    f=  6.29946D-04    |proj g|=  1.14190D-04\n",
      "\n",
      "At iterate   19    f=  3.15184D-04    |proj g|=  4.74609D-05\n",
      "\n",
      "At iterate   20    f=  1.57432D-04    |proj g|=  3.37823D-05\n",
      "\n",
      "At iterate   21    f=  1.45856D-04    |proj g|=  6.93259D-05\n",
      "\n",
      "At iterate   22    f=  4.60281D-05    |proj g|=  8.80496D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     22     27      1     0     0   8.805D-06   4.603D-05\n",
      "  F =   4.6028059255053962E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  7.92006D-02\n",
      "\n",
      "At iterate    1    f=  6.79451D-01    |proj g|=  5.28862D-02\n",
      "\n",
      "At iterate    2    f=  5.85359D-01    |proj g|=  6.18547D-02\n",
      "\n",
      "At iterate    3    f=  3.54846D-01    |proj g|=  1.88689D-02\n",
      "\n",
      "At iterate    4    f=  2.83205D-01    |proj g|=  1.33289D-02\n",
      "\n",
      "At iterate    5    f=  2.14554D-01    |proj g|=  8.37610D-03\n",
      "\n",
      "At iterate    6    f=  1.45645D-01    |proj g|=  5.26756D-03\n",
      "\n",
      "At iterate    7    f=  9.79827D-02    |proj g|=  4.69842D-03\n",
      "\n",
      "At iterate    8    f=  6.31949D-02    |proj g|=  9.45861D-03\n",
      "\n",
      "At iterate    9    f=  4.36667D-02    |proj g|=  4.85444D-03\n",
      "\n",
      "At iterate   10    f=  2.85563D-02    |proj g|=  1.77897D-03\n",
      "\n",
      "At iterate   11    f=  1.82370D-02    |proj g|=  8.28358D-04\n",
      "\n",
      "At iterate   12    f=  1.33897D-02    |proj g|=  7.87166D-03\n",
      "\n",
      "At iterate   13    f=  4.52980D-03    |proj g|=  1.02364D-03\n",
      "\n",
      "At iterate   14    f=  2.96646D-03    |proj g|=  4.72078D-04\n",
      "\n",
      "At iterate   15    f=  1.45240D-03    |proj g|=  1.50880D-04\n",
      "\n",
      "At iterate   16    f=  7.78367D-04    |proj g|=  1.02072D-04\n",
      "\n",
      "At iterate   17    f=  3.90131D-04    |proj g|=  5.67876D-05\n",
      "\n",
      "At iterate   18    f=  1.97344D-04    |proj g|=  3.31730D-05\n",
      "\n",
      "At iterate   19    f=  1.02598D-04    |proj g|=  2.30342D-05\n",
      "\n",
      "At iterate   20    f=  4.79421D-05    |proj g|=  6.40847D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     20     24      1     0     0   6.408D-06   4.794D-05\n",
      "  F =   4.7942078844509375E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  8.20419D-02\n",
      "\n",
      "At iterate    1    f=  6.82369D-01    |proj g|=  7.09028D-02\n",
      "\n",
      "At iterate    2    f=  6.27239D-01    |proj g|=  6.38986D-02\n",
      "\n",
      "At iterate    3    f=  3.28201D-01    |proj g|=  3.11713D-02\n",
      "\n",
      "At iterate    4    f=  2.58472D-01    |proj g|=  3.38224D-02\n",
      "\n",
      "At iterate    5    f=  2.09357D-01    |proj g|=  2.09354D-02\n",
      "\n",
      "At iterate    6    f=  1.59361D-01    |proj g|=  1.01053D-02\n",
      "\n",
      "At iterate    7    f=  1.29489D-01    |proj g|=  5.42280D-03\n",
      "\n",
      "At iterate    8    f=  1.01512D-01    |proj g|=  2.66621D-03\n",
      "\n",
      "At iterate    9    f=  7.37339D-02    |proj g|=  2.47975D-03\n",
      "\n",
      "At iterate   10    f=  4.10406D-02    |proj g|=  5.80421D-03\n",
      "\n",
      "At iterate   11    f=  2.21517D-02    |proj g|=  1.40665D-03\n",
      "\n",
      "At iterate   12    f=  1.69116D-02    |proj g|=  8.48101D-03\n",
      "\n",
      "At iterate   13    f=  8.31906D-03    |proj g|=  1.05294D-03\n",
      "\n",
      "At iterate   14    f=  6.59467D-03    |proj g|=  9.94642D-04\n",
      "\n",
      "At iterate   15    f=  5.53746D-03    |proj g|=  1.64171D-03\n",
      "\n",
      "At iterate   16    f=  2.43656D-03    |proj g|=  6.44127D-04\n",
      "\n",
      "At iterate   17    f=  1.41497D-03    |proj g|=  3.36557D-04\n",
      "\n",
      "At iterate   18    f=  6.96628D-04    |proj g|=  1.48789D-04\n",
      "\n",
      "At iterate   19    f=  3.59824D-04    |proj g|=  7.04828D-05\n",
      "\n",
      "At iterate   20    f=  1.80758D-04    |proj g|=  3.20475D-05\n",
      "\n",
      "At iterate   21    f=  9.13220D-05    |proj g|=  1.54252D-05\n",
      "\n",
      "At iterate   22    f=  4.58910D-05    |proj g|=  8.19170D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     22     26      1     0     0   8.192D-06   4.589D-05\n",
      "  F =   4.5891039849287408E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  8.52265D-02\n",
      "\n",
      "At iterate    1    f=  6.76460D-01    |proj g|=  6.34962D-02\n",
      "\n",
      "At iterate    2    f=  6.39694D-01    |proj g|=  6.13769D-02\n",
      "\n",
      "At iterate    3    f=  4.79672D-01    |proj g|=  5.41899D-02\n",
      "\n",
      "At iterate    4    f=  3.72566D-01    |proj g|=  3.89185D-02\n",
      "\n",
      "At iterate    5    f=  2.82875D-01    |proj g|=  1.83854D-02\n",
      "\n",
      "At iterate    6    f=  2.34173D-01    |proj g|=  6.24671D-03\n",
      "\n",
      "At iterate    7    f=  2.07081D-01    |proj g|=  4.12255D-03\n",
      "\n",
      "At iterate    8    f=  1.85995D-01    |proj g|=  3.46729D-03\n",
      "\n",
      "At iterate    9    f=  1.65046D-01    |proj g|=  8.03512D-03\n",
      "\n",
      "At iterate   10    f=  1.52660D-01    |proj g|=  6.75255D-03\n",
      "\n",
      "At iterate   11    f=  1.46294D-01    |proj g|=  4.13568D-03\n",
      "\n",
      "At iterate   12    f=  1.45640D-01    |proj g|=  4.95695D-03\n",
      "\n",
      "At iterate   13    f=  1.42318D-01    |proj g|=  3.15500D-03\n",
      "\n",
      "At iterate   14    f=  1.39079D-01    |proj g|=  2.32790D-03\n",
      "\n",
      "At iterate   15    f=  1.33209D-01    |proj g|=  1.49299D-03\n",
      "\n",
      "At iterate   16    f=  1.28853D-01    |proj g|=  1.83881D-03\n",
      "\n",
      "At iterate   17    f=  1.22724D-01    |proj g|=  3.08653D-03\n",
      "\n",
      "At iterate   18    f=  1.16344D-01    |proj g|=  1.20395D-03\n",
      "\n",
      "At iterate   19    f=  1.10547D-01    |proj g|=  2.03373D-03\n",
      "\n",
      "At iterate   20    f=  1.05217D-01    |proj g|=  2.41529D-03\n",
      "\n",
      "At iterate   21    f=  1.00405D-01    |proj g|=  1.16927D-03\n",
      "\n",
      "At iterate   22    f=  9.79830D-02    |proj g|=  1.28155D-03\n",
      "\n",
      "At iterate   23    f=  9.78673D-02    |proj g|=  2.40319D-03\n",
      "\n",
      "At iterate   24    f=  9.73506D-02    |proj g|=  2.20695D-03\n",
      "\n",
      "At iterate   25    f=  9.60196D-02    |proj g|=  1.58832D-03\n",
      "\n",
      "At iterate   26    f=  9.43807D-02    |proj g|=  1.16539D-03\n",
      "\n",
      "At iterate   27    f=  9.11205D-02    |proj g|=  1.16713D-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " 60%|██████    | 6/10 [00:00<00:00, 23.41it/s] This problem is unconstrained.\n",
      " This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      " This problem is unconstrained.\n",
      " 90%|█████████ | 9/10 [00:00<00:00, 25.15it/s] This problem is unconstrained.\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:592: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "100%|██████████| 10/10 [00:00<00:00, 25.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   28    f=  8.91925D-02    |proj g|=  1.70104D-03\n",
      "\n",
      "At iterate   29    f=  8.63535D-02    |proj g|=  3.46660D-03\n",
      "\n",
      "At iterate   30    f=  8.34615D-02    |proj g|=  1.39669D-03\n",
      "\n",
      "At iterate   31    f=  8.11074D-02    |proj g|=  1.41084D-03\n",
      "\n",
      "At iterate   32    f=  7.96720D-02    |proj g|=  1.60834D-03\n",
      "\n",
      "At iterate   33    f=  7.66277D-02    |proj g|=  2.28753D-03\n",
      "\n",
      "At iterate   34    f=  7.62839D-02    |proj g|=  2.77207D-03\n",
      "\n",
      "At iterate   35    f=  7.29966D-02    |proj g|=  1.77254D-03\n",
      "\n",
      "At iterate   36    f=  6.84275D-02    |proj g|=  9.29784D-04\n",
      "\n",
      "At iterate   37    f=  6.50071D-02    |proj g|=  1.84529D-03\n",
      "\n",
      "At iterate   38    f=  6.27308D-02    |proj g|=  2.72835D-03\n",
      "\n",
      "At iterate   39    f=  6.10895D-02    |proj g|=  1.20372D-03\n",
      "\n",
      "At iterate   40    f=  5.89340D-02    |proj g|=  1.40410D-03\n",
      "\n",
      "At iterate   41    f=  5.73404D-02    |proj g|=  1.59878D-03\n",
      "\n",
      "At iterate   42    f=  5.06635D-02    |proj g|=  1.35764D-03\n",
      "\n",
      "At iterate   43    f=  4.27111D-02    |proj g|=  1.45507D-03\n",
      "\n",
      "At iterate   44    f=  4.06056D-02    |proj g|=  3.55892D-03\n",
      "\n",
      "At iterate   45    f=  3.91642D-02    |proj g|=  4.78196D-03\n",
      "\n",
      "At iterate   46    f=  3.74887D-02    |proj g|=  2.95915D-03\n",
      "\n",
      "At iterate   47    f=  3.57617D-02    |proj g|=  2.14700D-03\n",
      "\n",
      "At iterate   48    f=  3.42811D-02    |proj g|=  2.10106D-03\n",
      "\n",
      "At iterate   49    f=  3.22853D-02    |proj g|=  2.49419D-03\n",
      "\n",
      "At iterate   50    f=  2.58394D-02    |proj g|=  2.63191D-03\n",
      "\n",
      "At iterate   51    f=  2.39757D-02    |proj g|=  2.76194D-03\n",
      "\n",
      "At iterate   52    f=  2.28410D-02    |proj g|=  1.84575D-03\n",
      "\n",
      "At iterate   53    f=  2.09204D-02    |proj g|=  1.32299D-03\n",
      "\n",
      "At iterate   54    f=  1.81694D-02    |proj g|=  1.76332D-03\n",
      "\n",
      "At iterate   55    f=  1.22088D-02    |proj g|=  1.92888D-03\n",
      "\n",
      "At iterate   56    f=  1.15569D-02    |proj g|=  1.97630D-03\n",
      "\n",
      "At iterate   57    f=  1.03101D-02    |proj g|=  9.84960D-04\n",
      "\n",
      "At iterate   58    f=  8.95977D-03    |proj g|=  1.11522D-03\n",
      "\n",
      "At iterate   59    f=  7.27403D-03    |proj g|=  7.17523D-04\n",
      "\n",
      "At iterate   60    f=  5.23485D-03    |proj g|=  4.59818D-04\n",
      "\n",
      "At iterate   61    f=  3.31278D-03    |proj g|=  1.03651D-03\n",
      "\n",
      "At iterate   62    f=  1.92388D-03    |proj g|=  4.25623D-04\n",
      "\n",
      "At iterate   63    f=  1.32008D-03    |proj g|=  1.93942D-04\n",
      "\n",
      "At iterate   64    f=  7.25484D-04    |proj g|=  7.44884D-05\n",
      "\n",
      "At iterate   65    f=  3.99319D-04    |proj g|=  4.71001D-05\n",
      "\n",
      "At iterate   66    f=  2.04064D-04    |proj g|=  1.92129D-05\n",
      "\n",
      "At iterate   67    f=  1.79753D-04    |proj g|=  9.04919D-05\n",
      "\n",
      "At iterate   68    f=  9.69257D-05    |proj g|=  3.10363D-05\n",
      "\n",
      "At iterate   69    f=  5.55028D-05    |proj g|=  1.53006D-05\n",
      "\n",
      "At iterate   70    f=  2.96920D-05    |proj g|=  7.64543D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     70     80      1     0     0   7.645D-06   2.969D-05\n",
      "  F =   2.9692006736532420E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  2.61218D-01\n",
      "\n",
      "At iterate    1    f=  5.58867D-01    |proj g|=  5.04352D-02\n",
      "\n",
      "At iterate    2    f=  5.38408D-01    |proj g|=  4.59172D-02\n",
      "\n",
      "At iterate    3    f=  3.93676D-01    |proj g|=  1.37716D-01\n",
      "\n",
      "At iterate    4    f=  3.14900D-01    |proj g|=  1.11536D-01\n",
      "\n",
      "At iterate    5    f=  2.05549D-01    |proj g|=  5.28165D-02\n",
      "\n",
      "At iterate    6    f=  1.53704D-01    |proj g|=  2.44144D-02\n",
      "\n",
      "At iterate    7    f=  1.17367D-01    |proj g|=  1.00217D-02\n",
      "\n",
      "At iterate    8    f=  9.01431D-02    |proj g|=  7.69907D-03\n",
      "\n",
      "At iterate    9    f=  5.92146D-02    |proj g|=  7.34815D-03\n",
      "\n",
      "At iterate   10    f=  2.77634D-02    |proj g|=  5.26603D-03\n",
      "\n",
      "At iterate   11    f=  1.06812D-02    |proj g|=  1.39562D-03\n",
      "\n",
      "At iterate   12    f=  6.58662D-03    |proj g|=  5.60088D-04\n",
      "\n",
      "At iterate   13    f=  3.36124D-03    |proj g|=  1.49720D-03\n",
      "\n",
      "At iterate   14    f=  2.01786D-03    |proj g|=  1.82036D-03\n",
      "\n",
      "At iterate   15    f=  8.97096D-04    |proj g|=  6.84076D-04\n",
      "\n",
      "At iterate   16    f=  5.39063D-04    |proj g|=  3.30010D-04\n",
      "\n",
      "At iterate   17    f=  3.04786D-04    |proj g|=  1.20062D-04\n",
      "\n",
      "At iterate   18    f=  1.80404D-04    |proj g|=  4.58875D-05\n",
      "\n",
      "At iterate   19    f=  1.04901D-04    |proj g|=  1.92363D-05\n",
      "\n",
      "At iterate   20    f=  6.00267D-05    |proj g|=  2.06870D-05\n",
      "\n",
      "At iterate   21    f=  3.18323D-05    |proj g|=  1.36234D-05\n",
      "\n",
      "At iterate   22    f=  1.65657D-05    |proj g|=  8.55930D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     22     25      1     0     0   8.559D-06   1.657D-05\n",
      "  F =   1.6565683368680238E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  5.53373D-02\n",
      "\n",
      "At iterate    1    f=  6.84536D-01    |proj g|=  6.75714D-02\n",
      "\n",
      "At iterate    2    f=  6.33647D-01    |proj g|=  6.42999D-02\n",
      "\n",
      "At iterate    3    f=  3.84316D-01    |proj g|=  3.83046D-02\n",
      "\n",
      "At iterate    4    f=  3.07889D-01    |proj g|=  2.92462D-02\n",
      "\n",
      "At iterate    5    f=  2.60345D-01    |proj g|=  2.53389D-02\n",
      "\n",
      "At iterate    6    f=  2.08027D-01    |proj g|=  5.34767D-03\n",
      "\n",
      "At iterate    7    f=  1.79400D-01    |proj g|=  7.04084D-03\n",
      "\n",
      "At iterate    8    f=  1.39063D-01    |proj g|=  1.08832D-02\n",
      "\n",
      "At iterate    9    f=  1.17057D-01    |proj g|=  4.75255D-03\n",
      "\n",
      "At iterate   10    f=  9.95195D-02    |proj g|=  3.68442D-03\n",
      "\n",
      "At iterate   11    f=  9.07304D-02    |proj g|=  4.28349D-03\n",
      "\n",
      "At iterate   12    f=  9.01066D-02    |proj g|=  4.68512D-03\n",
      "\n",
      "At iterate   13    f=  8.32152D-02    |proj g|=  3.49238D-03\n",
      "\n",
      "At iterate   14    f=  7.62031D-02    |proj g|=  1.88538D-03\n",
      "\n",
      "At iterate   15    f=  7.03635D-02    |proj g|=  1.99409D-03\n",
      "\n",
      "At iterate   16    f=  6.49025D-02    |proj g|=  2.01128D-03\n",
      "\n",
      "At iterate   17    f=  5.63925D-02    |proj g|=  1.58503D-03\n",
      "\n",
      "At iterate   18    f=  4.71029D-02    |proj g|=  2.27440D-03\n",
      "\n",
      "At iterate   19    f=  4.07156D-02    |proj g|=  1.69199D-03\n",
      "\n",
      "At iterate   20    f=  2.80366D-02    |proj g|=  1.54211D-03\n",
      "\n",
      "At iterate   21    f=  2.48136D-02    |proj g|=  2.30783D-03\n",
      "\n",
      "At iterate   22    f=  2.31732D-02    |proj g|=  1.56724D-03\n",
      "\n",
      "At iterate   23    f=  2.26592D-02    |proj g|=  1.02886D-03\n",
      "\n",
      "At iterate   24    f=  2.15201D-02    |proj g|=  7.92981D-04\n",
      "\n",
      "At iterate   25    f=  1.93461D-02    |proj g|=  1.04029D-03\n",
      "\n",
      "At iterate   26    f=  1.64643D-02    |proj g|=  1.63344D-03\n",
      "\n",
      "At iterate   27    f=  1.35139D-02    |proj g|=  1.22154D-03\n",
      "\n",
      "At iterate   28    f=  1.00413D-02    |proj g|=  1.22701D-03\n",
      "\n",
      "At iterate   29    f=  7.88822D-03    |proj g|=  3.11414D-03\n",
      "\n",
      "At iterate   30    f=  5.60962D-03    |proj g|=  1.82308D-03\n",
      "\n",
      "At iterate   31    f=  2.86733D-03    |proj g|=  4.87257D-04\n",
      "\n",
      "At iterate   32    f=  1.74649D-03    |proj g|=  2.62752D-04\n",
      "\n",
      "At iterate   33    f=  9.73466D-04    |proj g|=  1.48722D-04\n",
      "\n",
      "At iterate   34    f=  4.96154D-04    |proj g|=  6.66139D-05\n",
      "\n",
      "At iterate   35    f=  2.42194D-04    |proj g|=  9.70750D-05\n",
      "\n",
      "At iterate   36    f=  1.37180D-04    |proj g|=  4.79367D-05\n",
      "\n",
      "At iterate   37    f=  7.06670D-05    |proj g|=  2.25433D-05\n",
      "\n",
      "At iterate   38    f=  3.78571D-05    |proj g|=  1.07386D-05\n",
      "\n",
      "At iterate   39    f=  1.99354D-05    |proj g|=  4.88294D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     39     45      1     0     0   4.883D-06   1.994D-05\n",
      "  F =   1.9935433567162622E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  2.97262D-01\n",
      "\n",
      "At iterate    1    f=  5.20266D-01    |proj g|=  3.95779D-02\n",
      "\n",
      "At iterate    2    f=  5.01909D-01    |proj g|=  3.93375D-02\n",
      "\n",
      "At iterate    3    f=  3.85888D-01    |proj g|=  1.33718D-01\n",
      "\n",
      "At iterate    4    f=  3.20038D-01    |proj g|=  1.10968D-01\n",
      "\n",
      "At iterate    5    f=  2.19116D-01    |proj g|=  5.06002D-02\n",
      "\n",
      "At iterate    6    f=  1.75150D-01    |proj g|=  2.64502D-02\n",
      "\n",
      "At iterate    7    f=  1.43057D-01    |proj g|=  1.27905D-02\n",
      "\n",
      "At iterate    8    f=  1.17342D-01    |proj g|=  8.56932D-03\n",
      "\n",
      "At iterate    9    f=  8.17104D-02    |proj g|=  7.25375D-03\n",
      "\n",
      "At iterate   10    f=  3.73699D-02    |proj g|=  2.91076D-03\n",
      "\n",
      "At iterate   11    f=  1.89002D-02    |proj g|=  1.80136D-03\n",
      "\n",
      "At iterate   12    f=  8.78285D-03    |proj g|=  2.27365D-03\n",
      "\n",
      "At iterate   13    f=  4.14763D-03    |proj g|=  4.50058D-04\n",
      "\n",
      "At iterate   14    f=  1.88847D-03    |proj g|=  8.77019D-04\n",
      "\n",
      "At iterate   15    f=  9.44751D-04    |proj g|=  2.50505D-04\n",
      "\n",
      "At iterate   16    f=  5.14959D-04    |proj g|=  9.36130D-05\n",
      "\n",
      "At iterate   17    f=  2.58822D-04    |proj g|=  4.41050D-05\n",
      "\n",
      "At iterate   18    f=  1.30319D-04    |proj g|=  3.04372D-05\n",
      "\n",
      "At iterate   19    f=  7.18911D-05    |proj g|=  2.45847D-05\n",
      "\n",
      "At iterate   20    f=  4.28817D-05    |proj g|=  1.40574D-05\n",
      "\n",
      "At iterate   21    f=  2.01283D-05    |proj g|=  6.36586D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     21     24      1     0     0   6.366D-06   2.013D-05\n",
      "  F =   2.0128289768847820E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           74     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  6.93147D-01    |proj g|=  2.37355D-01\n",
      "\n",
      "At iterate    1    f=  5.99703D-01    |proj g|=  3.69949D-02\n",
      "\n",
      "At iterate    2    f=  5.81358D-01    |proj g|=  4.39845D-02\n",
      "\n",
      "At iterate    3    f=  4.43344D-01    |proj g|=  1.26704D-01\n",
      "\n",
      "At iterate    4    f=  3.74519D-01    |proj g|=  9.83913D-02\n",
      "\n",
      "At iterate    5    f=  2.93611D-01    |proj g|=  4.69263D-02\n",
      "\n",
      "At iterate    6    f=  2.54659D-01    |proj g|=  1.73023D-02\n",
      "\n",
      "At iterate    7    f=  2.31206D-01    |proj g|=  1.00673D-02\n",
      "\n",
      "At iterate    8    f=  2.12603D-01    |proj g|=  1.60198D-02\n",
      "\n",
      "At iterate    9    f=  1.84999D-01    |proj g|=  2.05103D-02\n",
      "\n",
      "At iterate   10    f=  1.27653D-01    |proj g|=  1.71370D-02\n",
      "\n",
      "At iterate   11    f=  6.60911D-02    |proj g|=  5.06182D-03\n",
      "\n",
      "At iterate   12    f=  4.92981D-02    |proj g|=  3.31497D-03\n",
      "\n",
      "At iterate   13    f=  3.82690D-02    |proj g|=  2.02826D-02\n",
      "\n",
      "At iterate   14    f=  2.33358D-02    |proj g|=  5.92354D-03\n",
      "\n",
      "At iterate   15    f=  1.99778D-02    |proj g|=  4.45771D-03\n",
      "\n",
      "At iterate   16    f=  1.59515D-02    |proj g|=  2.55095D-03\n",
      "\n",
      "At iterate   17    f=  1.32391D-02    |proj g|=  1.58350D-03\n",
      "\n",
      "At iterate   18    f=  1.09561D-02    |proj g|=  1.10627D-03\n",
      "\n",
      "At iterate   19    f=  8.38909D-03    |proj g|=  8.83221D-04\n",
      "\n",
      "At iterate   20    f=  5.02304D-03    |proj g|=  7.73038D-04\n",
      "\n",
      "At iterate   21    f=  2.19717D-03    |proj g|=  2.93772D-04\n",
      "\n",
      "At iterate   22    f=  1.02458D-03    |proj g|=  2.53585D-04\n",
      "\n",
      "At iterate   23    f=  5.20539D-04    |proj g|=  1.34467D-04\n",
      "\n",
      "At iterate   24    f=  2.57361D-04    |proj g|=  4.26509D-05\n",
      "\n",
      "At iterate   25    f=  1.44077D-04    |proj g|=  1.26897D-04\n",
      "\n",
      "At iterate   26    f=  5.20333D-05    |proj g|=  3.75936D-05\n",
      "\n",
      "At iterate   27    f=  3.13143D-05    |proj g|=  2.00751D-05\n",
      "\n",
      "At iterate   28    f=  1.53959D-05    |proj g|=  8.15040D-06\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   74     28     31      1     0     0   8.150D-06   1.540D-05\n",
      "  F =   1.5395873870856503E-005\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 13/13 [00:00<00:00, 5257.54it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 548.07it/s]\n",
      "100%|██████████| 13/13 [00:00<00:00, 1162.38it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 613.34it/s]\n"
     ]
    }
   ],
   "source": [
    "MLM_results_5_run_test = []\n",
    "MLM_results_5_run_train = []\n",
    "for run in range(5):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=None)    \n",
    "#     clf_1 = RandomForestClassifier()\n",
    "#     clf_1.fit(X_train, y_train)\n",
    "#     clf_2 = GradientBoostingClassifier()\n",
    "#     clf_2.fit(X_train, y_train)\n",
    "#     clf_3 = MLPClassifier(solver='lbfgs', alpha=1e-7, hidden_layer_sizes=(15, 2), random_state=1)\n",
    "#     clf_3.fit(X_train, y_train)\n",
    "    clf_4 = RandomForest()\n",
    "    clf_4.fit(X_train, y_train)\n",
    "#     clfs = [clf_1, clf_2, clf_3]\n",
    "    clfs = [clf_4]\n",
    "    sorted_clf = []\n",
    "    for i in clfs:\n",
    "        sorted_clf.append((i, roc_auc_score(np.round(i.predict(X_test)), y_test)))\n",
    "    sorted_clf = sorted(sorted_clf, key=lambda x: x[1])\n",
    "    avg_variance = np.mean(np.var(X_train, axis=0))\n",
    "    MLM = MixtureLinearModel(sorted_clf, verbose=True)\n",
    "    MLM.compute_kmeans_CELL(X_train, K=13, random_seed=None)\n",
    "    MLM.fit_LocalModels(X_train, y_train, \n",
    "                    eps=avg_variance, num_noise_samp=100, \n",
    "                    classification=True, alpha=0, max_iter=10000, random_seed=None)\n",
    "    pred_lmm_train = MLM.predict(X_train, covariance_tied=False, uniform_prior=False)\n",
    "    pred_lmm_test = MLM.predict(X_test, covariance_tied=False, uniform_prior=False)\n",
    "    MLM.fit_MergedLocalModels(10, classification=True, alpha=0, max_iter=10000, random_seed=None)\n",
    "    pred_epic_train = MLM.predict(X_train,  merged=True, \n",
    "                covariance_type='full', covariance_tied=False, uniform_prior=False)\n",
    "    pred_epic_test = MLM.predict(X_test, merged=True, \n",
    "                covariance_type='full', covariance_tied=False, uniform_prior=False)\n",
    "    \n",
    "    MLM_results_5_run_train.append(roc_auc_score(y_train, np.array(pred_epic_train)))\n",
    "    MLM_results_5_run_test.append(roc_auc_score(y_test, np.array(pred_epic_test)))\n",
    "# print('std test:', np.std(MLM_results_5_run_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "60b6424d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0.8086378737541527, 0.7644736842105263, 0.7654485049833888, 0.6617940199335549, 0.7036544850498339] \n",
      "Train: [0.9979180546302465, 1.0, 1.0, 1.0, 0.9917916666666666]\n",
      "\n",
      "std test: 0.05175268764033589\n",
      "\n",
      "Mean test: 0.7408017135862913\n"
     ]
    }
   ],
   "source": [
    "#Mixture\n",
    "print(\"Test:\", MLM_results_5_run_test, \"\\nTrain:\", MLM_results_5_run_train)\n",
    "print(\"\\nstd test:\", np.std(MLM_results_5_run_test))\n",
    "print(\"\\nMean test:\", np.mean(MLM_results_5_run_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "fed00540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0.7162790697674418, 0.6727656579873329, 0.6531084656084656, 0.6901737967914439, 0.7488372093023256] \n",
      "Train: [0.9894583333333333, 0.9966114457831325, 0.9979174476238077, 0.9977906540497729, 0.99325]\n",
      "\n",
      "std test: 0.033514782709916376\n",
      "\n",
      "Mean test: 0.696232839891402\n"
     ]
    }
   ],
   "source": [
    "#RF\n",
    "print(\"Test:\", MLM_results_5_run_test, \"\\nTrain:\", MLM_results_5_run_train)\n",
    "print(\"\\nstd test:\", np.std(MLM_results_5_run_test))\n",
    "print(\"\\nMean test:\", np.mean(MLM_results_5_run_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "cf8a154b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0.7321428571428572, 0.7832880434782609, 0.6802631578947368, 0.7731755424063116, 0.6823920265780732] \n",
      "Train: [0.9870113650555763, 0.9996248280461879, 0.9907596253902186, 1.0, 0.9966272485009994]\n",
      "\n",
      "std test: 0.04347019468949231\n",
      "\n",
      "Mean test: 0.7302523255000479\n"
     ]
    }
   ],
   "source": [
    "#GB\n",
    "print(\"Test:\", MLM_results_5_run_test, \"\\nTrain:\", MLM_results_5_run_train)\n",
    "print(\"\\nstd test:\", np.std(MLM_results_5_run_test))\n",
    "print(\"\\nMean test:\", np.mean(MLM_results_5_run_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "dc487a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0.7738095238095237, 0.75, 0.7145833333333333, 0.7747326203208557, 0.6757475083056478] \n",
      "Train: [1.0, 1.0, 0.9971577847439917, 1.0, 1.0]\n",
      "\n",
      "std test: 0.037944871481713945\n",
      "\n",
      "Mean test: 0.7377745971538722\n"
     ]
    }
   ],
   "source": [
    "#MLP\n",
    "print(\"Test:\", MLM_results_5_run_test, \"\\nTrain:\", MLM_results_5_run_train)\n",
    "print(\"\\nstd test:\", np.std(MLM_results_5_run_test))\n",
    "print(\"\\nMean test:\", np.mean(MLM_results_5_run_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "30356259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0.6638101516150297, 0.7598684210526315, 0.7501661129568107, 0.7168754119973633, 0.6966329966329966] \n",
      "Train: [1.0, 0.9972944849115504, 1.0, 1.0, 1.0]\n",
      "\n",
      "std test: 0.03515842852356534\n",
      "\n",
      "Mean test: 0.7174706188509663\n"
     ]
    }
   ],
   "source": [
    "#Forestry\n",
    "print(\"Test:\", MLM_results_5_run_test, \"\\nTrain:\", MLM_results_5_run_train)\n",
    "print(\"\\nstd test:\", np.std(MLM_results_5_run_test))\n",
    "print(\"\\nMean test:\", np.mean(MLM_results_5_run_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfdd809",
   "metadata": {},
   "source": [
    "# MLM End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfbbd1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_variances = np.var(X_train[:, :25], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9691260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc train: 1.0\n",
      "auc test: 0.7172050098879367\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn import tree as tr\n",
    "\n",
    "clf_1 = RandomForestClassifier()\n",
    "clf_1.fit(X_train, y_train)\n",
    "print('auc train:', roc_auc_score(clf_1.predict(X_train), y_train))\n",
    "print('auc test:', roc_auc_score(clf_1.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85831272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc train: 0.9903994335457537\n",
      "auc test: 0.7559523809523809\n"
     ]
    }
   ],
   "source": [
    "clf_2 = GradientBoostingClassifier()\n",
    "clf_2.fit(X_train, y_train)\n",
    "print('auc train:', roc_auc_score(clf_2.predict(X_train), y_train))\n",
    "print('auc test:', roc_auc_score(clf_2.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9cef865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc train: 0.9968553459119497\n",
      "auc test: 0.6011904761904762\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf_3 = MLPClassifier(solver='lbfgs', alpha=1e-7, hidden_layer_sizes=(15, 2), random_state=1)\n",
    "clf_3.fit(X_train, y_train)\n",
    "print('auc train:', roc_auc_score(clf_3.predict(X_train), y_train))\n",
    "print('auc test:', roc_auc_score(clf_3.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "198392e6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x does not have column names.  The check that columns are provided in the same order when training and predicting will be skipped\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 1.0\n",
      "Test 0.8127883981542517\n"
     ]
    }
   ],
   "source": [
    "from random_forestry import RandomForest\n",
    "clf_4 = RandomForest()\n",
    "clf_4.fit(X_train, y_train)\n",
    "\n",
    "forest_preds_train = clf_4.predict(X_train)\n",
    "forest_preds = clf_4.predict(X_test)\n",
    "\n",
    "print(\"Train\", roc_auc_score(y_train, forest_preds_train))\n",
    "print(\"Test\", roc_auc_score(y_test, forest_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8f7d60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DT: 1.0\n",
      "Test DT: 0.731578947368421 depth 12\n",
      "Train: 0.8225457120246574\n",
      "Test: 0.7640374331550803\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, GradientBoostingRegressor, RandomForestRegressor\n",
    "from cross_val import k_fold_cross_validation\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn import tree as tr\n",
    "\n",
    "m1 = tr.DecisionTreeClassifier()\n",
    "m1.fit(X_train, y_train)\n",
    "m1_pred = m1.predict(X_train)\n",
    "m1_pred_test = m1.predict(X_test)\n",
    "print(\"Train DT:\", roc_auc_score(m1_pred, y_train))\n",
    "print(\"Test DT:\", roc_auc_score(m1_pred_test, y_test), \"depth\", m1.get_depth())\n",
    "\n",
    "m = LogisticRegression()\n",
    "m.fit(X_train, y_train)\n",
    "m_pred = m.predict(X_train)\n",
    "m_pred_test = m.predict(X_test)\n",
    "print(\"Train:\", roc_auc_score(m_pred, y_train))\n",
    "print(\"Test:\", roc_auc_score(m_pred_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d005142b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1.get_n_leaves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa2928c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 0.880515318015318\n",
      "Test 0.5739644970414202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x does not have column names.  The check that columns are provided in the same order when training and predicting will be skipped\n"
     ]
    }
   ],
   "source": [
    "from random_forestry import RandomForest\n",
    "fr = RandomForest(ntree=1)\n",
    "fr.fit(X_train, y_train)\n",
    "\n",
    "forest_preds_train = fr.predict(X_train)\n",
    "forest_preds = fr.predict(X_test)\n",
    "\n",
    "print(\"Train\", roc_auc_score(y_train, forest_preds_train))\n",
    "print(\"Test\", roc_auc_score(y_test, forest_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b18ba8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method RandomForest.translate_tree of RandomForest(ntree=1, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=932, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x1476a455a8b0>, dataframe=<capsule object NULL at 0x1476a455bb10>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7   \\\n",
       "0    0.600000  0.432401  1.0  0.097087  0.485066  0.0  0.0  1.000000   \n",
       "1    0.906667  0.012610  1.0  0.145631  0.189974  0.0  0.0  0.342105   \n",
       "2    0.440000  0.006347  1.0  0.669903  0.187441  0.0  0.0  0.263158   \n",
       "3    0.453333  0.008184  1.0  1.000000  0.271873  0.0  0.0  0.947368   \n",
       "4    0.586667  0.145386  1.0  0.766990  0.201266  1.0  0.0  0.947368   \n",
       "..        ...       ...  ...       ...       ...  ...  ...       ...   \n",
       "305  0.666667  0.165261  1.0  0.262136  0.305330  0.0  0.0  0.947368   \n",
       "306  0.533333  0.220292  1.0  0.097087  0.212982  0.0  0.0  1.000000   \n",
       "307  0.720000  0.178539  1.0  0.077670  0.457309  1.0  0.0  0.657895   \n",
       "308  0.400000  0.054196  1.0  0.747573  0.901953  0.0  0.0  0.921053   \n",
       "309  0.440000  0.306054  1.0  0.203883  0.256148  0.0  0.0  0.947368   \n",
       "\n",
       "           8         9   ...   63   64   65   66   67   68   69   70   71   72  \n",
       "0    0.000000  1.000000  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
       "1    0.777778  0.323529  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2    0.000000  0.294118  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
       "3    0.000000  0.941176  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
       "4    0.000000  0.941176  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "..        ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "305  0.000000  0.941176  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "306  0.000000  1.000000  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "307  0.000000  0.617647  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
       "308  0.000000  0.911765  ...  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
       "309  0.000000  0.941176  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[310 rows x 73 columns], y=array([0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
       "       0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "       0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1.,\n",
       "       1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1.,\n",
       "       1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
       "       1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
       "       0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
       "       1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
       "       0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
       "       1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
       "       1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
       "       0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 1.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
       "       1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1.,\n",
       "       1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1.,\n",
       "       0., 1., 1., 0.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
       "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
       "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
       "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
       "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
       "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
       "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
       "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
       "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
       "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
       "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
       "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
       "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
       "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
       "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
       "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
       "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
       "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
       "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
       "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
       "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
       "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
       "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
       "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
       "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
       "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
       "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
       "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
       "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
       "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
       "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
       "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
       "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr.translate_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2cac8022",
   "metadata": {},
   "outputs": [],
   "source": [
    "fr.translate_tree(tree_ids=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "040f7199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24,  2, 14, -4, -4, 14,  8, 45, 14, 20, 14, -3, -3, -2, -2, 20, -2,\n",
       "       -2, -3, -3, -1, -1, -1, -1, 20, -4, -4, 52,  8, -1, -1, -4, -4, -1,\n",
       "       -1, 21, -1, -1,  4, -4, -4, 20, -4, -4, -3, -3,  5,  2, 17, 20, -1,\n",
       "       -1, 14, -4, -4, -1, -1, -1, -1, -2, -2, 34,  8, 33, 18, -3, -3, -2,\n",
       "       -2, -4, -4, -3, -3, -1, -1, 51,  1,  8, 64, -3, -3, -4, -4, 10,  1,\n",
       "       23, -3, -3, -2, -2,  4, 39, -4, -4, -4, -4,  2, 23, -4, -4, 16, -1,\n",
       "       -1, -4, -4, -2, -2,  1, -2, -2, 38, -2, -2, -3, -3, 69, 24,  5, 49,\n",
       "       38,  4,  1, -1, -1,  5, -2, -2, -4, -4,  1, 54, -3, -3, -2, -2, -1,\n",
       "       -1, -2, -2, -4, -4, 66, 10, -2, -2, 20,  2, 20, -3, -3, -3, -3,  3,\n",
       "       -1, -1, 39,  1, 16, 71, 14,  4, 24, -4, -4, -2, -2, -4, -4, -4, -4,\n",
       "       -3, -3,  8, -3, -3, -2, -2, 10, -1, -1,  2, 20, -4, -4, -2, -2, 33,\n",
       "        1, -2, -2, 59, -3, -3, -2, -2, -3, -3,  2, -1, -1, 14, 54,  1, 27,\n",
       "       24, 18, -2, -2, -4, -4,  5, -4, -4,  5, -2, -2, -3, -3, -1, -1, 24,\n",
       "       -1, -1,  2, -1, -1, -4, -4, -4, -4, -2, -2, 23, -3, -3, -3, -3, -3,\n",
       "       -3,  8, 11,  2, -2, -2, 10, -3, -3, -3, -3, 11, -2, -2, 13, -1, -1,\n",
       "       18, -2, -2, -4, -4, 14,  1, -1, -1, 10, -1, -1,  5, -1, -1, 40, -6,\n",
       "       -6, -1, -1,  8,  5, 24,  1, -4, -4, -4, -4, 13, -1, -1,  5, -2, -2,\n",
       "       28, 35, 24, -4, -4, 20, -3, -3, 27, -4, -4, -1, -1, -2, -2, -3, -3,\n",
       "        1, 10, -2, -2,  1, -1, -1,  5, -3, -3, -2, -2, -1, -1, 11, -1, -1,\n",
       "       20, 19, 70,  1, 11, -4, -4, -2, -2, -2, -2, -1, -1, -1, -1, -1, -1,\n",
       "       16, -4, -4, -1, -1, 28,  6, 24, -3, -3, 10, 23, -3, -3, 14, -2, -2,\n",
       "       -3, -3, 32, 14, -2, -2, 10, 71, -2, -2, -3, -3, -1, -1, -1, -1, -4,\n",
       "       -4, -2, -2], dtype=int32)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr.saved_forest[0]['feature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4fe7ddde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False,  True, False, False, False, False, False,\n",
       "       False,  True,  True, False,  True,  True,  True,  True, False,\n",
       "        True, False, False,  True,  True,  True, False,  True, False,\n",
       "        True, False,  True,  True, False, False, False, False,  True,\n",
       "       False,  True,  True,  True,  True, False, False, False, False,\n",
       "        True,  True,  True,  True,  True, False, False, False, False,\n",
       "        True,  True, False, False, False,  True,  True, False, False,\n",
       "        True,  True, False, False,  True, False,  True,  True,  True,\n",
       "       False,  True, False,  True,  True, False, False, False, False,\n",
       "       False, False, False,  True, False,  True,  True, False, False,\n",
       "        True,  True,  True,  True,  True, False, False,  True, False,\n",
       "       False, False,  True,  True, False,  True, False, False, False,\n",
       "       False, False, False, False,  True,  True,  True,  True,  True,\n",
       "       False,  True,  True, False,  True, False, False,  True,  True,\n",
       "       False, False,  True, False,  True,  True,  True, False,  True,\n",
       "       False, False, False, False, False, False,  True,  True, False,\n",
       "        True, False,  True,  True,  True, False,  True, False,  True,\n",
       "        True,  True,  True, False,  True,  True,  True, False, False,\n",
       "       False,  True, False,  True,  True, False,  True, False,  True,\n",
       "       False,  True,  True, False, False,  True, False,  True, False,\n",
       "        True, False,  True,  True, False, False, False, False,  True,\n",
       "        True, False,  True, False,  True, False, False, False,  True,\n",
       "       False,  True, False,  True,  True,  True,  True, False, False,\n",
       "        True, False,  True, False,  True,  True,  True, False,  True,\n",
       "       False, False, False, False, False,  True,  True,  True,  True,\n",
       "        True,  True, False,  True,  True, False, False, False,  True,\n",
       "       False, False,  True, False,  True,  True, False, False,  True,\n",
       "       False, False,  True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr.saved_forest[0]['na_left_count']==-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0ba1497c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(fr.saved_forest[0]['na_right_count']==-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "23ee5e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ntree': 1,\n",
       " 'replace': True,\n",
       " 'sampsize': 310,\n",
       " 'sample_fraction': None,\n",
       " 'mtry': 24,\n",
       " 'nodesize_spl': 5,\n",
       " 'nodesize_avg': 5,\n",
       " 'nodesize_strict_spl': 1,\n",
       " 'nodesize_strict_avg': 1,\n",
       " 'min_split_gain': 0.0,\n",
       " 'max_depth': 156,\n",
       " 'interaction_depth': 156,\n",
       " 'splitratio': 1.0,\n",
       " 'oob_honest': False,\n",
       " 'double_bootstrap': False,\n",
       " 'seed': 839,\n",
       " 'verbose': False,\n",
       " 'nthread': 0,\n",
       " 'splitrule': 'variance',\n",
       " 'middle_split': False,\n",
       " 'max_obs': 310,\n",
       " 'linear': False,\n",
       " 'min_trees_per_fold': 0,\n",
       " 'fold_size': 1,\n",
       " 'monotone_avg': False,\n",
       " 'overfit_penalty': 1,\n",
       " 'scale': False,\n",
       " 'double_tree': False,\n",
       " 'na_direction': False}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr.get_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42253447",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have a list of local models in each cluster, e.g., local_models_list\n",
    "\n",
    "def regression_distance(model_s, model_t, data_s, data_t, m):\n",
    "    n_s = len(data_s)\n",
    "    n_t = len(data_t)\n",
    "\n",
    "    sum_squared_diff = np.sum((model_s.predict(data_s) - model_t.predict(data_s))**2)\n",
    "    sum_squared_diff += np.sum((model_s.predict(data_t) - model_t.predict(data_t))**2)\n",
    "\n",
    "    distance = sum_squared_diff / (n_s + n_t + 2*m)\n",
    "    return distance\n",
    "\n",
    "def classification_distance(model_s, model_t, data_s, data_t, m):\n",
    "    predictions_s = model_s.predict(data_s)\n",
    "    predictions_t = model_t.predict(data_s)\n",
    "    \n",
    "    predictions_s = np.clip(np.round(np.concatenate((predictions_s, model_s.predict(data_t)))), 0, 1)\n",
    "    predictions_t = np.clip(np.round(np.concatenate((predictions_t, model_t.predict(data_t)))), 0, 1)\n",
    "\n",
    "    tp = np.sum((predictions_s == 1) & (predictions_t == 1))\n",
    "    fp = np.sum((predictions_s == 1) & (predictions_t == 0))\n",
    "    fn = np.sum((predictions_s == 0) & (predictions_t == 1))\n",
    "\n",
    "    if tp == 0:\n",
    "        distance = 10**10\n",
    "    else:\n",
    "        distance = (fp + fn) / (2 * tp)\n",
    "\n",
    "    return distance\n",
    "\n",
    "# Assuming local_models_list is a list of local models and X_clusters is a list of cluster subsets\n",
    "\n",
    "def dist_matrix(num_clusters, classification = False):\n",
    "    distance_matrix = np.zeros((num_clusters, num_clusters))\n",
    "    if classification == False:\n",
    "        for s in range(num_clusters):\n",
    "            for t in range(num_clusters):\n",
    "                if s == t:\n",
    "                    distance_matrix[s, t] = 0.0  # Distance to itself is 0\n",
    "                else:\n",
    "                    distance_matrix[s, t] = regression_distance(models_all[num_clusters][s], models_all[num_clusters][t], augmented_data[num_clusters][s], augmented_data[num_clusters][t], m)\n",
    "\n",
    "    elif classification == True:\n",
    "        for s in range(num_clusters):\n",
    "            for t in range(num_clusters):\n",
    "                if s == t:\n",
    "                    distance_matrix[s, t] = 0.0  # Distance to itself is 0\n",
    "                else:\n",
    "                    distance_matrix[s, t] = classification_distance(models_all[num_clusters][s], models_all[num_clusters][t], augmented_data[num_clusters][s], augmented_data[num_clusters][t], m)\n",
    "    return distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "343d2cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = X_train, y_train\n",
    "best_k = 35\n",
    "kmeans = KMeans(n_clusters=best_k, n_init=100)\n",
    "cluster_assignments = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea354874",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: 24\n",
      "cluster: 25\n",
      "cluster: 26\n",
      "cluster: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: 33\n",
      "cluster: 34\n",
      "cluster: 37\n",
      "cluster: 40\n",
      "cluster: 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster: 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best k: 50, Best R^2 score: 0.4303374540594722\n",
      "AUC: 0.8812228533244236\n"
     ]
    }
   ],
   "source": [
    "# epsilon = 0.01\n",
    "m = 100\n",
    "min_var = np.mean(np.var(X_train, axis=0))\n",
    "# clf = clf_3\n",
    "\n",
    "# X, y = X_train, y_train\n",
    "\n",
    "# Function to predict using the mixture of linear models\n",
    "def predict_mixture_of_models(X, local_models, kmeans):\n",
    "    cluster_assignments = kmeans.predict(X)\n",
    "    \n",
    "    predictions = np.zeros_like(X[:, 0])  # Initialize predictions\n",
    "\n",
    "    for cluster, model in enumerate(local_models):\n",
    "        cluster_indices = np.where(cluster_assignments == cluster)\n",
    "        X_cluster = X[cluster_indices]\n",
    "        predictions[cluster_indices] += model.predict(X_cluster)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Function to evaluate overall model performance for a given k\n",
    "def evaluate_overall_model(X, y, k, clf_models, classification=False):\n",
    "    kmeans = KMeans(n_clusters=k, n_init=10)\n",
    "    cluster_assignments = kmeans.fit_predict(X)\n",
    "    \n",
    "#     roc_score = float('-inf')\n",
    "#     whole_variances = np.var(X[:, :25], axis=0)\n",
    "\n",
    "    local_models = []\n",
    "    for cluster in range(k):\n",
    "        roc_score = float('-inf')\n",
    "        cluster_indices = np.where(cluster_assignments == cluster)\n",
    "        X_cluster = X[cluster_indices]\n",
    "        y_cluster = y[cluster_indices]\n",
    "        \n",
    "        if len(np.unique(y_cluster)) < 2:\n",
    "            clf = sorted_clf[-1][0]\n",
    "            print(\"cluster:\", cluster)\n",
    "        else:\n",
    "            for i in clf_models:\n",
    "#                 print(\"cluster:\", cluster, \"clf\", i, roc_auc_score(y_cluster, i.predict(X_cluster)), \n",
    "#                       accuracy_score(y_cluster, i.predict(X_cluster)))\n",
    "                if roc_auc_score(y_cluster, i.predict(X_cluster)) >= roc_score:\n",
    "                    roc_score = roc_auc_score(y_cluster, i.predict(X_cluster))\n",
    "                    clf = i\n",
    "#                 print(\"best clf\", clf)\n",
    "        \n",
    "#         def chol_sample(mean, cov, size=1):\n",
    "#             cholesky_cov = np.linalg.cholesky(cov)\n",
    "#             random_samp = np.array([mean + cholesky_cov @ np.random.standard_normal(mean.size) for i in range(size)])\n",
    "#             return random_samp\n",
    "        \n",
    "#         cov = np.diag(np.zeros(X_cluster.shape[1]) + min_var)\n",
    "        \n",
    "        sample_mean_cont = np.mean(X_cluster[:, :25], axis=0)\n",
    "        sample_mean_cat = np.mean(X_cluster[:, 25:], axis=0)\n",
    "        sample_mean = np.mean(X_cluster, axis=0)\n",
    "        \n",
    "#         print(\"Data\", X_cluster)\n",
    "#         print(\"Mean\", sample_mean_cont)\n",
    "\n",
    "        if len(X_cluster) < 8:\n",
    "            perturbed_samples_cont = np.random.normal(loc=sample_mean_cont, \n",
    "                                                      scale=np.sqrt(whole_variances)+0.001,\n",
    "                                                      size=(m, X_cluster[:, :25].shape[1]))\n",
    "#             perturbed_samples = chol_sample(sample_mean, cov, m)\n",
    "            perturbed_samples_cat = np.random.binomial(n=1, p=sample_mean_cat, \n",
    "                                                  size=(m, X_cluster[:, 25:].shape[1]))\n",
    "            perturbed_samples = np.hstack([perturbed_samples_cont, perturbed_samples_cat])\n",
    "#             print(\"Whole data SD\", np.sqrt(whole_variances))\n",
    "#             print(\"Data\", perturbed_samples.shape[1])\n",
    "        else:\n",
    "            variances = np.var(X_cluster[:, :25], axis=0)\n",
    "            \n",
    "#             mask_zeros = variances == 0\n",
    "#             variances[mask_zeros] = whole_variances[mask_zeros]\n",
    "#             print(\"variances:\", variances)\n",
    "            \n",
    "            perturbed_samples_cont = np.random.normal(loc=sample_mean_cont, \n",
    "                                                      scale=np.sqrt(variances)+0.001,\n",
    "                                                      size=(m, X_cluster[:, :25].shape[1]))\n",
    "#             perturbed_samples = chol_sample(sample_mean, cov, m)\n",
    "            perturbed_samples_cat = np.random.binomial(n=1, p=sample_mean_cat, \n",
    "                                                  size=(m, X_cluster[:, 25:].shape[1]))\n",
    "            perturbed_samples = np.hstack([perturbed_samples_cont, perturbed_samples_cat])\n",
    "#             print(\"SD\", np.sqrt(variances))\n",
    "#             print(\"Data\", perturbed_samples.shape[1])\n",
    "    \n",
    "        if len(np.unique(y_cluster)) < 2:\n",
    "                perturbed_predictions = np.round(clf.predict(perturbed_samples))\n",
    "        else:\n",
    "            perturbed_predictions = np.round(clf.predict(perturbed_samples))\n",
    "        \n",
    "        X_cluster = np.concatenate((X_cluster, perturbed_samples))\n",
    "        y_cluster = np.concatenate((y_cluster, perturbed_predictions))\n",
    "        augmented_data[k][cluster] = X_cluster\n",
    "        augmented_y[k][cluster] = y_cluster\n",
    "        clf_model[k][cluster] = clf\n",
    "\n",
    "        if classification == True:\n",
    "            if len(np.unique(y_cluster)) == 1:\n",
    "                model = LogisticRegression(solver='saga')\n",
    "                model.classes_ = np.unique(y_cluster)\n",
    "                model.coef_ = np.zeros((1, X.shape[1]))\n",
    "                model.intercept_ = 0.0\n",
    "            else:\n",
    "                model = LogisticRegression(solver='saga')\n",
    "                model.fit(X_cluster, y_cluster)\n",
    "        else:\n",
    "            model = LinearRegression()\n",
    "            model.fit(X_cluster, y_cluster)\n",
    "        local_models.append(model)\n",
    "        models_all[k][cluster] = model\n",
    "\n",
    "    predictions = predict_mixture_of_models(X, local_models, kmeans)\n",
    "    overall_score = r2_score(y, predictions)\n",
    "    \n",
    "    return overall_score, augmented_data, augmented_y, models_all\n",
    "\n",
    "# Set the maximum possible value of k\n",
    "max_k = 50\n",
    "\n",
    "# Iterate over different values of k and record the performance\n",
    "best_k = None\n",
    "best_score = float('-inf')  # Initialize with a very low value\n",
    "augmented_data = {}\n",
    "augmented_y = {}\n",
    "models_all = {}\n",
    "clf_model = {}\n",
    "\n",
    "clfs = [clf_1, clf_2, clf_3]\n",
    "\n",
    "sorted_clf = []\n",
    "for i in clfs:\n",
    "    sorted_clf.append((i, roc_auc_score(i.predict(X_test), y_test)))\n",
    "sorted_clf = sorted(sorted_clf, key=lambda x: x[1])\n",
    "\n",
    "for k in range(50, max_k + 1):\n",
    "    print(\"k:\", k)\n",
    "    augmented_data[k] = {}\n",
    "    augmented_y[k] = {}\n",
    "    models_all[k] = {}\n",
    "    clf_model[k] ={}\n",
    "    \n",
    "    overall_score = evaluate_overall_model(X, y, k, clfs, classification=True)\n",
    "\n",
    "    # Update best_k if a higher overall score is achieved\n",
    "    if overall_score[0] > best_score:\n",
    "        best_score = overall_score[0]\n",
    "        best_k = k\n",
    "\n",
    "# Final model using the best k\n",
    "kmeans = KMeans(n_clusters=best_k, n_init=10)\n",
    "cluster_assignments = kmeans.fit_predict(X)\n",
    "\n",
    "local_models = []\n",
    "for cluster in range(best_k):\n",
    "    cluster_indices = np.where(cluster_assignments == cluster)\n",
    "    X_cluster = X[cluster_indices]\n",
    "    y_cluster = y[cluster_indices]\n",
    "    \n",
    "#     print(\"unique classes:\", np.unique(y_cluster))\n",
    "\n",
    "    if len(np.unique(y_cluster)) == 1:\n",
    "        model = LogisticRegression(solver='saga')\n",
    "        model.classes_ = np.unique(y_cluster)\n",
    "        model.coef_ = np.zeros((1, X.shape[1]))\n",
    "        model.intercept_ = 0.0\n",
    "    else:\n",
    "    #     model = LinearRegression()\n",
    "        model = LogisticRegression()\n",
    "        model.fit(X_cluster, y_cluster)\n",
    "    local_models.append(model)\n",
    "\n",
    "predictions = predict_mixture_of_models(X, local_models, kmeans)\n",
    "print(f\"Best k: {best_k}, Best R^2 score: {best_score}\")\n",
    "print(\"AUC:\", roc_auc_score(y, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f7a8a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "dist = pd.DataFrame(dist_matrix(best_k, classification=True))\n",
    "\n",
    "CELL_centers = []\n",
    "CELL_variances = []\n",
    "for i in augmented_data[best_k]:\n",
    "    CELL_variances.append(np.sqrt(np.var(augmented_data[best_k][i], axis=0)))\n",
    "    CELL_centers.append(np.mean(augmented_data[best_k][i], axis=0))\n",
    "cells_df = pd.DataFrame(CELL_centers)\n",
    "\n",
    "y_local = []\n",
    "for i in range(len(CELL_centers)):\n",
    "    y_local.append(statistics.mode(augmented_y[best_k][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "48e04075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perturbed_samples = []\n",
    "# perturbed_predictions = []\n",
    "\n",
    "\n",
    "# for i in range(len(cells_df)):\n",
    "#     t = np.random.normal(loc=np.array(cells_df)[i], scale=CELL_variances[i], \n",
    "#                          size=(50, np.array(cells_df)[i].shape[0]))\n",
    "\n",
    "#     perturbed_samples.append(t)\n",
    "\n",
    "#     perturbed_predictions.append(clf_model[best_k][i].predict(t))\n",
    "#     # For using DNN\n",
    "# #     perturbed_predictions = [item for sublist in perturbed_predictions for item in sublist]\n",
    "    \n",
    "    \n",
    "# augmentation_x = [item for sublist in perturbed_samples for item in sublist]\n",
    "# # augmentation_y = np.round([item for sublist in perturbed_predictions for item in sublist])\n",
    "# augmentation_y = [item for sublist in perturbed_predictions for item in sublist]\n",
    "# # augmentation_y = [item for sublist in augmentation_y for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b2c629b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'slice'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m tree \u001b[38;5;241m=\u001b[39m dtc\u001b[38;5;241m.\u001b[39mDistanceDecisionTree(max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m), classification\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     25\u001b[0m tree\u001b[38;5;241m.\u001b[39mfit(np\u001b[38;5;241m.\u001b[39marray(cells_df), np\u001b[38;5;241m.\u001b[39marray(dist))\n\u001b[0;32m---> 27\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_leaf_logistic_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43maugmented_y\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbest_k\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcells_df\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugmented_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbest_k\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax Depth\u001b[39m\u001b[38;5;124m\"\u001b[39m, tree\u001b[38;5;241m.\u001b[39mfinal_depth)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Print the tree\u001b[39;00m\n",
      "File \u001b[0;32m/storage/work/eak5582/Research/PRUNING.py:151\u001b[0m, in \u001b[0;36mDistanceDecisionTree.fit_leaf_logistic_models\u001b[0;34m(self, X, y, X_test, y_test)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_leaf_logistic_models\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, X_test\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, y_test\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_leaf_logistic_models\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/storage/work/eak5582/Research/PRUNING.py:215\u001b[0m, in \u001b[0;36mDistanceDecisionTree._fit_leaf_logistic_models\u001b[0;34m(self, node, X, y)\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(X))\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m#             node['data_indices'] = np.arange(X.shape[0])\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m#             if node['data_indices'].size == 0:\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m#                 node['model'] = None\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m#                 else:\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m#                     node['accuracy'] = rmse(model.predict(X), y)\u001b[39;00m\n\u001b[0;32m--> 215\u001b[0m             left_indices \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfeature_index\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m node[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthreshold\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    216\u001b[0m             right_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39mleft_indices\n\u001b[1;32m    217\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_leaf_logistic_models(node[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m], X[left_indices], y[left_indices])\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'slice'"
     ]
    }
   ],
   "source": [
    "# import DT_with_R2_stopping as dtc\n",
    "# import Pruning/DECISION_TREE_CLASSIFIER as dtc\n",
    "import PRUNING as dtc\n",
    "\n",
    "ROC_test = []\n",
    "ROC_train = []\n",
    "ROC_beom_train = []\n",
    "ROC_beom_test = []\n",
    "Best_depth = []\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_n, y_n, test_size=0.2, random_state=0)\n",
    "\n",
    "# X_train_aug = np.concatenate((X_train, augmentation_x))\n",
    "# y_train_aug = np.concatenate((y_train, augmentation_y))\n",
    "\n",
    "# X_train_aug = X_train\n",
    "# y_train_aug = y_train\n",
    "\n",
    "for k in range(1, 2):\n",
    "    roc_curr = float('-inf')\n",
    "    roc_curr_train = float('-inf')\n",
    "    best_depth = float('inf')\n",
    "    \n",
    "    tree = dtc.DistanceDecisionTree(max_depth=float('inf'), classification=True)\n",
    "\n",
    "    tree.fit(np.array(cells_df), np.array(dist))\n",
    "\n",
    "    tree.fit_leaf_logistic_models(augmented_y[best_k], np.array(cells_df), augmented_data[best_k], 0.1)\n",
    "    print(\"Max Depth\", tree.final_depth)\n",
    "\n",
    "    # Print the tree\n",
    "    tree.print_tree()\n",
    "\n",
    "    for i in range(1, tree.final_depth + 1):\n",
    "        # for i in range(1, 5):\n",
    "        tree = dtc.DistanceDecisionTree(max_depth=i, classification=True)\n",
    "        # Assuming location_matrix and distance_matrix contain the location and pairwise distances\n",
    "        tree.fit(np.array(cells_df), np.array(dist))\n",
    "\n",
    "        # Now fit local logistic regression models using your actual data\n",
    "        # Assuming X_train and y_train are your training data\n",
    "        tree.fit_leaf_logistic_models(augmented_y[best_k], np.array(cells_df), augmented_data[best_k], 0.1)\n",
    "#         tree.print_tree()\n",
    "        \n",
    "        Y_pred_train = np.round(tree.predict(X_train, y_train))\n",
    "        Y_pred = np.round(tree.predict(X_test, y_test))\n",
    "\n",
    "        print(\"Current Accuracy test\", roc_auc_score(y_test, Y_pred))\n",
    "\n",
    "        if roc_auc_score(y_test, Y_pred) > roc_curr:\n",
    "            roc_curr = roc_auc_score(y_test, Y_pred)\n",
    "            roc_curr_train = roc_auc_score(y_train, Y_pred_train)\n",
    "            best_depth = tree.final_depth\n",
    "            best_tree = tree\n",
    "\n",
    "        print(\"Depth\", i, \"Accuracy train\", roc_curr_train)\n",
    "        print(\"Depth\", i, \"Accuracy test\", roc_curr)\n",
    "\n",
    "    ROC_test.append(roc_curr)\n",
    "    ROC_train.append(roc_curr_train)\n",
    "    Best_depth.append(best_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "06818b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   New test  New train  Depth\n",
      "0  0.780921   0.826943      2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.780921052631579"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Results_SKCM = pd.DataFrame()\n",
    "Results_SKCM['New test'] = ROC_test\n",
    "Results_SKCM['New train'] = ROC_train\n",
    "Results_SKCM['Depth'] = Best_depth\n",
    "\n",
    "\n",
    "def highlight_rows(x):\n",
    "    if x['MLM test'] >= x['New test']:\n",
    "        return ['background-color: green'] * len(x)\n",
    "    else:\n",
    "        return ['background-color: white'] * len(x)\n",
    "\n",
    "print(Results_SKCM)\n",
    "np.mean(Results_SKCM['New test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7122acf7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n",
      "Prediction is happening: 73 <class 'numpy.ndarray'> (73,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7725774555042847"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_preds = [item for sublist in np.round(best_tree.predict(X_test, y_test)) for item in sublist]\n",
    "roc_auc_score(tree_preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b46427ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_k 291"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80234c39",
   "metadata": {},
   "source": [
    "# Testing different clf models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a020158d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k = 35\n",
    "kmeans = KMeans(n_clusters=best_k, n_init=10)\n",
    "cluster_assignments = kmeans.fit_predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50b21356",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have a list of local models in each cluster, e.g., local_models_list\n",
    "\n",
    "def regression_distance(model_s, model_t, data_s, data_t, m):\n",
    "    n_s = len(data_s)\n",
    "    n_t = len(data_t)\n",
    "\n",
    "    sum_squared_diff = np.sum((model_s.predict(data_s) - model_t.predict(data_s))**2)\n",
    "    sum_squared_diff += np.sum((model_s.predict(data_t) - model_t.predict(data_t))**2)\n",
    "\n",
    "    distance = sum_squared_diff / (n_s + n_t + 2*m)\n",
    "    return distance\n",
    "\n",
    "def classification_distance(model_s, model_t, data_s, data_t, m):\n",
    "    predictions_s = model_s.predict(data_s)\n",
    "    predictions_t = model_t.predict(data_s)\n",
    "    \n",
    "    predictions_s = np.clip(np.round(np.concatenate((predictions_s, model_s.predict(data_t)))), 0, 1)\n",
    "    predictions_t = np.clip(np.round(np.concatenate((predictions_t, model_t.predict(data_t)))), 0, 1)\n",
    "\n",
    "    tp = np.sum((predictions_s == 1) & (predictions_t == 1))\n",
    "    fp = np.sum((predictions_s == 1) & (predictions_t == 0))\n",
    "    fn = np.sum((predictions_s == 0) & (predictions_t == 1))\n",
    "\n",
    "    if tp == 0:\n",
    "        distance = 10**10\n",
    "    else:\n",
    "        distance = (fp + fn) / (2 * tp)\n",
    "\n",
    "    return distance\n",
    "\n",
    "# Assuming local_models_list is a list of local models and X_clusters is a list of cluster subsets\n",
    "\n",
    "def dist_matrix(num_clusters, classification = False):\n",
    "    distance_matrix = np.zeros((num_clusters, num_clusters))\n",
    "    if classification == False:\n",
    "        for s in range(num_clusters):\n",
    "            for t in range(num_clusters):\n",
    "                if s == t:\n",
    "                    distance_matrix[s, t] = 0.0  # Distance to itself is 0\n",
    "                else:\n",
    "                    distance_matrix[s, t] = regression_distance(models_all[num_clusters][s], models_all[num_clusters][t], augmented_data[num_clusters][s], augmented_data[num_clusters][t], m)\n",
    "\n",
    "    elif classification == True:\n",
    "        for s in range(num_clusters):\n",
    "            for t in range(num_clusters):\n",
    "                if s == t:\n",
    "                    distance_matrix[s, t] = 0.0  # Distance to itself is 0\n",
    "                else:\n",
    "                    distance_matrix[s, t] = classification_distance(models_all[num_clusters][s], models_all[num_clusters][t], augmented_data[num_clusters][s], augmented_data[num_clusters][t], m)\n",
    "    return distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57fe15a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_var = np.mean(np.var(X_train, axis=0))\n",
    "# Function to predict using the mixture of linear models\n",
    "def predict_mixture_of_models(X, local_models, kmeans):\n",
    "    cluster_assignments = kmeans.predict(X)\n",
    "    \n",
    "    predictions = np.zeros_like(X[:, 0])  # Initialize predictions\n",
    "\n",
    "    for cluster, model in enumerate(local_models):\n",
    "        cluster_indices = np.where(cluster_assignments == cluster)\n",
    "        X_cluster = X[cluster_indices]\n",
    "        predictions[cluster_indices] += model.predict(X_cluster)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Function to evaluate overall model performance for a given k\n",
    "def evaluate_overall_model(X, y, k, clf_models, classification=False):\n",
    "    roc_score = float('-inf')\n",
    "\n",
    "    local_models = []\n",
    "    for cluster in range(k):\n",
    "#         roc_score = float('-inf')\n",
    "        cluster_indices = np.where(cluster_assignments == cluster)\n",
    "        X_cluster = X[cluster_indices]\n",
    "        y_cluster = y[cluster_indices]\n",
    "        \n",
    "        if len(np.unique(y_cluster)) < 2:\n",
    "            clf = sorted_clf[-1][0]\n",
    "#             print(\"cluster:\", cluster)\n",
    "        else:\n",
    "            for i in clf_models:\n",
    "                if roc_auc_score(y_cluster, i.predict(X_cluster)) >= roc_score:\n",
    "                    roc_score = roc_auc_score(y_cluster, i.predict(X_cluster))\n",
    "                    clf = i\n",
    "                    \n",
    "        def chol_sample(mean, cov, size=1):\n",
    "            cholesky_cov = np.linalg.cholesky(cov)\n",
    "            random_samp = np.array([mean + cholesky_cov @ np.random.standard_normal(mean.size) for i in range(size)])\n",
    "            return random_samp\n",
    "        \n",
    "#         cov = np.diag(np.zeros(X_cluster[:, :25].shape[1]) + min_var)\n",
    "        \n",
    "        cov = np.diag(np.zeros(X_cluster.shape[1]) + 0.1)\n",
    "        \n",
    "#         sample_mean_cont = np.mean(X_cluster[:, :25], axis=0)\n",
    "#         sample_mean_cat = np.mean(X_cluster[:, 25:], axis=0)\n",
    "\n",
    "                    \n",
    "#         if len(X_cluster) < 8:\n",
    "# #             perturbed_samples_cont = np.random.normal(loc=sample_mean_cont, \n",
    "# #                                                       scale=np.sqrt(np.var(X_train[:, :25], axis=0))+min_var,\n",
    "# #                                                       size=(m, X_cluster[:, :25].shape[1]))\n",
    "#             perturbed_samples_cont = chol_sample(sample_mean_cont, cov, m)\n",
    "    \n",
    "#             noise = np.random.normal(loc=sample_mean_cat, scale=0.00001, size=(m, X_cluster[:, 25:].shape[1]))\n",
    "#             perturbed_samples_cat = np.random.binomial(n=1, p=sample_mean_cat, \n",
    "#                                                   size=(m, X_cluster[:, 25:].shape[1])) + noise\n",
    "        \n",
    "#             perturbed_samples = np.hstack([perturbed_samples_cont, perturbed_samples_cat])\n",
    "# #             print(\"Whole data SD\", np.sqrt(whole_variances))\n",
    "# #             print(\"Data\", perturbed_samples.shape[1])\n",
    "#         else:\n",
    "#             variances = np.var(X_cluster[:, :25], axis=0)\n",
    "            \n",
    "# #             mask_zeros = variances == 0\n",
    "# #             variances[mask_zeros] = whole_variances[mask_zeros]\n",
    "# #             print(\"variances:\", variances)\n",
    "            \n",
    "# #             perturbed_samples_cont = np.random.normal(loc=sample_mean_cont, \n",
    "# #                                                       scale=np.sqrt(variances)+min_var,\n",
    "# #                                                       size=(m, X_cluster[:, :25].shape[1]))\n",
    "#             perturbed_samples_cont = chol_sample(sample_mean_cont, cov, m)\n",
    "    \n",
    "#             noise = np.random.normal(loc=sample_mean_cat, scale=0.00001, size=(m, X_cluster[:, 25:].shape[1]))\n",
    "#             perturbed_samples_cat = np.random.binomial(n=1, p=sample_mean_cat, \n",
    "#                                                   size=(m, X_cluster[:, 25:].shape[1])) + noise\n",
    "#             perturbed_samples = np.hstack([perturbed_samples_cont, perturbed_samples_cat])\n",
    "# #             print(\"SD\", np.sqrt(variances))\n",
    "    \n",
    "#         if len(np.unique(y_cluster)) < 2:\n",
    "#                 perturbed_predictions = np.round(clf.predict(perturbed_samples))\n",
    "#         else:\n",
    "#             perturbed_predictions = np.round(clf.predict(perturbed_samples))\n",
    "                    \n",
    "\n",
    "        \n",
    "#         sample_mean_cont = np.mean(X_cluster[:, :25], axis=0)\n",
    "#         sample_mean_cat = np.mean(X_cluster[:, 25:], axis=0)\n",
    "        sample_mean = np.mean(X_cluster, axis=0)\n",
    "        \n",
    "        if len(X_cluster) < 8:\n",
    "            perturbed_samples = chol_sample(sample_mean, cov, m)\n",
    "        else:\n",
    "#             variances = np.var(X_cluster[:, :25], axis=0)\n",
    "            \n",
    "            perturbed_samples = chol_sample(sample_mean, cov, m)\n",
    "        \n",
    "#         print(\"perturbed_samples:\", perturbed_samples)\n",
    "\n",
    "    \n",
    "        if len(np.unique(y_cluster)) < 2:\n",
    "                perturbed_predictions = np.round(clf.predict(perturbed_samples))\n",
    "        else:\n",
    "            perturbed_predictions = np.round(clf.predict(perturbed_samples))\n",
    "        \n",
    "        X_cluster = np.concatenate((X_cluster, perturbed_samples))\n",
    "        y_cluster = np.concatenate((y_cluster, perturbed_predictions))\n",
    "        augmented_data[k][cluster] = X_cluster\n",
    "        augmented_y[k][cluster] = y_cluster\n",
    "        clf_model[k][cluster] = clf\n",
    "\n",
    "        if classification == True:\n",
    "            if len(np.unique(y_cluster)) == 1:\n",
    "                model = LogisticRegression(solver='saga')\n",
    "                model.classes_ = np.unique(y_cluster)\n",
    "                model.coef_ = np.zeros((1, X.shape[1]))\n",
    "                model.intercept_ = 0.0\n",
    "            else:\n",
    "                model = LogisticRegression(solver='saga')\n",
    "                model.fit(X_cluster, y_cluster)\n",
    "        else:\n",
    "            model = LinearRegression()\n",
    "            model.fit(X_cluster, y_cluster)\n",
    "        local_models.append(model)\n",
    "        models_all[k][cluster] = model\n",
    "\n",
    "    predictions = predict_mixture_of_models(X, local_models, kmeans)\n",
    "    overall_score = r2_score(y, predictions)\n",
    "    \n",
    "    return overall_score, augmented_data, augmented_y, models_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eac77a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Depth 6\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.34331315318539013\n",
      "            Iterations: 486\n",
      "            Function evaluations: 486\n",
      "            Gradient evaluations: 486\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3317823122872113\n",
      "            Iterations: 438\n",
      "            Function evaluations: 439\n",
      "            Gradient evaluations: 438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3365496908864342\n",
      "            Iterations: 475\n",
      "            Function evaluations: 475\n",
      "            Gradient evaluations: 475\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.33280408875961026\n",
      "            Iterations: 425\n",
      "            Function evaluations: 425\n",
      "            Gradient evaluations: 425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 3 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.33757198964881313\n",
      "            Iterations: 470\n",
      "            Function evaluations: 470\n",
      "            Gradient evaluations: 470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3317171670659877\n",
      "            Iterations: 422\n",
      "            Function evaluations: 422\n",
      "            Gradient evaluations: 422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 2 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3425614238463391\n",
      "            Iterations: 477\n",
      "            Function evaluations: 477\n",
      "            Gradient evaluations: 477\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.33402306683572364\n",
      "            Iterations: 431\n",
      "            Function evaluations: 431\n",
      "            Gradient evaluations: 431\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3387429468146749\n",
      "            Iterations: 470\n",
      "            Function evaluations: 470\n",
      "            Gradient evaluations: 470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 2 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3293805297494928\n",
      "            Iterations: 443\n",
      "            Function evaluations: 443\n",
      "            Gradient evaluations: 443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.34147454797507465\n",
      "            Iterations: 455\n",
      "            Function evaluations: 456\n",
      "            Gradient evaluations: 455\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.33214563931049995\n",
      "            Iterations: 428\n",
      "            Function evaluations: 428\n",
      "            Gradient evaluations: 428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.34135657706822575\n",
      "            Iterations: 450\n",
      "            Function evaluations: 451\n",
      "            Gradient evaluations: 450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 6 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.33649503547230336\n",
      "            Iterations: 438\n",
      "            Function evaluations: 438\n",
      "            Gradient evaluations: 438\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3351935866058991\n",
      "            Iterations: 478\n",
      "            Function evaluations: 479\n",
      "            Gradient evaluations: 478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3331817763685866\n",
      "            Iterations: 425\n",
      "            Function evaluations: 425\n",
      "            Gradient evaluations: 425\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.33891502148261293\n",
      "            Iterations: 464\n",
      "            Function evaluations: 465\n",
      "            Gradient evaluations: 464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 2 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3364927871405943\n",
      "            Iterations: 432\n",
      "            Function evaluations: 432\n",
      "            Gradient evaluations: 432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3456108872412983\n",
      "            Iterations: 476\n",
      "            Function evaluations: 476\n",
      "            Gradient evaluations: 476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.33119524483033214\n",
      "            Iterations: 421\n",
      "            Function evaluations: 422\n",
      "            Gradient evaluations: 421\n",
      "Current Depth: 1\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2842706519018938\n",
      "            Iterations: 404\n",
      "            Function evaluations: 404\n",
      "            Gradient evaluations: 404\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.34480831900773123\n",
      "            Iterations: 355\n",
      "            Function evaluations: 355\n",
      "            Gradient evaluations: 355\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.255435231516831\n",
      "            Iterations: 320\n",
      "            Function evaluations: 321\n",
      "            Gradient evaluations: 320\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.32907058701854763\n",
      "            Iterations: 375\n",
      "            Function evaluations: 375\n",
      "            Gradient evaluations: 375\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2676023727336803\n",
      "            Iterations: 418\n",
      "            Function evaluations: 418\n",
      "            Gradient evaluations: 418\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3385870064694108\n",
      "            Iterations: 368\n",
      "            Function evaluations: 368\n",
      "            Gradient evaluations: 368\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.24778256133708507\n",
      "            Iterations: 312\n",
      "            Function evaluations: 312\n",
      "            Gradient evaluations: 312\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3290097851959581\n",
      "            Iterations: 366\n",
      "            Function evaluations: 366\n",
      "            Gradient evaluations: 366\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2837646469925095\n",
      "            Iterations: 394\n",
      "            Function evaluations: 394\n",
      "            Gradient evaluations: 394\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3363534245627977\n",
      "            Iterations: 388\n",
      "            Function evaluations: 389\n",
      "            Gradient evaluations: 388\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.258823844401182\n",
      "            Iterations: 341\n",
      "            Function evaluations: 341\n",
      "            Gradient evaluations: 341\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.32988358238069\n",
      "            Iterations: 371\n",
      "            Function evaluations: 371\n",
      "            Gradient evaluations: 371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.27896439124626166\n",
      "            Iterations: 400\n",
      "            Function evaluations: 400\n",
      "            Gradient evaluations: 400\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3428674040389717\n",
      "            Iterations: 361\n",
      "            Function evaluations: 362\n",
      "            Gradient evaluations: 361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 2 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.27565698332754157\n",
      "            Iterations: 316\n",
      "            Function evaluations: 317\n",
      "            Gradient evaluations: 316\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.32571263497599484\n",
      "            Iterations: 375\n",
      "            Function evaluations: 376\n",
      "            Gradient evaluations: 375\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2630914591658919\n",
      "            Iterations: 402\n",
      "            Function evaluations: 402\n",
      "            Gradient evaluations: 402\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3424769524748701\n",
      "            Iterations: 359\n",
      "            Function evaluations: 360\n",
      "            Gradient evaluations: 359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.259897129838612\n",
      "            Iterations: 335\n",
      "            Function evaluations: 335\n",
      "            Gradient evaluations: 335\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.32130268599149436\n",
      "            Iterations: 379\n",
      "            Function evaluations: 379\n",
      "            Gradient evaluations: 379\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.28648972235984044\n",
      "            Iterations: 383\n",
      "            Function evaluations: 384\n",
      "            Gradient evaluations: 383\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3385472167949609\n",
      "            Iterations: 357\n",
      "            Function evaluations: 357\n",
      "            Gradient evaluations: 357\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.25849000399374505\n",
      "            Iterations: 311\n",
      "            Function evaluations: 312\n",
      "            Gradient evaluations: 311\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.32774895182643565\n",
      "            Iterations: 366\n",
      "            Function evaluations: 367\n",
      "            Gradient evaluations: 366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.28139485373358014\n",
      "            Iterations: 394\n",
      "            Function evaluations: 394\n",
      "            Gradient evaluations: 394\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.34074322711915084\n",
      "            Iterations: 356\n",
      "            Function evaluations: 356\n",
      "            Gradient evaluations: 356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.27115575408545867\n",
      "            Iterations: 308\n",
      "            Function evaluations: 309\n",
      "            Gradient evaluations: 308\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3267542515564315\n",
      "            Iterations: 372\n",
      "            Function evaluations: 372\n",
      "            Gradient evaluations: 372\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2720544655034687\n",
      "            Iterations: 405\n",
      "            Function evaluations: 405\n",
      "            Gradient evaluations: 405\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3363943393942138\n",
      "            Iterations: 357\n",
      "            Function evaluations: 357\n",
      "            Gradient evaluations: 357\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.24559050283602785\n",
      "            Iterations: 335\n",
      "            Function evaluations: 336\n",
      "            Gradient evaluations: 335\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3318962463803156\n",
      "            Iterations: 367\n",
      "            Function evaluations: 368\n",
      "            Gradient evaluations: 367\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2793700580373122\n",
      "            Iterations: 398\n",
      "            Function evaluations: 398\n",
      "            Gradient evaluations: 398\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.34424634270371696\n",
      "            Iterations: 361\n",
      "            Function evaluations: 361\n",
      "            Gradient evaluations: 361\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2765034231485427\n",
      "            Iterations: 301\n",
      "            Function evaluations: 301\n",
      "            Gradient evaluations: 301\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.33122208556701205\n",
      "            Iterations: 367\n",
      "            Function evaluations: 367\n",
      "            Gradient evaluations: 367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.29172605291217774\n",
      "            Iterations: 385\n",
      "            Function evaluations: 386\n",
      "            Gradient evaluations: 385\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3469722588020247\n",
      "            Iterations: 349\n",
      "            Function evaluations: 350\n",
      "            Gradient evaluations: 349\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2628553077669102\n",
      "            Iterations: 322\n",
      "            Function evaluations: 322\n",
      "            Gradient evaluations: 322\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3263819086210682\n",
      "            Iterations: 364\n",
      "            Function evaluations: 364\n",
      "            Gradient evaluations: 364\n",
      "Current Depth: 2\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18722074741172678\n",
      "            Iterations: 445\n",
      "            Function evaluations: 446\n",
      "            Gradient evaluations: 445\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.23115810281555332\n",
      "            Iterations: 407\n",
      "            Function evaluations: 407\n",
      "            Gradient evaluations: 407\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2833187351300192\n",
      "            Iterations: 288\n",
      "            Function evaluations: 289\n",
      "            Gradient evaluations: 288\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3381295762132719\n",
      "            Iterations: 313\n",
      "            Function evaluations: 314\n",
      "            Gradient evaluations: 313\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.21272628360805118\n",
      "            Iterations: 358\n",
      "            Function evaluations: 358\n",
      "            Gradient evaluations: 358\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.11508371258757855\n",
      "            Iterations: 484\n",
      "            Function evaluations: 484\n",
      "            Gradient evaluations: 484\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3270113463941032\n",
      "            Iterations: 335\n",
      "            Function evaluations: 336\n",
      "            Gradient evaluations: 335\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.26629366382251757\n",
      "            Iterations: 286\n",
      "            Function evaluations: 287\n",
      "            Gradient evaluations: 286\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.17770174031454666\n",
      "            Iterations: 449\n",
      "            Function evaluations: 450\n",
      "            Gradient evaluations: 449\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18070575516228277\n",
      "            Iterations: 503\n",
      "            Function evaluations: 503\n",
      "            Gradient evaluations: 503\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2814361323033255\n",
      "            Iterations: 286\n",
      "            Function evaluations: 286\n",
      "            Gradient evaluations: 286\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3292814296646207\n",
      "            Iterations: 310\n",
      "            Function evaluations: 311\n",
      "            Gradient evaluations: 310\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1838786113546285\n",
      "            Iterations: 354\n",
      "            Function evaluations: 354\n",
      "            Gradient evaluations: 354\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.14672961025639925\n",
      "            Iterations: 453\n",
      "            Function evaluations: 453\n",
      "            Gradient evaluations: 453\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3206699598137714\n",
      "            Iterations: 324\n",
      "            Function evaluations: 324\n",
      "            Gradient evaluations: 324\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2708039484095037\n",
      "            Iterations: 279\n",
      "            Function evaluations: 279\n",
      "            Gradient evaluations: 279\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18451747012537464\n",
      "            Iterations: 469\n",
      "            Function evaluations: 469\n",
      "            Gradient evaluations: 469\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.22429593005094778\n",
      "            Iterations: 397\n",
      "            Function evaluations: 397\n",
      "            Gradient evaluations: 397\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.28547075464580496\n",
      "            Iterations: 286\n",
      "            Function evaluations: 287\n",
      "            Gradient evaluations: 286\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3223285148987246\n",
      "            Iterations: 334\n",
      "            Function evaluations: 334\n",
      "            Gradient evaluations: 334\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1693375004853346\n",
      "            Iterations: 417\n",
      "            Function evaluations: 417\n",
      "            Gradient evaluations: 417\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1872886906509787\n",
      "            Iterations: 382\n",
      "            Function evaluations: 383\n",
      "            Gradient evaluations: 382\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3289521974814265\n",
      "            Iterations: 309\n",
      "            Function evaluations: 309\n",
      "            Gradient evaluations: 309\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2713742309881582\n",
      "            Iterations: 286\n",
      "            Function evaluations: 286\n",
      "            Gradient evaluations: 286\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1842993251423955\n",
      "            Iterations: 432\n",
      "            Function evaluations: 432\n",
      "            Gradient evaluations: 432\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2001868862915926\n",
      "            Iterations: 469\n",
      "            Function evaluations: 469\n",
      "            Gradient evaluations: 469\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "n_folds = 10\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "ROC_val = []\n",
    "ROC_train = []\n",
    "ROC_beom_train = []\n",
    "ROC_beom_test = []\n",
    "Best_depth = []\n",
    "\n",
    "results = []\n",
    "m = 100\n",
    "\n",
    "clfs = [clf_1, clf_2, clf_3, clf_4]\n",
    "# clfs = [clf_1]\n",
    "for test in range(1):\n",
    "    # Set the maximum possible value of k\n",
    "    max_k = 35\n",
    "\n",
    "    # Iterate over different values of k and record the performance\n",
    "    best_k = None\n",
    "    best_score = float('-inf')  # Initialize with a very low value\n",
    "    augmented_data = {}\n",
    "    augmented_y = {}\n",
    "    models_all = {}\n",
    "    clf_model = {}\n",
    "\n",
    "    sorted_clf = []\n",
    "    for i in clfs:\n",
    "        sorted_clf.append((i, roc_auc_score(np.round(i.predict(X_train)), y_train)))\n",
    "    sorted_clf = sorted(sorted_clf, key=lambda x: x[1])\n",
    "\n",
    "    for k in range(35, max_k + 1):\n",
    "        print(\"k:\", k)\n",
    "        augmented_data[k] = {}\n",
    "        augmented_y[k] = {}\n",
    "        models_all[k] = {}\n",
    "        clf_model[k] ={}\n",
    "\n",
    "        overall_score = evaluate_overall_model(X_train, y_train, k, clfs, classification=True)\n",
    "\n",
    "        # Update best_k if a higher overall score is achieved\n",
    "        if overall_score[0] > best_score:\n",
    "            best_score = overall_score[0]\n",
    "            best_k = k\n",
    "\n",
    "#     Final model using the best k\n",
    "#     kmeans = KMeans(n_clusters=best_k, n_init=10)\n",
    "#     cluster_assignments = kmeans.fit_predict(X)\n",
    "\n",
    "    local_models = []\n",
    "    for cluster in range(best_k):\n",
    "        cluster_indices = np.where(cluster_assignments == cluster)\n",
    "        X_cluster = X_train[cluster_indices]\n",
    "        y_cluster = y_train[cluster_indices]\n",
    "\n",
    "    #     print(\"unique classes:\", np.unique(y_cluster))\n",
    "\n",
    "        if len(np.unique(y_cluster)) == 1:\n",
    "            model = LogisticRegression(solver='saga')\n",
    "            model.classes_ = np.unique(y_cluster)\n",
    "            model.coef_ = np.zeros((1, X_train.shape[1]))\n",
    "            model.intercept_ = 0.0\n",
    "        else:\n",
    "        #     model = LinearRegression()\n",
    "            model = LogisticRegression(solver='saga')\n",
    "            model.fit(X_cluster, y_cluster)\n",
    "        local_models.append(model)\n",
    "\n",
    "    predictions = np.round(predict_mixture_of_models(X_train, local_models, kmeans))\n",
    "    # print(f\"Best k: {best_k}, Best R^2 score: {best_score}\")\n",
    "    # print(\"AUC:\", roc_auc_score(y, predictions))\n",
    "\n",
    "    import statistics\n",
    "\n",
    "    dist = pd.DataFrame(dist_matrix(best_k, classification=True))\n",
    "\n",
    "    CELL_centers = []\n",
    "    CELL_variances = []\n",
    "    for i in augmented_data[best_k]:\n",
    "        CELL_variances.append(np.sqrt(np.var(augmented_data[best_k][i], axis=0)))\n",
    "        CELL_centers.append(np.mean(augmented_data[best_k][i], axis=0))\n",
    "    cells_df = pd.DataFrame(CELL_centers)\n",
    "\n",
    "    y_local = []\n",
    "    for i in range(len(CELL_centers)):\n",
    "        y_local.append(statistics.mode(augmented_y[best_k][i]))\n",
    "\n",
    "\n",
    "    # import DT_with_R2_stopping as dtc\n",
    "#     import DECISION_TREE_CLASSIFIER as dtc\n",
    "\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X_n, y_n, test_size=0.2, random_state=0)\n",
    "\n",
    "#     X_train_aug = np.concatenate((X_train, augmentation_x))\n",
    "#     y_train_aug = np.concatenate((y_train, augmentation_y))\n",
    "\n",
    "    # X_train_aug = X_train\n",
    "    # y_train_aug = y_train\n",
    "\n",
    "    for k in range(1, 2):\n",
    "        roc_curr = float('-inf')\n",
    "        roc_curr_train = float('-inf')\n",
    "        best_depth = float('inf')\n",
    "\n",
    "        tree = dtc(max_depth=float('inf'), classification=True)\n",
    "\n",
    "        tree.fit(np.array(cells_df), np.array(dist))\n",
    "        \n",
    "        data_transformed = [augmented_data[best_k][key] for key in range(len(augmented_data[best_k]))]\n",
    "        data_transformed_y = [augmented_y[best_k][key] for key in range(len(augmented_y[best_k]))]\n",
    "            \n",
    "        X_transformed = np.array([item for sublist in [arr.tolist() for arr in data_transformed] for item in sublist])\n",
    "        y_transformed = np.array([item for sublist in [arr.tolist() for arr in data_transformed_y] for item in sublist])\n",
    "        \n",
    "        \n",
    "        #CROSS-VALIDATION FOR SELECTION THE BEST DEPTH:\n",
    "\n",
    "#             tree.fit_leaf_logistic_models(X_train_fold, y_train_fold)\n",
    "    #         tree.fit_leaf_logistic_models_cluster_based(augmented_y[best_k], np.array(cells_df), augmented_data[best_k], 0.2)\n",
    "        print(\"Max Depth\", tree.final_depth)\n",
    "\n",
    "            # Print the tree\n",
    "        #     tree.print_tree()\n",
    "\n",
    "        cv_scores = {}\n",
    "        test_score = {}\n",
    "        train_score = {}\n",
    "        \n",
    "        for i in range(1, tree.final_depth + 1):\n",
    "            \n",
    "            fold_scores = []\n",
    "            fold_test = []\n",
    "            fold_train = []\n",
    "            \n",
    "            for train_idx, val_idx in kf.split(X_transformed):\n",
    "\n",
    "                X_train_fold, X_val = X_transformed[train_idx], X_transformed[val_idx]\n",
    "                y_train_fold, y_val = y_transformed[train_idx], y_transformed[val_idx]\n",
    "            \n",
    "                tree = dtc(max_depth=i, classification=True)\n",
    "\n",
    "                tree.fit(np.array(cells_df), np.array(dist))\n",
    "\n",
    "                tree.fit_leaf_logistic_models(X_train_fold, y_train_fold, np.array(X_test), np.array(y_test))\n",
    "#                 tree.fit_leaf_logistic_models_cluster_based(augmented_y[best_k], np.array(cells_df), augmented_data[best_k], 0.2)\n",
    "#                 tree.print_tree()\n",
    "\n",
    "                Y_pred_train = np.round(tree.predict(X_train_fold, y_train_fold))\n",
    "                Y_pred = np.round(tree.predict(X_val, y_val))\n",
    "\n",
    "#                 print(\"Current Accuracy val\", roc_auc_score(y_val, Y_pred))\n",
    "                \n",
    "                fold_accuracy = roc_auc_score(y_val, Y_pred)\n",
    "                \n",
    "                fold_scores.append(fold_accuracy)\n",
    "                fold_train.append(roc_auc_score(y_train, np.round(tree.predict(X_train, y_train))))\n",
    "                fold_test.append(roc_auc_score(y_test, np.round(tree.predict(X_test, y_test))))\n",
    "\n",
    "#                 if roc_auc_score(y_val, Y_pred) > roc_curr:\n",
    "#                     roc_curr = roc_auc_score(y_val, Y_pred)\n",
    "#                     roc_curr_train = roc_auc_score(y_train_fold, Y_pred_train)\n",
    "#                     best_depth = tree.final_depth\n",
    "#                     best_tree = tree\n",
    "                    \n",
    "            print(f\"Current Depth: {i}\")        \n",
    "            cv_scores[i] = np.mean(fold_scores)\n",
    "            test_score[i] = np.mean(fold_test)\n",
    "            train_score[i] = np.mean(fold_train)\n",
    "                \n",
    "#         print(\"Test scores:\", test_score)\n",
    "#         print(\"cv_scores:\", cv_scores)\n",
    "\n",
    "    best_depth = max(cv_scores, key=lambda k: cv_scores[k])\n",
    "    print(\"Best Depth:\", best_depth, \"Train:\", train_score[best_depth], \"Test:\", \n",
    "          test_score[best_depth], \"Val:\", cv_scores[best_depth])\n",
    "    print(\"Models\", clfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7055f04a",
   "metadata": {},
   "source": [
    "## Regression Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d243f6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# import utils\n",
    "import importlib\n",
    "# importlib.reload(utils)\n",
    "# import utils\n",
    "# from utils import plot_mosaic, plot_ci, explainable_tree, explainable_condition, explainable_dim, highest_explainable_dim, plot_id_1d, plot_id_2d, plot_id_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2ed9abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 39 <= 0.0905900226539378\n",
      "Left:\n",
      "  Feature 12 <= 0.9887797295828311\n",
      "  Left:\n",
      "    Leaf: Accuracy Test= 0.8787\n",
      "    Model: <statsmodels.discrete.discrete_model.L1BinaryResultsWrapper object at 0x152e24d11a20>\n",
      "  Right:\n",
      "    Leaf: Accuracy Test= 0.8986\n",
      "    Model: <statsmodels.discrete.discrete_model.L1BinaryResultsWrapper object at 0x152e188d37f0>\n",
      "Right:\n",
      "  Feature 70 <= 0.03991398357855075\n",
      "  Left:\n",
      "    Leaf: Accuracy Test= 0.8514\n",
      "    Model: <statsmodels.discrete.discrete_model.L1BinaryResultsWrapper object at 0x152e188d2020>\n",
      "  Right:\n",
      "    Leaf: Accuracy Test= 0.8544\n",
      "    Model: <statsmodels.discrete.discrete_model.L1BinaryResultsWrapper object at 0x152e0dbf0e20>\n"
     ]
    }
   ],
   "source": [
    "tree.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c8f6558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<statsmodels.discrete.discrete_model.L1BinaryResultsWrapper at 0x152e24d11a20>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.tree['left']['left']['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d69edf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ce9640fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS.append(tree.tree['left']['right']['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b2995410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<statsmodels.discrete.discrete_model.L1BinaryResultsWrapper at 0x152e0dbf0e20>,\n",
       " <statsmodels.discrete.discrete_model.L1BinaryResultsWrapper at 0x152e188d2020>,\n",
       " <statsmodels.discrete.discrete_model.L1BinaryResultsWrapper at 0x152e24d11a20>,\n",
       " <statsmodels.discrete.discrete_model.L1BinaryResultsWrapper at 0x152e188d37f0>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d95c1af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "89c81367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ci(MODELS, feature_id, epic_id=None, feature_names=feature_names, intercept=True, sort=True, log_trans=True, ax=None, yticklabels=True, title=True):\n",
    "    if ax == None:\n",
    "        f, ax = plt.subplots(1,1,figsize=(7,3),dpi=300)\n",
    "    \n",
    "    param = np.array([MODELS[i].params for i in range(len(MODELS))])\n",
    "    cilb = np.transpose([MODELS[i].conf_int() for i in range(len(MODELS))])[0]\n",
    "    ciub = np.transpose([MODELS[i].conf_int() for i in range(len(MODELS))])[1]\n",
    "\n",
    "    if epic_id == None:\n",
    "        epic_id = range(param.shape[0])\n",
    "\n",
    "    def log(x): return np.sign(x)*np.log(np.abs(x)+1)\n",
    "    if log_trans:\n",
    "        param = log(param)\n",
    "        cilb = log(cilb)\n",
    "        ciub = log(ciub)\n",
    "\n",
    "#     if sort:\n",
    "#         sorted_postid, sorted_postsize = sort_EPIC(MLM.loc_prob)\n",
    "#         param = param[sorted_postid[epic_id],:]\n",
    "#         cilb = cilb[:,sorted_postid[epic_id]]\n",
    "#         ciub = ciub[:,sorted_postid[epic_id]]\n",
    "#     else:\n",
    "    param = param[epic_id,:]\n",
    "    cilb = cilb[:,epic_id]\n",
    "    ciub = ciub[:,epic_id]\n",
    "\n",
    "#     if feature_names == None:\n",
    "#         if 'feature_names' in MLM.__dict__.keys():\n",
    "#             feature_names = MLM.feature_names\n",
    "#         else:\n",
    "#             feature_names = ['v'+str(i) for i in range(param.shape[1])]\n",
    "#     if intercept:\n",
    "#         feature_names = np.concatenate([['intercept'],feature_names])\n",
    "\n",
    "    data_dict = {}\n",
    "    data_dict['category'] = ['ELC'+str(i+1) for i in epic_id]\n",
    "    data_dict['coef'] = param[:,feature_id]\n",
    "    data_dict['lower'] = cilb[feature_id,:]\n",
    "    data_dict['upper'] = ciub[feature_id,:]\n",
    "    dataset = pd.DataFrame(data_dict)\n",
    "    \n",
    "    defcol = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "    \n",
    "    for coef,lower,upper,y in zip(dataset['coef'],dataset['lower'],dataset['upper'],range(len(dataset))):\n",
    "        ax.plot((lower,upper),(y,y),'-',color='#74a9cf', linewidth=3, alpha=0.9)\n",
    "        ax.scatter(coef,y,color='black')\n",
    "    ax.set_yticks(range(len(dataset)))\n",
    "    ax.set_yticklabels(list(dataset['category']))\n",
    "    ax.axvline(x=0,color='black',linewidth=1)\n",
    "    if title:\n",
    "        ax.set_title(feature_names[feature_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "99298887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "def plot_mosaic(MODELS, epic_id=None, feature_names=feature_names, intercept=True, sort=True, log_trans=True, ax=None):\n",
    "    if ax == None:\n",
    "        f, ax = plt.subplots(figsize=(7,3),dpi=300)\n",
    "    \n",
    "    cmap=plt.get_cmap('RdBu_r')\n",
    "    param = np.array([MODELS[i].params for i in range(len(MODELS))])\n",
    "\n",
    "    if epic_id == None:\n",
    "        epic_id = range(param.shape[0])\n",
    "\n",
    "    if log_trans:\n",
    "        param = np.sign(param)*np.log(np.abs(param)+1)\n",
    "\n",
    "#     if sort:\n",
    "#         sorted_postid, sorted_postsize = sort_EPIC(MLM.loc_prob)\n",
    "#         param = param[sorted_postid[epic_id],:]\n",
    "#     else:\n",
    "    param = param[epic_id,:]\n",
    "\n",
    "#     if feature_names == None:\n",
    "#         if 'feature_names' in MLM.__dict__.keys():\n",
    "#             feature_names = MLM.feature_names\n",
    "#         else:\n",
    "#             feature_names = ['v'+str(i) for i in range(param.shape[1])]\n",
    "    if intercept:\n",
    "        feature_names = np.concatenate([['intercept'],feature_names])\n",
    "\n",
    "    vmax = np.max(np.abs(param))\n",
    "    im = ax.imshow(param, cmap=cmap, vmin=-vmax, vmax=vmax)\n",
    "\n",
    "    plt.xticks(ticks=range(0,len(feature_names)))\n",
    "    ax.set_xticklabels(feature_names, rotation = 90)\n",
    "    plt.yticks(ticks=range(0,len(epic_id)), labels = [i+1 for i in epic_id])\n",
    "    plt.ylabel('ELC')\n",
    "    plt.xlabel('Variable')\n",
    "\n",
    "\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    plt.colorbar(im, cax=cax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "541eaf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['Age',\n",
    " 'DAYS_TO_COLLECTION',\n",
    " 'ETHNICITY',\n",
    " 'FORM_COMPLETION_DATE',\n",
    " 'FRACTION_GENOME_ALTERED',\n",
    " 'HISTORY_NEOADJUVANT_TRTYN',\n",
    " 'History malignancy',\n",
    " 'ICD_10',\n",
    " 'ICD_O_3_HISTOLOGY',\n",
    " 'ICD_O_3_SITE',\n",
    " 'INITIAL_PATHOLOGIC_DX_YEAR',\n",
    " 'NEW_TUMOR_EVENT_AFTER_INITIAL_TREATMENT',\n",
    " 'OCT_EMBEDDED',\n",
    " 'OS_MONTHS',\n",
    " 'PRIMARY_MELANOMA_KNOWN_DX',\n",
    " 'PROSPECTIVE_COLLECTION',\n",
    " 'Radiation treatment',\n",
    " 'RETROSPECTIVE_COLLECTION',\n",
    " 'SAMPLE_COUNT',\n",
    " 'SAMPLE_INITIAL_WEIGHT',\n",
    " 'SAMPLE_TYPE',\n",
    " 'SAMPLE_TYPE_ID',\n",
    " 'Gender',\n",
    " 'SUBMITTED_TUMOR_DX_DAYS_TO',\n",
    " 'VIAL_NUMBER',\n",
    " 'AJCC_NODES_PATHOLOGIC_PN0',\n",
    " 'AJCC_NODES_PATHOLOGIC_PN1',\n",
    " 'AJCC_NODES_PATHOLOGIC_PN2',\n",
    " 'AJCC_NODES_PATHOLOGIC_PN3',\n",
    " 'AJCC_NODES_PATHOLOGIC_PN4',\n",
    " 'AJCC_NODES_PATHOLOGIC_PN5',\n",
    " 'AJCC_NODES_PATHOLOGIC_PN6',\n",
    " 'AJCC_NODES_PATHOLOGIC_PN7',\n",
    " 'AJCC_NODES_PATHOLOGIC_PN8',\n",
    " 'AJCC_STAGING_EDITION0',\n",
    " 'AJCC_STAGING_EDITION1',\n",
    " 'AJCC_STAGING_EDITION2',\n",
    " 'AJCC_STAGING_EDITION3',\n",
    " 'AJCC_STAGING_EDITION4',\n",
    " 'AJCC_STAGING_EDITION5',\n",
    " 'RACE0',\n",
    " 'RACE1',\n",
    " 'TISSUE_SOURCE_SITE0',\n",
    " 'TISSUE_SOURCE_SITE1',\n",
    " 'TISSUE_SOURCE_SITE2',\n",
    " 'TISSUE_SOURCE_SITE3',\n",
    " 'TISSUE_SOURCE_SITE4',\n",
    " 'TISSUE_SOURCE_SITE5',\n",
    " 'TISSUE_SOURCE_SITE6',\n",
    " 'TISSUE_SOURCE_SITE7',\n",
    " 'TISSUE_SOURCE_SITE8',\n",
    " 'TISSUE_SOURCE_SITE9',\n",
    " 'TISSUE_SOURCE_SITE10',\n",
    " 'TISSUE_SOURCE_SITE11',\n",
    " 'TISSUE_SOURCE_SITE12',\n",
    " 'TISSUE_SOURCE_SITE13',\n",
    " 'TISSUE_SOURCE_SITE14',\n",
    " 'TISSUE_SOURCE_SITE15',\n",
    " 'TISSUE_SOURCE_SITE16',\n",
    " 'TISSUE_SOURCE_SITE17',\n",
    " 'TISSUE_SOURCE_SITE18',\n",
    " 'TISSUE_SOURCE_SITE19',\n",
    " 'TISSUE_SOURCE_SITE20',\n",
    " 'TUMOR_SITE0',\n",
    " 'TUMOR_SITE1',\n",
    " 'TUMOR_SITE2',\n",
    " 'TUMOR_SITE3',\n",
    " 'TUMOR_SITE4',\n",
    " 'TUMOR_SITE5',\n",
    " 'TUMOR_SITE6',\n",
    " 'TUMOR_SITE7',\n",
    " 'TUMOR_SITE8',\n",
    " 'TUMOR_SITE9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2de1569a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "plot_mosaic(MODELS, epic_id=range(4), log_trans=False, ax=ax)\n",
    "plt.savefig('SKCM_heatmap.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "81762fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 3000x6000 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "f, axs = plt.subplots(4, 1, figsize=(5,10), dpi=600)\n",
    "# f.tight_layout()\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "for i in range(4):\n",
    "    j = (0,22,16,6)[i]\n",
    "    ax = plt.subplot(4,1,i+1)\n",
    "    plot_ci(MODELS,j,epic_id=range(4),ax=ax,title=True)\n",
    "    \n",
    "    \n",
    "# f, axs = plt.subplots(2,2,figsize=(10,5),dpi=300)\n",
    "# f.tight_layout()\n",
    "# plt.subplots_adjust(hspace=0.3)\n",
    "# for i in range(11):\n",
    "#     j = (2,3,5,4)[i]\n",
    "#     ax = plt.subplot(2,2,i+1)\n",
    "#     plot_ci(models_top,j,epic_id=range(5),ax=ax,title=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84deff50",
   "metadata": {},
   "source": [
    "### Regression Coefficients End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43223427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_1 = RF\n",
    "# clf_2 = GB\n",
    "# clf_3 = MLP\n",
    "# clf_4 = Forestry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "903f5a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 3 <= 0.14099194650688512\n",
      "Left:\n",
      "  Feature 1 <= 0.7717255248977114\n",
      "  Left:\n",
      "    Leaf: Accuracy = 0.7939\n",
      "    Model: LogisticRegression(penalty='l1', solver='saga')\n",
      "  Right:\n",
      "    Leaf: Accuracy = 0.9189\n",
      "    Model: LogisticRegression(penalty='l1', solver='saga')\n",
      "Right:\n",
      "  Feature 38 <= 0.03365618354331364\n",
      "  Left:\n",
      "    Leaf: Accuracy = 0.7984\n",
      "    Model: LogisticRegression(penalty='l1', solver='saga')\n",
      "  Right:\n",
      "    Leaf: Accuracy = 0.8285\n",
      "    Model: LogisticRegression(penalty='l1', solver='saga')\n"
     ]
    }
   ],
   "source": [
    "tree.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78d7cdf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feature_index': 1,\n",
       " 'threshold': 0.17664530618928118,\n",
       " 'left': {'feature_index': 53,\n",
       "  'threshold': 0.019692314600863087,\n",
       "  'left': {'leaf': True,\n",
       "   'data_indices': array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "           13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "           26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "           39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "           52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "           65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "           78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "           91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "          104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "          117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "          130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "          143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "          156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "          169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "          182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "          195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "          208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "          221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
       "          234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "          247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
       "          260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
       "          273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
       "          286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
       "          299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
       "          312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n",
       "          325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n",
       "          338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
       "          351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
       "          364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376,\n",
       "          377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389,\n",
       "          390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402,\n",
       "          403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415,\n",
       "          416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428,\n",
       "          429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441,\n",
       "          442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454,\n",
       "          455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,\n",
       "          468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480,\n",
       "          481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493,\n",
       "          494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506,\n",
       "          507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519,\n",
       "          520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532,\n",
       "          533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545,\n",
       "          546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558,\n",
       "          559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571,\n",
       "          572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584,\n",
       "          585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597,\n",
       "          598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610,\n",
       "          611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623,\n",
       "          624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636,\n",
       "          637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649,\n",
       "          650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662,\n",
       "          663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675,\n",
       "          676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688,\n",
       "          689, 690, 691, 692, 693, 694, 695, 696, 697, 698]),\n",
       "   'accuracy': 0.8183118741058655,\n",
       "   'location_matrix': array([[ 6.47571251e-01,  7.19804063e-03,  1.04973712e+00,\n",
       "            3.76820704e-01,  3.92688305e-01,  3.43496703e-02,\n",
       "            9.84842304e-04,  3.75128467e-01,  1.44854736e-02,\n",
       "            2.86386918e-01,  9.64893829e-01,  6.50966636e-02,\n",
       "            8.92732262e-01,  8.66943974e-02,  1.01502051e+00,\n",
       "            1.03825339e+00,  1.07304531e-01,  2.71880304e-02,\n",
       "            3.11041141e-02,  2.26673707e-01,  9.57874345e-01,\n",
       "            1.47384173e-02,  1.45556648e-01, -3.83249553e-02,\n",
       "           -1.70228442e-02,  1.17258336e-01,  5.67414332e-02,\n",
       "           -9.95901093e-03,  2.10651927e-02,  1.26839928e-03,\n",
       "            3.51146414e-02,  1.60301355e-01,  3.62768836e-03,\n",
       "            5.95811980e-02, -2.17676636e-02, -1.49459116e-02,\n",
       "           -4.57498968e-02, -1.55133087e-02, -3.73263101e-03,\n",
       "            1.02155502e+00, -3.06753571e-02,  8.70931229e-01,\n",
       "            3.41751014e-02,  1.18006932e-02,  1.56722135e-01,\n",
       "            1.18465106e-02,  7.92907348e-01, -4.20108525e-02,\n",
       "            1.26421343e-02,  3.87211533e-02, -2.05003322e-02,\n",
       "            2.13698743e-02,  1.08781137e-02, -3.45186206e-02,\n",
       "           -1.36646031e-02,  2.03496316e-04,  1.29743546e-02,\n",
       "           -1.04955910e-02,  8.99418246e-02,  3.88821328e-02,\n",
       "            3.93492791e-02, -9.36181760e-03, -3.29794725e-02,\n",
       "           -5.39292672e-02, -3.19216528e-02, -1.71314699e-02,\n",
       "            2.70011135e-01, -6.49812838e-02,  1.17864989e-03,\n",
       "            1.45209367e-02,  6.31539789e-01, -3.96897897e-02,\n",
       "            1.70637390e-02],\n",
       "          [ 4.72160251e-01,  1.34327874e-01,  1.05108861e+00,\n",
       "            2.93523265e-01,  3.78806403e-01,  6.78306029e-02,\n",
       "            1.59778418e-01,  8.77346369e-01,  1.20022623e-03,\n",
       "            8.72981121e-01,  8.40749026e-01,  9.33986397e-01,\n",
       "            8.88858123e-01,  1.64922670e-01,  1.16121616e-01,\n",
       "            8.08343105e-02,  6.49492256e-01,  9.35163999e-01,\n",
       "           -2.91048158e-02,  6.90726908e-02,  2.24798236e-03,\n",
       "            9.66674995e-01,  3.87800717e-01,  3.68170557e-02,\n",
       "            2.31757881e-03,  9.01733039e-02,  2.84443062e-02,\n",
       "            3.04824718e-01,  8.94874877e-03,  1.07687383e-02,\n",
       "           -5.62789400e-03,  1.33204256e-01,  6.52223167e-01,\n",
       "            7.47041668e-03, -1.43475429e-02,  3.92433014e-02,\n",
       "            2.33770192e-02,  7.27943498e-03,  5.68108194e-02,\n",
       "            8.85069081e-01,  2.83668556e-03,  1.04362097e+00,\n",
       "            2.02723619e-02,  6.90344604e-01,  5.45141837e-03,\n",
       "           -3.99717981e-02,  4.10312126e-02,  1.25123889e-01,\n",
       "           -3.81635836e-02,  8.78390376e-02, -6.26082253e-03,\n",
       "           -2.46258617e-02, -1.00672116e-02,  5.45867517e-03,\n",
       "            8.59552361e-03,  5.75092571e-03,  7.04068511e-02,\n",
       "           -1.21944288e-03, -2.81837378e-02,  1.09235419e-01,\n",
       "           -4.86305911e-02, -3.03933639e-02,  4.52250358e-02,\n",
       "           -5.17795939e-06, -1.68308698e-02,  2.13490815e-02,\n",
       "            1.27789634e-01, -2.91117623e-02, -1.66365535e-02,\n",
       "            9.40463078e-01, -1.36836762e-02, -2.88037074e-02,\n",
       "            6.81047558e-02],\n",
       "          [ 6.48687731e-01,  1.95060735e-02,  9.65047090e-01,\n",
       "            5.30493404e-01,  4.11197515e-01, -4.34279554e-02,\n",
       "            7.76786537e-02,  3.88517878e-01,  6.28427477e-03,\n",
       "            3.81199813e-01,  9.49885791e-01, -2.93021328e-03,\n",
       "            9.42390069e-01,  5.38055597e-02,  9.76909729e-01,\n",
       "            1.00143189e+00,  3.62137813e-02,  5.49505095e-03,\n",
       "           -2.44929745e-02,  2.69528090e-01,  9.58792378e-01,\n",
       "            6.01467866e-03,  6.10660710e-01,  9.62508409e-03,\n",
       "            1.97800299e-02,  1.08957792e-01,  1.91705439e-02,\n",
       "            4.68277906e-02,  9.11157937e-02, -3.15365360e-02,\n",
       "            9.36792008e-02, -4.56762720e-04,  5.69562286e-03,\n",
       "            2.28682189e-01,  2.46204417e-02, -8.02603277e-03,\n",
       "            3.32394833e-02, -1.79918264e-03, -2.56653458e-02,\n",
       "            1.00404557e+00,  1.20655752e-03,  6.37205040e-01,\n",
       "           -1.99706394e-02,  2.65708359e-02, -5.07769855e-03,\n",
       "           -3.47662072e-03,  8.62271075e-01, -1.62671050e-03,\n",
       "            3.04249273e-02,  3.55169891e-02, -3.63041704e-02,\n",
       "            7.11357277e-02,  7.03068376e-02, -4.60607989e-02,\n",
       "           -3.71416655e-02,  1.21570845e-02,  1.91699905e-02,\n",
       "            1.64293328e-02, -1.58520363e-02,  9.35649731e-02,\n",
       "            4.32572973e-02,  1.20314703e-02, -6.14748441e-02,\n",
       "            1.03212424e+00, -5.37131626e-04,  1.55146978e-02,\n",
       "           -7.27650956e-03,  1.99913752e-02,  3.31959868e-02,\n",
       "           -6.11874534e-02, -4.52833420e-02,  8.01822266e-03,\n",
       "           -2.65315488e-02],\n",
       "          [ 5.86057049e-01,  1.27025186e-01,  1.01994659e+00,\n",
       "            4.29762451e-01,  3.04291332e-01, -3.41618003e-03,\n",
       "           -3.75204548e-02,  6.13950338e-01,  2.85930520e-02,\n",
       "            6.39905666e-01,  8.89924815e-01,  9.72869584e-01,\n",
       "            1.13638401e-02,  1.50982506e-01,  1.02968984e+00,\n",
       "            9.78329812e-01,  5.73856545e-03,  2.16628704e-02,\n",
       "           -2.64741372e-02,  1.60008126e-01,  5.12339812e-02,\n",
       "            8.97006491e-01,  4.50992257e-01,  1.40242230e-01,\n",
       "           -1.86455217e-02,  4.25473716e-03,  1.47159420e-01,\n",
       "            2.60191883e-03, -3.47407263e-02,  1.08009278e-01,\n",
       "            8.83552428e-03, -1.18836214e-02,  4.12918457e-03,\n",
       "            5.49113595e-02,  5.55502821e-03,  2.52568152e-03,\n",
       "            9.18398656e-02, -1.45451473e-02,  1.90652728e-01,\n",
       "            6.71458296e-01, -5.21331035e-02,  9.95089030e-01,\n",
       "            2.73837265e-02, -3.12265421e-02, -4.31986645e-03,\n",
       "           -6.16915688e-04, -2.07866614e-03, -3.31364906e-02,\n",
       "           -2.64883834e-02,  8.06313415e-01, -3.02465452e-03,\n",
       "           -3.35380027e-02,  1.12639583e-01,  1.96923146e-02,\n",
       "            3.05991427e-02, -1.27484544e-02, -4.52099259e-02,\n",
       "           -1.13709353e-02, -3.08126586e-02,  3.12066569e-02,\n",
       "            4.09796484e-02,  1.26327984e-03, -2.34317880e-02,\n",
       "            5.38587187e-01,  1.96636867e-02,  1.34832373e-02,\n",
       "           -1.19189900e-02,  2.53018920e-01, -3.25518249e-02,\n",
       "           -5.44425862e-02,  1.82519138e-01,  4.09896793e-02,\n",
       "            2.91319948e-02],\n",
       "          [ 6.73690938e-01,  7.07885666e-03,  9.89284609e-01,\n",
       "            4.45454687e-01,  2.12653157e-01,  4.10042578e-02,\n",
       "            2.27999723e-02,  9.98413064e-01,  2.96759829e-02,\n",
       "            9.72583303e-01,  1.01392941e+00,  5.84554182e-01,\n",
       "            9.97869678e-01, -6.31072806e-04,  1.03678883e+00,\n",
       "            1.01236202e+00, -2.35143992e-02,  4.22682862e-02,\n",
       "            1.55312363e-02,  4.91983177e-01,  1.89349550e-02,\n",
       "            1.02685029e+00,  4.36127000e-01, -2.22189591e-03,\n",
       "           -1.87898181e-02,  6.01480112e-01, -6.48357840e-02,\n",
       "           -2.52910109e-03, -1.69491761e-02, -1.81547636e-02,\n",
       "            1.85679114e-01, -6.61116710e-03,  2.13663007e-01,\n",
       "            1.35422449e-02, -4.87981256e-02, -1.24786461e-02,\n",
       "            3.67921660e-03,  1.69018608e-02,  5.73420545e-02,\n",
       "            1.02127192e+00, -3.35201364e-02,  4.09865090e-01,\n",
       "            1.28671952e-02,  5.09925498e-02,  2.82790360e-01,\n",
       "            1.87049982e-02,  7.82727806e-01,  3.79794679e-03,\n",
       "           -4.22522152e-02,  5.93005634e-03, -1.22828304e-02,\n",
       "            1.23189822e-02, -6.41387523e-02,  1.57774910e-02,\n",
       "            3.61348383e-02,  4.65092165e-03,  1.59943376e-02,\n",
       "            1.06975667e-02, -2.61810921e-02,  2.22841961e-02,\n",
       "           -2.86050933e-02, -5.73067346e-03, -3.83337828e-02,\n",
       "            7.98542535e-01,  2.82021584e-02,  2.64586402e-02,\n",
       "            2.88185057e-01,  4.00831314e-02,  5.34411338e-02,\n",
       "            3.23447085e-02,  3.13175787e-02, -4.07312397e-02,\n",
       "            9.14959069e-03],\n",
       "          [ 8.07521241e-01,  1.59303939e-02,  8.79252230e-01,\n",
       "            2.87861204e-01,  3.25542681e-01,  8.11325737e-03,\n",
       "            1.80295945e-02,  4.10812352e-01,  5.07602511e-01,\n",
       "            3.41921185e-01,  9.95188609e-01,  3.44045590e-02,\n",
       "            9.78502928e-01,  8.51418785e-02,  9.84177844e-01,\n",
       "            9.71113266e-01, -2.04984312e-02,  3.23941133e-02,\n",
       "           -3.83947917e-02,  5.24666015e-02,  9.90173038e-01,\n",
       "           -2.70561460e-02,  5.54180577e-01, -4.46527799e-03,\n",
       "            1.26819450e-02,  4.65134475e-03, -6.82961592e-03,\n",
       "           -2.00403718e-02,  2.13705688e-01, -1.19360892e-02,\n",
       "            3.62909941e-02, -2.02059543e-02,  9.24592992e-02,\n",
       "            6.89035813e-03,  3.15074239e-03,  3.01329479e-02,\n",
       "            7.25611556e-03, -3.94645832e-02,  9.00627303e-04,\n",
       "            1.00890593e+00, -5.05450378e-03,  9.71704564e-01,\n",
       "            9.78335957e-01, -5.00695051e-02,  3.79719816e-02,\n",
       "           -1.46439424e-03,  5.86399596e-03, -5.00069333e-02,\n",
       "           -1.85653523e-03, -3.89824510e-02,  9.93444476e-03,\n",
       "           -3.70128832e-02, -2.16342951e-02,  7.47909104e-03,\n",
       "            1.13639060e-02, -5.18454442e-03,  4.20415530e-03,\n",
       "           -1.76025116e-02, -1.56622132e-02, -9.26559672e-03,\n",
       "            2.52759584e-02,  5.32996039e-03, -1.87930671e-02,\n",
       "            9.84171319e-01,  8.03440895e-04, -5.22322327e-02,\n",
       "            1.21382533e-02, -2.25667922e-02,  2.59291160e-04,\n",
       "            2.02953436e-02,  4.51956990e-03, -9.03940200e-03,\n",
       "           -3.65076775e-02],\n",
       "          [ 6.19027462e-01,  8.41346462e-02,  1.02536037e+00,\n",
       "            1.67004315e-02,  3.02734534e-01,  3.07382632e-02,\n",
       "           -1.96213671e-02,  7.17400874e-01,  4.88471774e-05,\n",
       "            6.77316920e-01,  8.69196482e-01,  8.59920587e-01,\n",
       "            9.96344418e-01,  1.16971391e-01,  5.52721159e-01,\n",
       "            3.24502989e-02,  2.35649606e-02,  9.72588364e-01,\n",
       "           -1.64442764e-02,  1.55392177e-01,  1.14666880e-01,\n",
       "            9.08468998e-01,  9.03030240e-01,  6.64105504e-02,\n",
       "           -1.58900338e-02,  1.75389065e-02,  4.99528789e-03,\n",
       "           -1.44158047e-02, -1.36055973e-02,  1.13296211e-01,\n",
       "            5.68428838e-01,  2.13949949e-02, -2.07781409e-03,\n",
       "           -1.87126461e-02,  8.19805141e-03,  5.15041042e-02,\n",
       "            9.68331837e-02,  9.03931649e-03,  6.46008031e-02,\n",
       "            8.11644017e-01,  2.40367376e-02,  9.88758004e-01,\n",
       "           -5.26359337e-02,  6.43892971e-02,  5.49584616e-02,\n",
       "            1.01269231e+00, -6.62954588e-03, -1.03202810e-02,\n",
       "            6.89924398e-03, -6.86830032e-02, -1.95408694e-03,\n",
       "            1.25072761e-02,  1.93372032e-02,  1.16388386e-02,\n",
       "            9.60582442e-03, -2.10634324e-02,  5.30030066e-02,\n",
       "           -6.92922619e-03, -9.12468692e-03, -4.03835332e-02,\n",
       "           -1.87114289e-02, -6.55145612e-03, -2.06276136e-02,\n",
       "            5.68861753e-02, -4.93250150e-02, -1.78216267e-02,\n",
       "            8.60768461e-02, -3.69123977e-02,  3.43746255e-01,\n",
       "            1.57776376e-01,  3.33112497e-01,  1.22091273e-02,\n",
       "            1.57126712e-02],\n",
       "          [ 4.70513648e-01, -1.17888251e-02,  9.88231159e-01,\n",
       "            7.40290767e-01,  3.13061390e-01, -3.55685487e-02,\n",
       "            1.33852406e-01,  8.06497936e-01,  8.21818915e-02,\n",
       "            7.76101989e-01,  9.76946174e-01,  8.86334132e-01,\n",
       "            5.09306359e-01,  1.95267613e-02,  9.60650537e-01,\n",
       "            9.92271315e-01,  3.91959123e-02, -4.98722613e-02,\n",
       "            4.07394571e-03,  1.65489198e-01,  4.93266153e-02,\n",
       "            1.03866830e+00,  2.80381715e-01,  2.12345176e-02,\n",
       "            1.57922516e-02,  9.48892486e-04,  4.55847806e-03,\n",
       "            2.76240925e-01, -1.74430912e-02,  6.23559168e-03,\n",
       "            2.76182894e-01, -2.03979755e-02,  3.99175161e-01,\n",
       "            5.00932462e-02, -3.92721618e-02, -4.45386601e-02,\n",
       "           -3.21933330e-02, -8.45415103e-03,  4.56416876e-02,\n",
       "            1.00595513e+00,  7.06038599e-03,  1.01185321e+00,\n",
       "           -2.23478122e-02,  6.05573303e-03, -4.90614410e-03,\n",
       "           -1.88530362e-03,  4.64519568e-03,  1.42740238e-02,\n",
       "            1.12811391e-01,  4.43685472e-03, -1.99417769e-02,\n",
       "            1.64457084e-01, -4.35175178e-02, -2.65981228e-02,\n",
       "           -1.52519235e-02,  2.83505390e-02,  2.30368565e-02,\n",
       "           -4.54797751e-02, -3.56313841e-02,  6.19544685e-01,\n",
       "            1.07678819e-02,  1.49507604e-02,  6.12821563e-02,\n",
       "            6.16284171e-01,  4.83759593e-02,  3.13163580e-02,\n",
       "           -2.18901055e-02, -8.49292484e-03,  2.29213253e-04,\n",
       "           -3.40541004e-02,  4.18683564e-01, -2.47554236e-02,\n",
       "            3.87667922e-02],\n",
       "          [ 6.39689006e-01,  7.77026037e-03,  9.92653228e-01,\n",
       "            6.08762547e-01,  3.78731343e-01,  3.43149498e-02,\n",
       "            2.18054479e-02,  3.08180934e-01,  1.07982927e-01,\n",
       "            3.24559655e-01,  9.61348271e-01,  5.97769657e-02,\n",
       "            9.16940773e-01,  6.07909584e-04,  1.01454559e+00,\n",
       "            9.78295746e-01,  4.92947821e-02,  8.68741341e-04,\n",
       "            6.89810892e-02,  3.21463736e-01,  1.01679214e+00,\n",
       "            1.86331101e-02,  8.70288608e-01, -1.87708862e-02,\n",
       "           -5.70677651e-02,  4.44548387e-02,  2.85816505e-02,\n",
       "            7.53068015e-03,  2.62540275e-02,  4.97045418e-03,\n",
       "           -4.06935705e-02, -1.26121995e-02,  1.45339433e-01,\n",
       "            3.90873715e-01, -2.37111003e-02, -1.65789671e-02,\n",
       "           -2.52264395e-02,  1.83261699e-02,  3.23640665e-02,\n",
       "            1.00954094e+00,  3.03009569e-02,  1.04584574e+00,\n",
       "            1.48212710e-02,  1.00937311e-02, -3.84641935e-02,\n",
       "           -8.68177301e-03,  8.37540968e-01, -2.63703897e-02,\n",
       "            1.42233031e-01,  9.86008579e-03, -1.13462365e-02,\n",
       "            5.28830576e-02,  9.20076972e-03,  6.86135489e-04,\n",
       "           -3.65150322e-02, -6.39785786e-02, -3.29386132e-02,\n",
       "           -1.33055421e-02,  8.65769278e-03,  4.28133154e-02,\n",
       "            9.28510073e-03,  1.35114571e-02, -1.30154607e-02,\n",
       "           -2.87784456e-02,  6.60133237e-02, -3.22982870e-03,\n",
       "            1.32784749e-01, -1.46997937e-02,  7.87538790e-03,\n",
       "           -2.91625176e-03,  8.41961788e-01, -6.53118156e-03,\n",
       "           -3.00525184e-02],\n",
       "          [ 6.40411125e-01,  1.76645306e-01,  9.72984924e-01,\n",
       "            3.64989654e-01,  2.78387115e-01, -4.25091814e-03,\n",
       "            1.79403821e-02,  8.98154197e-01,  7.01030167e-03,\n",
       "            8.43641782e-01,  8.10872214e-01,  2.25078638e-01,\n",
       "            9.84662922e-01,  1.62991269e-01, -2.21208593e-02,\n",
       "            3.86656176e-02,  8.24245772e-02,  1.01713624e+00,\n",
       "            5.38810739e-02,  3.81511772e-02,  1.35741527e-02,\n",
       "            9.86982678e-01,  9.08103340e-01, -4.81015176e-02,\n",
       "           -3.12768656e-02, -3.39477120e-02, -5.88794640e-02,\n",
       "            1.24494745e-01,  9.96703145e-02,  1.97666119e-01,\n",
       "            3.78847973e-01, -4.82020893e-02,  4.76779166e-02,\n",
       "            2.04805614e-01, -6.99154838e-03,  6.60379255e-03,\n",
       "            1.81520189e-01,  2.50153603e-01,  6.36906533e-01,\n",
       "            3.29740823e-02,  3.01038663e-02,  1.08663660e+00,\n",
       "            1.39571257e-03, -4.08344666e-03, -1.95538737e-02,\n",
       "            1.62627968e-01,  2.45156319e-02,  5.79119100e-01,\n",
       "            4.48510005e-02,  2.56087965e-02,  3.26787380e-02,\n",
       "           -7.91917614e-03,  1.26825245e-02,  1.51639845e-02,\n",
       "            1.08986511e-02, -1.36845780e-03,  1.96383912e-02,\n",
       "            2.17997933e-01,  1.25136233e-01, -1.77608973e-02,\n",
       "           -3.54384139e-02, -1.66351575e-02,  6.38793082e-02,\n",
       "            2.08750919e-02, -1.24531979e-04, -3.11225576e-02,\n",
       "           -1.90971187e-04,  1.33397607e-04, -2.27016691e-03,\n",
       "            8.25845717e-01,  2.85392348e-03, -2.42534283e-02,\n",
       "           -8.85496200e-03]]),\n",
       "   'parent': {...},\n",
       "   'model': LogisticRegression(penalty='l1', solver='saga')},\n",
       "  'right': {'leaf': True,\n",
       "   'data_indices': array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "           13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "           26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "           39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "           52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "           65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "           78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "           91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "          104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "          117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "          130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "          143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "          156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "          169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "          182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "          195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "          208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "          221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
       "          234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "          247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
       "          260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
       "          273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
       "          286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
       "          299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
       "          312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n",
       "          325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n",
       "          338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
       "          351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
       "          364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376,\n",
       "          377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389,\n",
       "          390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402,\n",
       "          403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415,\n",
       "          416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428,\n",
       "          429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441,\n",
       "          442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454,\n",
       "          455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,\n",
       "          468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480,\n",
       "          481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493,\n",
       "          494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506,\n",
       "          507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519,\n",
       "          520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532,\n",
       "          533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545,\n",
       "          546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558,\n",
       "          559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571,\n",
       "          572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584,\n",
       "          585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597,\n",
       "          598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610,\n",
       "          611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623,\n",
       "          624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636,\n",
       "          637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649,\n",
       "          650, 651, 652, 653, 654, 655]),\n",
       "   'accuracy': 0.8307926829268293,\n",
       "   'location_matrix': array([[ 6.21772936e-01,  1.19193527e-01,  9.72365975e-01,\n",
       "            3.00398599e-01,  3.10017896e-01,  6.13350927e-02,\n",
       "            5.70474759e-02,  7.37855419e-01, -5.05773303e-04,\n",
       "            6.77253205e-01,  8.20995933e-01,  1.00666937e+00,\n",
       "            9.95231802e-01,  1.22522676e-01,  1.00882301e+00,\n",
       "            1.99004051e-01, -1.38684168e-02,  7.68867787e-01,\n",
       "           -1.77428496e-02,  9.55046176e-02,  4.45470503e-02,\n",
       "            1.03153723e+00,  6.25656319e-01,  5.65003960e-02,\n",
       "            4.01917331e-02, -3.53228131e-03,  1.55884813e-01,\n",
       "            7.12058079e-02,  1.53470662e-02,  7.46356142e-02,\n",
       "            1.30185275e-03, -1.06762781e-02,  1.06682555e-01,\n",
       "           -2.12461516e-02,  1.58266748e-02,  3.48038726e-03,\n",
       "            2.13442721e-02, -2.90216226e-02,  1.00364321e+00,\n",
       "            3.29200980e-02, -1.84521024e-02,  1.02949776e+00,\n",
       "           -2.59341800e-03,  7.51860559e-03, -2.12531268e-02,\n",
       "           -7.43363639e-03,  5.69783616e-02,  8.11219136e-01,\n",
       "           -1.38745211e-02,  4.61279495e-02, -3.77813492e-02,\n",
       "           -2.77979443e-02,  1.93268863e-02,  7.60174235e-02,\n",
       "           -1.26665903e-02,  1.10497279e-03, -6.46045045e-02,\n",
       "            5.83297598e-02,  1.74549408e-02, -3.50735204e-02,\n",
       "           -1.50495796e-02, -2.01137214e-03, -3.83862609e-02,\n",
       "            1.43925354e-02,  8.35495628e-02, -3.64073702e-02,\n",
       "           -8.68493264e-03,  2.13753126e-02, -1.43905502e-02,\n",
       "           -4.99924113e-02,  9.65389120e-01,  3.01363131e-02,\n",
       "            1.34660617e-02],\n",
       "          [ 6.65682158e-01,  1.08381126e-01,  9.96847204e-01,\n",
       "            2.93543924e-01,  4.07448916e-01, -3.12204588e-02,\n",
       "            9.67157922e-02,  7.06999668e-01, -2.65997053e-02,\n",
       "            6.93539984e-01,  7.83709388e-01,  1.00797934e+00,\n",
       "            9.90232997e-01,  1.66551861e-01,  1.00665822e+00,\n",
       "            6.91394917e-02,  1.38459883e-02,  8.71077434e-01,\n",
       "           -1.80059973e-02,  1.29958979e-01, -1.94553191e-03,\n",
       "            9.82047912e-01,  5.26175918e-01,  8.79735065e-02,\n",
       "            2.00786933e-02,  3.14340742e-02,  7.71382484e-02,\n",
       "            1.29412798e-01, -3.31060719e-02, -3.66203134e-02,\n",
       "           -6.77519747e-03,  5.11927937e-02,  1.21046830e-01,\n",
       "            3.44040524e-02, -2.90750055e-02, -2.15654042e-02,\n",
       "            3.92421039e-03, -7.90470663e-03,  9.87631718e-01,\n",
       "            5.94316997e-03, -2.34927424e-02,  9.66392067e-01,\n",
       "            4.16845356e-02, -2.76614900e-02, -3.50284471e-02,\n",
       "            7.68819994e-02,  2.64426735e-02,  7.59840824e-01,\n",
       "           -3.35173583e-02,  2.09862133e-02, -1.27157824e-02,\n",
       "           -1.95956460e-02, -3.00259249e-02,  3.91023168e-02,\n",
       "           -1.56075048e-02,  2.12211824e-02,  6.69698528e-03,\n",
       "           -1.90389059e-02,  1.21697416e-02,  6.14885856e-02,\n",
       "            1.91428920e-02,  1.30931238e-02, -1.23567001e-02,\n",
       "            9.98108473e-01,  5.66530246e-02,  7.31537519e-03,\n",
       "            1.92709493e-02, -3.93300744e-02,  3.74817840e-02,\n",
       "           -6.12790015e-03, -9.99165306e-04,  2.42813370e-02,\n",
       "            6.47656900e-03],\n",
       "          [ 5.95063251e-01,  1.06336460e-01,  9.48874196e-01,\n",
       "            2.43646693e-01,  4.42387953e-01,  4.70926729e-02,\n",
       "           -2.01050501e-02,  5.12133210e-01,  1.81404215e-02,\n",
       "            4.54778267e-01,  8.37978193e-01,  8.45883711e-01,\n",
       "            7.14680069e-01,  1.25222506e-01,  9.64139506e-01,\n",
       "           -2.62132580e-02, -1.17769848e-03,  1.00789185e+00,\n",
       "           -3.98843435e-02,  2.32377106e-02,  8.77392917e-02,\n",
       "            8.78131820e-01,  7.96537808e-01,  5.17828567e-02,\n",
       "            1.08813293e-02,  9.72462232e-03,  1.17334123e-02,\n",
       "            3.37550660e-01, -3.40371735e-03,  1.12398554e-01,\n",
       "            3.36436517e-02,  1.15135692e-02,  8.23477351e-03,\n",
       "            1.06282269e-01,  2.97023976e-02,  2.55606862e-02,\n",
       "            2.66857553e-02, -2.38101300e-02,  8.81191321e-01,\n",
       "            1.64527879e-01, -1.38129100e-02,  1.01982972e+00,\n",
       "            2.85142279e-03,  1.54057088e-02,  3.00044034e-02,\n",
       "            1.13077464e-01, -2.31341624e-02,  3.62860308e-01,\n",
       "            2.65926824e-02, -1.17869424e-02,  2.20823745e-01,\n",
       "           -8.73806412e-04,  1.52454327e-02,  7.99306345e-02,\n",
       "            6.67847318e-02,  1.18172363e-02, -2.29509484e-02,\n",
       "           -7.16839366e-03, -2.52967473e-02,  1.06467780e-01,\n",
       "           -3.89513269e-03, -7.56936548e-02, -5.01204993e-02,\n",
       "            3.93190068e-02, -1.80576657e-02, -1.78938781e-02,\n",
       "            1.01727949e+00, -3.17886936e-02, -1.23953181e-02,\n",
       "            4.86508418e-02, -2.47772954e-02, -2.76869781e-02,\n",
       "           -4.05514816e-02],\n",
       "          [ 7.48936560e-01,  7.51217555e-02,  9.70183960e-01,\n",
       "            2.39526146e-01,  3.64866652e-01, -2.77894739e-02,\n",
       "           -5.25243746e-02,  2.84244848e-01,  9.64930389e-02,\n",
       "            3.31612564e-01,  9.83693757e-01, -2.30308253e-02,\n",
       "            8.89110720e-01,  8.79632525e-03,  1.04412423e+00,\n",
       "            9.94668612e-01,  5.46182248e-02, -2.33019309e-02,\n",
       "           -8.31308659e-03,  7.57506094e-02,  9.79182124e-01,\n",
       "            3.95638816e-02,  5.91170592e-01, -3.57388671e-03,\n",
       "           -7.01139030e-03, -1.91613379e-03,  5.93560665e-03,\n",
       "            4.48504806e-02,  5.65695482e-02,  8.87108004e-02,\n",
       "           -5.49637610e-03, -2.95482729e-02,  1.35046187e-01,\n",
       "           -8.95261134e-03, -1.42114895e-02,  1.72702863e-02,\n",
       "            1.99061955e-02, -4.06494019e-03,  2.50231259e-02,\n",
       "            1.04289145e+00, -4.02355425e-02,  1.04929003e+00,\n",
       "            8.44519482e-01, -4.08985456e-02, -1.01834286e-02,\n",
       "            2.36210980e-03, -3.66059981e-02,  1.39855616e-02,\n",
       "            2.65267182e-02,  1.23809253e-01,  2.11731511e-02,\n",
       "           -2.33406698e-03, -3.20058475e-02,  2.93447773e-02,\n",
       "           -4.40267342e-02,  1.58522662e-02,  9.57453012e-03,\n",
       "           -3.10468274e-02,  3.03417025e-02, -5.12251548e-03,\n",
       "            2.17160185e-02, -1.43465402e-02,  2.74009692e-02,\n",
       "            1.82945628e-02, -4.16191790e-02, -1.27790223e-02,\n",
       "            9.30031027e-02, -1.48772703e-02,  1.72209078e-02,\n",
       "            1.47844382e-02,  8.56762347e-01, -6.46016597e-02,\n",
       "            4.08947631e-02],\n",
       "          [ 5.43717634e-01,  1.08376867e-01,  9.72280485e-01,\n",
       "            4.55098342e-01,  9.56506465e-02,  6.79173742e-02,\n",
       "            2.31871107e-03,  2.81073955e-01,  4.95955726e-01,\n",
       "            3.01517014e-01,  7.29465609e-01,  6.68255524e-01,\n",
       "            1.00569953e+00,  7.87968025e-02,  9.66807839e-01,\n",
       "            6.12695388e-02,  6.63597013e-02,  1.00358657e+00,\n",
       "           -1.02941904e-02,  1.47251835e-01,  1.02741386e+00,\n",
       "            1.81627583e-02,  9.75028767e-01,  1.51301345e-02,\n",
       "            8.16722540e-03, -2.39480025e-03, -1.41687075e-02,\n",
       "           -3.42979460e-02,  1.20464959e-02,  3.16606787e-02,\n",
       "            3.32055425e-01, -4.12877475e-02,  6.73300819e-01,\n",
       "           -3.10603752e-02,  1.14754208e-02, -1.47111867e-02,\n",
       "            2.92029186e-02, -2.30245487e-02,  1.00209349e+00,\n",
       "            2.90590073e-02,  1.13999353e-02,  1.02330779e+00,\n",
       "           -1.31368897e-02,  2.50568793e-02, -1.46214621e-02,\n",
       "            4.80967394e-02,  1.48253872e-02,  8.81795153e-04,\n",
       "            2.00373872e-02, -3.15839555e-03,  7.07048445e-04,\n",
       "            1.02934760e-03, -2.50001985e-02,  9.67177559e-01,\n",
       "            6.45990075e-02,  3.13261607e-02,  6.29246477e-02,\n",
       "           -6.50608209e-02,  1.94662376e-02, -5.05993279e-02,\n",
       "            2.37713505e-02, -1.37849422e-02,  3.50523583e-02,\n",
       "            3.93367352e-02, -1.95948762e-02,  1.02858694e-02,\n",
       "            2.39473819e-02, -1.08068617e-02,  4.16376881e-02,\n",
       "           -2.37360267e-03,  1.02873732e+00, -2.66329292e-02,\n",
       "            5.19907699e-02],\n",
       "          [ 6.39756287e-01,  1.23293084e-01,  1.01303360e+00,\n",
       "            6.93916395e-01,  3.79737416e-01,  1.99082774e-01,\n",
       "            9.42329979e-03,  9.44373854e-01,  1.10227537e-02,\n",
       "            9.65534366e-01,  9.75762774e-01,  9.85347462e-01,\n",
       "            6.12437902e-01,  5.04303211e-03,  1.00328635e+00,\n",
       "           -1.34732007e-02,  1.70223365e-02,  9.75393486e-01,\n",
       "            1.04286731e-02,  1.41585403e-01,  1.93126568e-01,\n",
       "            8.20640243e-01,  3.42356506e-01,  2.08353102e-02,\n",
       "           -1.04406492e-02,  3.77344012e-02, -5.14111971e-03,\n",
       "            7.64149526e-02,  2.05518601e-02, -4.00680237e-02,\n",
       "            3.23712469e-03,  4.15360557e-03,  1.57897584e-01,\n",
       "            8.29869995e-01, -8.88292534e-03, -5.01294073e-02,\n",
       "            1.72035478e-02, -7.31363543e-03,  1.91842913e-01,\n",
       "            8.15829112e-01,  8.47008217e-03,  9.89918668e-01,\n",
       "            1.37964964e-02, -5.03274388e-02,  2.64635419e-02,\n",
       "           -9.79414801e-03,  9.28471166e-03, -1.44184685e-02,\n",
       "           -6.41941455e-02, -1.63288130e-03, -4.85187163e-02,\n",
       "           -2.61813345e-02,  4.16757094e-02,  9.52793692e-01,\n",
       "           -7.73850853e-04,  2.56964923e-02, -1.49618130e-03,\n",
       "            1.01958304e-03, -3.76895743e-02, -1.42145290e-02,\n",
       "           -3.12394953e-03, -2.98208725e-03,  2.94156309e-03,\n",
       "            5.77933879e-01,  1.32889144e-03,  9.64432821e-03,\n",
       "            3.35863048e-02,  5.61895501e-02, -1.46633670e-02,\n",
       "            3.21535523e-02,  4.41503022e-01, -1.54470073e-02,\n",
       "            6.35443535e-02]]),\n",
       "   'parent': {...},\n",
       "   'model': LogisticRegression(penalty='l1', solver='saga')},\n",
       "  'parent': {...}},\n",
       " 'right': {'feature_index': 5,\n",
       "  'threshold': 0.02076292546505908,\n",
       "  'left': {'leaf': True,\n",
       "   'data_indices': array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "           13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "           26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "           39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "           52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "           65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "           78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "           91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "          104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "          117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "          130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "          143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "          156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "          169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "          182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "          195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "          208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "          221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
       "          234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "          247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
       "          260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
       "          273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
       "          286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
       "          299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
       "          312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n",
       "          325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n",
       "          338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
       "          351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
       "          364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376,\n",
       "          377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389,\n",
       "          390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402,\n",
       "          403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415,\n",
       "          416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428,\n",
       "          429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441,\n",
       "          442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454,\n",
       "          455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,\n",
       "          468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480,\n",
       "          481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493,\n",
       "          494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506,\n",
       "          507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519,\n",
       "          520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532,\n",
       "          533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545,\n",
       "          546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558,\n",
       "          559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571,\n",
       "          572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584,\n",
       "          585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597,\n",
       "          598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610,\n",
       "          611, 612, 613, 614, 615, 616, 617]),\n",
       "   'accuracy': 0.8349514563106796,\n",
       "   'location_matrix': array([[ 4.70928176e-01,  4.39822760e-01,  9.53272257e-01,\n",
       "            5.82535740e-01,  4.37961703e-01, -9.34131996e-03,\n",
       "            2.68664577e-02,  1.01519438e+00,  2.11196402e-02,\n",
       "            9.60445620e-01,  5.22449697e-01,  1.00106956e+00,\n",
       "            3.34218143e-01,  2.85715279e-01,  9.67271753e-01,\n",
       "            1.81224591e-02,  2.07322509e-02,  9.66535412e-01,\n",
       "            9.85842477e-03,  5.69676499e-02, -1.18806301e-02,\n",
       "            9.34588638e-01,  9.27138067e-01,  2.52468169e-01,\n",
       "           -2.44704326e-02,  8.71124365e-02, -3.74135172e-02,\n",
       "            7.75470403e-02,  8.82775887e-04,  2.98393143e-03,\n",
       "            1.54131334e-03,  3.11411213e-01, -1.79350636e-02,\n",
       "            3.48961798e-02, -7.80747336e-02, -5.02902706e-02,\n",
       "            6.33562271e-01,  3.31039240e-01, -2.66396457e-03,\n",
       "           -9.34862006e-03, -4.97382107e-04,  1.01734314e+00,\n",
       "           -2.31935808e-02, -1.72793679e-02,  4.39900303e-02,\n",
       "           -1.24688322e-02, -3.48182885e-02, -3.98694120e-02,\n",
       "           -3.14561029e-02, -3.84410221e-02,  6.61308247e-01,\n",
       "            2.35184704e-02,  1.60256340e-02,  3.61716734e-02,\n",
       "           -3.69963159e-02,  4.22011486e-02,  2.90440150e-02,\n",
       "            4.59053525e-02,  3.45795394e-01, -3.50318520e-02,\n",
       "            2.46769907e-03, -4.26178863e-02,  1.17691100e-02,\n",
       "            1.07121479e+00,  4.77317434e-02, -4.02948584e-03,\n",
       "            3.50676730e-02,  1.32701999e-03, -4.30558525e-02,\n",
       "            5.25036196e-02, -3.69679155e-02, -2.28079734e-02,\n",
       "           -2.72813048e-02],\n",
       "          [ 6.73216709e-01,  1.78737629e-01,  9.71381957e-01,\n",
       "            4.90584140e-01,  1.77504512e-01, -6.20201057e-04,\n",
       "            2.26468627e-01,  5.46972795e-01, -1.35637977e-02,\n",
       "            5.15474590e-01,  7.74060142e-01,  8.64917341e-01,\n",
       "            9.18560840e-01,  1.31140050e-01,  9.79937332e-01,\n",
       "            1.04683640e+00,  1.24921427e-02, -2.26512916e-02,\n",
       "            8.46025446e-03,  1.46374095e-01,  2.56235795e-01,\n",
       "            6.67370976e-01,  5.51678390e-01,  5.68801323e-02,\n",
       "           -2.12802980e-03, -2.33967027e-02,  2.14449414e-01,\n",
       "           -1.99395004e-02,  1.25538112e-01, -2.70547016e-02,\n",
       "           -4.78446438e-02, -1.08922168e-02, -9.45268644e-03,\n",
       "            1.24887797e-01, -6.79228681e-03, -1.68696449e-02,\n",
       "            4.55441287e-02,  1.00747181e-01,  7.48450435e-01,\n",
       "            9.10938230e-02,  1.68151628e-02,  9.96535341e-01,\n",
       "           -3.47761607e-02,  2.52419235e-02,  9.11890832e-03,\n",
       "            1.41010201e-02, -2.03314978e-03, -2.59901326e-02,\n",
       "            1.00448484e+00,  2.89675210e-02,  2.16370000e-02,\n",
       "            1.12080971e-02,  2.63055184e-02,  3.71689040e-02,\n",
       "            2.60641605e-02, -7.13164318e-03,  3.36037740e-02,\n",
       "           -1.03828794e-02, -2.43233413e-02, -9.85273860e-03,\n",
       "           -3.01476075e-02,  2.95791212e-02, -3.13031972e-03,\n",
       "            3.62938965e-02,  4.97107903e-03,  4.03924398e-02,\n",
       "            2.39201680e-01,  6.62984726e-01,  3.34768655e-02,\n",
       "           -6.49787054e-02,  8.33334708e-02, -1.95493928e-02,\n",
       "           -2.38761095e-02],\n",
       "          [ 5.57741870e-01,  4.39575080e-01,  1.00019927e+00,\n",
       "            4.69863247e-01,  2.81967850e-01,  2.07629255e-02,\n",
       "            1.13994474e-01,  8.92929264e-01,  1.65135546e-02,\n",
       "            8.67470087e-01,  4.85808290e-01,  9.82989125e-01,\n",
       "            9.64922557e-01,  3.44545470e-01,  9.46727388e-01,\n",
       "            1.73232128e-02,  3.37586858e-03,  1.02472607e+00,\n",
       "           -1.61036560e-02,  4.61254077e-02, -1.80770760e-02,\n",
       "            9.82634414e-01,  9.22598211e-01,  2.26931956e-01,\n",
       "            5.25065794e-02,  9.81291749e-02,  6.75170698e-04,\n",
       "            8.18891337e-03, -5.04428053e-02,  2.31612632e-02,\n",
       "            6.37334250e-02,  1.04976902e-03, -4.49633845e-02,\n",
       "            4.96783443e-02,  4.63509560e-02,  2.86911697e-02,\n",
       "            5.26211292e-01,  3.57765963e-01, -5.38867936e-03,\n",
       "            3.72532753e-02,  2.53491717e-02,  9.79659371e-01,\n",
       "           -1.16519744e-02,  3.32198503e-02,  3.94470029e-02,\n",
       "            4.51668667e-02, -6.00766409e-03,  7.17145361e-01,\n",
       "            4.39602993e-03,  2.00648561e-03, -2.26248475e-02,\n",
       "           -2.94945997e-02,  7.61337551e-03, -5.41802510e-03,\n",
       "            3.67219417e-02, -1.10348555e-02, -1.20127213e-02,\n",
       "            5.56221255e-04,  2.21069526e-01, -1.34597275e-02,\n",
       "           -3.73619619e-02,  3.41160665e-03,  3.19432774e-02,\n",
       "            6.24587446e-03, -2.99197291e-02, -9.55576592e-03,\n",
       "            6.89450641e-02,  5.88089837e-02, -1.37710898e-02,\n",
       "            6.45008065e-03,  8.37951134e-01,  1.75259582e-03,\n",
       "           -1.44653185e-02],\n",
       "          [ 5.46820144e-01,  4.42285371e-01,  1.00587972e+00,\n",
       "            1.51094010e-01,  3.44419176e-01,  1.86625330e-02,\n",
       "            1.06664948e-02,  8.53939403e-01,  2.52805114e-02,\n",
       "            8.14860805e-01,  4.91780723e-01,  9.55364961e-01,\n",
       "           -1.85500119e-02,  3.15722816e-01,  1.01423336e+00,\n",
       "            7.90114251e-03,  3.25279149e-02,  9.95946266e-01,\n",
       "           -3.48357311e-02,  9.55418315e-02,  1.17401270e-01,\n",
       "            8.90969360e-01,  6.92493164e-01,  3.37690316e-01,\n",
       "           -6.21128741e-03, -7.35583715e-02,  1.48022415e-01,\n",
       "           -2.82383374e-02, -4.20457871e-02,  3.23509868e-02,\n",
       "            5.47565494e-03,  3.49434358e-02,  1.04318119e-01,\n",
       "           -2.00038752e-02,  1.94278778e-01,  2.72123631e-02,\n",
       "            9.96961446e-02,  4.56903931e-01,  6.96633124e-03,\n",
       "            1.01704390e-01, -3.23561010e-02,  9.96449827e-01,\n",
       "           -7.24151224e-02, -3.52779383e-02, -5.85099696e-04,\n",
       "           -7.41829681e-03, -4.10838203e-03,  7.40038893e-03,\n",
       "           -7.90530303e-03,  6.81450073e-02,  1.00015355e+00,\n",
       "           -6.24188524e-03, -2.64374306e-03,  2.12367791e-03,\n",
       "           -1.24778924e-02, -3.29151276e-03,  1.18442155e-02,\n",
       "           -3.84782433e-02,  1.64640492e-02, -2.73980165e-02,\n",
       "            3.98553152e-02,  3.22366886e-02,  3.22645765e-02,\n",
       "           -2.61058174e-02,  2.02534340e-02,  1.36057798e-02,\n",
       "            2.01277076e-02,  9.92815395e-03,  1.13271878e-02,\n",
       "           -6.95111791e-02,  9.83292290e-01,  1.07076652e-02,\n",
       "            2.68236015e-02]]),\n",
       "   'parent': {...},\n",
       "   'model': LogisticRegression(penalty='l1', solver='saga')},\n",
       "  'right': {'leaf': True,\n",
       "   'data_indices': array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "           13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "           26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "           39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "           52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "           65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "           78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "           91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "          104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "          117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "          130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "          143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "          156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "          169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "          182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "          195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "          208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "          221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
       "          234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "          247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
       "          260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
       "          273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
       "          286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
       "          299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
       "          312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n",
       "          325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n",
       "          338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
       "          351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
       "          364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376,\n",
       "          377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389,\n",
       "          390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402,\n",
       "          403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415,\n",
       "          416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428,\n",
       "          429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441,\n",
       "          442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454,\n",
       "          455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,\n",
       "          468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480,\n",
       "          481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493,\n",
       "          494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506,\n",
       "          507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519,\n",
       "          520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532,\n",
       "          533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545,\n",
       "          546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558,\n",
       "          559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571,\n",
       "          572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584,\n",
       "          585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597,\n",
       "          598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610,\n",
       "          611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623,\n",
       "          624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636,\n",
       "          637, 638, 639, 640, 641, 642, 643, 644, 645]),\n",
       "   'accuracy': 0.8421052631578947,\n",
       "   'location_matrix': array([[ 4.80231751e-01,  2.18474392e-01,  9.32513462e-01,\n",
       "            2.03066414e-01,  3.40123881e-01,  2.59733097e-01,\n",
       "           -3.55661627e-03,  7.94440357e-01,  5.08903722e-02,\n",
       "            7.94748220e-01,  6.93407167e-01,  9.66777335e-01,\n",
       "            1.00002194e+00,  2.33281339e-01,  9.95167448e-01,\n",
       "           -3.68520919e-02,  8.47071791e-05,  9.96648369e-01,\n",
       "            8.07774168e-02,  7.44009481e-02, -1.11190084e-02,\n",
       "            1.00580154e+00,  6.04454405e-01,  1.32085088e-01,\n",
       "            1.47414306e-02,  6.74955347e-02,  3.54060887e-02,\n",
       "            1.96213378e-01, -1.04204711e-02,  9.73622860e-02,\n",
       "            3.17449490e-02,  3.66636395e-02,  5.98639684e-02,\n",
       "           -4.99138731e-03,  3.32541254e-03, -5.83095472e-05,\n",
       "            8.64929056e-03, -6.75740027e-02,  1.35622083e-03,\n",
       "            9.37704551e-01,  5.45512811e-02,  1.00292278e+00,\n",
       "            1.35687786e-02,  1.00499676e+00, -8.95660898e-03,\n",
       "           -1.16974931e-02, -4.09130340e-03, -3.05227841e-02,\n",
       "            2.58161974e-04, -3.70122410e-02, -8.19760729e-04,\n",
       "           -1.34698893e-02, -2.27851145e-02,  2.11989538e-02,\n",
       "            8.98546710e-03,  1.26430616e-02,  1.98853294e-02,\n",
       "            2.87295593e-02, -7.20224762e-03,  2.52460456e-02,\n",
       "            1.30961555e-02, -2.77313296e-02,  4.86147494e-02,\n",
       "            9.28616405e-01,  4.62999677e-03, -2.19982598e-03,\n",
       "            1.24165583e-02,  5.47036933e-02,  2.20952925e-02,\n",
       "           -1.60717446e-02, -1.82119937e-02,  5.27808149e-03,\n",
       "           -7.69002455e-03],\n",
       "          [ 5.48441928e-01,  1.87036959e-01,  1.00490238e+00,\n",
       "            5.21090108e-01,  4.01870404e-01,  6.37740958e-02,\n",
       "            5.16552312e-02,  4.54522328e-01,  7.21725609e-02,\n",
       "            4.50584179e-01,  7.53992583e-01,  7.29728517e-01,\n",
       "            1.02357543e+00,  1.45203048e-01,  7.83726025e-01,\n",
       "            1.00106006e+00,  1.25634793e-01,  1.85218128e-02,\n",
       "            1.34666867e-01,  1.30753692e-01,  1.00020429e-01,\n",
       "            9.39376757e-01,  6.81485256e-01,  5.30810935e-02,\n",
       "           -2.43729518e-02, -4.92114261e-03, -1.14808938e-02,\n",
       "           -3.13961138e-02, -1.00683104e-02,  1.85090901e-02,\n",
       "           -6.64174173e-02,  2.64389543e-02,  4.53042569e-01,\n",
       "           -1.31064289e-02,  3.97769944e-02, -1.02475665e-02,\n",
       "           -4.32664617e-03,  1.38921097e-01,  7.98745645e-01,\n",
       "            3.74303433e-02,  3.81114332e-03,  9.86084398e-01,\n",
       "            4.21948862e-03,  2.64204112e-02, -2.66846046e-02,\n",
       "           -3.07396121e-02,  1.56200697e-02,  5.15458807e-03,\n",
       "            9.29916328e-01, -1.49223916e-02, -4.04027716e-02,\n",
       "           -1.65440737e-02, -3.20172179e-02, -1.30547131e-02,\n",
       "           -7.46296244e-03,  3.73848289e-02, -2.41765598e-02,\n",
       "            2.95412005e-02,  6.40753031e-04,  9.03114555e-02,\n",
       "            2.78595599e-02, -4.68525468e-02,  1.33221720e-03,\n",
       "            8.30829415e-01, -9.26971928e-04, -1.12409310e-02,\n",
       "           -1.37708475e-02,  1.45113133e-02,  7.61989252e-02,\n",
       "            1.05051904e-01,  1.57746677e-02, -3.29666348e-02,\n",
       "            1.17087579e-02],\n",
       "          [ 4.88528660e-01,  2.25545567e-01,  8.69504157e-01,\n",
       "            2.14872437e-01,  3.25299559e-01,  1.79522578e-01,\n",
       "            1.52721857e-01,  6.45060108e-01,  2.42595389e-02,\n",
       "            6.09588330e-01,  7.56633755e-01,  1.01507698e+00,\n",
       "            1.02734391e+00,  2.01911523e-01,  8.58881602e-01,\n",
       "           -7.43950593e-04,  1.52354906e-01,  1.03630664e+00,\n",
       "            3.00666937e-02,  6.69974783e-02,  4.99854751e-02,\n",
       "            9.53960360e-01,  9.71802522e-01,  1.08866993e-01,\n",
       "           -2.48456873e-02,  2.69690319e-02, -4.41143822e-02,\n",
       "            1.82767695e-01,  2.68079207e-02,  5.43005180e-02,\n",
       "            2.51157725e-02,  5.22539002e-02,  1.22316736e-01,\n",
       "           -4.27350644e-02,  6.60657658e-02, -1.93455009e-02,\n",
       "            3.07465499e-02, -4.86109203e-03, -2.97391809e-02,\n",
       "            1.01299643e+00,  1.50688255e-02,  1.01070353e+00,\n",
       "            2.60862486e-02,  1.01682585e+00,  4.64596357e-03,\n",
       "           -9.11275363e-03,  3.49750806e-02, -6.45357089e-02,\n",
       "           -1.62046927e-02,  1.55778149e-02, -4.27675403e-02,\n",
       "           -2.31522924e-03,  4.27285271e-02, -1.54955607e-02,\n",
       "            1.50651904e-02,  6.00883951e-03,  2.18031405e-02,\n",
       "            3.21691155e-02,  1.75840982e-02, -2.04420018e-03,\n",
       "            2.78433110e-02, -8.86591851e-03, -1.81364113e-02,\n",
       "           -1.15634223e-02,  4.34943370e-02,  6.44628243e-02,\n",
       "            1.96434578e-01,  3.65071776e-03,  1.57536566e-01,\n",
       "           -3.76802392e-03,  6.27436048e-01, -1.11491742e-02,\n",
       "            2.29636820e-02],\n",
       "          [ 4.60732452e-01,  4.64745632e-01,  1.01136090e+00,\n",
       "            3.25904495e-01,  3.82784338e-01,  2.21372955e-02,\n",
       "            1.14864063e-01,  7.92342207e-01, -1.44288051e-02,\n",
       "            7.55640773e-01,  4.89874868e-01,  9.72846216e-01,\n",
       "            9.19274924e-01,  4.14186087e-01,  9.97134812e-01,\n",
       "            1.01308478e-02,  3.77824923e-02,  9.74370192e-01,\n",
       "            2.22958370e-03,  8.66374555e-02,  7.03529559e-03,\n",
       "            1.02146796e+00,  4.85996692e-01,  3.03032570e-01,\n",
       "           -1.21450144e-02,  1.43146524e-01, -7.10169156e-02,\n",
       "           -4.15737635e-02,  1.15533774e-02, -2.03695553e-02,\n",
       "           -4.37097428e-03,  2.53711935e-02, -3.83293537e-02,\n",
       "            5.60895587e-03,  8.36007530e-02,  1.29845310e-01,\n",
       "            2.93397274e-02,  5.94786601e-01,  2.16722707e-02,\n",
       "            3.85797331e-02, -1.08155187e-02,  9.73203787e-01,\n",
       "           -3.09394905e-02, -1.02886977e-02,  3.50910571e-03,\n",
       "            7.30992611e-02,  1.02480676e-02,  7.78605646e-01,\n",
       "           -5.41377222e-02,  1.12861029e-01,  2.04191020e-03,\n",
       "           -1.65598984e-02, -1.93653527e-02, -4.08759700e-03,\n",
       "           -3.88845935e-02,  1.61174083e-02, -6.64977655e-03,\n",
       "            3.56249887e-02,  1.06214625e-01, -2.84114432e-02,\n",
       "           -3.08041700e-02, -2.15418118e-02,  7.28509811e-03,\n",
       "            1.02751241e+00, -3.36954524e-02, -1.07917541e-03,\n",
       "            4.99474627e-02, -3.59119278e-02, -7.40491171e-04,\n",
       "            3.98900838e-02, -1.46732097e-02,  2.34268874e-02,\n",
       "           -3.18499818e-02],\n",
       "          [ 4.77313846e-01,  2.32479869e-01,  1.00347374e+00,\n",
       "            4.47168478e-01,  4.11402175e-01,  8.78386061e-02,\n",
       "            1.23008232e-02,  5.42479547e-01,  3.41972689e-02,\n",
       "            5.25117391e-01,  7.73207821e-01,  6.41418894e-01,\n",
       "            6.54142918e-01,  2.52854106e-01,  1.06770190e+00,\n",
       "            1.03395577e+00, -1.17438120e-02,  7.62883742e-03,\n",
       "            8.69892005e-02,  1.70075044e-01,  1.76677643e-02,\n",
       "            1.05359542e+00,  9.94496727e-01,  1.88273741e-01,\n",
       "            2.94676941e-02,  2.36598371e-01,  1.47360723e-02,\n",
       "            8.89289121e-02,  5.72349551e-02,  1.41173685e-02,\n",
       "           -2.92918479e-02, -2.13916703e-02, -8.10366064e-03,\n",
       "            4.15438656e-01,  3.53180655e-02,  9.76634632e-02,\n",
       "            8.73831485e-02,  8.90128670e-02,  3.75376151e-01,\n",
       "            3.10470347e-01,  1.35567352e-02,  9.75961966e-01,\n",
       "            2.08788660e-02,  1.03704272e-02,  3.33966380e-01,\n",
       "           -1.32624207e-02,  9.50648356e-02, -1.90559432e-02,\n",
       "            3.02458471e-01,  3.82905651e-02,  3.67711522e-02,\n",
       "            3.66925935e-02, -3.41733750e-02, -3.40418246e-02,\n",
       "           -1.40263960e-02,  8.37476073e-02,  2.62115447e-03,\n",
       "            4.18384342e-02,  3.21723396e-04, -2.27853573e-03,\n",
       "           -2.47153594e-02, -1.12748297e-03,  2.54340034e-02,\n",
       "            8.28567810e-03,  1.76582324e-02,  1.40615627e-02,\n",
       "           -2.55352398e-02, -4.17512903e-02, -4.02209421e-02,\n",
       "           -6.32437833e-03,  9.98853782e-01,  1.16453381e-02,\n",
       "           -2.82440281e-02],\n",
       "          [ 5.23765117e-01,  2.42082477e-01,  9.95832612e-01,\n",
       "            3.54705674e-01,  3.82117803e-01,  9.70800987e-02,\n",
       "            8.61004273e-02,  8.23761018e-01, -2.49360209e-02,\n",
       "            8.40824498e-01,  7.36698211e-01,  9.02073512e-01,\n",
       "           -2.71589415e-02,  1.41117021e-01,  9.74637653e-01,\n",
       "            5.36168586e-03,  4.65451398e-02,  9.97500782e-01,\n",
       "            2.60625833e-02,  8.38655496e-02, -2.37108690e-02,\n",
       "            9.84565502e-01,  3.34542028e-01,  9.96592544e-02,\n",
       "           -3.04441079e-02,  1.26240425e-02,  1.70295672e-01,\n",
       "            8.44042984e-02,  4.14809752e-02,  5.13021974e-03,\n",
       "            7.74610886e-02,  1.30991657e-01,  8.28156550e-02,\n",
       "            2.22202784e-02, -1.47798115e-02,  6.65901039e-02,\n",
       "           -3.04006426e-02, -1.25017457e-02,  9.24996018e-01,\n",
       "           -3.74294257e-03,  3.79454948e-03,  1.00855801e+00,\n",
       "            2.80897225e-02, -2.07796213e-02, -2.50994160e-02,\n",
       "            3.00280954e-02,  5.06880205e-03, -2.07872687e-02,\n",
       "            9.96632389e-03,  1.93710764e-02,  9.62091414e-01,\n",
       "           -1.01443567e-02,  5.69038473e-02,  8.73170860e-02,\n",
       "           -2.80631168e-02, -7.55630231e-03,  7.19519584e-02,\n",
       "           -4.83065168e-02, -2.40829392e-02,  3.10765041e-02,\n",
       "           -1.79011628e-02, -4.48019763e-03,  1.60161299e-02,\n",
       "            7.25885586e-01,  3.68313579e-02, -7.40216556e-03,\n",
       "           -3.07470166e-03, -6.20081805e-02, -1.86331197e-02,\n",
       "           -2.92731326e-02,  1.80626583e-01,  7.04051068e-02,\n",
       "           -6.21058858e-03]]),\n",
       "   'parent': {...},\n",
       "   'model': LogisticRegression(penalty='l1', solver='saga')},\n",
       "  'parent': {...}},\n",
       " 'parent': None}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.tree['right']['parent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "97959816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.67470176e-01,  1.83987479e-01,  7.68244069e-01,  7.41853130e-01,\n",
       "        3.46406810e-02,  2.17383519e-01,  1.32025607e-01,  6.89027554e-01,\n",
       "        2.64345428e-01,  6.99131843e-01,  6.53582280e-01,  1.10367420e+00,\n",
       "        3.29246949e-01,  3.63960555e-01, -1.36792300e-03, -1.12700211e-01,\n",
       "       -7.89019599e-02,  1.18726525e+00,  3.03781441e-01,  2.80657973e-01,\n",
       "       -4.86788034e-01,  1.31451269e+00,  3.81882805e-01,  1.13696532e-01,\n",
       "        1.84683456e-01, -2.04600655e-06, -7.06117362e-06,  1.45454255e+00,\n",
       "       -4.03092645e-06, -9.84199707e-07,  9.09283469e-02,  1.06172717e-05,\n",
       "        3.63623398e-01, -1.21642537e-05,  2.47462488e-06,  6.72347990e-06,\n",
       "        1.91303856e-05,  1.05107220e-05, -8.61568343e-06,  2.00001220e+00,\n",
       "       -8.87019427e-07,  2.00000438e+00,  1.17639480e-05,  1.81817181e+00,\n",
       "        6.41473471e-06,  1.23528347e-05, -1.42980055e-07, -1.42119802e-05,\n",
       "       -9.53273812e-07, -8.74276095e-06, -1.84386733e-06,  1.07917389e-05,\n",
       "       -1.06524037e-05, -1.34870914e-05,  1.45062523e-05,  1.62915848e-05,\n",
       "        9.09197647e-02,  7.18969169e-06, -1.15568293e-05,  9.09127475e-02,\n",
       "        1.63390389e-05,  5.42933990e-06, -1.65435422e-07, -6.53933275e-06,\n",
       "        5.42212909e-06, -6.03883762e-06, -1.00212464e-05,  1.02327018e-05,\n",
       "        1.81818143e-01,  1.72726986e+00,  1.68483138e-05, -1.04244947e-05,\n",
       "       -4.19755148e-06])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_data[best_k][25][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "26610498",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LogisticRegression' object has no attribute 'summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mright\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary\u001b[49m())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LogisticRegression' object has no attribute 'summary'"
     ]
    }
   ],
   "source": [
    "print(best_tree.tree['left']['right']['model'].summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c9b807a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 35 <= -2.8377564766031216e-07\n",
      "Left:\n",
      "  Feature 5 <= 0.0493415782166494\n",
      "  Left:\n",
      "    Leaf: Accuracy = 0.9250\n",
      "    Model: <statsmodels.discrete.discrete_model.L1BinaryResultsWrapper object at 0x14dcb15ba860>\n",
      "  Right:\n",
      "    Leaf: Accuracy = 0.9389\n",
      "    Model: <statsmodels.discrete.discrete_model.L1BinaryResultsWrapper object at 0x14dcb15b9570>\n",
      "Right:\n",
      "  Feature 61 <= -2.552137048309395e-07\n",
      "  Left:\n",
      "    Leaf: Accuracy = 0.8985\n",
      "    Model: <statsmodels.discrete.discrete_model.L1BinaryResultsWrapper object at 0x14dcb14c86a0>\n",
      "  Right:\n",
      "    Leaf: Accuracy = 0.8842\n",
      "    Model: <statsmodels.discrete.discrete_model.L1BinaryResultsWrapper object at 0x14dcb413d7e0>\n"
     ]
    }
   ],
   "source": [
    "best_tree.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "298d44a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier()\n",
      "GradientBoostingClassifier()\n",
      "MLPClassifier(alpha=1e-07, hidden_layer_sizes=(15, 2), random_state=1,\n",
      "              solver='lbfgs')\n",
      "Avg result: 0.8280398671096346\n",
      "Standard deviation: 0.018847021180888435\n",
      "   New test  New train  Depth\n",
      "0  0.803987   0.883661      1\n",
      "1  0.836545   0.921844      1\n",
      "2  0.839203   0.920470      1\n",
      "3  0.807973   0.933045      1\n",
      "4  0.852492   0.916056      1\n"
     ]
    }
   ],
   "source": [
    "# Mixture of teachers\n",
    "print(clfs[0])\n",
    "print(clfs[1])\n",
    "print(clfs[2])\n",
    "print(\"Avg result:\", np.mean(results))\n",
    "print(\"Standard deviation:\", np.sqrt(np.var(Results_SKCM['New test'])))\n",
    "print(Results_SKCM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3ae00da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(alpha=1e-07, hidden_layer_sizes=(15, 2), random_state=1,\n",
      "              solver='lbfgs')\n",
      "MLPClassifier(alpha=1e-07, hidden_layer_sizes=(15, 2), random_state=1,\n",
      "              solver='lbfgs')\n",
      "MLPClassifier(alpha=1e-07, hidden_layer_sizes=(15, 2), random_state=1,\n",
      "              solver='lbfgs')\n",
      "Avg result: 0.8176744186046512\n",
      "   New test  New train  Depth\n",
      "0  0.842525   0.884619      1\n",
      "1  0.801329   0.935626      1\n",
      "2  0.805316   0.919720      1\n",
      "3  0.837209   0.912975      3\n",
      "4  0.801993   0.939873      1\n"
     ]
    }
   ],
   "source": [
    "# MLP\n",
    "print(clfs[0])\n",
    "print(clfs[1])\n",
    "print(clfs[2])\n",
    "print(Results_SKCM)\n",
    "print(\"Avg result:\", np.mean(results))\n",
    "print(\"Standard deviation:\", np.sqrt(np.var(Results_SKCM['New test'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "05264bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01824807592706694"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = [0.842525, 0.801329, 0.805316, 0.837209, 0.801993]\n",
    "np.sqrt(np.var(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "106c3abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier()\n",
      "RandomForestClassifier()\n",
      "RandomForestClassifier()\n",
      "   New test  New train  Depth\n",
      "0  0.760133   0.829364      2\n",
      "1  0.753488   0.813749      2\n",
      "2  0.764784   0.811126      2\n",
      "3  0.811960   0.827199      1\n",
      "4  0.745515   0.834569      1\n",
      "Avg result: 0.7671760797342193\n",
      "Standard deviation: 0.023313076156373028\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "print(clfs[0])\n",
    "print(clfs[1])\n",
    "print(clfs[2])\n",
    "print(Results_SKCM)\n",
    "print(\"Avg result:\", np.mean(results))\n",
    "print(\"Standard deviation:\", np.sqrt(np.var(Results_SKCM['New test'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "625bfdbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier()\n",
      "GradientBoostingClassifier()\n",
      "GradientBoostingClassifier()\n",
      "   New test  New train  Depth\n",
      "0  0.697674   0.750208      3\n",
      "1  0.788040   0.852348      1\n",
      "2  0.778738   0.870836      2\n",
      "3  0.781395   0.852224      2\n",
      "4  0.806645   0.755871      3\n",
      "Avg result: 0.770498338870432\n",
      "Standard deviation: 0.037694042361776696\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting\n",
    "print(clfs[0])\n",
    "print(clfs[1])\n",
    "print(clfs[2])\n",
    "print(Results_SKCM)\n",
    "print(\"Avg result:\", np.mean(results))\n",
    "print(\"Standard deviation:\", np.sqrt(np.var(Results_SKCM['New test'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fd0d0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c81211f",
   "metadata": {},
   "source": [
    "# Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d89d3642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resubstitution_error(tree, R=None):\n",
    "    if R is None:\n",
    "        R = []\n",
    "\n",
    "    R.append(tree['R_node'])\n",
    "\n",
    "    if 'leaf' in tree:\n",
    "#         result = sum(R)\n",
    "#         print(R)\n",
    "        return tree['R_node']\n",
    "    else:\n",
    "        left_result = resubstitution_error(tree['left'], R)\n",
    "        right_result = resubstitution_error(tree['right'], R)\n",
    "    \n",
    "    return sum(R)\n",
    "\n",
    "def number_of_leaf_nodes(tree, number=None):\n",
    "    if number is None:\n",
    "        number = 0\n",
    "    \n",
    "    if 'leaf' in tree:\n",
    "        number += 1\n",
    "        \n",
    "    else:\n",
    "        left = number_of_leaf_nodes(tree['left'], number)\n",
    "        right = number_of_leaf_nodes(tree['right'], number)\n",
    "        return left + right\n",
    "\n",
    "    return number\n",
    "\n",
    "def subtrees(tree):\n",
    "    if 'leaf' in tree:\n",
    "        return [tree]\n",
    "        \n",
    "    left_subtrees = [tree['left']] + subtrees(tree['left'])\n",
    "    right_subtrees = [tree['right']] + subtrees(tree['right'])\n",
    "\n",
    "    return left_subtrees + right_subtrees\n",
    "\n",
    "def weakest_link_pruning(tree, previous_alpha=0):\n",
    "#     print('previous alpha', type(previous_alpha))\n",
    "    all_subtrees = subtrees(tree)\n",
    "    unique_models = set()\n",
    "    unique_trees = [t for t in all_subtrees if not (t['model'] in unique_models or unique_models.add(t['model']))]\n",
    "#     print(len(unique_trees))\n",
    "    h = {}\n",
    "    unique_subtrees = {}\n",
    "    for t in unique_trees:\n",
    "        if 'leaf' not in t:\n",
    "            unique_subtrees[t['model']] = t\n",
    "            h[t['model']] = (resubstitution_error(tree) - resubstitution_error(t))/number_of_leaf_nodes(t) - 1\n",
    "    \n",
    "    selected_h = []\n",
    "    for model, value in h.items():\n",
    "#         print('model', model, 'value', value)\n",
    "        if value <= previous_alpha:\n",
    "            selected_h.append((model, value))\n",
    "    selected_h.append(min(h.items(), key=lambda x: x[1] if x[1] > previous_alpha else float('inf')))\n",
    "    \n",
    "#     print(\"Selected h:\", selected_h)\n",
    "    \n",
    "    if not selected_h:\n",
    "        return None\n",
    "    else:\n",
    "        return selected_h, {model: unique_subtrees[model] for model, _ in selected_h}\n",
    "    \n",
    "def cut_off_subtree(tree, target_model):\n",
    "    # Base case: If the current node is None, return None\n",
    "    if tree is None:\n",
    "        return None\n",
    "\n",
    "    # Check if the current node has the target model\n",
    "    if tree['model'] == target_model:\n",
    "#         print(tree['model'])\n",
    "        tree['left'] = None\n",
    "        tree['right'] = None\n",
    "        tree['leaf'] = True\n",
    "        return tree  # Cut off the current node\n",
    "    else:\n",
    "        # Recursively check the left subtree if it exists\n",
    "        if 'left' in tree:\n",
    "#             print('left step')\n",
    "            tree['left'] = cut_off_subtree(tree['left'], target_model)\n",
    "\n",
    "        # Recursively check the right subtree if it exists\n",
    "        if 'right' in tree:\n",
    "#             print('right step')\n",
    "            tree['right'] = cut_off_subtree(tree['right'], target_model)\n",
    "\n",
    "    return tree\n",
    "\n",
    "def alpha_sequence(tree):\n",
    "    alpha = [0]\n",
    "    tree_sequence = []\n",
    "#     for i in range(2):\n",
    "    while number_of_leaf_nodes(tree.tree) > 2:\n",
    "#         print('alpha', alpha[-1])\n",
    "        model_to_cut = weakest_link_pruning(tree.tree, alpha[-1])[0]\n",
    "        print(model_to_cut)\n",
    "        print(model_to_cut[-1][1])\n",
    "#         print(model_to_cut)\n",
    "        for one_model_to_cut in model_to_cut:\n",
    "            if one_model_to_cut is not None:\n",
    "                cut_off_subtree(tree.tree, one_model_to_cut[0])\n",
    "    #             print(\"\\n Test fold result\", roc_auc_score(y_test, pd.DataFrame(tree.predict(np.array(X_test), np.array(y_test)))))\n",
    "    #             print(\"\\n Test result\", roc_auc_score(y, pd.DataFrame(tree.predict(np.array(X), np.array(y)))))\n",
    "        tree_sequence.append(tree.copy())\n",
    "        alpha.append(model_to_cut[-1][1])\n",
    "    #         print(number_of_leaf_nodes(tree))\n",
    "    return alpha, tree_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4a60fb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf1 auc train: 1.0\n",
      "clf1 auc test: 0.6677807486631018\n",
      "clf2 auc train: 0.9935483870967742\n",
      "clf2 auc test: 0.7821357943309163\n",
      "clf3 auc train: 0.9936305732484076\n",
      "clf3 auc test: 0.7754010695187166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x does not have column names.  The check that columns are provided in the same order when training and predicting will be skipped\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 1.0\n",
      "Test 0.7986842105263158\n",
      "k: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best k: 100, Best AUC score: 0.8806451612903226\n",
      "AUC: 0.8935483870967742\n",
      "k: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Depth 9\n",
      "Iteration limit reached    (Exit mode 9)\n",
      "            Current function value: 0.3457309165014807\n",
      "            Iterations: 1000\n",
      "            Function evaluations: 1000\n",
      "            Gradient evaluations: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 32 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.24801415681363692\n",
      "            Iterations: 824\n",
      "            Function evaluations: 825\n",
      "            Gradient evaluations: 824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 22 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.250528071041181\n",
      "            Iterations: 691\n",
      "            Function evaluations: 692\n",
      "            Gradient evaluations: 691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 7 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.20482910329396403\n",
      "            Iterations: 609\n",
      "            Function evaluations: 609\n",
      "            Gradient evaluations: 609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.0428317108200096\n",
      "            Iterations: 571\n",
      "            Function evaluations: 572\n",
      "            Gradient evaluations: 571\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.04606490761046171\n",
      "            Iterations: 547\n",
      "            Function evaluations: 548\n",
      "            Gradient evaluations: 547\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2521694924855512\n",
      "            Iterations: 538\n",
      "            Function evaluations: 539\n",
      "            Gradient evaluations: 538\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.21813123083926755\n",
      "            Iterations: 487\n",
      "            Function evaluations: 487\n",
      "            Gradient evaluations: 487\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1746687302776342\n",
      "            Iterations: 587\n",
      "            Function evaluations: 587\n",
      "            Gradient evaluations: 587\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.14026471265043516\n",
      "            Iterations: 97\n",
      "            Function evaluations: 97\n",
      "            Gradient evaluations: 97\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.17407601492140334\n",
      "            Iterations: 527\n",
      "            Function evaluations: 528\n",
      "            Gradient evaluations: 527\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1980256136368656\n",
      "            Iterations: 374\n",
      "            Function evaluations: 374\n",
      "            Gradient evaluations: 374\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1933774701163688\n",
      "            Iterations: 384\n",
      "            Function evaluations: 384\n",
      "            Gradient evaluations: 384\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1851825820205814\n",
      "            Iterations: 454\n",
      "            Function evaluations: 454\n",
      "            Gradient evaluations: 454\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.20546444488947088\n",
      "            Iterations: 548\n",
      "            Function evaluations: 548\n",
      "            Gradient evaluations: 548\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.21810903321925695\n",
      "            Iterations: 442\n",
      "            Function evaluations: 443\n",
      "            Gradient evaluations: 442\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.25637506942341437\n",
      "            Iterations: 503\n",
      "            Function evaluations: 503\n",
      "            Gradient evaluations: 503\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.21115612863702338\n",
      "            Iterations: 548\n",
      "            Function evaluations: 548\n",
      "            Gradient evaluations: 548\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18546458419076362\n",
      "            Iterations: 507\n",
      "            Function evaluations: 508\n",
      "            Gradient evaluations: 507\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18987949730988207\n",
      "            Iterations: 485\n",
      "            Function evaluations: 485\n",
      "            Gradient evaluations: 485\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2048571674539592\n",
      "            Iterations: 444\n",
      "            Function evaluations: 444\n",
      "            Gradient evaluations: 444\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.21038189466808543\n",
      "            Iterations: 494\n",
      "            Function evaluations: 495\n",
      "            Gradient evaluations: 494\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2535297241155146\n",
      "            Iterations: 551\n",
      "            Function evaluations: 552\n",
      "            Gradient evaluations: 551\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18790568267982782\n",
      "            Iterations: 577\n",
      "            Function evaluations: 577\n",
      "            Gradient evaluations: 577\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.15274325117488213\n",
      "            Iterations: 515\n",
      "            Function evaluations: 515\n",
      "            Gradient evaluations: 515\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1714580529927982\n",
      "            Iterations: 552\n",
      "            Function evaluations: 553\n",
      "            Gradient evaluations: 552\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.185313170201455\n",
      "            Iterations: 432\n",
      "            Function evaluations: 432\n",
      "            Gradient evaluations: 432\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.16140588504689463\n",
      "            Iterations: 590\n",
      "            Function evaluations: 590\n",
      "            Gradient evaluations: 590\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.26174594052079203\n",
      "            Iterations: 468\n",
      "            Function evaluations: 469\n",
      "            Gradient evaluations: 468\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.19891799946511185\n",
      "            Iterations: 548\n",
      "            Function evaluations: 548\n",
      "            Gradient evaluations: 548\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.15339402572415073\n",
      "            Iterations: 545\n",
      "            Function evaluations: 545\n",
      "            Gradient evaluations: 545\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.19686952442013486\n",
      "            Iterations: 568\n",
      "            Function evaluations: 568\n",
      "            Gradient evaluations: 568\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.17061006671988166\n",
      "            Iterations: 575\n",
      "            Function evaluations: 576\n",
      "            Gradient evaluations: 575\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1790781475228736\n",
      "            Iterations: 291\n",
      "            Function evaluations: 291\n",
      "            Gradient evaluations: 291\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.17615699213826502\n",
      "            Iterations: 531\n",
      "            Function evaluations: 532\n",
      "            Gradient evaluations: 531\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.23464110116824496\n",
      "            Iterations: 699\n",
      "            Function evaluations: 700\n",
      "            Gradient evaluations: 699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 5 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.19233809292118917\n",
      "            Iterations: 610\n",
      "            Function evaluations: 610\n",
      "            Gradient evaluations: 610\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.12620955339865572\n",
      "            Iterations: 577\n",
      "            Function evaluations: 578\n",
      "            Gradient evaluations: 577\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.12987122999411235\n",
      "            Iterations: 540\n",
      "            Function evaluations: 540\n",
      "            Gradient evaluations: 540\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.13104691034912674\n",
      "            Iterations: 532\n",
      "            Function evaluations: 532\n",
      "            Gradient evaluations: 532\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.14609075986003206\n",
      "            Iterations: 447\n",
      "            Function evaluations: 447\n",
      "            Gradient evaluations: 447\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.22740080244364252\n",
      "            Iterations: 46\n",
      "            Function evaluations: 46\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.12991354046024534\n",
      "            Iterations: 415\n",
      "            Function evaluations: 415\n",
      "            Gradient evaluations: 415\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18158032986219066\n",
      "            Iterations: 592\n",
      "            Function evaluations: 592\n",
      "            Gradient evaluations: 592\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1252211339851636\n",
      "            Iterations: 443\n",
      "            Function evaluations: 443\n",
      "            Gradient evaluations: 443\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.14365501663900682\n",
      "            Iterations: 378\n",
      "            Function evaluations: 378\n",
      "            Gradient evaluations: 378\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.12136239010501818\n",
      "            Iterations: 184\n",
      "            Function evaluations: 184\n",
      "            Gradient evaluations: 184\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.17816576308450094\n",
      "            Iterations: 597\n",
      "            Function evaluations: 598\n",
      "            Gradient evaluations: 597\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.14074855959089416\n",
      "            Iterations: 608\n",
      "            Function evaluations: 609\n",
      "            Gradient evaluations: 608\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.14906428151528772\n",
      "            Iterations: 607\n",
      "            Function evaluations: 607\n",
      "            Gradient evaluations: 607\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.16428973415962123\n",
      "            Iterations: 461\n",
      "            Function evaluations: 461\n",
      "            Gradient evaluations: 461\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.14526023641511998\n",
      "            Iterations: 573\n",
      "            Function evaluations: 573\n",
      "            Gradient evaluations: 573\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.23058017823834065\n",
      "            Iterations: 708\n",
      "            Function evaluations: 708\n",
      "            Gradient evaluations: 708\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.10759305552487824\n",
      "            Iterations: 690\n",
      "            Function evaluations: 691\n",
      "            Gradient evaluations: 690\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.08637595349222547\n",
      "            Iterations: 513\n",
      "            Function evaluations: 514\n",
      "            Gradient evaluations: 513\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.06449077457197896\n",
      "            Iterations: 240\n",
      "            Function evaluations: 240\n",
      "            Gradient evaluations: 240\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.09942783318489165\n",
      "            Iterations: 426\n",
      "            Function evaluations: 427\n",
      "            Gradient evaluations: 426\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1083860760091108\n",
      "            Iterations: 746\n",
      "            Function evaluations: 746\n",
      "            Gradient evaluations: 746\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1342833595967633\n",
      "            Iterations: 505\n",
      "            Function evaluations: 506\n",
      "            Gradient evaluations: 505\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.10772714242719542\n",
      "            Iterations: 704\n",
      "            Function evaluations: 704\n",
      "            Gradient evaluations: 704\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.24766265272667032\n",
      "            Iterations: 506\n",
      "            Function evaluations: 506\n",
      "            Gradient evaluations: 506\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2527163668431971\n",
      "            Iterations: 481\n",
      "            Function evaluations: 482\n",
      "            Gradient evaluations: 481\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2463568940372008\n",
      "            Iterations: 488\n",
      "            Function evaluations: 488\n",
      "            Gradient evaluations: 488\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.24245551181409147\n",
      "            Iterations: 420\n",
      "            Function evaluations: 420\n",
      "            Gradient evaluations: 420\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.21662049540151435\n",
      "            Iterations: 395\n",
      "            Function evaluations: 395\n",
      "            Gradient evaluations: 395\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1937703410092322\n",
      "            Iterations: 496\n",
      "            Function evaluations: 496\n",
      "            Gradient evaluations: 496\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.17237420892279343\n",
      "            Iterations: 547\n",
      "            Function evaluations: 548\n",
      "            Gradient evaluations: 547\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.37401111757169087\n",
      "            Iterations: 955\n",
      "            Function evaluations: 955\n",
      "            Gradient evaluations: 955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 18 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.35305322570721026\n",
      "            Iterations: 846\n",
      "            Function evaluations: 846\n",
      "            Gradient evaluations: 846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 9 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.34839163459119316\n",
      "            Iterations: 663\n",
      "            Function evaluations: 663\n",
      "            Gradient evaluations: 663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.31741544603015426\n",
      "            Iterations: 562\n",
      "            Function evaluations: 563\n",
      "            Gradient evaluations: 562\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.27892313584542777\n",
      "            Iterations: 532\n",
      "            Function evaluations: 532\n",
      "            Gradient evaluations: 532\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.24040612398446148\n",
      "            Iterations: 503\n",
      "            Function evaluations: 504\n",
      "            Gradient evaluations: 503\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.20519952754864995\n",
      "            Iterations: 511\n",
      "            Function evaluations: 511\n",
      "            Gradient evaluations: 511\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.21599033473353996\n",
      "            Iterations: 392\n",
      "            Function evaluations: 392\n",
      "            Gradient evaluations: 392\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.09395770156630678\n",
      "            Iterations: 88\n",
      "            Function evaluations: 88\n",
      "            Gradient evaluations: 88\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2653330038800978\n",
      "            Iterations: 312\n",
      "            Function evaluations: 313\n",
      "            Gradient evaluations: 312\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2289449166727858\n",
      "            Iterations: 419\n",
      "            Function evaluations: 419\n",
      "            Gradient evaluations: 419\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.20506743142789396\n",
      "            Iterations: 495\n",
      "            Function evaluations: 495\n",
      "            Gradient evaluations: 495\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1747947733899215\n",
      "            Iterations: 284\n",
      "            Function evaluations: 284\n",
      "            Gradient evaluations: 284\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.24084718315700787\n",
      "            Iterations: 447\n",
      "            Function evaluations: 447\n",
      "            Gradient evaluations: 447\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.22642560885724852\n",
      "            Iterations: 580\n",
      "            Function evaluations: 581\n",
      "            Gradient evaluations: 580\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1930047883534599\n",
      "            Iterations: 622\n",
      "            Function evaluations: 622\n",
      "            Gradient evaluations: 622\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.22914103476786868\n",
      "            Iterations: 505\n",
      "            Function evaluations: 505\n",
      "            Gradient evaluations: 505\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.21808306480583883\n",
      "            Iterations: 327\n",
      "            Function evaluations: 327\n",
      "            Gradient evaluations: 327\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18857309082454837\n",
      "            Iterations: 321\n",
      "            Function evaluations: 321\n",
      "            Gradient evaluations: 321\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.21803297979728709\n",
      "            Iterations: 405\n",
      "            Function evaluations: 405\n",
      "            Gradient evaluations: 405\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.11520500525857559\n",
      "            Iterations: 111\n",
      "            Function evaluations: 111\n",
      "            Gradient evaluations: 111\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2434674412777388\n",
      "            Iterations: 415\n",
      "            Function evaluations: 415\n",
      "            Gradient evaluations: 415\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2102980902928589\n",
      "            Iterations: 626\n",
      "            Function evaluations: 626\n",
      "            Gradient evaluations: 626\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.20804766555303533\n",
      "            Iterations: 486\n",
      "            Function evaluations: 486\n",
      "            Gradient evaluations: 486\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.19157732825095325\n",
      "            Iterations: 493\n",
      "            Function evaluations: 494\n",
      "            Gradient evaluations: 493\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.27877131871591565\n",
      "            Iterations: 474\n",
      "            Function evaluations: 475\n",
      "            Gradient evaluations: 474\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2476988581623814\n",
      "            Iterations: 541\n",
      "            Function evaluations: 542\n",
      "            Gradient evaluations: 541\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18892615427399312\n",
      "            Iterations: 474\n",
      "            Function evaluations: 475\n",
      "            Gradient evaluations: 474\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2618976684192927\n",
      "            Iterations: 512\n",
      "            Function evaluations: 512\n",
      "            Gradient evaluations: 512\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.27462960450328516\n",
      "            Iterations: 521\n",
      "            Function evaluations: 522\n",
      "            Gradient evaluations: 521\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.17743457413380076\n",
      "            Iterations: 575\n",
      "            Function evaluations: 576\n",
      "            Gradient evaluations: 575\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.21583067523947858\n",
      "            Iterations: 509\n",
      "            Function evaluations: 510\n",
      "            Gradient evaluations: 509\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.20281115406143146\n",
      "            Iterations: 457\n",
      "            Function evaluations: 457\n",
      "            Gradient evaluations: 457\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.21329277465953755\n",
      "            Iterations: 417\n",
      "            Function evaluations: 417\n",
      "            Gradient evaluations: 417\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.25932451542014023\n",
      "            Iterations: 406\n",
      "            Function evaluations: 406\n",
      "            Gradient evaluations: 406\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3403718621921351\n",
      "            Iterations: 509\n",
      "            Function evaluations: 510\n",
      "            Gradient evaluations: 509\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.17238021473845544\n",
      "            Iterations: 553\n",
      "            Function evaluations: 554\n",
      "            Gradient evaluations: 553\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3543721367582754\n",
      "            Iterations: 488\n",
      "            Function evaluations: 488\n",
      "            Gradient evaluations: 488\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.17830731923436804\n",
      "            Iterations: 600\n",
      "            Function evaluations: 600\n",
      "            Gradient evaluations: 600\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3456902344156028\n",
      "            Iterations: 469\n",
      "            Function evaluations: 469\n",
      "            Gradient evaluations: 469\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.31550436616197125\n",
      "            Iterations: 458\n",
      "            Function evaluations: 459\n",
      "            Gradient evaluations: 458\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.25465799736360134\n",
      "            Iterations: 484\n",
      "            Function evaluations: 485\n",
      "            Gradient evaluations: 484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.27579192001788244\n",
      "            Iterations: 466\n",
      "            Function evaluations: 467\n",
      "            Gradient evaluations: 466\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.29832892165434843\n",
      "            Iterations: 455\n",
      "            Function evaluations: 456\n",
      "            Gradient evaluations: 455\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3426063396842468\n",
      "            Iterations: 748\n",
      "            Function evaluations: 748\n",
      "            Gradient evaluations: 748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 7 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3432027299558256\n",
      "            Iterations: 550\n",
      "            Function evaluations: 550\n",
      "            Gradient evaluations: 550\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2626652367435371\n",
      "            Iterations: 533\n",
      "            Function evaluations: 534\n",
      "            Gradient evaluations: 533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18217433087754267\n",
      "            Iterations: 584\n",
      "            Function evaluations: 584\n",
      "            Gradient evaluations: 584\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18755945997911955\n",
      "            Iterations: 505\n",
      "            Function evaluations: 505\n",
      "            Gradient evaluations: 505\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.17446293461940937\n",
      "            Iterations: 457\n",
      "            Function evaluations: 457\n",
      "            Gradient evaluations: 457\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2275712139195692\n",
      "            Iterations: 268\n",
      "            Function evaluations: 269\n",
      "            Gradient evaluations: 268\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1960582607324482\n",
      "            Iterations: 451\n",
      "            Function evaluations: 451\n",
      "            Gradient evaluations: 451\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2305028225231628\n",
      "            Iterations: 153\n",
      "            Function evaluations: 153\n",
      "            Gradient evaluations: 153\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.21027408762924876\n",
      "            Iterations: 437\n",
      "            Function evaluations: 438\n",
      "            Gradient evaluations: 437\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.27670373659492703\n",
      "            Iterations: 452\n",
      "            Function evaluations: 453\n",
      "            Gradient evaluations: 452\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.20314175202048468\n",
      "            Iterations: 538\n",
      "            Function evaluations: 539\n",
      "            Gradient evaluations: 538\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1845082744553199\n",
      "            Iterations: 495\n",
      "            Function evaluations: 495\n",
      "            Gradient evaluations: 495\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2395935005171004\n",
      "            Iterations: 482\n",
      "            Function evaluations: 482\n",
      "            Gradient evaluations: 482\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2719704755087666\n",
      "            Iterations: 484\n",
      "            Function evaluations: 484\n",
      "            Gradient evaluations: 484\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.23243326156523436\n",
      "            Iterations: 502\n",
      "            Function evaluations: 503\n",
      "            Gradient evaluations: 502\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.23045998053394393\n",
      "            Iterations: 387\n",
      "            Function evaluations: 388\n",
      "            Gradient evaluations: 387\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2313121349020823\n",
      "            Iterations: 520\n",
      "            Function evaluations: 520\n",
      "            Gradient evaluations: 520\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2251051808589845\n",
      "            Iterations: 355\n",
      "            Function evaluations: 355\n",
      "            Gradient evaluations: 355\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.19789988664781205\n",
      "            Iterations: 625\n",
      "            Function evaluations: 626\n",
      "            Gradient evaluations: 625\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.13450666019575716\n",
      "            Iterations: 599\n",
      "            Function evaluations: 599\n",
      "            Gradient evaluations: 599\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.201365509507936\n",
      "            Iterations: 170\n",
      "            Function evaluations: 170\n",
      "            Gradient evaluations: 170\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.13750705901887472\n",
      "            Iterations: 585\n",
      "            Function evaluations: 585\n",
      "            Gradient evaluations: 585\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.21313110964151444\n",
      "            Iterations: 571\n",
      "            Function evaluations: 571\n",
      "            Gradient evaluations: 571\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.17373053982058048\n",
      "            Iterations: 448\n",
      "            Function evaluations: 449\n",
      "            Gradient evaluations: 448\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.19729612385083062\n",
      "            Iterations: 579\n",
      "            Function evaluations: 579\n",
      "            Gradient evaluations: 579\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.30924209968111777\n",
      "            Iterations: 636\n",
      "            Function evaluations: 636\n",
      "            Gradient evaluations: 636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1946973368829374\n",
      "            Iterations: 525\n",
      "            Function evaluations: 525\n",
      "            Gradient evaluations: 525\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1140358952146655\n",
      "            Iterations: 440\n",
      "            Function evaluations: 440\n",
      "            Gradient evaluations: 440\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2108222511441295\n",
      "            Iterations: 325\n",
      "            Function evaluations: 326\n",
      "            Gradient evaluations: 325\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2342615097926005\n",
      "            Iterations: 456\n",
      "            Function evaluations: 457\n",
      "            Gradient evaluations: 456\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2959812267301243\n",
      "            Iterations: 604\n",
      "            Function evaluations: 604\n",
      "            Gradient evaluations: 604\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2194045769415856\n",
      "            Iterations: 478\n",
      "            Function evaluations: 478\n",
      "            Gradient evaluations: 478\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18316421994508142\n",
      "            Iterations: 416\n",
      "            Function evaluations: 417\n",
      "            Gradient evaluations: 416\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.20105064312847368\n",
      "            Iterations: 529\n",
      "            Function evaluations: 529\n",
      "            Gradient evaluations: 529\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2572241686729775\n",
      "            Iterations: 542\n",
      "            Function evaluations: 542\n",
      "            Gradient evaluations: 542\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3608025866975373\n",
      "            Iterations: 697\n",
      "            Function evaluations: 697\n",
      "            Gradient evaluations: 697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 4 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.28339562931456574\n",
      "            Iterations: 512\n",
      "            Function evaluations: 512\n",
      "            Gradient evaluations: 512\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2841151992460961\n",
      "            Iterations: 429\n",
      "            Function evaluations: 429\n",
      "            Gradient evaluations: 429\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.29588495052202274\n",
      "            Iterations: 466\n",
      "            Function evaluations: 466\n",
      "            Gradient evaluations: 466\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.20489538822927933\n",
      "            Iterations: 458\n",
      "            Function evaluations: 458\n",
      "            Gradient evaluations: 458\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.23626122731904214\n",
      "            Iterations: 510\n",
      "            Function evaluations: 511\n",
      "            Gradient evaluations: 510\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.19113227843780897\n",
      "            Iterations: 483\n",
      "            Function evaluations: 483\n",
      "            Gradient evaluations: 483\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2434417974201895\n",
      "            Iterations: 507\n",
      "            Function evaluations: 507\n",
      "            Gradient evaluations: 507\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2399301617729891\n",
      "            Iterations: 364\n",
      "            Function evaluations: 364\n",
      "            Gradient evaluations: 364\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.23658261510215367\n",
      "            Iterations: 519\n",
      "            Function evaluations: 519\n",
      "            Gradient evaluations: 519\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.35898756313894015\n",
      "            Iterations: 687\n",
      "            Function evaluations: 687\n",
      "            Gradient evaluations: 687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 3 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.34771898704638116\n",
      "            Iterations: 537\n",
      "            Function evaluations: 537\n",
      "            Gradient evaluations: 537\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3167172756431896\n",
      "            Iterations: 468\n",
      "            Function evaluations: 469\n",
      "            Gradient evaluations: 468\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2367576468329265\n",
      "            Iterations: 506\n",
      "            Function evaluations: 506\n",
      "            Gradient evaluations: 506\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.24030649067939885\n",
      "            Iterations: 357\n",
      "            Function evaluations: 357\n",
      "            Gradient evaluations: 357\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.21228140633035192\n",
      "            Iterations: 181\n",
      "            Function evaluations: 181\n",
      "            Gradient evaluations: 181\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.25251296960180186\n",
      "            Iterations: 312\n",
      "            Function evaluations: 312\n",
      "            Gradient evaluations: 312\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.23922389690057372\n",
      "            Iterations: 490\n",
      "            Function evaluations: 490\n",
      "            Gradient evaluations: 490\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.22668920526586517\n",
      "            Iterations: 603\n",
      "            Function evaluations: 604\n",
      "            Gradient evaluations: 603\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.23396196775878467\n",
      "            Iterations: 311\n",
      "            Function evaluations: 311\n",
      "            Gradient evaluations: 311\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.21051762031963223\n",
      "            Iterations: 563\n",
      "            Function evaluations: 563\n",
      "            Gradient evaluations: 563\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2925284103923019\n",
      "            Iterations: 223\n",
      "            Function evaluations: 223\n",
      "            Gradient evaluations: 223\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.21736234775877367\n",
      "            Iterations: 532\n",
      "            Function evaluations: 532\n",
      "            Gradient evaluations: 532\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3153376710436725\n",
      "            Iterations: 487\n",
      "            Function evaluations: 487\n",
      "            Gradient evaluations: 487\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.21583278803076725\n",
      "            Iterations: 546\n",
      "            Function evaluations: 547\n",
      "            Gradient evaluations: 546\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.196415296707087\n",
      "            Iterations: 546\n",
      "            Function evaluations: 546\n",
      "            Gradient evaluations: 546\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.15196349828922265\n",
      "            Iterations: 477\n",
      "            Function evaluations: 478\n",
      "            Gradient evaluations: 477\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.19386575658195582\n",
      "            Iterations: 439\n",
      "            Function evaluations: 439\n",
      "            Gradient evaluations: 439\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.5532480193781357\n",
      "            Iterations: 67\n",
      "            Function evaluations: 67\n",
      "            Gradient evaluations: 67\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18927828960172202\n",
      "            Iterations: 414\n",
      "            Function evaluations: 414\n",
      "            Gradient evaluations: 414\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1730883910064826\n",
      "            Iterations: 491\n",
      "            Function evaluations: 491\n",
      "            Gradient evaluations: 491\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1786372052433471\n",
      "            Iterations: 353\n",
      "            Function evaluations: 353\n",
      "            Gradient evaluations: 353\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.19625192163177924\n",
      "            Iterations: 284\n",
      "            Function evaluations: 285\n",
      "            Gradient evaluations: 284\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.29317076535471753\n",
      "            Iterations: 479\n",
      "            Function evaluations: 479\n",
      "            Gradient evaluations: 479\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.19393228104919621\n",
      "            Iterations: 496\n",
      "            Function evaluations: 496\n",
      "            Gradient evaluations: 496\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1484705325207406\n",
      "            Iterations: 209\n",
      "            Function evaluations: 209\n",
      "            Gradient evaluations: 209\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1844954717876211\n",
      "            Iterations: 393\n",
      "            Function evaluations: 393\n",
      "            Gradient evaluations: 393\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2304955239444183\n",
      "            Iterations: 559\n",
      "            Function evaluations: 560\n",
      "            Gradient evaluations: 559\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3220043630565249\n",
      "            Iterations: 528\n",
      "            Function evaluations: 528\n",
      "            Gradient evaluations: 528\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.31105292071335056\n",
      "            Iterations: 463\n",
      "            Function evaluations: 463\n",
      "            Gradient evaluations: 463\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.22233709865001763\n",
      "            Iterations: 559\n",
      "            Function evaluations: 560\n",
      "            Gradient evaluations: 559\n",
      "[(<statsmodels.discrete.discrete_model.L1BinaryResultsWrapper object at 0x14e4f5eaa080>, 54.58064516129032)]\n",
      "54.58064516129032\n",
      "[(<statsmodels.discrete.discrete_model.L1BinaryResultsWrapper object at 0x14e4de084b20>, 76.45714285714286)]\n",
      "76.45714285714286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 7 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 9 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 9 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 6 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 8 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 8 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration limit reached    (Exit mode 9)\n",
      "            Current function value: 0.34621156545941034\n",
      "            Iterations: 1000\n",
      "            Function evaluations: 1000\n",
      "            Gradient evaluations: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 34 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.24968693504293457\n",
      "            Iterations: 816\n",
      "            Function evaluations: 816\n",
      "            Gradient evaluations: 816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 7 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.24895787810462996\n",
      "            Iterations: 667\n",
      "            Function evaluations: 667\n",
      "            Gradient evaluations: 667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2017134397151566\n",
      "            Iterations: 600\n",
      "            Function evaluations: 600\n",
      "            Gradient evaluations: 600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.03701144473942953\n",
      "            Iterations: 529\n",
      "            Function evaluations: 529\n",
      "            Gradient evaluations: 529\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.03997443898258962\n",
      "            Iterations: 513\n",
      "            Function evaluations: 514\n",
      "            Gradient evaluations: 513\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.24909973909182734\n",
      "            Iterations: 533\n",
      "            Function evaluations: 534\n",
      "            Gradient evaluations: 533\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.22300803327000623\n",
      "            Iterations: 481\n",
      "            Function evaluations: 481\n",
      "            Gradient evaluations: 481\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.17915204089023717\n",
      "            Iterations: 510\n",
      "            Function evaluations: 511\n",
      "            Gradient evaluations: 510\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.14026471265043516\n",
      "            Iterations: 97\n",
      "            Function evaluations: 97\n",
      "            Gradient evaluations: 97\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1792464686309822\n",
      "            Iterations: 486\n",
      "            Function evaluations: 486\n",
      "            Gradient evaluations: 486\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.19455601751412077\n",
      "            Iterations: 414\n",
      "            Function evaluations: 414\n",
      "            Gradient evaluations: 414\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1888481504248138\n",
      "            Iterations: 370\n",
      "            Function evaluations: 370\n",
      "            Gradient evaluations: 370\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.19069527735250838\n",
      "            Iterations: 421\n",
      "            Function evaluations: 421\n",
      "            Gradient evaluations: 421\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2068771693203775\n",
      "            Iterations: 541\n",
      "            Function evaluations: 542\n",
      "            Gradient evaluations: 541\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.20572755792350406\n",
      "            Iterations: 408\n",
      "            Function evaluations: 408\n",
      "            Gradient evaluations: 408\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.25332915070679296\n",
      "            Iterations: 448\n",
      "            Function evaluations: 449\n",
      "            Gradient evaluations: 448\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1935696266341701\n",
      "            Iterations: 547\n",
      "            Function evaluations: 548\n",
      "            Gradient evaluations: 547\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.17779186923122958\n",
      "            Iterations: 557\n",
      "            Function evaluations: 557\n",
      "            Gradient evaluations: 557\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2038766907391641\n",
      "            Iterations: 509\n",
      "            Function evaluations: 510\n",
      "            Gradient evaluations: 509\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18758222038714306\n",
      "            Iterations: 418\n",
      "            Function evaluations: 418\n",
      "            Gradient evaluations: 418\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2131375795785134\n",
      "            Iterations: 474\n",
      "            Function evaluations: 475\n",
      "            Gradient evaluations: 474\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2504596094445324\n",
      "            Iterations: 545\n",
      "            Function evaluations: 545\n",
      "            Gradient evaluations: 545\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1797077540751867\n",
      "            Iterations: 591\n",
      "            Function evaluations: 591\n",
      "            Gradient evaluations: 591\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.13691210922687863\n",
      "            Iterations: 539\n",
      "            Function evaluations: 539\n",
      "            Gradient evaluations: 539\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.16525686351116559\n",
      "            Iterations: 543\n",
      "            Function evaluations: 543\n",
      "            Gradient evaluations: 543\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.19675367848382241\n",
      "            Iterations: 419\n",
      "            Function evaluations: 420\n",
      "            Gradient evaluations: 419\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1546277000295137\n",
      "            Iterations: 520\n",
      "            Function evaluations: 521\n",
      "            Gradient evaluations: 520\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2574474424771398\n",
      "            Iterations: 464\n",
      "            Function evaluations: 464\n",
      "            Gradient evaluations: 464\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.19754841217162028\n",
      "            Iterations: 528\n",
      "            Function evaluations: 529\n",
      "            Gradient evaluations: 528\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1655146596387578\n",
      "            Iterations: 545\n",
      "            Function evaluations: 546\n",
      "            Gradient evaluations: 545\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1939537279376644\n",
      "            Iterations: 564\n",
      "            Function evaluations: 565\n",
      "            Gradient evaluations: 564\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1502309911578012\n",
      "            Iterations: 584\n",
      "            Function evaluations: 584\n",
      "            Gradient evaluations: 584\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.16839077969223856\n",
      "            Iterations: 314\n",
      "            Function evaluations: 314\n",
      "            Gradient evaluations: 314\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.15748503474240172\n",
      "            Iterations: 497\n",
      "            Function evaluations: 498\n",
      "            Gradient evaluations: 497\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.23881777852792954\n",
      "            Iterations: 673\n",
      "            Function evaluations: 673\n",
      "            Gradient evaluations: 673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 4 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.19601753351713289\n",
      "            Iterations: 639\n",
      "            Function evaluations: 639\n",
      "            Gradient evaluations: 639\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1313664966259432\n",
      "            Iterations: 518\n",
      "            Function evaluations: 518\n",
      "            Gradient evaluations: 518\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1397518902056798\n",
      "            Iterations: 501\n",
      "            Function evaluations: 502\n",
      "            Gradient evaluations: 501\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1412078419236037\n",
      "            Iterations: 515\n",
      "            Function evaluations: 515\n",
      "            Gradient evaluations: 515\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.15716752186926444\n",
      "            Iterations: 380\n",
      "            Function evaluations: 381\n",
      "            Gradient evaluations: 380\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3016595192294672\n",
      "            Iterations: 49\n",
      "            Function evaluations: 49\n",
      "            Gradient evaluations: 49\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.14113758063515486\n",
      "            Iterations: 340\n",
      "            Function evaluations: 341\n",
      "            Gradient evaluations: 340\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18689530147507843\n",
      "            Iterations: 594\n",
      "            Function evaluations: 594\n",
      "            Gradient evaluations: 594\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.13457829895306825\n",
      "            Iterations: 423\n",
      "            Function evaluations: 423\n",
      "            Gradient evaluations: 423\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.15904037714281508\n",
      "            Iterations: 405\n",
      "            Function evaluations: 406\n",
      "            Gradient evaluations: 405\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.12136239010501818\n",
      "            Iterations: 184\n",
      "            Function evaluations: 184\n",
      "            Gradient evaluations: 184\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1816169296565267\n",
      "            Iterations: 590\n",
      "            Function evaluations: 590\n",
      "            Gradient evaluations: 590\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.14637287962829723\n",
      "            Iterations: 567\n",
      "            Function evaluations: 568\n",
      "            Gradient evaluations: 567\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.15547663556910102\n",
      "            Iterations: 620\n",
      "            Function evaluations: 620\n",
      "            Gradient evaluations: 620\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1724924518305977\n",
      "            Iterations: 466\n",
      "            Function evaluations: 466\n",
      "            Gradient evaluations: 466\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1451410986109778\n",
      "            Iterations: 563\n",
      "            Function evaluations: 563\n",
      "            Gradient evaluations: 563\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.22436504733101234\n",
      "            Iterations: 676\n",
      "            Function evaluations: 677\n",
      "            Gradient evaluations: 676\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.10706169682419317\n",
      "            Iterations: 704\n",
      "            Function evaluations: 704\n",
      "            Gradient evaluations: 704\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.09008610950515417\n",
      "            Iterations: 512\n",
      "            Function evaluations: 512\n",
      "            Gradient evaluations: 512\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.06701038836905482\n",
      "            Iterations: 250\n",
      "            Function evaluations: 250\n",
      "            Gradient evaluations: 250\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.10613910578841507\n",
      "            Iterations: 408\n",
      "            Function evaluations: 408\n",
      "            Gradient evaluations: 408\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1129488596260877\n",
      "            Iterations: 768\n",
      "            Function evaluations: 769\n",
      "            Gradient evaluations: 768\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.14025880023013898\n",
      "            Iterations: 522\n",
      "            Function evaluations: 522\n",
      "            Gradient evaluations: 522\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1128889847771522\n",
      "            Iterations: 685\n",
      "            Function evaluations: 685\n",
      "            Gradient evaluations: 685\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.24628704707145144\n",
      "            Iterations: 510\n",
      "            Function evaluations: 510\n",
      "            Gradient evaluations: 510\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.24988565362835582\n",
      "            Iterations: 477\n",
      "            Function evaluations: 477\n",
      "            Gradient evaluations: 477\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.25230078559097197\n",
      "            Iterations: 511\n",
      "            Function evaluations: 511\n",
      "            Gradient evaluations: 511\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.25385573105309556\n",
      "            Iterations: 474\n",
      "            Function evaluations: 474\n",
      "            Gradient evaluations: 474\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.21800738503248018\n",
      "            Iterations: 422\n",
      "            Function evaluations: 422\n",
      "            Gradient evaluations: 422\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18358453880206838\n",
      "            Iterations: 480\n",
      "            Function evaluations: 480\n",
      "            Gradient evaluations: 480\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.16366844466250835\n",
      "            Iterations: 563\n",
      "            Function evaluations: 563\n",
      "            Gradient evaluations: 563\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3745518774470673\n",
      "            Iterations: 901\n",
      "            Function evaluations: 901\n",
      "            Gradient evaluations: 901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 12 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration limit reached    (Exit mode 9)\n",
      "            Current function value: 0.34386955023873733\n",
      "            Iterations: 1000\n",
      "            Function evaluations: 1000\n",
      "            Gradient evaluations: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 39 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.24772060294017417\n",
      "            Iterations: 822\n",
      "            Function evaluations: 823\n",
      "            Gradient evaluations: 822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 12 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2531882292738131\n",
      "            Iterations: 667\n",
      "            Function evaluations: 667\n",
      "            Gradient evaluations: 667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 5 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.21074322861529168\n",
      "            Iterations: 556\n",
      "            Function evaluations: 556\n",
      "            Gradient evaluations: 556\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.04457350140752317\n",
      "            Iterations: 595\n",
      "            Function evaluations: 595\n",
      "            Gradient evaluations: 595\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.04743622304388583\n",
      "            Iterations: 582\n",
      "            Function evaluations: 582\n",
      "            Gradient evaluations: 582\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.25250694561733245\n",
      "            Iterations: 538\n",
      "            Function evaluations: 538\n",
      "            Gradient evaluations: 538\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.22053928003690906\n",
      "            Iterations: 492\n",
      "            Function evaluations: 493\n",
      "            Gradient evaluations: 492\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.17781481130385907\n",
      "            Iterations: 566\n",
      "            Function evaluations: 566\n",
      "            Gradient evaluations: 566\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.17586582322273697\n",
      "            Iterations: 85\n",
      "            Function evaluations: 85\n",
      "            Gradient evaluations: 85\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1726672348576283\n",
      "            Iterations: 540\n",
      "            Function evaluations: 541\n",
      "            Gradient evaluations: 540\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.20761355152800606\n",
      "            Iterations: 384\n",
      "            Function evaluations: 384\n",
      "            Gradient evaluations: 384\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.20197298108449935\n",
      "            Iterations: 370\n",
      "            Function evaluations: 370\n",
      "            Gradient evaluations: 370\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18830653521488688\n",
      "            Iterations: 496\n",
      "            Function evaluations: 497\n",
      "            Gradient evaluations: 496\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18704633208189175\n",
      "            Iterations: 515\n",
      "            Function evaluations: 515\n",
      "            Gradient evaluations: 515\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2042787325797935\n",
      "            Iterations: 469\n",
      "            Function evaluations: 470\n",
      "            Gradient evaluations: 469\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.24120427385615853\n",
      "            Iterations: 441\n",
      "            Function evaluations: 442\n",
      "            Gradient evaluations: 441\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2118085089532638\n",
      "            Iterations: 535\n",
      "            Function evaluations: 535\n",
      "            Gradient evaluations: 535\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18003471127431603\n",
      "            Iterations: 503\n",
      "            Function evaluations: 503\n",
      "            Gradient evaluations: 503\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18400884474636844\n",
      "            Iterations: 580\n",
      "            Function evaluations: 580\n",
      "            Gradient evaluations: 580\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.21829856412422372\n",
      "            Iterations: 425\n",
      "            Function evaluations: 425\n",
      "            Gradient evaluations: 425\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.22443473790838264\n",
      "            Iterations: 504\n",
      "            Function evaluations: 504\n",
      "            Gradient evaluations: 504\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2528263341216045\n",
      "            Iterations: 509\n",
      "            Function evaluations: 510\n",
      "            Gradient evaluations: 509\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1774410350263846\n",
      "            Iterations: 564\n",
      "            Function evaluations: 564\n",
      "            Gradient evaluations: 564\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.15302687618229266\n",
      "            Iterations: 500\n",
      "            Function evaluations: 501\n",
      "            Gradient evaluations: 500\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.15773395148446356\n",
      "            Iterations: 581\n",
      "            Function evaluations: 581\n",
      "            Gradient evaluations: 581\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.19709131528152984\n",
      "            Iterations: 458\n",
      "            Function evaluations: 458\n",
      "            Gradient evaluations: 458\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.14857242940628346\n",
      "            Iterations: 600\n",
      "            Function evaluations: 600\n",
      "            Gradient evaluations: 600\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2642359107887976\n",
      "            Iterations: 470\n",
      "            Function evaluations: 470\n",
      "            Gradient evaluations: 470\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.20510573782287095\n",
      "            Iterations: 565\n",
      "            Function evaluations: 565\n",
      "            Gradient evaluations: 565\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.14887578619630473\n",
      "            Iterations: 530\n",
      "            Function evaluations: 531\n",
      "            Gradient evaluations: 530\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.20060921325966463\n",
      "            Iterations: 581\n",
      "            Function evaluations: 582\n",
      "            Gradient evaluations: 581\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1575615399107247\n",
      "            Iterations: 588\n",
      "            Function evaluations: 588\n",
      "            Gradient evaluations: 588\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1674832601787757\n",
      "            Iterations: 272\n",
      "            Function evaluations: 272\n",
      "            Gradient evaluations: 272\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1695327095054248\n",
      "            Iterations: 538\n",
      "            Function evaluations: 538\n",
      "            Gradient evaluations: 538\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2288289268582533\n",
      "            Iterations: 698\n",
      "            Function evaluations: 699\n",
      "            Gradient evaluations: 698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 2 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18128411709521758\n",
      "            Iterations: 632\n",
      "            Function evaluations: 633\n",
      "            Gradient evaluations: 632\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.12327038236156401\n",
      "            Iterations: 586\n",
      "            Function evaluations: 586\n",
      "            Gradient evaluations: 586\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1327532631830769\n",
      "            Iterations: 530\n",
      "            Function evaluations: 530\n",
      "            Gradient evaluations: 530\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.13420320955936882\n",
      "            Iterations: 524\n",
      "            Function evaluations: 525\n",
      "            Gradient evaluations: 524\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.13265341756351184\n",
      "            Iterations: 359\n",
      "            Function evaluations: 360\n",
      "            Gradient evaluations: 359\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.22740080244364252\n",
      "            Iterations: 46\n",
      "            Function evaluations: 46\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.11429966090245607\n",
      "            Iterations: 367\n",
      "            Function evaluations: 367\n",
      "            Gradient evaluations: 367\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.16442385482750166\n",
      "            Iterations: 635\n",
      "            Function evaluations: 635\n",
      "            Gradient evaluations: 635\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.12398638138774387\n",
      "            Iterations: 444\n",
      "            Function evaluations: 444\n",
      "            Gradient evaluations: 444\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.13037148838334248\n",
      "            Iterations: 380\n",
      "            Function evaluations: 380\n",
      "            Gradient evaluations: 380\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.12408646515796015\n",
      "            Iterations: 131\n",
      "            Function evaluations: 131\n",
      "            Gradient evaluations: 131\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.16206953967727242\n",
      "            Iterations: 615\n",
      "            Function evaluations: 615\n",
      "            Gradient evaluations: 615\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.12636579250155222\n",
      "            Iterations: 558\n",
      "            Function evaluations: 558\n",
      "            Gradient evaluations: 558\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.155020455496286\n",
      "            Iterations: 562\n",
      "            Function evaluations: 563\n",
      "            Gradient evaluations: 562\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.170986306514109\n",
      "            Iterations: 482\n",
      "            Function evaluations: 482\n",
      "            Gradient evaluations: 482\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.15353340068301205\n",
      "            Iterations: 564\n",
      "            Function evaluations: 564\n",
      "            Gradient evaluations: 564\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2254145489871809\n",
      "            Iterations: 678\n",
      "            Function evaluations: 678\n",
      "            Gradient evaluations: 678\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.09978053487465338\n",
      "            Iterations: 746\n",
      "            Function evaluations: 746\n",
      "            Gradient evaluations: 746\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.07296665185491\n",
      "            Iterations: 460\n",
      "            Function evaluations: 460\n",
      "            Gradient evaluations: 460\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.043888053424125065\n",
      "            Iterations: 203\n",
      "            Function evaluations: 204\n",
      "            Gradient evaluations: 203\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.09929889837574318\n",
      "            Iterations: 390\n",
      "            Function evaluations: 390\n",
      "            Gradient evaluations: 390\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.10544584744866403\n",
      "            Iterations: 708\n",
      "            Function evaluations: 708\n",
      "            Gradient evaluations: 708\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1323237866597734\n",
      "            Iterations: 462\n",
      "            Function evaluations: 462\n",
      "            Gradient evaluations: 462\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.11261449384904126\n",
      "            Iterations: 689\n",
      "            Function evaluations: 689\n",
      "            Gradient evaluations: 689\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.24095883735149065\n",
      "            Iterations: 514\n",
      "            Function evaluations: 514\n",
      "            Gradient evaluations: 514\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.25207404842426573\n",
      "            Iterations: 480\n",
      "            Function evaluations: 481\n",
      "            Gradient evaluations: 480\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2544294204269171\n",
      "            Iterations: 487\n",
      "            Function evaluations: 487\n",
      "            Gradient evaluations: 487\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.25189657667727416\n",
      "            Iterations: 424\n",
      "            Function evaluations: 425\n",
      "            Gradient evaluations: 424\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.22390131092530718\n",
      "            Iterations: 387\n",
      "            Function evaluations: 387\n",
      "            Gradient evaluations: 387\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.17387285953575268\n",
      "            Iterations: 439\n",
      "            Function evaluations: 439\n",
      "            Gradient evaluations: 439\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1758405560999695\n",
      "            Iterations: 553\n",
      "            Function evaluations: 553\n",
      "            Gradient evaluations: 553\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.37166612313735997\n",
      "            Iterations: 931\n",
      "            Function evaluations: 931\n",
      "            Gradient evaluations: 931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 14 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration limit reached    (Exit mode 9)\n",
      "            Current function value: 0.3470487418139503\n",
      "            Iterations: 1000\n",
      "            Function evaluations: 1000\n",
      "            Gradient evaluations: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 34 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2499961630784015\n",
      "            Iterations: 833\n",
      "            Function evaluations: 834\n",
      "            Gradient evaluations: 833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 6 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.25018140716797777\n",
      "            Iterations: 680\n",
      "            Function evaluations: 680\n",
      "            Gradient evaluations: 680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 4 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.19555935634049104\n",
      "            Iterations: 622\n",
      "            Function evaluations: 622\n",
      "            Gradient evaluations: 622\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.0447838520281157\n",
      "            Iterations: 593\n",
      "            Function evaluations: 593\n",
      "            Gradient evaluations: 593\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.04802643986518185\n",
      "            Iterations: 582\n",
      "            Function evaluations: 582\n",
      "            Gradient evaluations: 582\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2384159259446283\n",
      "            Iterations: 547\n",
      "            Function evaluations: 547\n",
      "            Gradient evaluations: 547\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18548101305313897\n",
      "            Iterations: 564\n",
      "            Function evaluations: 565\n",
      "            Gradient evaluations: 564\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.17430106780233107\n",
      "            Iterations: 554\n",
      "            Function evaluations: 554\n",
      "            Gradient evaluations: 554\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.17332380443435147\n",
      "            Iterations: 581\n",
      "            Function evaluations: 581\n",
      "            Gradient evaluations: 581\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.21414495339376377\n",
      "            Iterations: 360\n",
      "            Function evaluations: 361\n",
      "            Gradient evaluations: 360\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.20750022134853427\n",
      "            Iterations: 345\n",
      "            Function evaluations: 345\n",
      "            Gradient evaluations: 345\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.17801710835337062\n",
      "            Iterations: 548\n",
      "            Function evaluations: 548\n",
      "            Gradient evaluations: 548\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.20355943102388235\n",
      "            Iterations: 542\n",
      "            Function evaluations: 543\n",
      "            Gradient evaluations: 542\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.21010821966868096\n",
      "            Iterations: 452\n",
      "            Function evaluations: 452\n",
      "            Gradient evaluations: 452\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.25596088073068335\n",
      "            Iterations: 454\n",
      "            Function evaluations: 455\n",
      "            Gradient evaluations: 454\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.213198842364824\n",
      "            Iterations: 575\n",
      "            Function evaluations: 575\n",
      "            Gradient evaluations: 575\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1852492521673011\n",
      "            Iterations: 569\n",
      "            Function evaluations: 570\n",
      "            Gradient evaluations: 569\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.19188451614536384\n",
      "            Iterations: 489\n",
      "            Function evaluations: 490\n",
      "            Gradient evaluations: 489\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.21400202872789548\n",
      "            Iterations: 413\n",
      "            Function evaluations: 413\n",
      "            Gradient evaluations: 413\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.222006184060085\n",
      "            Iterations: 513\n",
      "            Function evaluations: 513\n",
      "            Gradient evaluations: 513\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.25815539697058854\n",
      "            Iterations: 540\n",
      "            Function evaluations: 540\n",
      "            Gradient evaluations: 540\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18627114361862906\n",
      "            Iterations: 564\n",
      "            Function evaluations: 564\n",
      "            Gradient evaluations: 564\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.14996331424230525\n",
      "            Iterations: 517\n",
      "            Function evaluations: 517\n",
      "            Gradient evaluations: 517\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.16761578036004549\n",
      "            Iterations: 560\n",
      "            Function evaluations: 560\n",
      "            Gradient evaluations: 560\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.21719351620140798\n",
      "            Iterations: 421\n",
      "            Function evaluations: 421\n",
      "            Gradient evaluations: 421\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.16153286007943923\n",
      "            Iterations: 552\n",
      "            Function evaluations: 553\n",
      "            Gradient evaluations: 552\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.25566499154403477\n",
      "            Iterations: 477\n",
      "            Function evaluations: 477\n",
      "            Gradient evaluations: 477\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.21171816074623348\n",
      "            Iterations: 504\n",
      "            Function evaluations: 504\n",
      "            Gradient evaluations: 504\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.16041357152065752\n",
      "            Iterations: 530\n",
      "            Function evaluations: 530\n",
      "            Gradient evaluations: 530\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2050932429655707\n",
      "            Iterations: 561\n",
      "            Function evaluations: 561\n",
      "            Gradient evaluations: 561\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.16030623376203484\n",
      "            Iterations: 606\n",
      "            Function evaluations: 606\n",
      "            Gradient evaluations: 606\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1872188738070495\n",
      "            Iterations: 287\n",
      "            Function evaluations: 287\n",
      "            Gradient evaluations: 287\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.16659656913627174\n",
      "            Iterations: 503\n",
      "            Function evaluations: 503\n",
      "            Gradient evaluations: 503\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.23659117599445675\n",
      "            Iterations: 706\n",
      "            Function evaluations: 707\n",
      "            Gradient evaluations: 706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1972432910981511\n",
      "            Iterations: 593\n",
      "            Function evaluations: 594\n",
      "            Gradient evaluations: 593\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.12880957081761646\n",
      "            Iterations: 609\n",
      "            Function evaluations: 609\n",
      "            Gradient evaluations: 609\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.13164220125805728\n",
      "            Iterations: 573\n",
      "            Function evaluations: 574\n",
      "            Gradient evaluations: 573\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.13237153039500077\n",
      "            Iterations: 573\n",
      "            Function evaluations: 573\n",
      "            Gradient evaluations: 573\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.15722996043500392\n",
      "            Iterations: 440\n",
      "            Function evaluations: 440\n",
      "            Gradient evaluations: 440\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.22740080244364252\n",
      "            Iterations: 46\n",
      "            Function evaluations: 46\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1415842597234965\n",
      "            Iterations: 378\n",
      "            Function evaluations: 379\n",
      "            Gradient evaluations: 378\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18178193923094124\n",
      "            Iterations: 581\n",
      "            Function evaluations: 581\n",
      "            Gradient evaluations: 581\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.12887617092735354\n",
      "            Iterations: 457\n",
      "            Function evaluations: 457\n",
      "            Gradient evaluations: 457\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1496173344681938\n",
      "            Iterations: 383\n",
      "            Function evaluations: 383\n",
      "            Gradient evaluations: 383\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1316636101096506\n",
      "            Iterations: 173\n",
      "            Function evaluations: 173\n",
      "            Gradient evaluations: 173\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.17504953791454494\n",
      "            Iterations: 599\n",
      "            Function evaluations: 600\n",
      "            Gradient evaluations: 599\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.14219902751085245\n",
      "            Iterations: 594\n",
      "            Function evaluations: 594\n",
      "            Gradient evaluations: 594\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.14412898434939042\n",
      "            Iterations: 628\n",
      "            Function evaluations: 628\n",
      "            Gradient evaluations: 628\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.15690910480169773\n",
      "            Iterations: 473\n",
      "            Function evaluations: 474\n",
      "            Gradient evaluations: 473\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.14567338855518375\n",
      "            Iterations: 552\n",
      "            Function evaluations: 552\n",
      "            Gradient evaluations: 552\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.22193548200416363\n",
      "            Iterations: 691\n",
      "            Function evaluations: 692\n",
      "            Gradient evaluations: 691\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1022121428689166\n",
      "            Iterations: 735\n",
      "            Function evaluations: 735\n",
      "            Gradient evaluations: 735\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.08584814739015453\n",
      "            Iterations: 475\n",
      "            Function evaluations: 475\n",
      "            Gradient evaluations: 475\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.06898269542799629\n",
      "            Iterations: 218\n",
      "            Function evaluations: 218\n",
      "            Gradient evaluations: 218\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.10449998333391541\n",
      "            Iterations: 389\n",
      "            Function evaluations: 389\n",
      "            Gradient evaluations: 389\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.10457692085476432\n",
      "            Iterations: 726\n",
      "            Function evaluations: 727\n",
      "            Gradient evaluations: 726\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.14549966240988804\n",
      "            Iterations: 515\n",
      "            Function evaluations: 515\n",
      "            Gradient evaluations: 515\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.10626970058041932\n",
      "            Iterations: 669\n",
      "            Function evaluations: 670\n",
      "            Gradient evaluations: 669\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2333263069315208\n",
      "            Iterations: 527\n",
      "            Function evaluations: 528\n",
      "            Gradient evaluations: 527\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2487997186581113\n",
      "            Iterations: 485\n",
      "            Function evaluations: 485\n",
      "            Gradient evaluations: 485\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.23200873928164117\n",
      "            Iterations: 526\n",
      "            Function evaluations: 526\n",
      "            Gradient evaluations: 526\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2511317554135294\n",
      "            Iterations: 532\n",
      "            Function evaluations: 532\n",
      "            Gradient evaluations: 532\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.21620007269724084\n",
      "            Iterations: 412\n",
      "            Function evaluations: 412\n",
      "            Gradient evaluations: 412\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.20295756511692237\n",
      "            Iterations: 491\n",
      "            Function evaluations: 491\n",
      "            Gradient evaluations: 491\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.15997693325951812\n",
      "            Iterations: 579\n",
      "            Function evaluations: 579\n",
      "            Gradient evaluations: 579\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3746151854169114\n",
      "            Iterations: 929\n",
      "            Function evaluations: 929\n",
      "            Gradient evaluations: 929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 16 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration limit reached    (Exit mode 9)\n",
      "            Current function value: 0.3450789217774115\n",
      "            Iterations: 1000\n",
      "            Function evaluations: 1000\n",
      "            Gradient evaluations: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 33 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.24841998038543606\n",
      "            Iterations: 832\n",
      "            Function evaluations: 832\n",
      "            Gradient evaluations: 832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 11 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.25312434619232654\n",
      "            Iterations: 665\n",
      "            Function evaluations: 665\n",
      "            Gradient evaluations: 665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 3 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.20650500118273316\n",
      "            Iterations: 621\n",
      "            Function evaluations: 622\n",
      "            Gradient evaluations: 621\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.04691514642113082\n",
      "            Iterations: 560\n",
      "            Function evaluations: 561\n",
      "            Gradient evaluations: 560\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.05052921795991723\n",
      "            Iterations: 563\n",
      "            Function evaluations: 564\n",
      "            Gradient evaluations: 563\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.24791791347338804\n",
      "            Iterations: 525\n",
      "            Function evaluations: 525\n",
      "            Gradient evaluations: 525\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.21941170168653967\n",
      "            Iterations: 519\n",
      "            Function evaluations: 519\n",
      "            Gradient evaluations: 519\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18485151953547263\n",
      "            Iterations: 519\n",
      "            Function evaluations: 520\n",
      "            Gradient evaluations: 519\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.14026471265043516\n",
      "            Iterations: 97\n",
      "            Function evaluations: 97\n",
      "            Gradient evaluations: 97\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18623746037276462\n",
      "            Iterations: 535\n",
      "            Function evaluations: 535\n",
      "            Gradient evaluations: 535\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.17317101438123506\n",
      "            Iterations: 326\n",
      "            Function evaluations: 326\n",
      "            Gradient evaluations: 326\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.17600021834188215\n",
      "            Iterations: 306\n",
      "            Function evaluations: 306\n",
      "            Gradient evaluations: 306\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.198058463032858\n",
      "            Iterations: 479\n",
      "            Function evaluations: 479\n",
      "            Gradient evaluations: 479\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2102734826810042\n",
      "            Iterations: 544\n",
      "            Function evaluations: 545\n",
      "            Gradient evaluations: 544\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2302878098944619\n",
      "            Iterations: 459\n",
      "            Function evaluations: 460\n",
      "            Gradient evaluations: 459\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.26867351413235735\n",
      "            Iterations: 454\n",
      "            Function evaluations: 454\n",
      "            Gradient evaluations: 454\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2026427436547542\n",
      "            Iterations: 554\n",
      "            Function evaluations: 555\n",
      "            Gradient evaluations: 554\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18303426215972368\n",
      "            Iterations: 598\n",
      "            Function evaluations: 598\n",
      "            Gradient evaluations: 598\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18333718430505935\n",
      "            Iterations: 473\n",
      "            Function evaluations: 473\n",
      "            Gradient evaluations: 473\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.22376910120809138\n",
      "            Iterations: 426\n",
      "            Function evaluations: 426\n",
      "            Gradient evaluations: 426\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.21272363558053903\n",
      "            Iterations: 477\n",
      "            Function evaluations: 478\n",
      "            Gradient evaluations: 477\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2550493893837139\n",
      "            Iterations: 527\n",
      "            Function evaluations: 528\n",
      "            Gradient evaluations: 527\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18999852658581243\n",
      "            Iterations: 570\n",
      "            Function evaluations: 570\n",
      "            Gradient evaluations: 570\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.16303203863258087\n",
      "            Iterations: 500\n",
      "            Function evaluations: 500\n",
      "            Gradient evaluations: 500\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.17448478695149702\n",
      "            Iterations: 563\n",
      "            Function evaluations: 563\n",
      "            Gradient evaluations: 563\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1797977477794837\n",
      "            Iterations: 391\n",
      "            Function evaluations: 391\n",
      "            Gradient evaluations: 391\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.16529808960578607\n",
      "            Iterations: 612\n",
      "            Function evaluations: 612\n",
      "            Gradient evaluations: 612\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2552488496218764\n",
      "            Iterations: 465\n",
      "            Function evaluations: 466\n",
      "            Gradient evaluations: 465\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.19749114941679743\n",
      "            Iterations: 529\n",
      "            Function evaluations: 529\n",
      "            Gradient evaluations: 529\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.15910576575120153\n",
      "            Iterations: 508\n",
      "            Function evaluations: 509\n",
      "            Gradient evaluations: 508\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.20263081668087443\n",
      "            Iterations: 583\n",
      "            Function evaluations: 583\n",
      "            Gradient evaluations: 583\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.15353003328479195\n",
      "            Iterations: 564\n",
      "            Function evaluations: 564\n",
      "            Gradient evaluations: 564\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.20660182306098754\n",
      "            Iterations: 312\n",
      "            Function evaluations: 312\n",
      "            Gradient evaluations: 312\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.15706824793223376\n",
      "            Iterations: 520\n",
      "            Function evaluations: 520\n",
      "            Gradient evaluations: 520\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.23385770066515923\n",
      "            Iterations: 692\n",
      "            Function evaluations: 692\n",
      "            Gradient evaluations: 692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 4 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.19054020362433072\n",
      "            Iterations: 624\n",
      "            Function evaluations: 625\n",
      "            Gradient evaluations: 624\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.12392864359635586\n",
      "            Iterations: 623\n",
      "            Function evaluations: 623\n",
      "            Gradient evaluations: 623\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1357765387316309\n",
      "            Iterations: 558\n",
      "            Function evaluations: 558\n",
      "            Gradient evaluations: 558\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.13722207722456634\n",
      "            Iterations: 553\n",
      "            Function evaluations: 554\n",
      "            Gradient evaluations: 553\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.14924003173754388\n",
      "            Iterations: 422\n",
      "            Function evaluations: 422\n",
      "            Gradient evaluations: 422\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.22740080244364252\n",
      "            Iterations: 46\n",
      "            Function evaluations: 46\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1277385248800485\n",
      "            Iterations: 403\n",
      "            Function evaluations: 403\n",
      "            Gradient evaluations: 403\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.17822598845613574\n",
      "            Iterations: 603\n",
      "            Function evaluations: 604\n",
      "            Gradient evaluations: 603\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.13604327638510744\n",
      "            Iterations: 445\n",
      "            Function evaluations: 445\n",
      "            Gradient evaluations: 445\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.14722814634502096\n",
      "            Iterations: 393\n",
      "            Function evaluations: 393\n",
      "            Gradient evaluations: 393\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1284249003863191\n",
      "            Iterations: 130\n",
      "            Function evaluations: 131\n",
      "            Gradient evaluations: 130\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1740112072732135\n",
      "            Iterations: 607\n",
      "            Function evaluations: 608\n",
      "            Gradient evaluations: 607\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1442515400552531\n",
      "            Iterations: 572\n",
      "            Function evaluations: 572\n",
      "            Gradient evaluations: 572\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.12620869899040157\n",
      "            Iterations: 581\n",
      "            Function evaluations: 581\n",
      "            Gradient evaluations: 581\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.178471141446427\n",
      "            Iterations: 442\n",
      "            Function evaluations: 442\n",
      "            Gradient evaluations: 442\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.13579700150489826\n",
      "            Iterations: 562\n",
      "            Function evaluations: 562\n",
      "            Gradient evaluations: 562\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.23183895933747767\n",
      "            Iterations: 647\n",
      "            Function evaluations: 648\n",
      "            Gradient evaluations: 647\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.11487370198593028\n",
      "            Iterations: 669\n",
      "            Function evaluations: 669\n",
      "            Gradient evaluations: 669\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.09845846113193057\n",
      "            Iterations: 488\n",
      "            Function evaluations: 488\n",
      "            Gradient evaluations: 488\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.06917088838583285\n",
      "            Iterations: 241\n",
      "            Function evaluations: 242\n",
      "            Gradient evaluations: 241\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.11410538534300514\n",
      "            Iterations: 400\n",
      "            Function evaluations: 400\n",
      "            Gradient evaluations: 400\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.11210010802564299\n",
      "            Iterations: 764\n",
      "            Function evaluations: 764\n",
      "            Gradient evaluations: 764\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.13760229721872239\n",
      "            Iterations: 503\n",
      "            Function evaluations: 503\n",
      "            Gradient evaluations: 503\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.11163175808880041\n",
      "            Iterations: 675\n",
      "            Function evaluations: 675\n",
      "            Gradient evaluations: 675\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.25101208745899084\n",
      "            Iterations: 507\n",
      "            Function evaluations: 507\n",
      "            Gradient evaluations: 507\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.23989736012245222\n",
      "            Iterations: 492\n",
      "            Function evaluations: 492\n",
      "            Gradient evaluations: 492\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2197220921690638\n",
      "            Iterations: 526\n",
      "            Function evaluations: 527\n",
      "            Gradient evaluations: 526\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.24896459539364357\n",
      "            Iterations: 450\n",
      "            Function evaluations: 451\n",
      "            Gradient evaluations: 450\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.20468120213785718\n",
      "            Iterations: 418\n",
      "            Function evaluations: 418\n",
      "            Gradient evaluations: 418\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2028343100275387\n",
      "            Iterations: 523\n",
      "            Function evaluations: 523\n",
      "            Gradient evaluations: 523\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.17856410153480218\n",
      "            Iterations: 555\n",
      "            Function evaluations: 555\n",
      "            Gradient evaluations: 555\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.371345944217783\n",
      "            Iterations: 913\n",
      "            Function evaluations: 913\n",
      "            Gradient evaluations: 913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 15 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration limit reached    (Exit mode 9)\n",
      "            Current function value: 0.34681589515361166\n",
      "            Iterations: 1000\n",
      "            Function evaluations: 1000\n",
      "            Gradient evaluations: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 42 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.24480291312585978\n",
      "            Iterations: 831\n",
      "            Function evaluations: 831\n",
      "            Gradient evaluations: 831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 9 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.23988381722519778\n",
      "            Iterations: 667\n",
      "            Function evaluations: 668\n",
      "            Gradient evaluations: 667\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18846502780234964\n",
      "            Iterations: 623\n",
      "            Function evaluations: 623\n",
      "            Gradient evaluations: 623\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.046561524963857064\n",
      "            Iterations: 593\n",
      "            Function evaluations: 593\n",
      "            Gradient evaluations: 593\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.05015655646725956\n",
      "            Iterations: 560\n",
      "            Function evaluations: 560\n",
      "            Gradient evaluations: 560\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.23935327022412103\n",
      "            Iterations: 530\n",
      "            Function evaluations: 531\n",
      "            Gradient evaluations: 530\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18891880047076703\n",
      "            Iterations: 534\n",
      "            Function evaluations: 534\n",
      "            Gradient evaluations: 534\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.15948940366015363\n",
      "            Iterations: 564\n",
      "            Function evaluations: 564\n",
      "            Gradient evaluations: 564\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.14026471265043516\n",
      "            Iterations: 97\n",
      "            Function evaluations: 97\n",
      "            Gradient evaluations: 97\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.16196815533897543\n",
      "            Iterations: 513\n",
      "            Function evaluations: 514\n",
      "            Gradient evaluations: 513\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1766869306184038\n",
      "            Iterations: 346\n",
      "            Function evaluations: 347\n",
      "            Gradient evaluations: 346\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2023009787538872\n",
      "            Iterations: 344\n",
      "            Function evaluations: 344\n",
      "            Gradient evaluations: 344\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18672708549373526\n",
      "            Iterations: 468\n",
      "            Function evaluations: 468\n",
      "            Gradient evaluations: 468\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.20806374392111704\n",
      "            Iterations: 569\n",
      "            Function evaluations: 569\n",
      "            Gradient evaluations: 569\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.21186882535445284\n",
      "            Iterations: 477\n",
      "            Function evaluations: 477\n",
      "            Gradient evaluations: 477\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.26098148839585866\n",
      "            Iterations: 399\n",
      "            Function evaluations: 400\n",
      "            Gradient evaluations: 399\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.20981107703002283\n",
      "            Iterations: 528\n",
      "            Function evaluations: 529\n",
      "            Gradient evaluations: 528\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1884450580795908\n",
      "            Iterations: 523\n",
      "            Function evaluations: 524\n",
      "            Gradient evaluations: 523\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.19677991233186753\n",
      "            Iterations: 504\n",
      "            Function evaluations: 504\n",
      "            Gradient evaluations: 504\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1950014572302435\n",
      "            Iterations: 413\n",
      "            Function evaluations: 413\n",
      "            Gradient evaluations: 413\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.22237966440632048\n",
      "            Iterations: 522\n",
      "            Function evaluations: 522\n",
      "            Gradient evaluations: 522\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2466413134272654\n",
      "            Iterations: 516\n",
      "            Function evaluations: 517\n",
      "            Gradient evaluations: 516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.17287768548898574\n",
      "            Iterations: 602\n",
      "            Function evaluations: 603\n",
      "            Gradient evaluations: 602\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.15566309656284089\n",
      "            Iterations: 534\n",
      "            Function evaluations: 534\n",
      "            Gradient evaluations: 534\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.15864875447458557\n",
      "            Iterations: 562\n",
      "            Function evaluations: 562\n",
      "            Gradient evaluations: 562\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.14300287428166938\n",
      "            Iterations: 406\n",
      "            Function evaluations: 407\n",
      "            Gradient evaluations: 406\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1660040992712854\n",
      "            Iterations: 554\n",
      "            Function evaluations: 554\n",
      "            Gradient evaluations: 554\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2499507768097044\n",
      "            Iterations: 440\n",
      "            Function evaluations: 441\n",
      "            Gradient evaluations: 440\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1859147641656247\n",
      "            Iterations: 587\n",
      "            Function evaluations: 588\n",
      "            Gradient evaluations: 587\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1310965968156178\n",
      "            Iterations: 551\n",
      "            Function evaluations: 552\n",
      "            Gradient evaluations: 551\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2092236679381326\n",
      "            Iterations: 567\n",
      "            Function evaluations: 567\n",
      "            Gradient evaluations: 567\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.16933942349182948\n",
      "            Iterations: 573\n",
      "            Function evaluations: 574\n",
      "            Gradient evaluations: 573\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.19221089936718944\n",
      "            Iterations: 314\n",
      "            Function evaluations: 314\n",
      "            Gradient evaluations: 314\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.166460654072501\n",
      "            Iterations: 502\n",
      "            Function evaluations: 502\n",
      "            Gradient evaluations: 502\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.23566422057731853\n",
      "            Iterations: 684\n",
      "            Function evaluations: 685\n",
      "            Gradient evaluations: 684\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.19134964325754492\n",
      "            Iterations: 603\n",
      "            Function evaluations: 603\n",
      "            Gradient evaluations: 603\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.127199490259916\n",
      "            Iterations: 537\n",
      "            Function evaluations: 537\n",
      "            Gradient evaluations: 537\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.13436926260876345\n",
      "            Iterations: 539\n",
      "            Function evaluations: 540\n",
      "            Gradient evaluations: 539\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.13576796567422938\n",
      "            Iterations: 531\n",
      "            Function evaluations: 531\n",
      "            Gradient evaluations: 531\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.14548516087733043\n",
      "            Iterations: 411\n",
      "            Function evaluations: 412\n",
      "            Gradient evaluations: 411\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.14355621607644384\n",
      "            Iterations: 419\n",
      "            Function evaluations: 419\n",
      "            Gradient evaluations: 419\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.17904068749500107\n",
      "            Iterations: 619\n",
      "            Function evaluations: 619\n",
      "            Gradient evaluations: 619\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.12047616889381839\n",
      "            Iterations: 389\n",
      "            Function evaluations: 389\n",
      "            Gradient evaluations: 389\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1468809088950731\n",
      "            Iterations: 347\n",
      "            Function evaluations: 347\n",
      "            Gradient evaluations: 347\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.08033727932919761\n",
      "            Iterations: 137\n",
      "            Function evaluations: 138\n",
      "            Gradient evaluations: 137\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1806698748651745\n",
      "            Iterations: 602\n",
      "            Function evaluations: 603\n",
      "            Gradient evaluations: 602\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.13979917468889203\n",
      "            Iterations: 602\n",
      "            Function evaluations: 602\n",
      "            Gradient evaluations: 602\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.15732606100546956\n",
      "            Iterations: 650\n",
      "            Function evaluations: 650\n",
      "            Gradient evaluations: 650\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1514010334298974\n",
      "            Iterations: 426\n",
      "            Function evaluations: 426\n",
      "            Gradient evaluations: 426\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.14885678341964767\n",
      "            Iterations: 543\n",
      "            Function evaluations: 543\n",
      "            Gradient evaluations: 543\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.23271832648733487\n",
      "            Iterations: 663\n",
      "            Function evaluations: 664\n",
      "            Gradient evaluations: 663\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.10429657578694629\n",
      "            Iterations: 700\n",
      "            Function evaluations: 700\n",
      "            Gradient evaluations: 700\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.0901055330693217\n",
      "            Iterations: 526\n",
      "            Function evaluations: 526\n",
      "            Gradient evaluations: 526\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.07692846174282779\n",
      "            Iterations: 236\n",
      "            Function evaluations: 237\n",
      "            Gradient evaluations: 236\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.09122048324426081\n",
      "            Iterations: 420\n",
      "            Function evaluations: 420\n",
      "            Gradient evaluations: 420\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.10442302665158179\n",
      "            Iterations: 743\n",
      "            Function evaluations: 744\n",
      "            Gradient evaluations: 743\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.13620592232907025\n",
      "            Iterations: 537\n",
      "            Function evaluations: 537\n",
      "            Gradient evaluations: 537\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.10394341999838846\n",
      "            Iterations: 665\n",
      "            Function evaluations: 665\n",
      "            Gradient evaluations: 665\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.257506123294949\n",
      "            Iterations: 512\n",
      "            Function evaluations: 512\n",
      "            Gradient evaluations: 512\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2617093224430468\n",
      "            Iterations: 463\n",
      "            Function evaluations: 463\n",
      "            Gradient evaluations: 463\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2538014429345837\n",
      "            Iterations: 545\n",
      "            Function evaluations: 545\n",
      "            Gradient evaluations: 545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.23490687208568983\n",
      "            Iterations: 433\n",
      "            Function evaluations: 433\n",
      "            Gradient evaluations: 433\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.23564054442583865\n",
      "            Iterations: 384\n",
      "            Function evaluations: 384\n",
      "            Gradient evaluations: 384\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2008175283941841\n",
      "            Iterations: 502\n",
      "            Function evaluations: 502\n",
      "            Gradient evaluations: 502\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.175233888473026\n",
      "            Iterations: 537\n",
      "            Function evaluations: 538\n",
      "            Gradient evaluations: 537\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3745175568838421\n",
      "            Iterations: 927\n",
      "            Function evaluations: 927\n",
      "            Gradient evaluations: 927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 17 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration limit reached    (Exit mode 9)\n",
      "            Current function value: 0.3460209257998608\n",
      "            Iterations: 1000\n",
      "            Function evaluations: 1000\n",
      "            Gradient evaluations: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 37 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.24578320963240222\n",
      "            Iterations: 847\n",
      "            Function evaluations: 847\n",
      "            Gradient evaluations: 847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 6 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.24626522830316525\n",
      "            Iterations: 715\n",
      "            Function evaluations: 715\n",
      "            Gradient evaluations: 715\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.20348785930788818\n",
      "            Iterations: 594\n",
      "            Function evaluations: 594\n",
      "            Gradient evaluations: 594\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.03802809613129276\n",
      "            Iterations: 629\n",
      "            Function evaluations: 629\n",
      "            Gradient evaluations: 629\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.04033883427053275\n",
      "            Iterations: 598\n",
      "            Function evaluations: 599\n",
      "            Gradient evaluations: 598\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2533748463007168\n",
      "            Iterations: 509\n",
      "            Function evaluations: 509\n",
      "            Gradient evaluations: 509\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2211335424881188\n",
      "            Iterations: 496\n",
      "            Function evaluations: 496\n",
      "            Gradient evaluations: 496\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.17571779464128431\n",
      "            Iterations: 571\n",
      "            Function evaluations: 571\n",
      "            Gradient evaluations: 571\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.14026471265043516\n",
      "            Iterations: 97\n",
      "            Function evaluations: 97\n",
      "            Gradient evaluations: 97\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.17263273806130744\n",
      "            Iterations: 571\n",
      "            Function evaluations: 571\n",
      "            Gradient evaluations: 571\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.19838764335885828\n",
      "            Iterations: 409\n",
      "            Function evaluations: 410\n",
      "            Gradient evaluations: 409\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.19693380145000233\n",
      "            Iterations: 352\n",
      "            Function evaluations: 352\n",
      "            Gradient evaluations: 352\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1914623288180619\n",
      "            Iterations: 491\n",
      "            Function evaluations: 492\n",
      "            Gradient evaluations: 491\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.21873846700443533\n",
      "            Iterations: 539\n",
      "            Function evaluations: 539\n",
      "            Gradient evaluations: 539\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.23363737953660846\n",
      "            Iterations: 419\n",
      "            Function evaluations: 419\n",
      "            Gradient evaluations: 419\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.23657325003833796\n",
      "            Iterations: 456\n",
      "            Function evaluations: 457\n",
      "            Gradient evaluations: 456\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.21473632136116363\n",
      "            Iterations: 553\n",
      "            Function evaluations: 554\n",
      "            Gradient evaluations: 553\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18089408399156\n",
      "            Iterations: 565\n",
      "            Function evaluations: 565\n",
      "            Gradient evaluations: 565\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.19046611741387504\n",
      "            Iterations: 464\n",
      "            Function evaluations: 465\n",
      "            Gradient evaluations: 464\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.206069365758066\n",
      "            Iterations: 463\n",
      "            Function evaluations: 463\n",
      "            Gradient evaluations: 463\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.21349782223670732\n",
      "            Iterations: 490\n",
      "            Function evaluations: 490\n",
      "            Gradient evaluations: 490\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.24607842825679435\n",
      "            Iterations: 571\n",
      "            Function evaluations: 571\n",
      "            Gradient evaluations: 571\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.16271140706753112\n",
      "            Iterations: 579\n",
      "            Function evaluations: 580\n",
      "            Gradient evaluations: 579\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.15988183190446914\n",
      "            Iterations: 537\n",
      "            Function evaluations: 537\n",
      "            Gradient evaluations: 537\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.15041578844770476\n",
      "            Iterations: 578\n",
      "            Function evaluations: 578\n",
      "            Gradient evaluations: 578\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.16803043618785476\n",
      "            Iterations: 442\n",
      "            Function evaluations: 442\n",
      "            Gradient evaluations: 442\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.14083802911474613\n",
      "            Iterations: 630\n",
      "            Function evaluations: 630\n",
      "            Gradient evaluations: 630\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.263249564003144\n",
      "            Iterations: 458\n",
      "            Function evaluations: 458\n",
      "            Gradient evaluations: 458\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1935718040983302\n",
      "            Iterations: 564\n",
      "            Function evaluations: 564\n",
      "            Gradient evaluations: 564\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1613079656695002\n",
      "            Iterations: 550\n",
      "            Function evaluations: 550\n",
      "            Gradient evaluations: 550\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1935419992941359\n",
      "            Iterations: 565\n",
      "            Function evaluations: 565\n",
      "            Gradient evaluations: 565\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.17882945536026776\n",
      "            Iterations: 547\n",
      "            Function evaluations: 547\n",
      "            Gradient evaluations: 547\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.19375850847999765\n",
      "            Iterations: 281\n",
      "            Function evaluations: 281\n",
      "            Gradient evaluations: 281\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1830437376516233\n",
      "            Iterations: 507\n",
      "            Function evaluations: 507\n",
      "            Gradient evaluations: 507\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.23292959247139627\n",
      "            Iterations: 689\n",
      "            Function evaluations: 690\n",
      "            Gradient evaluations: 689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 7 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18687279943487048\n",
      "            Iterations: 620\n",
      "            Function evaluations: 620\n",
      "            Gradient evaluations: 620\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.13161502086419927\n",
      "            Iterations: 591\n",
      "            Function evaluations: 591\n",
      "            Gradient evaluations: 591\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1341872829367809\n",
      "            Iterations: 564\n",
      "            Function evaluations: 564\n",
      "            Gradient evaluations: 564\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1355374255235969\n",
      "            Iterations: 545\n",
      "            Function evaluations: 545\n",
      "            Gradient evaluations: 545\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.15473951983627904\n",
      "            Iterations: 455\n",
      "            Function evaluations: 456\n",
      "            Gradient evaluations: 455\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.22740080244364252\n",
      "            Iterations: 46\n",
      "            Function evaluations: 46\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.13677216009036866\n",
      "            Iterations: 414\n",
      "            Function evaluations: 414\n",
      "            Gradient evaluations: 414\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.181440504285423\n",
      "            Iterations: 597\n",
      "            Function evaluations: 597\n",
      "            Gradient evaluations: 597\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.11612729186476121\n",
      "            Iterations: 408\n",
      "            Function evaluations: 408\n",
      "            Gradient evaluations: 408\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.12968752706867903\n",
      "            Iterations: 311\n",
      "            Function evaluations: 311\n",
      "            Gradient evaluations: 311\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.12136239010501818\n",
      "            Iterations: 184\n",
      "            Function evaluations: 184\n",
      "            Gradient evaluations: 184\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1803038281158289\n",
      "            Iterations: 597\n",
      "            Function evaluations: 597\n",
      "            Gradient evaluations: 597\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.14317079523348597\n",
      "            Iterations: 545\n",
      "            Function evaluations: 545\n",
      "            Gradient evaluations: 545\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.15273608377457926\n",
      "            Iterations: 590\n",
      "            Function evaluations: 591\n",
      "            Gradient evaluations: 590\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.16976600426057656\n",
      "            Iterations: 466\n",
      "            Function evaluations: 466\n",
      "            Gradient evaluations: 466\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.15676106789244526\n",
      "            Iterations: 613\n",
      "            Function evaluations: 613\n",
      "            Gradient evaluations: 613\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.23327369709273396\n",
      "            Iterations: 667\n",
      "            Function evaluations: 668\n",
      "            Gradient evaluations: 667\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.10803210436997132\n",
      "            Iterations: 719\n",
      "            Function evaluations: 719\n",
      "            Gradient evaluations: 719\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.06917697781131577\n",
      "            Iterations: 441\n",
      "            Function evaluations: 441\n",
      "            Gradient evaluations: 441\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.07102482849724259\n",
      "            Iterations: 247\n",
      "            Function evaluations: 248\n",
      "            Gradient evaluations: 247\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.0685562344403027\n",
      "            Iterations: 372\n",
      "            Function evaluations: 372\n",
      "            Gradient evaluations: 372\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.11299774494774495\n",
      "            Iterations: 744\n",
      "            Function evaluations: 744\n",
      "            Gradient evaluations: 744\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.12708415577512228\n",
      "            Iterations: 454\n",
      "            Function evaluations: 454\n",
      "            Gradient evaluations: 454\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.11330060480672918\n",
      "            Iterations: 666\n",
      "            Function evaluations: 667\n",
      "            Gradient evaluations: 666\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.25105084303532554\n",
      "            Iterations: 522\n",
      "            Function evaluations: 522\n",
      "            Gradient evaluations: 522\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.25164089884204777\n",
      "            Iterations: 473\n",
      "            Function evaluations: 473\n",
      "            Gradient evaluations: 473\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.250043846963396\n",
      "            Iterations: 518\n",
      "            Function evaluations: 519\n",
      "            Gradient evaluations: 518\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2278820259681039\n",
      "            Iterations: 435\n",
      "            Function evaluations: 436\n",
      "            Gradient evaluations: 435\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.22373223229228417\n",
      "            Iterations: 409\n",
      "            Function evaluations: 409\n",
      "            Gradient evaluations: 409\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1982980662477914\n",
      "            Iterations: 462\n",
      "            Function evaluations: 463\n",
      "            Gradient evaluations: 462\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18085545446955464\n",
      "            Iterations: 536\n",
      "            Function evaluations: 537\n",
      "            Gradient evaluations: 536\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3743980378096274\n",
      "            Iterations: 925\n",
      "            Function evaluations: 925\n",
      "            Gradient evaluations: 925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 10 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration limit reached    (Exit mode 9)\n",
      "            Current function value: 0.34563678061886477\n",
      "            Iterations: 1000\n",
      "            Function evaluations: 1000\n",
      "            Gradient evaluations: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 31 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.24743717411558722\n",
      "            Iterations: 850\n",
      "            Function evaluations: 850\n",
      "            Gradient evaluations: 850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 5 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2483750401167791\n",
      "            Iterations: 687\n",
      "            Function evaluations: 687\n",
      "            Gradient evaluations: 687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 2 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.19587095518518408\n",
      "            Iterations: 649\n",
      "            Function evaluations: 649\n",
      "            Gradient evaluations: 649\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.04475180312751818\n",
      "            Iterations: 578\n",
      "            Function evaluations: 578\n",
      "            Gradient evaluations: 578\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.04818590284663307\n",
      "            Iterations: 542\n",
      "            Function evaluations: 543\n",
      "            Gradient evaluations: 542\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2458307041100389\n",
      "            Iterations: 536\n",
      "            Function evaluations: 537\n",
      "            Gradient evaluations: 536\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.20940799464208348\n",
      "            Iterations: 554\n",
      "            Function evaluations: 554\n",
      "            Gradient evaluations: 554\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.17289348949855607\n",
      "            Iterations: 572\n",
      "            Function evaluations: 572\n",
      "            Gradient evaluations: 572\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.17077772680025416\n",
      "            Iterations: 84\n",
      "            Function evaluations: 84\n",
      "            Gradient evaluations: 84\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.17212269911983566\n",
      "            Iterations: 582\n",
      "            Function evaluations: 582\n",
      "            Gradient evaluations: 582\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.19119514368624124\n",
      "            Iterations: 365\n",
      "            Function evaluations: 365\n",
      "            Gradient evaluations: 365\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18506695040703558\n",
      "            Iterations: 354\n",
      "            Function evaluations: 354\n",
      "            Gradient evaluations: 354\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1772845221939026\n",
      "            Iterations: 492\n",
      "            Function evaluations: 492\n",
      "            Gradient evaluations: 492\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.20222908218482777\n",
      "            Iterations: 543\n",
      "            Function evaluations: 543\n",
      "            Gradient evaluations: 543\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.20834026244769263\n",
      "            Iterations: 477\n",
      "            Function evaluations: 477\n",
      "            Gradient evaluations: 477\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.26252805574826016\n",
      "            Iterations: 457\n",
      "            Function evaluations: 458\n",
      "            Gradient evaluations: 457\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2052437967609385\n",
      "            Iterations: 544\n",
      "            Function evaluations: 544\n",
      "            Gradient evaluations: 544\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1705821208920475\n",
      "            Iterations: 540\n",
      "            Function evaluations: 540\n",
      "            Gradient evaluations: 540\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.19457785934348715\n",
      "            Iterations: 467\n",
      "            Function evaluations: 467\n",
      "            Gradient evaluations: 467\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18960793973095663\n",
      "            Iterations: 394\n",
      "            Function evaluations: 394\n",
      "            Gradient evaluations: 394\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.21821182031237318\n",
      "            Iterations: 474\n",
      "            Function evaluations: 474\n",
      "            Gradient evaluations: 474\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2441314527111293\n",
      "            Iterations: 583\n",
      "            Function evaluations: 583\n",
      "            Gradient evaluations: 583\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.19481349725309985\n",
      "            Iterations: 552\n",
      "            Function evaluations: 552\n",
      "            Gradient evaluations: 552\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.15996719321489608\n",
      "            Iterations: 487\n",
      "            Function evaluations: 487\n",
      "            Gradient evaluations: 487\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.17489319611217538\n",
      "            Iterations: 540\n",
      "            Function evaluations: 541\n",
      "            Gradient evaluations: 540\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1822255580408159\n",
      "            Iterations: 408\n",
      "            Function evaluations: 408\n",
      "            Gradient evaluations: 408\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.16131521285242012\n",
      "            Iterations: 595\n",
      "            Function evaluations: 595\n",
      "            Gradient evaluations: 595\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.24343347430686824\n",
      "            Iterations: 496\n",
      "            Function evaluations: 497\n",
      "            Gradient evaluations: 496\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.15993603696656428\n",
      "            Iterations: 612\n",
      "            Function evaluations: 613\n",
      "            Gradient evaluations: 612\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.15816867055151684\n",
      "            Iterations: 584\n",
      "            Function evaluations: 585\n",
      "            Gradient evaluations: 584\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1908144284751494\n",
      "            Iterations: 561\n",
      "            Function evaluations: 562\n",
      "            Gradient evaluations: 561\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18531878608092028\n",
      "            Iterations: 546\n",
      "            Function evaluations: 546\n",
      "            Gradient evaluations: 546\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.17546688040512093\n",
      "            Iterations: 313\n",
      "            Function evaluations: 313\n",
      "            Gradient evaluations: 313\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.19254132007653135\n",
      "            Iterations: 521\n",
      "            Function evaluations: 522\n",
      "            Gradient evaluations: 521\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.23372576450710392\n",
      "            Iterations: 670\n",
      "            Function evaluations: 670\n",
      "            Gradient evaluations: 670\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.17350750696135966\n",
      "            Iterations: 624\n",
      "            Function evaluations: 624\n",
      "            Gradient evaluations: 624\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.11274347381079058\n",
      "            Iterations: 559\n",
      "            Function evaluations: 559\n",
      "            Gradient evaluations: 559\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.11232450336977351\n",
      "            Iterations: 561\n",
      "            Function evaluations: 562\n",
      "            Gradient evaluations: 561\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.11319327292280652\n",
      "            Iterations: 561\n",
      "            Function evaluations: 562\n",
      "            Gradient evaluations: 561\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.14967384317309063\n",
      "            Iterations: 468\n",
      "            Function evaluations: 468\n",
      "            Gradient evaluations: 468\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.22740080244364252\n",
      "            Iterations: 46\n",
      "            Function evaluations: 46\n",
      "            Gradient evaluations: 46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.13542451893655316\n",
      "            Iterations: 396\n",
      "            Function evaluations: 396\n",
      "            Gradient evaluations: 396\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.15958728353928534\n",
      "            Iterations: 609\n",
      "            Function evaluations: 609\n",
      "            Gradient evaluations: 609\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1120296231500823\n",
      "            Iterations: 383\n",
      "            Function evaluations: 383\n",
      "            Gradient evaluations: 383\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.11638323663699612\n",
      "            Iterations: 358\n",
      "            Function evaluations: 359\n",
      "            Gradient evaluations: 358\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.13261745762325633\n",
      "            Iterations: 165\n",
      "            Function evaluations: 165\n",
      "            Gradient evaluations: 165\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.15380446464970726\n",
      "            Iterations: 643\n",
      "            Function evaluations: 643\n",
      "            Gradient evaluations: 643\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.13347174311286164\n",
      "            Iterations: 581\n",
      "            Function evaluations: 581\n",
      "            Gradient evaluations: 581\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.12660188384873786\n",
      "            Iterations: 627\n",
      "            Function evaluations: 628\n",
      "            Gradient evaluations: 627\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.16709550550410196\n",
      "            Iterations: 458\n",
      "            Function evaluations: 458\n",
      "            Gradient evaluations: 458\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.12360507006070767\n",
      "            Iterations: 576\n",
      "            Function evaluations: 577\n",
      "            Gradient evaluations: 576\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2388478272703094\n",
      "            Iterations: 661\n",
      "            Function evaluations: 661\n",
      "            Gradient evaluations: 661\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.11114083135715866\n",
      "            Iterations: 679\n",
      "            Function evaluations: 679\n",
      "            Gradient evaluations: 679\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.08522694829218083\n",
      "            Iterations: 471\n",
      "            Function evaluations: 472\n",
      "            Gradient evaluations: 471\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.06583746504758711\n",
      "            Iterations: 246\n",
      "            Function evaluations: 246\n",
      "            Gradient evaluations: 246\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.09497426717698523\n",
      "            Iterations: 365\n",
      "            Function evaluations: 366\n",
      "            Gradient evaluations: 365\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.11326525213268232\n",
      "            Iterations: 698\n",
      "            Function evaluations: 698\n",
      "            Gradient evaluations: 698\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.13809401616250006\n",
      "            Iterations: 466\n",
      "            Function evaluations: 466\n",
      "            Gradient evaluations: 466\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.11513199490901607\n",
      "            Iterations: 676\n",
      "            Function evaluations: 676\n",
      "            Gradient evaluations: 676\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.24845091939408515\n",
      "            Iterations: 520\n",
      "            Function evaluations: 520\n",
      "            Gradient evaluations: 520\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.24737814463009433\n",
      "            Iterations: 477\n",
      "            Function evaluations: 477\n",
      "            Gradient evaluations: 477\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.24227878125070518\n",
      "            Iterations: 538\n",
      "            Function evaluations: 538\n",
      "            Gradient evaluations: 538\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2521594496182011\n",
      "            Iterations: 448\n",
      "            Function evaluations: 448\n",
      "            Gradient evaluations: 448\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.21601062719230446\n",
      "            Iterations: 413\n",
      "            Function evaluations: 413\n",
      "            Gradient evaluations: 413\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18849707847780195\n",
      "            Iterations: 452\n",
      "            Function evaluations: 452\n",
      "            Gradient evaluations: 452\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18333074117863787\n",
      "            Iterations: 544\n",
      "            Function evaluations: 545\n",
      "            Gradient evaluations: 544\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.37535122078286765\n",
      "            Iterations: 932\n",
      "            Function evaluations: 932\n",
      "            Gradient evaluations: 932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 12 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration limit reached    (Exit mode 9)\n",
      "            Current function value: 0.34425516050580157\n",
      "            Iterations: 1000\n",
      "            Function evaluations: 1000\n",
      "            Gradient evaluations: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 40 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2462737159238545\n",
      "            Iterations: 811\n",
      "            Function evaluations: 811\n",
      "            Gradient evaluations: 811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 8 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.24941919317922384\n",
      "            Iterations: 689\n",
      "            Function evaluations: 689\n",
      "            Gradient evaluations: 689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 3 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.20353636788413537\n",
      "            Iterations: 597\n",
      "            Function evaluations: 598\n",
      "            Gradient evaluations: 597\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.03992085604820312\n",
      "            Iterations: 601\n",
      "            Function evaluations: 601\n",
      "            Gradient evaluations: 601\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.04302950411462227\n",
      "            Iterations: 626\n",
      "            Function evaluations: 626\n",
      "            Gradient evaluations: 626\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.25013976859233017\n",
      "            Iterations: 527\n",
      "            Function evaluations: 528\n",
      "            Gradient evaluations: 527\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.21660317094436338\n",
      "            Iterations: 482\n",
      "            Function evaluations: 483\n",
      "            Gradient evaluations: 482\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1782037650191756\n",
      "            Iterations: 542\n",
      "            Function evaluations: 542\n",
      "            Gradient evaluations: 542\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.14026471265043516\n",
      "            Iterations: 97\n",
      "            Function evaluations: 97\n",
      "            Gradient evaluations: 97\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1788638377334395\n",
      "            Iterations: 548\n",
      "            Function evaluations: 548\n",
      "            Gradient evaluations: 548\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.21059083914694907\n",
      "            Iterations: 352\n",
      "            Function evaluations: 352\n",
      "            Gradient evaluations: 352\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.20751278652428612\n",
      "            Iterations: 332\n",
      "            Function evaluations: 332\n",
      "            Gradient evaluations: 332\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1913370096107807\n",
      "            Iterations: 501\n",
      "            Function evaluations: 501\n",
      "            Gradient evaluations: 501\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.21683614451270636\n",
      "            Iterations: 541\n",
      "            Function evaluations: 541\n",
      "            Gradient evaluations: 541\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2299929624139549\n",
      "            Iterations: 450\n",
      "            Function evaluations: 451\n",
      "            Gradient evaluations: 450\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.27081717056491494\n",
      "            Iterations: 452\n",
      "            Function evaluations: 452\n",
      "            Gradient evaluations: 452\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2075770992530593\n",
      "            Iterations: 555\n",
      "            Function evaluations: 555\n",
      "            Gradient evaluations: 555\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.20146371907946983\n",
      "            Iterations: 500\n",
      "            Function evaluations: 500\n",
      "            Gradient evaluations: 500\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.20242231325850293\n",
      "            Iterations: 478\n",
      "            Function evaluations: 478\n",
      "            Gradient evaluations: 478\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2080995050870287\n",
      "            Iterations: 428\n",
      "            Function evaluations: 428\n",
      "            Gradient evaluations: 428\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18862768330878998\n",
      "            Iterations: 499\n",
      "            Function evaluations: 499\n",
      "            Gradient evaluations: 499\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2474610297560774\n",
      "            Iterations: 549\n",
      "            Function evaluations: 549\n",
      "            Gradient evaluations: 549\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18506682218655696\n",
      "            Iterations: 570\n",
      "            Function evaluations: 570\n",
      "            Gradient evaluations: 570\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1462379934210955\n",
      "            Iterations: 550\n",
      "            Function evaluations: 550\n",
      "            Gradient evaluations: 550\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1773340398389309\n",
      "            Iterations: 552\n",
      "            Function evaluations: 552\n",
      "            Gradient evaluations: 552\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18710819208862375\n",
      "            Iterations: 443\n",
      "            Function evaluations: 444\n",
      "            Gradient evaluations: 443\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1596652861821543\n",
      "            Iterations: 574\n",
      "            Function evaluations: 575\n",
      "            Gradient evaluations: 574\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.25743697509647445\n",
      "            Iterations: 475\n",
      "            Function evaluations: 475\n",
      "            Gradient evaluations: 475\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.19278040355602497\n",
      "            Iterations: 555\n",
      "            Function evaluations: 555\n",
      "            Gradient evaluations: 555\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.15460928976352384\n",
      "            Iterations: 501\n",
      "            Function evaluations: 502\n",
      "            Gradient evaluations: 501\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.19923596931696136\n",
      "            Iterations: 585\n",
      "            Function evaluations: 585\n",
      "            Gradient evaluations: 585\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.16675889151228857\n",
      "            Iterations: 624\n",
      "            Function evaluations: 624\n",
      "            Gradient evaluations: 624\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18567101755164273\n",
      "            Iterations: 299\n",
      "            Function evaluations: 299\n",
      "            Gradient evaluations: 299\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.16738403939497049\n",
      "            Iterations: 542\n",
      "            Function evaluations: 542\n",
      "            Gradient evaluations: 542\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.23268236968537753\n",
      "            Iterations: 662\n",
      "            Function evaluations: 662\n",
      "            Gradient evaluations: 662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 4 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.19032644376764973\n",
      "            Iterations: 611\n",
      "            Function evaluations: 611\n",
      "            Gradient evaluations: 611\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.12936834525954322\n",
      "            Iterations: 561\n",
      "            Function evaluations: 562\n",
      "            Gradient evaluations: 561\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.13632288804648224\n",
      "            Iterations: 517\n",
      "            Function evaluations: 517\n",
      "            Gradient evaluations: 517\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.13688556492643902\n",
      "            Iterations: 525\n",
      "            Function evaluations: 525\n",
      "            Gradient evaluations: 525\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.12926335866857966\n",
      "            Iterations: 408\n",
      "            Function evaluations: 408\n",
      "            Gradient evaluations: 408\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.22740080244364252\n",
      "            Iterations: 46\n",
      "            Function evaluations: 46\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.10145585837321092\n",
      "            Iterations: 314\n",
      "            Function evaluations: 314\n",
      "            Gradient evaluations: 314\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.17677339885782675\n",
      "            Iterations: 600\n",
      "            Function evaluations: 600\n",
      "            Gradient evaluations: 600\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.12406855460217453\n",
      "            Iterations: 378\n",
      "            Function evaluations: 378\n",
      "            Gradient evaluations: 378\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.15226659100525863\n",
      "            Iterations: 357\n",
      "            Function evaluations: 358\n",
      "            Gradient evaluations: 357\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1265838356154605\n",
      "            Iterations: 194\n",
      "            Function evaluations: 195\n",
      "            Gradient evaluations: 194\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.17078615829747307\n",
      "            Iterations: 588\n",
      "            Function evaluations: 589\n",
      "            Gradient evaluations: 588\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.14226217418409043\n",
      "            Iterations: 614\n",
      "            Function evaluations: 614\n",
      "            Gradient evaluations: 614\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.15420828317625568\n",
      "            Iterations: 627\n",
      "            Function evaluations: 627\n",
      "            Gradient evaluations: 627\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1687485879739113\n",
      "            Iterations: 475\n",
      "            Function evaluations: 476\n",
      "            Gradient evaluations: 475\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.15420261407683966\n",
      "            Iterations: 576\n",
      "            Function evaluations: 576\n",
      "            Gradient evaluations: 576\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.22771108384651098\n",
      "            Iterations: 656\n",
      "            Function evaluations: 656\n",
      "            Gradient evaluations: 656\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.09304933354587952\n",
      "            Iterations: 714\n",
      "            Function evaluations: 714\n",
      "            Gradient evaluations: 714\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.0909463960921564\n",
      "            Iterations: 500\n",
      "            Function evaluations: 500\n",
      "            Gradient evaluations: 500\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.06705359417017491\n",
      "            Iterations: 249\n",
      "            Function evaluations: 249\n",
      "            Gradient evaluations: 249\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.10593943356946994\n",
      "            Iterations: 436\n",
      "            Function evaluations: 436\n",
      "            Gradient evaluations: 436\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.10261294061204379\n",
      "            Iterations: 710\n",
      "            Function evaluations: 710\n",
      "            Gradient evaluations: 710\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1435513567808927\n",
      "            Iterations: 506\n",
      "            Function evaluations: 507\n",
      "            Gradient evaluations: 506\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.10496727629964103\n",
      "            Iterations: 699\n",
      "            Function evaluations: 700\n",
      "            Gradient evaluations: 699\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.25131871243013004\n",
      "            Iterations: 504\n",
      "            Function evaluations: 504\n",
      "            Gradient evaluations: 504\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.250013901563111\n",
      "            Iterations: 488\n",
      "            Function evaluations: 488\n",
      "            Gradient evaluations: 488\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.23669896784634326\n",
      "            Iterations: 500\n",
      "            Function evaluations: 500\n",
      "            Gradient evaluations: 500\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2444324969089419\n",
      "            Iterations: 449\n",
      "            Function evaluations: 449\n",
      "            Gradient evaluations: 449\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2170167972986219\n",
      "            Iterations: 402\n",
      "            Function evaluations: 403\n",
      "            Gradient evaluations: 402\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1785110962426602\n",
      "            Iterations: 445\n",
      "            Function evaluations: 445\n",
      "            Gradient evaluations: 445\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.17347392423882452\n",
      "            Iterations: 572\n",
      "            Function evaluations: 572\n",
      "            Gradient evaluations: 572\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.37249662049317467\n",
      "            Iterations: 937\n",
      "            Function evaluations: 937\n",
      "            Gradient evaluations: 937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 12 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration limit reached    (Exit mode 9)\n",
      "            Current function value: 0.3460141370281328\n",
      "            Iterations: 1000\n",
      "            Function evaluations: 1000\n",
      "            Gradient evaluations: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 31 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.24951599237047695\n",
      "            Iterations: 811\n",
      "            Function evaluations: 811\n",
      "            Gradient evaluations: 811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 13 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.25326894960841295\n",
      "            Iterations: 672\n",
      "            Function evaluations: 672\n",
      "            Gradient evaluations: 672\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.20437276258809361\n",
      "            Iterations: 593\n",
      "            Function evaluations: 594\n",
      "            Gradient evaluations: 593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 2 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.040779924743033866\n",
      "            Iterations: 584\n",
      "            Function evaluations: 585\n",
      "            Gradient evaluations: 584\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.043757033020070175\n",
      "            Iterations: 567\n",
      "            Function evaluations: 567\n",
      "            Gradient evaluations: 567\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.25920738725915543\n",
      "            Iterations: 506\n",
      "            Function evaluations: 507\n",
      "            Gradient evaluations: 506\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.22631845381874893\n",
      "            Iterations: 488\n",
      "            Function evaluations: 488\n",
      "            Gradient evaluations: 488\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18296673607197056\n",
      "            Iterations: 568\n",
      "            Function evaluations: 568\n",
      "            Gradient evaluations: 568\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.15618114879547157\n",
      "            Iterations: 79\n",
      "            Function evaluations: 79\n",
      "            Gradient evaluations: 79\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18733060006857216\n",
      "            Iterations: 545\n",
      "            Function evaluations: 545\n",
      "            Gradient evaluations: 545\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.22894154296534092\n",
      "            Iterations: 374\n",
      "            Function evaluations: 375\n",
      "            Gradient evaluations: 374\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.23010645555599935\n",
      "            Iterations: 300\n",
      "            Function evaluations: 301\n",
      "            Gradient evaluations: 300\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18744299655244043\n",
      "            Iterations: 466\n",
      "            Function evaluations: 467\n",
      "            Gradient evaluations: 466\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.22181065799022381\n",
      "            Iterations: 532\n",
      "            Function evaluations: 532\n",
      "            Gradient evaluations: 532\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.23205045504869962\n",
      "            Iterations: 438\n",
      "            Function evaluations: 438\n",
      "            Gradient evaluations: 438\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2654747335741704\n",
      "            Iterations: 469\n",
      "            Function evaluations: 470\n",
      "            Gradient evaluations: 469\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.22034653028941262\n",
      "            Iterations: 533\n",
      "            Function evaluations: 533\n",
      "            Gradient evaluations: 533\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.19463666765039211\n",
      "            Iterations: 528\n",
      "            Function evaluations: 529\n",
      "            Gradient evaluations: 528\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.190787618936547\n",
      "            Iterations: 534\n",
      "            Function evaluations: 535\n",
      "            Gradient evaluations: 534\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.21085299098483434\n",
      "            Iterations: 489\n",
      "            Function evaluations: 489\n",
      "            Gradient evaluations: 489\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2105012208927079\n",
      "            Iterations: 488\n",
      "            Function evaluations: 489\n",
      "            Gradient evaluations: 488\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.24762991696768044\n",
      "            Iterations: 568\n",
      "            Function evaluations: 568\n",
      "            Gradient evaluations: 568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.17508020881969005\n",
      "            Iterations: 579\n",
      "            Function evaluations: 579\n",
      "            Gradient evaluations: 579\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.13633811254954287\n",
      "            Iterations: 480\n",
      "            Function evaluations: 481\n",
      "            Gradient evaluations: 480\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.16820433181353062\n",
      "            Iterations: 573\n",
      "            Function evaluations: 573\n",
      "            Gradient evaluations: 573\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18132275295803724\n",
      "            Iterations: 463\n",
      "            Function evaluations: 464\n",
      "            Gradient evaluations: 463\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1617789694221054\n",
      "            Iterations: 556\n",
      "            Function evaluations: 556\n",
      "            Gradient evaluations: 556\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2612123971146962\n",
      "            Iterations: 509\n",
      "            Function evaluations: 509\n",
      "            Gradient evaluations: 509\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2025273314144545\n",
      "            Iterations: 539\n",
      "            Function evaluations: 539\n",
      "            Gradient evaluations: 539\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.16350901299329962\n",
      "            Iterations: 539\n",
      "            Function evaluations: 540\n",
      "            Gradient evaluations: 539\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.20119288964554893\n",
      "            Iterations: 539\n",
      "            Function evaluations: 539\n",
      "            Gradient evaluations: 539\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1763714884371797\n",
      "            Iterations: 542\n",
      "            Function evaluations: 542\n",
      "            Gradient evaluations: 542\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18932032701688176\n",
      "            Iterations: 285\n",
      "            Function evaluations: 286\n",
      "            Gradient evaluations: 285\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.186065306039954\n",
      "            Iterations: 529\n",
      "            Function evaluations: 529\n",
      "            Gradient evaluations: 529\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.23234084990230197\n",
      "            Iterations: 663\n",
      "            Function evaluations: 663\n",
      "            Gradient evaluations: 663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.19313191530414583\n",
      "            Iterations: 620\n",
      "            Function evaluations: 621\n",
      "            Gradient evaluations: 620\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.12386654255435794\n",
      "            Iterations: 542\n",
      "            Function evaluations: 543\n",
      "            Gradient evaluations: 542\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.12883249851691636\n",
      "            Iterations: 543\n",
      "            Function evaluations: 544\n",
      "            Gradient evaluations: 543\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.12999226279387105\n",
      "            Iterations: 566\n",
      "            Function evaluations: 566\n",
      "            Gradient evaluations: 566\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.15673189388460174\n",
      "            Iterations: 481\n",
      "            Function evaluations: 481\n",
      "            Gradient evaluations: 481\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.22740080244364252\n",
      "            Iterations: 46\n",
      "            Function evaluations: 46\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.14046127439517758\n",
      "            Iterations: 419\n",
      "            Function evaluations: 419\n",
      "            Gradient evaluations: 419\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18893048599317727\n",
      "            Iterations: 560\n",
      "            Function evaluations: 560\n",
      "            Gradient evaluations: 560\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.12698258529503675\n",
      "            Iterations: 440\n",
      "            Function evaluations: 440\n",
      "            Gradient evaluations: 440\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.14564799855910887\n",
      "            Iterations: 410\n",
      "            Function evaluations: 410\n",
      "            Gradient evaluations: 410\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.12136239010501818\n",
      "            Iterations: 184\n",
      "            Function evaluations: 184\n",
      "            Gradient evaluations: 184\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1881567161640032\n",
      "            Iterations: 571\n",
      "            Function evaluations: 571\n",
      "            Gradient evaluations: 571\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.15162732747716487\n",
      "            Iterations: 550\n",
      "            Function evaluations: 551\n",
      "            Gradient evaluations: 550\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.15172411408773873\n",
      "            Iterations: 602\n",
      "            Function evaluations: 602\n",
      "            Gradient evaluations: 602\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.15374331639227287\n",
      "            Iterations: 456\n",
      "            Function evaluations: 457\n",
      "            Gradient evaluations: 456\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.15024235798299032\n",
      "            Iterations: 614\n",
      "            Function evaluations: 614\n",
      "            Gradient evaluations: 614\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.23025273953010572\n",
      "            Iterations: 675\n",
      "            Function evaluations: 675\n",
      "            Gradient evaluations: 675\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1042315657810314\n",
      "            Iterations: 689\n",
      "            Function evaluations: 690\n",
      "            Gradient evaluations: 689\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.08600775099424804\n",
      "            Iterations: 470\n",
      "            Function evaluations: 470\n",
      "            Gradient evaluations: 470\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.06663169867649982\n",
      "            Iterations: 247\n",
      "            Function evaluations: 247\n",
      "            Gradient evaluations: 247\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.10528703210594656\n",
      "            Iterations: 408\n",
      "            Function evaluations: 408\n",
      "            Gradient evaluations: 408\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.11202410276608579\n",
      "            Iterations: 696\n",
      "            Function evaluations: 697\n",
      "            Gradient evaluations: 696\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.13500202530288644\n",
      "            Iterations: 506\n",
      "            Function evaluations: 506\n",
      "            Gradient evaluations: 506\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.11122970984073732\n",
      "            Iterations: 662\n",
      "            Function evaluations: 663\n",
      "            Gradient evaluations: 662\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.24728727282623797\n",
      "            Iterations: 515\n",
      "            Function evaluations: 515\n",
      "            Gradient evaluations: 515\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.24890011570671638\n",
      "            Iterations: 491\n",
      "            Function evaluations: 491\n",
      "            Gradient evaluations: 491\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.24527961887220515\n",
      "            Iterations: 513\n",
      "            Function evaluations: 514\n",
      "            Gradient evaluations: 513\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.25354638974571747\n",
      "            Iterations: 435\n",
      "            Function evaluations: 435\n",
      "            Gradient evaluations: 435\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.22401195286187786\n",
      "            Iterations: 393\n",
      "            Function evaluations: 393\n",
      "            Gradient evaluations: 393\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18727733348263487\n",
      "            Iterations: 444\n",
      "            Function evaluations: 444\n",
      "            Gradient evaluations: 444\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1624224981558771\n",
      "            Iterations: 574\n",
      "            Function evaluations: 575\n",
      "            Gradient evaluations: 574\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3727995233173051\n",
      "            Iterations: 915\n",
      "            Function evaluations: 916\n",
      "            Gradient evaluations: 915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 20 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration limit reached    (Exit mode 9)\n",
      "            Current function value: 0.34448319573748715\n",
      "            Iterations: 1000\n",
      "            Function evaluations: 1000\n",
      "            Gradient evaluations: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 32 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.24394511561966548\n",
      "            Iterations: 856\n",
      "            Function evaluations: 856\n",
      "            Gradient evaluations: 856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 5 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.24673123789651372\n",
      "            Iterations: 664\n",
      "            Function evaluations: 664\n",
      "            Gradient evaluations: 664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 6 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.20747078246395445\n",
      "            Iterations: 586\n",
      "            Function evaluations: 586\n",
      "            Gradient evaluations: 586\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.04727653025824971\n",
      "            Iterations: 575\n",
      "            Function evaluations: 575\n",
      "            Gradient evaluations: 575\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.05062655278348556\n",
      "            Iterations: 549\n",
      "            Function evaluations: 550\n",
      "            Gradient evaluations: 549\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.24807168743179517\n",
      "            Iterations: 522\n",
      "            Function evaluations: 523\n",
      "            Gradient evaluations: 522\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.22224419694101483\n",
      "            Iterations: 504\n",
      "            Function evaluations: 505\n",
      "            Gradient evaluations: 504\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18707670691854378\n",
      "            Iterations: 535\n",
      "            Function evaluations: 535\n",
      "            Gradient evaluations: 535\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.15838706266859603\n",
      "            Iterations: 92\n",
      "            Function evaluations: 92\n",
      "            Gradient evaluations: 92\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1817816261086131\n",
      "            Iterations: 524\n",
      "            Function evaluations: 524\n",
      "            Gradient evaluations: 524\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.20998732548940696\n",
      "            Iterations: 350\n",
      "            Function evaluations: 350\n",
      "            Gradient evaluations: 350\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.17586610617194606\n",
      "            Iterations: 340\n",
      "            Function evaluations: 341\n",
      "            Gradient evaluations: 340\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1963105090559169\n",
      "            Iterations: 454\n",
      "            Function evaluations: 454\n",
      "            Gradient evaluations: 454\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.20875497956002959\n",
      "            Iterations: 556\n",
      "            Function evaluations: 556\n",
      "            Gradient evaluations: 556\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.22737200675194658\n",
      "            Iterations: 476\n",
      "            Function evaluations: 476\n",
      "            Gradient evaluations: 476\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2509089659478354\n",
      "            Iterations: 483\n",
      "            Function evaluations: 483\n",
      "            Gradient evaluations: 483\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.20015024603256276\n",
      "            Iterations: 578\n",
      "            Function evaluations: 578\n",
      "            Gradient evaluations: 578\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.17795005956111182\n",
      "            Iterations: 518\n",
      "            Function evaluations: 518\n",
      "            Gradient evaluations: 518\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18293537824870537\n",
      "            Iterations: 500\n",
      "            Function evaluations: 500\n",
      "            Gradient evaluations: 500\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2030282164963501\n",
      "            Iterations: 406\n",
      "            Function evaluations: 406\n",
      "            Gradient evaluations: 406\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2034691381293221\n",
      "            Iterations: 495\n",
      "            Function evaluations: 496\n",
      "            Gradient evaluations: 495\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2507936526492363\n",
      "            Iterations: 553\n",
      "            Function evaluations: 553\n",
      "            Gradient evaluations: 553\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.19959616045910705\n",
      "            Iterations: 522\n",
      "            Function evaluations: 523\n",
      "            Gradient evaluations: 522\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.16087907831617998\n",
      "            Iterations: 524\n",
      "            Function evaluations: 525\n",
      "            Gradient evaluations: 524\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18276839896762462\n",
      "            Iterations: 529\n",
      "            Function evaluations: 529\n",
      "            Gradient evaluations: 529\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18668977392485658\n",
      "            Iterations: 407\n",
      "            Function evaluations: 407\n",
      "            Gradient evaluations: 407\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.17231670046155403\n",
      "            Iterations: 580\n",
      "            Function evaluations: 581\n",
      "            Gradient evaluations: 580\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.25300947083418746\n",
      "            Iterations: 497\n",
      "            Function evaluations: 497\n",
      "            Gradient evaluations: 497\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.19017860199381395\n",
      "            Iterations: 551\n",
      "            Function evaluations: 551\n",
      "            Gradient evaluations: 551\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.15557449454804334\n",
      "            Iterations: 527\n",
      "            Function evaluations: 528\n",
      "            Gradient evaluations: 527\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.18994957774283117\n",
      "            Iterations: 522\n",
      "            Function evaluations: 523\n",
      "            Gradient evaluations: 522\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1621850024959376\n",
      "            Iterations: 509\n",
      "            Function evaluations: 509\n",
      "            Gradient evaluations: 509\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.181033415596963\n",
      "            Iterations: 269\n",
      "            Function evaluations: 269\n",
      "            Gradient evaluations: 269\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.16322120186662736\n",
      "            Iterations: 505\n",
      "            Function evaluations: 505\n",
      "            Gradient evaluations: 505\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2277828417649004\n",
      "            Iterations: 672\n",
      "            Function evaluations: 672\n",
      "            Gradient evaluations: 672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 3 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1940060627744666\n",
      "            Iterations: 607\n",
      "            Function evaluations: 607\n",
      "            Gradient evaluations: 607\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.12377518969294088\n",
      "            Iterations: 550\n",
      "            Function evaluations: 551\n",
      "            Gradient evaluations: 550\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.12675918926668514\n",
      "            Iterations: 509\n",
      "            Function evaluations: 509\n",
      "            Gradient evaluations: 509\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.12787875128488652\n",
      "            Iterations: 527\n",
      "            Function evaluations: 528\n",
      "            Gradient evaluations: 527\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.14389796673406652\n",
      "            Iterations: 398\n",
      "            Function evaluations: 398\n",
      "            Gradient evaluations: 398\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.22740080244364252\n",
      "            Iterations: 46\n",
      "            Function evaluations: 46\n",
      "            Gradient evaluations: 46\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.12199182026520539\n",
      "            Iterations: 369\n",
      "            Function evaluations: 370\n",
      "            Gradient evaluations: 369\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.17999726401194355\n",
      "            Iterations: 557\n",
      "            Function evaluations: 558\n",
      "            Gradient evaluations: 557\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1376098605857513\n",
      "            Iterations: 409\n",
      "            Function evaluations: 409\n",
      "            Gradient evaluations: 409\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.15622383763171666\n",
      "            Iterations: 377\n",
      "            Function evaluations: 377\n",
      "            Gradient evaluations: 377\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.13509889374083356\n",
      "            Iterations: 188\n",
      "            Function evaluations: 188\n",
      "            Gradient evaluations: 188\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.17167273274364694\n",
      "            Iterations: 572\n",
      "            Function evaluations: 573\n",
      "            Gradient evaluations: 572\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.13754983163903042\n",
      "            Iterations: 547\n",
      "            Function evaluations: 547\n",
      "            Gradient evaluations: 547\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.14341784781639288\n",
      "            Iterations: 617\n",
      "            Function evaluations: 617\n",
      "            Gradient evaluations: 617\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.17165199263210718\n",
      "            Iterations: 503\n",
      "            Function evaluations: 503\n",
      "            Gradient evaluations: 503\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1459909886627485\n",
      "            Iterations: 571\n",
      "            Function evaluations: 572\n",
      "            Gradient evaluations: 571\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.21587783162305926\n",
      "            Iterations: 691\n",
      "            Function evaluations: 692\n",
      "            Gradient evaluations: 691\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.10931251049244496\n",
      "            Iterations: 707\n",
      "            Function evaluations: 707\n",
      "            Gradient evaluations: 707\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.09190483598855054\n",
      "            Iterations: 516\n",
      "            Function evaluations: 516\n",
      "            Gradient evaluations: 516\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.06417866980748729\n",
      "            Iterations: 221\n",
      "            Function evaluations: 221\n",
      "            Gradient evaluations: 221\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1057627395564702\n",
      "            Iterations: 400\n",
      "            Function evaluations: 401\n",
      "            Gradient evaluations: 400\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.11050252081076825\n",
      "            Iterations: 742\n",
      "            Function evaluations: 742\n",
      "            Gradient evaluations: 742\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1348989909092411\n",
      "            Iterations: 491\n",
      "            Function evaluations: 492\n",
      "            Gradient evaluations: 491\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.10851352366157013\n",
      "            Iterations: 640\n",
      "            Function evaluations: 640\n",
      "            Gradient evaluations: 640\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.21135534331920086\n",
      "            Iterations: 563\n",
      "            Function evaluations: 563\n",
      "            Gradient evaluations: 563\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.20292146959028\n",
      "            Iterations: 601\n",
      "            Function evaluations: 602\n",
      "            Gradient evaluations: 601\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.19434631151814086\n",
      "            Iterations: 567\n",
      "            Function evaluations: 567\n",
      "            Gradient evaluations: 567\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.1974819445587826\n",
      "            Iterations: 465\n",
      "            Function evaluations: 465\n",
      "            Gradient evaluations: 465\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.23611284270942276\n",
      "            Iterations: 422\n",
      "            Function evaluations: 422\n",
      "            Gradient evaluations: 422\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2137623500120408\n",
      "            Iterations: 481\n",
      "            Function evaluations: 481\n",
      "            Gradient evaluations: 481\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.17020898003939539\n",
      "            Iterations: 539\n",
      "            Function evaluations: 539\n",
      "            Gradient evaluations: 539\n",
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3745345812296388\n",
      "            Iterations: 913\n",
      "            Function evaluations: 913\n",
      "            Gradient evaluations: 913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 20 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 7 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 9 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 9 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 6 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 8 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 8 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration limit reached    (Exit mode 9)\n",
      "            Current function value: 0.3470193541205792\n",
      "            Iterations: 1000\n",
      "            Function evaluations: 1000\n",
      "            Gradient evaluations: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 38 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.24695215281032604\n",
      "            Iterations: 824\n",
      "            Function evaluations: 824\n",
      "            Gradient evaluations: 824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 5 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3751436024476856\n",
      "            Iterations: 930\n",
      "            Function evaluations: 930\n",
      "            Gradient evaluations: 930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 17 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration limit reached    (Exit mode 9)\n",
      "            Current function value: 0.34486921118246455\n",
      "            Iterations: 1000\n",
      "            Function evaluations: 1000\n",
      "            Gradient evaluations: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 30 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.24772917744453768\n",
      "            Iterations: 832\n",
      "            Function evaluations: 832\n",
      "            Gradient evaluations: 832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 3 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3737674274630739\n",
      "            Iterations: 934\n",
      "            Function evaluations: 935\n",
      "            Gradient evaluations: 934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 19 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration limit reached    (Exit mode 9)\n",
      "            Current function value: 0.34357459847165295\n",
      "            Iterations: 1000\n",
      "            Function evaluations: 1000\n",
      "            Gradient evaluations: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 34 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.24701012867469302\n",
      "            Iterations: 831\n",
      "            Function evaluations: 831\n",
      "            Gradient evaluations: 831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 12 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3714536351871702\n",
      "            Iterations: 900\n",
      "            Function evaluations: 900\n",
      "            Gradient evaluations: 900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 18 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration limit reached    (Exit mode 9)\n",
      "            Current function value: 0.34818962379104906\n",
      "            Iterations: 1000\n",
      "            Function evaluations: 1000\n",
      "            Gradient evaluations: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 33 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.24607949025756964\n",
      "            Iterations: 833\n",
      "            Function evaluations: 833\n",
      "            Gradient evaluations: 833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 9 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.37647223824755416\n",
      "            Iterations: 907\n",
      "            Function evaluations: 908\n",
      "            Gradient evaluations: 907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 21 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration limit reached    (Exit mode 9)\n",
      "            Current function value: 0.3483061216568116\n",
      "            Iterations: 1000\n",
      "            Function evaluations: 1000\n",
      "            Gradient evaluations: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 38 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.24723381680174064\n",
      "            Iterations: 869\n",
      "            Function evaluations: 869\n",
      "            Gradient evaluations: 869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 13 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.37772350606226335\n",
      "            Iterations: 939\n",
      "            Function evaluations: 939\n",
      "            Gradient evaluations: 939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 16 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration limit reached    (Exit mode 9)\n",
      "            Current function value: 0.34463305083844337\n",
      "            Iterations: 1000\n",
      "            Function evaluations: 1000\n",
      "            Gradient evaluations: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 44 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2480717101897493\n",
      "            Iterations: 823\n",
      "            Function evaluations: 823\n",
      "            Gradient evaluations: 823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 10 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3731460480113197\n",
      "            Iterations: 931\n",
      "            Function evaluations: 932\n",
      "            Gradient evaluations: 931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 17 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration limit reached    (Exit mode 9)\n",
      "            Current function value: 0.34420993066017885\n",
      "            Iterations: 1000\n",
      "            Function evaluations: 1000\n",
      "            Gradient evaluations: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 41 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.24776887138773843\n",
      "            Iterations: 830\n",
      "            Function evaluations: 830\n",
      "            Gradient evaluations: 830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 11 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.37268055627373803\n",
      "            Iterations: 910\n",
      "            Function evaluations: 911\n",
      "            Gradient evaluations: 910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 15 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration limit reached    (Exit mode 9)\n",
      "            Current function value: 0.34484563091883114\n",
      "            Iterations: 1000\n",
      "            Function evaluations: 1000\n",
      "            Gradient evaluations: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 33 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.24709822801212125\n",
      "            Iterations: 835\n",
      "            Function evaluations: 836\n",
      "            Gradient evaluations: 835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 11 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3733131043110764\n",
      "            Iterations: 917\n",
      "            Function evaluations: 917\n",
      "            Gradient evaluations: 917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 24 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration limit reached    (Exit mode 9)\n",
      "            Current function value: 0.3423973437089993\n",
      "            Iterations: 1000\n",
      "            Function evaluations: 1000\n",
      "            Gradient evaluations: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 38 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.2436902062040401\n",
      "            Iterations: 845\n",
      "            Function evaluations: 845\n",
      "            Gradient evaluations: 845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 1 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3684704294219454\n",
      "            Iterations: 934\n",
      "            Function evaluations: 934\n",
      "            Gradient evaluations: 934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 16 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration limit reached    (Exit mode 9)\n",
      "            Current function value: 0.34730076479059796\n",
      "            Iterations: 1000\n",
      "            Function evaluations: 1000\n",
      "            Gradient evaluations: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 37 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.25159472336132166\n",
      "            Iterations: 825\n",
      "            Function evaluations: 825\n",
      "            Gradient evaluations: 825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 7 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.3736655379313934\n",
      "            Iterations: 913\n",
      "            Function evaluations: 913\n",
      "            Gradient evaluations: 913\n",
      "Test scores: {<PRUNING.DistanceDecisionTree object at 0x14e4f5e1f700>: 0.7934210526315788, <PRUNING.DistanceDecisionTree object at 0x14e4f5e1d9c0>: 0.8261184210526314}\n",
      "cv_scores: {<PRUNING.DistanceDecisionTree object at 0x14e4f5e1f700>: 0.9038846679839937, <PRUNING.DistanceDecisionTree object at 0x14e4f5e1d9c0>: 0.9291041542320417}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 15 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration limit reached    (Exit mode 9)\n",
      "            Current function value: 0.3457309165014807\n",
      "            Iterations: 1000\n",
      "            Function evaluations: 1000\n",
      "            Gradient evaluations: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 32 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.24801415681363692\n",
      "            Iterations: 824\n",
      "            Function evaluations: 825\n",
      "            Gradient evaluations: 824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 22 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.37401111757169087\n",
      "            Iterations: 955\n",
      "            Function evaluations: 955\n",
      "            Gradient evaluations: 955\n",
      "\n",
      "Final result whole training data: 0.9349593686009972 Final result whole test data: 0.8269736842105264\n",
      "Best Depth: <PRUNING.DistanceDecisionTree object at 0x14e4f5e1d9c0> Train: 0.9296357960457856 Test: 0.8261184210526314 Val: 0.9291041542320417\n",
      "Models "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:71: ConvergenceWarning: QC check did not pass for 18 out of 73 parameters\n",
      "Try increasing solver accuracy or number of iterations, decreasing alpha, or switch solvers\n",
      "  warnings.warn(message, ConvergenceWarning)\n",
      "/storage/home/eak5582/.local/lib/python3.10/site-packages/statsmodels/base/l1_solvers_common.py:144: ConvergenceWarning: Could not trim params automatically due to failed QC check. Trimming using trim_mode == 'size' will still work.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RandomForestClassifier(), GradientBoostingClassifier(), MLPClassifier(alpha=1e-05, hidden_layer_sizes=(16, 2), random_state=1,\n",
      "              solver='lbfgs'), RandomForest(ntree=500, replace=True, sampsize=310, sample_fraction=None, mtry=24, nodesize_spl=5, nodesize_avg=5, nodesize_strict_spl=1, nodesize_strict_avg=1, min_split_gain=0.0, max_depth=156, interaction_depth=156, splitratio=1.0, oob_honest=False, double_bootstrap=False, seed=251, verbose=False, nthread=0, splitrule='variance', middle_split=False, max_obs=310, linear=False, min_trees_per_fold=0, fold_size=1, monotone_avg=False, overfit_penalty=1, scale=False, double_tree=False, na_direction=False, forest=<capsule object NULL at 0x14e4dec99dd0>, dataframe=<capsule object NULL at 0x14e4dec99d10>, processed_dta=ProcessedDta(processed_x=           0         1    2         3         4    5    6         7   \\\n",
      "0    0.826667  0.209269  1.0  0.689320  0.742586  0.0  0.0  0.657895   \n",
      "1    0.773333  0.012860  1.0  0.660194  0.256570  0.0  0.0  0.263158   \n",
      "2    0.826667  0.004509  1.0  0.320388  0.159683  0.0  0.0  0.263158   \n",
      "3    0.866667  0.068058  1.0  0.262136  0.669657  0.0  0.0  0.921053   \n",
      "4    0.546667  0.008852  1.0  0.524272  0.000844  0.0  0.0  0.421053   \n",
      "..        ...       ...  ...       ...       ...  ...  ...       ...   \n",
      "305  0.666667  0.005595  1.0  0.640777  0.335726  0.0  0.0  0.421053   \n",
      "306  0.306667  0.050104  1.0  0.077670  0.730554  0.0  0.0  0.500000   \n",
      "307  0.506667  0.247766  1.0  0.825243  0.071979  0.0  0.0  0.236842   \n",
      "308  0.320000  0.010438  1.0  0.000000  0.085383  0.0  0.0  0.868421   \n",
      "309  0.546667  0.006430  1.0  0.446602  0.482427  0.0  0.0  0.368421   \n",
      "\n",
      "           8         9   ...   63   64   65   66   67   68   69   70   71   72  \n",
      "0    0.000000  0.617647  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1    0.111111  0.294118  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "2    0.000000  0.294118  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "3    0.000000  0.911765  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.000000  0.382353  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "..        ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
      "305  0.000000  0.382353  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "306  0.000000  0.470588  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "307  0.000000  0.264706  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "308  0.000000  0.852941  ...  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "309  0.000000  0.352941  ...  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[310 rows x 73 columns], y=array([0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "       0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1.,\n",
      "       0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1.,\n",
      "       1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1.,\n",
      "       0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
      "       0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
      "       1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1.,\n",
      "       0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "       0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1.,\n",
      "       0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1.,\n",
      "       1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
      "       0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
      "       1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
      "       0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
      "       0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0.,\n",
      "       0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
      "       1., 0., 1., 0.]), categorical_feature_cols=array([], dtype=int64), categorical_feature_mapping=[], feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), deep_feature_weights=array([0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863, 0.01369863, 0.01369863,\n",
      "       0.01369863, 0.01369863, 0.01369863]), deep_feature_weights_variables=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), observation_weights=array([0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581,\n",
      "       0.00322581, 0.00322581, 0.00322581, 0.00322581, 0.00322581]), monotonic_constraints=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0], dtype=int32), linear_feature_cols=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
      "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
      "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n",
      "       68, 69, 70, 71, 72], dtype=uint64), group_memberships=array(0), col_means=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), col_sd=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0.]), has_nas=False, na_direction=False, n_observations=310, num_columns=73, feat_names=None), saved_forest=[])]\n"
     ]
    }
   ],
   "source": [
    "ALL_RESULTS_TRAIN = []\n",
    "ALL_RESULTS_TEST = []\n",
    "ALL_RESULTS_VAL = []\n",
    "ALL_RESULTS_LEAVES = []\n",
    "\n",
    "for attempt in range(1):\n",
    "    # random split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=None)\n",
    "    \n",
    "    # training 4 bb models\n",
    "    clf_1 = RandomForestClassifier()\n",
    "    clf_1.fit(X_train, y_train)\n",
    "    print('clf1 auc train:', roc_auc_score(clf_1.predict(X_train), y_train))\n",
    "    print('clf1 auc test:', roc_auc_score(clf_1.predict(X_test), y_test))\n",
    "    clf_2 = GradientBoostingClassifier()\n",
    "    clf_2.fit(X_train, y_train)\n",
    "    print('clf2 auc train:', roc_auc_score(clf_2.predict(X_train), y_train))\n",
    "    print('clf2 auc test:', roc_auc_score(clf_2.predict(X_test), y_test))\n",
    "    clf_3 = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(16, 2), random_state=1)\n",
    "    clf_3.fit(X_train, y_train)\n",
    "    print('clf3 auc train:', roc_auc_score(clf_3.predict(X_train), y_train))\n",
    "    print('clf3 auc test:', roc_auc_score(clf_3.predict(X_test), y_test))\n",
    "    clf_4 = RandomForest()\n",
    "    clf_4.fit(X_train, y_train)\n",
    "    forest_preds_train = clf_4.predict(X_train)\n",
    "    forest_preds = clf_4.predict(X_test)\n",
    "    print(\"Train\", roc_auc_score(y_train, forest_preds_train))\n",
    "    print(\"Test\", roc_auc_score(y_test, forest_preds))\n",
    "    \n",
    "    my_clfs = [clf_1, clf_2, clf_3, clf_4]\n",
    "#     my_clfs = [clf_2]\n",
    "    \n",
    "    # best k\n",
    "    \n",
    "    m = 100\n",
    "    min_var = np.mean(np.var(X_train, axis=0))\n",
    "\n",
    "    def predict_mixture_of_models(X, local_models, kmeans):\n",
    "        cluster_assignments = kmeans.predict(X)\n",
    "\n",
    "        predictions = np.zeros_like(X[:, 0]) \n",
    "\n",
    "        for cluster, model in enumerate(local_models):\n",
    "            cluster_indices = np.where(cluster_assignments == cluster)\n",
    "            X_cluster = X[cluster_indices]\n",
    "            predictions[cluster_indices] += model.predict(X_cluster)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def evaluate_overall_model(X, y, k, clf_models, classification=False):\n",
    "        kmeans = KMeans(n_clusters=k, n_init=100)\n",
    "        cluster_assignments = kmeans.fit_predict(X)\n",
    "\n",
    "        roc_score = float('-inf')\n",
    "\n",
    "        local_models = []\n",
    "        for cluster in range(k):\n",
    "            cluster_indices = np.where(cluster_assignments == cluster)\n",
    "\n",
    "            X_cluster = X[cluster_indices]\n",
    "            y_cluster = y[cluster_indices]\n",
    "\n",
    "            if len(np.unique(y_cluster)) < 2:\n",
    "                clf = sorted_clf[-1][0]\n",
    "    #             print(\"cluster:\", cluster)\n",
    "            else:\n",
    "                for i in clf_models:\n",
    "    #                 print(\"model in cluster:\", i, \"accuracy\", roc_auc_score(y_cluster, i.predict(X_cluster)))\n",
    "                    if roc_auc_score(y_cluster, i.predict(X_cluster)) >= roc_score:\n",
    "                        roc_score = roc_auc_score(y_cluster, i.predict(X_cluster))\n",
    "                        clf = i\n",
    "    #                 print(\"best model:\", clf)\n",
    "\n",
    "            def chol_sample(mean, cov, size=1):\n",
    "                cholesky_cov = np.linalg.cholesky(cov)\n",
    "                random_samp = np.array([mean + cholesky_cov @ np.random.standard_normal(mean.size) for i in range(size)])\n",
    "                return random_samp\n",
    "\n",
    "            cov = np.diag(np.zeros(X_cluster.shape[1]) + 0.01)\n",
    "\n",
    "            sample_mean = np.mean(X_cluster, axis=0)\n",
    "\n",
    "            perturbed_samples = chol_sample(sample_mean, cov, m)\n",
    "\n",
    "            perturbed_predictions = np.round(clf.predict(perturbed_samples))\n",
    "\n",
    "            X_cluster = np.concatenate((X_cluster, perturbed_samples))\n",
    "            y_cluster = np.concatenate((y_cluster, perturbed_predictions))\n",
    "            augmented_data[k][cluster] = X_cluster\n",
    "            augmented_y[k][cluster] = y_cluster\n",
    "            clf_model[k][cluster] = clf\n",
    "\n",
    "            if classification == True:\n",
    "                if len(np.unique(y_cluster)) == 1:\n",
    "                    model = LogisticRegression(solver='saga')\n",
    "                    model.classes_ = np.unique(y_cluster)\n",
    "                    model.coef_ = np.zeros((1, X.shape[1]))\n",
    "                    model.intercept_ = 0.0\n",
    "                else:\n",
    "                    model = LogisticRegression(solver='saga')\n",
    "                    model.fit(X_cluster, y_cluster)\n",
    "            else:\n",
    "                model = LinearRegression()\n",
    "                model.fit(X_cluster, y_cluster)\n",
    "            local_models.append(model)\n",
    "            models_all[k][cluster] = model\n",
    "\n",
    "        predictions = predict_mixture_of_models(X, local_models, kmeans)\n",
    "        overall_score = roc_auc_score(y, predictions)\n",
    "\n",
    "        return overall_score, augmented_data, augmented_y, models_all\n",
    "    \n",
    "    X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "    X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "\n",
    "    # Set the maximum possible value of k\n",
    "    max_k = 100\n",
    "\n",
    "    # Iterate over different values of k and record the performance\n",
    "    best_k = None\n",
    "    best_score = float('-inf')  # Initialize with a very low value\n",
    "    augmented_data = {}\n",
    "    augmented_y = {}\n",
    "    models_all = {}\n",
    "    clf_model = {}\n",
    "\n",
    "    clfs = my_clfs\n",
    "#     clfs = [clf_1]\n",
    "\n",
    "    sorted_clf = []\n",
    "    for i in clfs:\n",
    "        sorted_clf.append((i, roc_auc_score(np.round(i.predict(X_train)), y_train)))\n",
    "    sorted_clf = sorted(sorted_clf, key=lambda x: x[1])\n",
    "\n",
    "    for k in range(100, max_k + 1, 10):\n",
    "        print(\"k:\", k)\n",
    "        augmented_data[k] = {}\n",
    "        augmented_y[k] = {}\n",
    "        models_all[k] = {}\n",
    "        clf_model[k] ={}\n",
    "\n",
    "        overall_score = evaluate_overall_model(X_train, y_train, k, clfs, classification=True)\n",
    "\n",
    "        # Update best_k if a higher overall score is achieved\n",
    "        if overall_score[0] > best_score:\n",
    "            best_score = overall_score[0]\n",
    "            best_k = k\n",
    "\n",
    "    # Final model using the best k\n",
    "    kmeans = KMeans(n_clusters=best_k, n_init=100)\n",
    "    cluster_assignments = kmeans.fit_predict(X_train)\n",
    "\n",
    "    local_models = []\n",
    "    for cluster in range(best_k):\n",
    "        cluster_indices = np.where(cluster_assignments == cluster)\n",
    "        X_cluster = X_train[cluster_indices]\n",
    "        y_cluster = y_train[cluster_indices]\n",
    "\n",
    "    #     print(\"unique classes:\", np.unique(y_cluster))\n",
    "\n",
    "        if len(np.unique(y_cluster)) == 1:\n",
    "            model = LogisticRegression(solver='saga')\n",
    "            model.classes_ = np.unique(y_cluster)\n",
    "            model.coef_ = np.zeros((1, X_train.shape[1]))\n",
    "            model.intercept_ = 0.0\n",
    "        else:\n",
    "        #     model = LinearRegression()\n",
    "            model = LogisticRegression()\n",
    "            model.fit(X_cluster, y_cluster)\n",
    "        local_models.append(model)\n",
    "\n",
    "    predictions = predict_mixture_of_models(X_train, local_models, kmeans)\n",
    "    print(f\"Best k: {best_k}, Best AUC score: {best_score}\")\n",
    "    print(\"AUC:\", roc_auc_score(y_train, predictions))\n",
    "    \n",
    "    # collecting results\n",
    "    #With Pruning\n",
    "\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "    results = []\n",
    "    m = 100\n",
    "\n",
    "    X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "    X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "\n",
    "    n_folds = 10\n",
    "    # kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    stratified_kfold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=None)\n",
    "\n",
    "\n",
    "    clfs = my_clfs\n",
    "#     clfs = [clf_4]\n",
    "\n",
    "    for test in range(1):\n",
    "        # Set the maximum possible value of k\n",
    "        max_k = best_k\n",
    "\n",
    "        # Iterate over different values of k and record the performance\n",
    "        best_k = None\n",
    "        best_score = float('-inf')  # Initialize with a very low value\n",
    "        augmented_data = {}\n",
    "        augmented_y = {}\n",
    "        models_all = {}\n",
    "        clf_model = {}\n",
    "\n",
    "        sorted_clf = []\n",
    "        for i in clfs:\n",
    "            sorted_clf.append((i, roc_auc_score(np.round(i.predict(X_train)), y_train)))\n",
    "        sorted_clf = sorted(sorted_clf, key=lambda x: x[1])\n",
    "\n",
    "        for k in range(max_k, max_k + 1):\n",
    "            print(\"k:\", k)\n",
    "            augmented_data[k] = {}\n",
    "            augmented_y[k] = {}\n",
    "            models_all[k] = {}\n",
    "            clf_model[k] ={}\n",
    "\n",
    "            overall_score = evaluate_overall_model(X_train, y_train, k, clfs, classification=True)\n",
    "\n",
    "            # Update best_k if a higher overall score is achieved\n",
    "            if overall_score[0] > best_score:\n",
    "                best_score = overall_score[0]\n",
    "                best_k = k\n",
    "\n",
    "        local_models = []\n",
    "        for cluster in range(best_k):\n",
    "            cluster_indices = np.where(cluster_assignments == cluster)\n",
    "            X_cluster = X_train[cluster_indices]\n",
    "            y_cluster = y_train[cluster_indices]\n",
    "\n",
    "        #     print(\"unique classes:\", np.unique(y_cluster))\n",
    "\n",
    "            if len(np.unique(y_cluster)) == 1:\n",
    "                model = LogisticRegression(solver='saga')\n",
    "                model.classes_ = np.unique(y_cluster)\n",
    "                model.coef_ = np.zeros((1, X_train.shape[1]))\n",
    "                model.intercept_ = 0.0\n",
    "            else:\n",
    "            #     model = LinearRegression()\n",
    "                model = LogisticRegression()\n",
    "                model.fit(X_cluster, y_cluster)\n",
    "            local_models.append(model)\n",
    "\n",
    "        predictions = predict_mixture_of_models(X_train, local_models, kmeans)\n",
    "\n",
    "        import statistics\n",
    "\n",
    "        dist = pd.DataFrame(dist_matrix(best_k, classification=True))\n",
    "\n",
    "        CELL_centers = []\n",
    "        CELL_variances = []\n",
    "        for i in augmented_data[best_k]:\n",
    "            CELL_variances.append(np.sqrt(np.var(augmented_data[best_k][i], axis=0)))\n",
    "            CELL_centers.append(np.mean(augmented_data[best_k][i], axis=0))\n",
    "        cells_df = pd.DataFrame(CELL_centers)\n",
    "\n",
    "        y_local = []\n",
    "        for i in range(len(CELL_centers)):\n",
    "            y_local.append(statistics.mode(augmented_y[best_k][i]))\n",
    "\n",
    "\n",
    "        # import DT_with_R2_stopping as dtc\n",
    "        import PRUNING as dtc\n",
    "\n",
    "        for k in range(1, 2):\n",
    "            roc_curr = float('-inf')\n",
    "            roc_curr_train = float('-inf')\n",
    "            best_depth = float('inf')\n",
    "\n",
    "            tree = dtc.DistanceDecisionTree(max_depth=float('inf'), classification=True)\n",
    "\n",
    "            tree.fit(np.array(cells_df), np.array(dist))\n",
    "\n",
    "            data_transformed = [augmented_data[best_k][key] for key in range(len(augmented_data[best_k]))]\n",
    "            data_transformed_y = [augmented_y[best_k][key] for key in range(len(augmented_y[best_k]))]\n",
    "\n",
    "            X_transformed = np.array([item for sublist in [arr.tolist() for arr in data_transformed] for item in sublist])\n",
    "            y_transformed = np.array([item for sublist in [arr.tolist() for arr in data_transformed_y] for item in sublist])\n",
    "\n",
    "            print(\"Max Depth\", tree.final_depth)\n",
    "\n",
    "            tree.fit_leaf_logistic_models(X_transformed, y_transformed)\n",
    "            alpha_sequence_full = alpha_sequence(tree)[1]\n",
    "            alpha_sequence_full_end = alpha_sequence_full.copy()\n",
    "\n",
    "            cv_scores = {}\n",
    "            test_score = {}\n",
    "            train_score = {}\n",
    "\n",
    "    #         fold_data = {}\n",
    "    #         fold_data_val = {}\n",
    "    #         fold_y = {}\n",
    "    #         fold_y_val = {}\n",
    "\n",
    "            for sequence_tree in alpha_sequence_full:\n",
    "\n",
    "                fold_data = {}\n",
    "                fold_data_val = {}\n",
    "                fold_y = {}\n",
    "                fold_y_val = {}\n",
    "\n",
    "                fold_scores = []\n",
    "                fold_test = []\n",
    "                fold_train = []\n",
    "\n",
    "    #             fold_data = []\n",
    "    #             fold_data_val = []\n",
    "    #             fold_y = []\n",
    "    #             fold_y_val = []\n",
    "\n",
    "    #             data_transformed = [augmented_data[best_k][key] for key in range(len(augmented_data[best_k]))]\n",
    "    #             data_transformed_y = [augmented_y[best_k][key] for key in range(len(augmented_y[best_k]))]\n",
    "\n",
    "    #             X_transformed = np.array([item for sublist in [arr.tolist() for arr in data_transformed] for item in sublist])\n",
    "    #             y_transformed = np.array([item for sublist in [arr.tolist() for arr in data_transformed_y] for item in sublist])\n",
    "\n",
    "                for cluster in augmented_data[best_k].keys():\n",
    "                    for fold, (train_indices, val_indices) in enumerate(stratified_kfold.split(augmented_data[best_k][cluster], augmented_y[best_k][cluster])):\n",
    "                        if fold in fold_data:\n",
    "                            fold_data[fold] = np.concatenate([fold_data[fold], augmented_data[best_k][cluster][train_indices]])\n",
    "                            fold_data_val[fold] = np.concatenate([fold_data_val[fold], augmented_data[best_k][cluster][val_indices]])\n",
    "                            fold_y[fold] = np.concatenate([fold_y[fold], augmented_y[best_k][cluster][train_indices]])\n",
    "                            fold_y_val[fold] = np.concatenate([fold_y_val[fold], augmented_y[best_k][cluster][val_indices]])\n",
    "                        else:\n",
    "                            fold_data[fold] = augmented_data[best_k][cluster][train_indices]\n",
    "                            fold_data_val[fold] = augmented_data[best_k][cluster][val_indices]\n",
    "                            fold_y[fold] = augmented_y[best_k][cluster][train_indices]\n",
    "                            fold_y_val[fold] = augmented_y[best_k][cluster][val_indices]\n",
    "\n",
    "                for p in range(n_folds):\n",
    "\n",
    "                    X_train_fold, X_val = fold_data[p], fold_data_val[p]\n",
    "                    y_train_fold, y_val = fold_y[p], fold_y_val[p]\n",
    "\n",
    "    #                 print(\"X_train_fold\", X_train_fold)\n",
    "    #                 print(\"y_train_fold\", y_train_fold)\n",
    "\n",
    "    #                 tree = dtc.DistanceDecisionTree(max_depth=i, classification=True)\n",
    "\n",
    "    #                 tree.fit(np.array(cells_df), np.array(dist))\n",
    "\n",
    "    #                 print(\"y: \", y_train_fold)\n",
    "    #                 tree.fit_leaf_logistic_models_cluster_based(np.array(y_train_fold), np.array(cells_df), np.array(X_train_fold), 0.2)\n",
    "                    sequence_tree.fit_leaf_logistic_models(X_train_fold, y_train_fold)\n",
    "\n",
    "    #                 print(\"Results:\", alpha_sequence(tree))\n",
    "\n",
    "                    Y_pred_train = sequence_tree.predict(X_train_fold, y_train_fold)\n",
    "                    Y_pred = sequence_tree.predict(X_val, y_val)\n",
    "    #                 print(Y_pred)\n",
    "    #                 if Y_pred is not None:\n",
    "    #                 Y_pred = [a[0] for a in Y_pred]\n",
    "                    [print(\"None\") for a in Y_pred if a is None]\n",
    "                    Y_pred = [a[0] if a is not None else 0 for a in Y_pred]\n",
    "    #                 print(Y_pred)\n",
    "\n",
    "#                     print(\"Current Accuracy val\", roc_auc_score(y_val, Y_pred))\n",
    "\n",
    "\n",
    "                    fold_accuracy = roc_auc_score(y_val, Y_pred)\n",
    "\n",
    "                    fold_scores.append(fold_accuracy)\n",
    "                    fold_train.append(roc_auc_score(y_train, sequence_tree.predict(X_train, y_train)))\n",
    "                    fold_test.append(roc_auc_score(y_test, sequence_tree.predict(X_test, y_test)))\n",
    "\n",
    "                cv_scores[sequence_tree] = np.mean(fold_scores)\n",
    "                test_score[sequence_tree] = np.mean(fold_test)\n",
    "                train_score[sequence_tree] = np.mean(fold_train)\n",
    "\n",
    "            print(\"Test scores:\", test_score)\n",
    "            print(\"cv_scores:\", cv_scores)\n",
    "\n",
    "        best_depth = max(cv_scores, key=lambda k: cv_scores[k])\n",
    "        best_tree_object = next((obj for obj in alpha_sequence_full_end if obj == best_depth), None)\n",
    "        best_tree_object.fit_leaf_logistic_models(X_transformed, y_transformed)\n",
    "        \n",
    "        Y_pred_train = best_tree_object.predict(X_transformed, y_transformed)\n",
    "        Y_pred_train = [a[0] if type(a[0])==np.float64 else a[0][0] for a in Y_pred_train]\n",
    "        Y_pred_test = best_tree_object.predict(X_test, y_test)\n",
    "        Y_pred_test = [a[0] if type(a[0])==np.float64 else a[0][0] for a in Y_pred_test]\n",
    "        \n",
    "        print(\"\\nFinal result whole training data:\", roc_auc_score(y_transformed, Y_pred_train), \"Final result whole test data:\",\n",
    "              roc_auc_score(y_test, Y_pred_test))\n",
    "        \n",
    "        print(\"Best Depth:\", best_depth, \"Train:\", train_score[best_depth], \n",
    "              \"Test:\", test_score[best_depth], \"Val:\", cv_scores[best_depth])  \n",
    "        print(\"Models\", clfs)\n",
    "    \n",
    "#     ALL_RESULTS_TRAIN.append(train_score[best_depth])\n",
    "#     ALL_RESULTS_TEST.append(test_score[best_depth])\n",
    "#     ALL_RESULTS_VAL.append(cv_scores[best_depth])\n",
    "    ALL_RESULTS_TRAIN.append(roc_auc_score(y_transformed, Y_pred_train))\n",
    "    ALL_RESULTS_TEST.append(roc_auc_score(y_test, Y_pred_test))\n",
    "#     ALL_RESULTS_VAL.append(cv_scores[best_depth])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c5b3c56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_leaf_nodes(best_tree_object.tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf194ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 10 <= 0.7835065767614282Model: <statsmodels.discrete.discrete_model.L1BinaryResultsWrapper object at 0x14e4e0aace80> R_node: 1613\n",
      "Left:\n",
      "  Leaf: Accuracy Test= 0.8988\n",
      "  Model: <statsmodels.discrete.discrete_model.L1BinaryResultsWrapper object at 0x14e4e0aae680>   R_node: 400\n",
      "Right:\n",
      "  Leaf: Accuracy Test= 0.8273\n",
      "  Model: <statsmodels.discrete.discrete_model.L1BinaryResultsWrapper object at 0x14e4e0aae890>   R_node: 1098\n"
     ]
    }
   ],
   "source": [
    "best_tree_object.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe523e25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<PRUNING.DistanceDecisionTree at 0x14b33c29bf70>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_sequence_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d384ac80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04807292603937304"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std([ALL_RESULTS_TEST])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "07563bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alpha_sequence_full[0].print_tree()\n",
    "number_of_leaf_nodes(alpha_sequence_full[0].tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1b0f6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train: [0.9331209129909617, 0.9112558100582053, 0.9226148851148853, 0.9239456635318704, 0.8897768897768898]\n",
      "\n",
      "Test: [0.8289021164021164, 0.7484285714285714, 0.7566732412886259, 0.8413194444444445, 0.8956493078444299]\n",
      "\n",
      "Valid: [0.8827901518172968, 0.9027239835439002, 0.9282141407666952, 0.9142747883045503, 0.8894094241720495]\n"
     ]
    }
   ],
   "source": [
    "# Mixture\n",
    "print(\"\\nTrain:\", ALL_RESULTS_TRAIN)\n",
    "print(\"\\nTest:\", ALL_RESULTS_TEST)\n",
    "print(\"\\nValid:\", ALL_RESULTS_VAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e719510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train: [0.8850630698139129, 0.8749385647049023, 0.8847166666666666, 0.8578785229590775, 0.8948019492690241]\n",
      "\n",
      "Test: [0.7992105263157895, 0.9179144385026741, 0.7619269102990033, 0.8341931216931217, 0.802807486631016]\n",
      "\n",
      "Valid: [0.9363756737499838, 0.9314190536669671, 0.927557225030726, 0.9318706696829209, 0.9349257521091798]\n"
     ]
    }
   ],
   "source": [
    "# RF\n",
    "print(\"\\nTrain:\", ALL_RESULTS_TRAIN)\n",
    "print(\"\\nTest:\", ALL_RESULTS_TEST)\n",
    "print(\"\\nValid:\", ALL_RESULTS_VAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "574c863d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train: [0.9371543970686209, 0.8698010323010325, 0.8555472295075143, 0.8930777555777556, 0.91018368111958]\n",
      "\n",
      "Test: [0.7589980224126565, 0.8619986850756082, 0.808994708994709, 0.7750821827744904, 0.8669312169312169]\n",
      "\n",
      "Valid: [0.8730543057116014, 0.8378462239264447, 0.8120634781695119, 0.9001631805018775, 0.8812243651330449]\n"
     ]
    }
   ],
   "source": [
    "# GB\n",
    "print(\"\\nTrain:\", ALL_RESULTS_TRAIN)\n",
    "print(\"\\nTest:\", ALL_RESULTS_TEST)\n",
    "print(\"\\nValid:\", ALL_RESULTS_VAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f8835e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train: [0.9188308697290732, 0.9210248085248086, 0.9096596596596598, 0.9500249750249751, 0.9010118254497004]\n",
      "\n",
      "Test: [0.7859999999999999, 0.8744904667981592, 0.8356211393273851, 0.8332893869479235, 0.8460465116279069]\n",
      "\n",
      "Valid: [0.9583900960268675, 0.9418756660961647, 0.9559559789847356, 0.9654428366670951, 0.8996387605071237]\n"
     ]
    }
   ],
   "source": [
    "# MLP\n",
    "print(\"\\nTrain:\", ALL_RESULTS_TRAIN)\n",
    "print(\"\\nTest:\", ALL_RESULTS_TEST)\n",
    "print(\"\\nValid:\", ALL_RESULTS_VAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b5c47ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train: [0.8642521127346905, 0.8924412070759626, 0.9161022949727184, 0.9087703962703962, 0.9291557332666918]\n",
      "\n",
      "Test: [0.835383597883598, 0.8023684210526316, 0.8354497354497354, 0.879750164365549, 0.7848262032085562]\n",
      "\n",
      "Valid: [0.8932732885457506, 0.8614669647155557, 0.9005804762535694, 0.9019178435292629, 0.9014061605524735]\n"
     ]
    }
   ],
   "source": [
    "# Forestry\n",
    "print(\"\\nTrain:\", ALL_RESULTS_TRAIN)\n",
    "print(\"\\nTest:\", ALL_RESULTS_TEST)\n",
    "print(\"\\nValid:\", ALL_RESULTS_VAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a778e1a3",
   "metadata": {},
   "source": [
    "# Fixed results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6ded691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train: [0.9323624878103546, 0.9435560172847312, 0.9262799837287183, 0.9409193362193362, 0.9369778744350699]\n",
      "\n",
      "Test: [0.8342105263157895, 0.8356344510190664, 0.8650793650793651, 0.8154761904761906, 0.7448912326961107]\n"
     ]
    }
   ],
   "source": [
    "# RF\n",
    "print(\"\\nTrain:\", ALL_RESULTS_TRAIN)\n",
    "print(\"\\nTest:\", ALL_RESULTS_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ef6bed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train: []\n",
      "\n",
      "Test: []\n"
     ]
    }
   ],
   "source": [
    "# GB\n",
    "print(\"\\nTrain:\", ALL_RESULTS_TRAIN)\n",
    "print(\"\\nTest:\", ALL_RESULTS_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88be9325",
   "metadata": {},
   "source": [
    "# CART PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1298d359",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "dot_data = export_graphviz(m1, out_file='tree.dot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b77ba23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (0)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"3198pt\" height=\"1309pt\"\n",
       " viewBox=\"0.00 0.00 3198.00 1309.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 1305)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-1305 3194,-1305 3194,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"1156,-1301 1006,-1301 1006,-1233 1156,-1233 1156,-1301\"/>\n",
       "<text text-anchor=\"middle\" x=\"1081\" y=\"-1285.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[1] &lt;= 0.018</text>\n",
       "<text text-anchor=\"middle\" x=\"1081\" y=\"-1270.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.5</text>\n",
       "<text text-anchor=\"middle\" x=\"1081\" y=\"-1255.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 310</text>\n",
       "<text text-anchor=\"middle\" x=\"1081\" y=\"-1240.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [158, 152]</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"874,-1197 742,-1197 742,-1129 874,-1129 874,-1197\"/>\n",
       "<text text-anchor=\"middle\" x=\"808\" y=\"-1181.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[13] &lt;= 0.036</text>\n",
       "<text text-anchor=\"middle\" x=\"808\" y=\"-1166.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.316</text>\n",
       "<text text-anchor=\"middle\" x=\"808\" y=\"-1151.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 66</text>\n",
       "<text text-anchor=\"middle\" x=\"808\" y=\"-1136.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [13, 53]</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1005.6789,-1238.3062C967.7047,-1223.8399 921.7656,-1206.3393 883.7339,-1191.851\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"884.8358,-1188.5255 874.2449,-1188.2362 882.3438,-1195.0669 884.8358,-1188.5255\"/>\n",
       "<text text-anchor=\"middle\" x=\"884.4713\" y=\"-1207.3489\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">True</text>\n",
       "</g>\n",
       "<!-- 26 -->\n",
       "<g id=\"node27\" class=\"node\">\n",
       "<title>26</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"1560.5,-1197 1419.5,-1197 1419.5,-1129 1560.5,-1129 1560.5,-1197\"/>\n",
       "<text text-anchor=\"middle\" x=\"1490\" y=\"-1181.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[13] &lt;= 0.138</text>\n",
       "<text text-anchor=\"middle\" x=\"1490\" y=\"-1166.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.482</text>\n",
       "<text text-anchor=\"middle\" x=\"1490\" y=\"-1151.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 244</text>\n",
       "<text text-anchor=\"middle\" x=\"1490\" y=\"-1136.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [145, 99]</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;26 -->\n",
       "<g id=\"edge26\" class=\"edge\">\n",
       "<title>0&#45;&gt;26</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1156.0169,-1247.9248C1227.8566,-1229.6575 1335.9124,-1202.1812 1409.521,-1183.4641\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1410.3919,-1186.8541 1419.2209,-1180.9976 1408.6668,-1180.07 1410.3919,-1186.8541\"/>\n",
       "<text text-anchor=\"middle\" x=\"1406.4449\" y=\"-1198.7865\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">False</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"461.5,-1093 334.5,-1093 334.5,-1025 461.5,-1025 461.5,-1093\"/>\n",
       "<text text-anchor=\"middle\" x=\"398\" y=\"-1077.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[19] &lt;= 0.075</text>\n",
       "<text text-anchor=\"middle\" x=\"398\" y=\"-1062.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.436</text>\n",
       "<text text-anchor=\"middle\" x=\"398\" y=\"-1047.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 28</text>\n",
       "<text text-anchor=\"middle\" x=\"398\" y=\"-1032.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [9, 19]</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M741.6707,-1146.175C667.7078,-1127.4137 548.4073,-1097.1521 471.6148,-1077.673\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"472.0899,-1074.1828 461.5363,-1075.1165 470.3688,-1080.9679 472.0899,-1074.1828\"/>\n",
       "</g>\n",
       "<!-- 15 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>15</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"869.5,-1093 746.5,-1093 746.5,-1025 869.5,-1025 869.5,-1093\"/>\n",
       "<text text-anchor=\"middle\" x=\"808\" y=\"-1077.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[3] &lt;= 0.117</text>\n",
       "<text text-anchor=\"middle\" x=\"808\" y=\"-1062.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.188</text>\n",
       "<text text-anchor=\"middle\" x=\"808\" y=\"-1047.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 38</text>\n",
       "<text text-anchor=\"middle\" x=\"808\" y=\"-1032.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [4, 34]</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;15 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>1&#45;&gt;15</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M808,-1128.9465C808,-1120.776 808,-1111.9318 808,-1103.3697\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"811.5001,-1103.13 808,-1093.13 804.5001,-1103.13 811.5001,-1103.13\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"248,-989 130,-989 130,-921 248,-921 248,-989\"/>\n",
       "<text text-anchor=\"middle\" x=\"189\" y=\"-973.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[1] &lt;= 0.005</text>\n",
       "<text text-anchor=\"middle\" x=\"189\" y=\"-958.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.32</text>\n",
       "<text text-anchor=\"middle\" x=\"189\" y=\"-943.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 5</text>\n",
       "<text text-anchor=\"middle\" x=\"189\" y=\"-928.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [4, 1]</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M334.4419,-1027.373C310.0549,-1015.2379 282.1345,-1001.3445 257.3194,-988.9963\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"258.8419,-985.8445 248.3298,-984.523 255.7234,-992.1115 258.8419,-985.8445\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>6</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"459.5,-989 336.5,-989 336.5,-921 459.5,-921 459.5,-989\"/>\n",
       "<text text-anchor=\"middle\" x=\"398\" y=\"-973.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[0] &lt;= 0.827</text>\n",
       "<text text-anchor=\"middle\" x=\"398\" y=\"-958.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.34</text>\n",
       "<text text-anchor=\"middle\" x=\"398\" y=\"-943.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 23</text>\n",
       "<text text-anchor=\"middle\" x=\"398\" y=\"-928.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [5, 18]</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;6 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>2&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M398,-1024.9465C398,-1016.776 398,-1007.9318 398,-999.3697\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"401.5001,-999.13 398,-989.13 394.5001,-999.13 401.5001,-999.13\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"114,-877.5 0,-877.5 0,-824.5 114,-824.5 114,-877.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"57\" y=\"-862.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"57\" y=\"-847.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 1</text>\n",
       "<text text-anchor=\"middle\" x=\"57\" y=\"-832.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 1]</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>3&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M145.7783,-920.9465C130.6794,-909.0504 113.7676,-895.726 98.7878,-883.9237\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"100.7091,-880.9817 90.6881,-877.5422 96.377,-886.4802 100.7091,-880.9817\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"246,-877.5 132,-877.5 132,-824.5 246,-824.5 246,-877.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"189\" y=\"-862.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"189\" y=\"-847.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 4</text>\n",
       "<text text-anchor=\"middle\" x=\"189\" y=\"-832.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [4, 0]</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;5 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>3&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M189,-920.9465C189,-910.2621 189,-898.4254 189,-887.5742\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"192.5001,-887.5421 189,-877.5422 185.5001,-887.5422 192.5001,-887.5421\"/>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>7</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"387.5,-885 264.5,-885 264.5,-817 387.5,-817 387.5,-885\"/>\n",
       "<text text-anchor=\"middle\" x=\"326\" y=\"-869.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[4] &lt;= 0.038</text>\n",
       "<text text-anchor=\"middle\" x=\"326\" y=\"-854.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.198</text>\n",
       "<text text-anchor=\"middle\" x=\"326\" y=\"-839.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 18</text>\n",
       "<text text-anchor=\"middle\" x=\"326\" y=\"-824.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [2, 16]</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;7 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>6&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M374.4245,-920.9465C368.3329,-912.1475 361.7004,-902.5672 355.3534,-893.3993\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"358.1983,-891.3597 349.6285,-885.13 352.4429,-895.3442 358.1983,-891.3597\"/>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>12</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"532.5,-885 405.5,-885 405.5,-817 532.5,-817 532.5,-885\"/>\n",
       "<text text-anchor=\"middle\" x=\"469\" y=\"-869.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[13] &lt;= 0.026</text>\n",
       "<text text-anchor=\"middle\" x=\"469\" y=\"-854.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.48</text>\n",
       "<text text-anchor=\"middle\" x=\"469\" y=\"-839.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 5</text>\n",
       "<text text-anchor=\"middle\" x=\"469\" y=\"-824.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [3, 2]</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;12 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>6&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M421.2481,-920.9465C427.255,-912.1475 433.7955,-902.5672 440.0543,-893.3993\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"442.952,-895.3623 445.6997,-885.13 437.1707,-891.4155 442.952,-895.3623\"/>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>8</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"246,-773.5 132,-773.5 132,-720.5 246,-720.5 246,-773.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"189\" y=\"-758.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"189\" y=\"-743.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 1</text>\n",
       "<text text-anchor=\"middle\" x=\"189\" y=\"-728.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [1, 0]</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;8 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>7&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M281.1411,-816.9465C265.3252,-804.9403 247.5927,-791.4791 231.9393,-779.5962\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"234.0455,-776.8009 223.9642,-773.5422 229.8129,-782.3764 234.0455,-776.8009\"/>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>9</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"387.5,-781 264.5,-781 264.5,-713 387.5,-713 387.5,-781\"/>\n",
       "<text text-anchor=\"middle\" x=\"326\" y=\"-765.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[25] &lt;= 0.5</text>\n",
       "<text text-anchor=\"middle\" x=\"326\" y=\"-750.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.111</text>\n",
       "<text text-anchor=\"middle\" x=\"326\" y=\"-735.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 17</text>\n",
       "<text text-anchor=\"middle\" x=\"326\" y=\"-720.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [1, 16]</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;9 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>7&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M326,-816.9465C326,-808.776 326,-799.9318 326,-791.3697\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"329.5001,-791.13 326,-781.13 322.5001,-791.13 329.5001,-791.13\"/>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>10</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"319.5,-669.5 196.5,-669.5 196.5,-616.5 319.5,-616.5 319.5,-669.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"258\" y=\"-654.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"258\" y=\"-639.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 16</text>\n",
       "<text text-anchor=\"middle\" x=\"258\" y=\"-624.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 16]</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;10 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>9&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M303.7343,-712.9465C296.4602,-701.8215 288.3694,-689.4473 281.0396,-678.237\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"283.7564,-675.9965 275.3545,-669.5422 277.8976,-679.8273 283.7564,-675.9965\"/>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>11</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"452,-669.5 338,-669.5 338,-616.5 452,-616.5 452,-669.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"395\" y=\"-654.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"395\" y=\"-639.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 1</text>\n",
       "<text text-anchor=\"middle\" x=\"395\" y=\"-624.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [1, 0]</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;11 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>9&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M348.5932,-712.9465C356.0473,-701.7113 364.3467,-689.2021 371.8423,-677.9043\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"374.7782,-679.81 377.3903,-669.5422 368.9453,-675.94 374.7782,-679.81\"/>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>13</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"521,-773.5 407,-773.5 407,-720.5 521,-720.5 521,-773.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"464\" y=\"-758.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"464\" y=\"-743.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 2</text>\n",
       "<text text-anchor=\"middle\" x=\"464\" y=\"-728.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 2]</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;13 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>12&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M467.3628,-816.9465C466.8491,-806.2621 466.2801,-794.4254 465.7584,-783.5742\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"469.2523,-783.3625 465.2761,-773.5422 462.2604,-783.6987 469.2523,-783.3625\"/>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>14</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"653,-773.5 539,-773.5 539,-720.5 653,-720.5 653,-773.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"596\" y=\"-758.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"596\" y=\"-743.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 3</text>\n",
       "<text text-anchor=\"middle\" x=\"596\" y=\"-728.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [3, 0]</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;14 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>12&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M510.5845,-816.9465C525.1115,-805.0504 541.3827,-791.726 555.7951,-779.9237\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"558.0686,-782.5858 563.5879,-773.5422 553.6335,-777.17 558.0686,-782.5858\"/>\n",
       "</g>\n",
       "<!-- 16 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>16</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"797,-989 683,-989 683,-921 797,-921 797,-989\"/>\n",
       "<text text-anchor=\"middle\" x=\"740\" y=\"-973.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[66] &lt;= 0.5</text>\n",
       "<text text-anchor=\"middle\" x=\"740\" y=\"-958.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.444</text>\n",
       "<text text-anchor=\"middle\" x=\"740\" y=\"-943.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 3</text>\n",
       "<text text-anchor=\"middle\" x=\"740\" y=\"-928.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [2, 1]</text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;16 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>15&#45;&gt;16</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M785.7343,-1024.9465C780.0398,-1016.2373 773.8448,-1006.7626 767.9062,-997.6801\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"770.7177,-995.5843 762.3158,-989.13 764.8589,-999.4151 770.7177,-995.5843\"/>\n",
       "</g>\n",
       "<!-- 19 -->\n",
       "<g id=\"node20\" class=\"node\">\n",
       "<title>19</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"938.5,-989 815.5,-989 815.5,-921 938.5,-921 938.5,-989\"/>\n",
       "<text text-anchor=\"middle\" x=\"877\" y=\"-973.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[53] &lt;= 0.5</text>\n",
       "<text text-anchor=\"middle\" x=\"877\" y=\"-958.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.108</text>\n",
       "<text text-anchor=\"middle\" x=\"877\" y=\"-943.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 35</text>\n",
       "<text text-anchor=\"middle\" x=\"877\" y=\"-928.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [2, 33]</text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;19 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>15&#45;&gt;19</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M830.5932,-1024.9465C836.3714,-1016.2373 842.6575,-1006.7626 848.6834,-997.6801\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"851.744,-999.3978 854.3561,-989.13 845.911,-995.5278 851.744,-999.3978\"/>\n",
       "</g>\n",
       "<!-- 17 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>17</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"665,-877.5 551,-877.5 551,-824.5 665,-824.5 665,-877.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"608\" y=\"-862.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"608\" y=\"-847.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 2</text>\n",
       "<text text-anchor=\"middle\" x=\"608\" y=\"-832.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [2, 0]</text>\n",
       "</g>\n",
       "<!-- 16&#45;&gt;17 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>16&#45;&gt;17</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M696.7783,-920.9465C681.6794,-909.0504 664.7676,-895.726 649.7878,-883.9237\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"651.7091,-880.9817 641.6881,-877.5422 647.377,-886.4802 651.7091,-880.9817\"/>\n",
       "</g>\n",
       "<!-- 18 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>18</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"797,-877.5 683,-877.5 683,-824.5 797,-824.5 797,-877.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"740\" y=\"-862.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"740\" y=\"-847.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 1</text>\n",
       "<text text-anchor=\"middle\" x=\"740\" y=\"-832.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 1]</text>\n",
       "</g>\n",
       "<!-- 16&#45;&gt;18 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>16&#45;&gt;18</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M740,-920.9465C740,-910.2621 740,-898.4254 740,-887.5742\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"743.5001,-887.5421 740,-877.5422 736.5001,-887.5422 743.5001,-887.5421\"/>\n",
       "</g>\n",
       "<!-- 20 -->\n",
       "<g id=\"node21\" class=\"node\">\n",
       "<title>20</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"938.5,-885 815.5,-885 815.5,-817 938.5,-817 938.5,-885\"/>\n",
       "<text text-anchor=\"middle\" x=\"877\" y=\"-869.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[1] &lt;= 0.006</text>\n",
       "<text text-anchor=\"middle\" x=\"877\" y=\"-854.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.057</text>\n",
       "<text text-anchor=\"middle\" x=\"877\" y=\"-839.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 34</text>\n",
       "<text text-anchor=\"middle\" x=\"877\" y=\"-824.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [1, 33]</text>\n",
       "</g>\n",
       "<!-- 19&#45;&gt;20 -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>19&#45;&gt;20</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M877,-920.9465C877,-912.776 877,-903.9318 877,-895.3697\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"880.5001,-895.13 877,-885.13 873.5001,-895.13 880.5001,-895.13\"/>\n",
       "</g>\n",
       "<!-- 25 -->\n",
       "<g id=\"node26\" class=\"node\">\n",
       "<title>25</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"1071,-877.5 957,-877.5 957,-824.5 1071,-824.5 1071,-877.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"1014\" y=\"-862.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"1014\" y=\"-847.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 1</text>\n",
       "<text text-anchor=\"middle\" x=\"1014\" y=\"-832.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [1, 0]</text>\n",
       "</g>\n",
       "<!-- 19&#45;&gt;25 -->\n",
       "<g id=\"edge25\" class=\"edge\">\n",
       "<title>19&#45;&gt;25</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M921.8589,-920.9465C937.6748,-908.9403 955.4073,-895.4791 971.0607,-883.5962\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"973.1871,-886.3764 979.0358,-877.5422 968.9545,-880.8009 973.1871,-886.3764\"/>\n",
       "</g>\n",
       "<!-- 21 -->\n",
       "<g id=\"node22\" class=\"node\">\n",
       "<title>21</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"798.5,-781 671.5,-781 671.5,-713 798.5,-713 798.5,-781\"/>\n",
       "<text text-anchor=\"middle\" x=\"735\" y=\"-765.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[23] &lt;= 0.001</text>\n",
       "<text text-anchor=\"middle\" x=\"735\" y=\"-750.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.198</text>\n",
       "<text text-anchor=\"middle\" x=\"735\" y=\"-735.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 9</text>\n",
       "<text text-anchor=\"middle\" x=\"735\" y=\"-720.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [1, 8]</text>\n",
       "</g>\n",
       "<!-- 20&#45;&gt;21 -->\n",
       "<g id=\"edge21\" class=\"edge\">\n",
       "<title>20&#45;&gt;21</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M830.5039,-816.9465C817.5092,-807.4293 803.2663,-796.9978 789.8379,-787.163\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"791.7363,-784.2151 781.6006,-781.13 787.6002,-789.8624 791.7363,-784.2151\"/>\n",
       "</g>\n",
       "<!-- 24 -->\n",
       "<g id=\"node25\" class=\"node\">\n",
       "<title>24</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"939.5,-773.5 816.5,-773.5 816.5,-720.5 939.5,-720.5 939.5,-773.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"878\" y=\"-758.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"878\" y=\"-743.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 25</text>\n",
       "<text text-anchor=\"middle\" x=\"878\" y=\"-728.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 25]</text>\n",
       "</g>\n",
       "<!-- 20&#45;&gt;24 -->\n",
       "<g id=\"edge24\" class=\"edge\">\n",
       "<title>20&#45;&gt;24</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M877.3274,-816.9465C877.4302,-806.2621 877.544,-794.4254 877.6483,-783.5742\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"881.1484,-783.5754 877.7448,-773.5422 874.1487,-783.508 881.1484,-783.5754\"/>\n",
       "</g>\n",
       "<!-- 22 -->\n",
       "<g id=\"node23\" class=\"node\">\n",
       "<title>22</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"726,-669.5 612,-669.5 612,-616.5 726,-616.5 726,-669.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"669\" y=\"-654.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"669\" y=\"-639.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 8</text>\n",
       "<text text-anchor=\"middle\" x=\"669\" y=\"-624.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 8]</text>\n",
       "</g>\n",
       "<!-- 21&#45;&gt;22 -->\n",
       "<g id=\"edge22\" class=\"edge\">\n",
       "<title>21&#45;&gt;22</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M713.3891,-712.9465C706.329,-701.8215 698.4761,-689.4473 691.3619,-678.237\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"694.1575,-676.1101 685.8441,-669.5422 688.2472,-679.8609 694.1575,-676.1101\"/>\n",
       "</g>\n",
       "<!-- 23 -->\n",
       "<g id=\"node24\" class=\"node\">\n",
       "<title>23</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"858,-669.5 744,-669.5 744,-616.5 858,-616.5 858,-669.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"801\" y=\"-654.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"801\" y=\"-639.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 1</text>\n",
       "<text text-anchor=\"middle\" x=\"801\" y=\"-624.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [1, 0]</text>\n",
       "</g>\n",
       "<!-- 21&#45;&gt;23 -->\n",
       "<g id=\"edge23\" class=\"edge\">\n",
       "<title>21&#45;&gt;23</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M756.6109,-712.9465C763.671,-701.8215 771.5239,-689.4473 778.6381,-678.237\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"781.7528,-679.8609 784.1559,-669.5422 775.8425,-676.1101 781.7528,-679.8609\"/>\n",
       "</g>\n",
       "<!-- 27 -->\n",
       "<g id=\"node28\" class=\"node\">\n",
       "<title>27</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"1556,-1093 1424,-1093 1424,-1025 1556,-1025 1556,-1093\"/>\n",
       "<text text-anchor=\"middle\" x=\"1490\" y=\"-1077.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[1] &lt;= 0.061</text>\n",
       "<text text-anchor=\"middle\" x=\"1490\" y=\"-1062.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.331</text>\n",
       "<text text-anchor=\"middle\" x=\"1490\" y=\"-1047.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 110</text>\n",
       "<text text-anchor=\"middle\" x=\"1490\" y=\"-1032.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [87, 23]</text>\n",
       "</g>\n",
       "<!-- 26&#45;&gt;27 -->\n",
       "<g id=\"edge27\" class=\"edge\">\n",
       "<title>26&#45;&gt;27</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1490,-1128.9465C1490,-1120.776 1490,-1111.9318 1490,-1103.3697\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1493.5001,-1103.13 1490,-1093.13 1486.5001,-1103.13 1493.5001,-1103.13\"/>\n",
       "</g>\n",
       "<!-- 50 -->\n",
       "<g id=\"node51\" class=\"node\">\n",
       "<title>50</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"2472,-1093 2340,-1093 2340,-1025 2472,-1025 2472,-1093\"/>\n",
       "<text text-anchor=\"middle\" x=\"2406\" y=\"-1077.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[1] &lt;= 0.427</text>\n",
       "<text text-anchor=\"middle\" x=\"2406\" y=\"-1062.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.491</text>\n",
       "<text text-anchor=\"middle\" x=\"2406\" y=\"-1047.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 134</text>\n",
       "<text text-anchor=\"middle\" x=\"2406\" y=\"-1032.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [58, 76]</text>\n",
       "</g>\n",
       "<!-- 26&#45;&gt;50 -->\n",
       "<g id=\"edge50\" class=\"edge\">\n",
       "<title>26&#45;&gt;50</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1560.735,-1154.969C1729.6331,-1135.7928 2155.3223,-1087.4612 2329.7057,-1067.6622\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"2330.2826,-1071.1193 2339.8239,-1066.5134 2329.4928,-1064.164 2330.2826,-1071.1193\"/>\n",
       "</g>\n",
       "<!-- 28 -->\n",
       "<g id=\"node29\" class=\"node\">\n",
       "<title>28</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"1420,-989 1288,-989 1288,-921 1420,-921 1420,-989\"/>\n",
       "<text text-anchor=\"middle\" x=\"1354\" y=\"-973.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[13] &lt;= 0.058</text>\n",
       "<text text-anchor=\"middle\" x=\"1354\" y=\"-958.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.498</text>\n",
       "<text text-anchor=\"middle\" x=\"1354\" y=\"-943.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 32</text>\n",
       "<text text-anchor=\"middle\" x=\"1354\" y=\"-928.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [15, 17]</text>\n",
       "</g>\n",
       "<!-- 27&#45;&gt;28 -->\n",
       "<g id=\"edge28\" class=\"edge\">\n",
       "<title>27&#45;&gt;28</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1445.4685,-1024.9465C1433.1403,-1015.519 1419.6391,-1005.1946 1406.885,-995.4415\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1408.7012,-992.4243 1398.6315,-989.13 1404.449,-997.9848 1408.7012,-992.4243\"/>\n",
       "</g>\n",
       "<!-- 39 -->\n",
       "<g id=\"node40\" class=\"node\">\n",
       "<title>39</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"1753.5,-989 1630.5,-989 1630.5,-921 1753.5,-921 1753.5,-989\"/>\n",
       "<text text-anchor=\"middle\" x=\"1692\" y=\"-973.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[57] &lt;= 0.5</text>\n",
       "<text text-anchor=\"middle\" x=\"1692\" y=\"-958.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.142</text>\n",
       "<text text-anchor=\"middle\" x=\"1692\" y=\"-943.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 78</text>\n",
       "<text text-anchor=\"middle\" x=\"1692\" y=\"-928.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [72, 6]</text>\n",
       "</g>\n",
       "<!-- 27&#45;&gt;39 -->\n",
       "<g id=\"edge39\" class=\"edge\">\n",
       "<title>27&#45;&gt;39</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1556.1424,-1024.9465C1576.983,-1014.2167 1600.0805,-1002.3249 1621.2299,-991.4361\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1622.8738,-994.5264 1630.1625,-986.8371 1619.6695,-988.3029 1622.8738,-994.5264\"/>\n",
       "</g>\n",
       "<!-- 29 -->\n",
       "<g id=\"node30\" class=\"node\">\n",
       "<title>29</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"1207,-885 1089,-885 1089,-817 1207,-817 1207,-885\"/>\n",
       "<text text-anchor=\"middle\" x=\"1148\" y=\"-869.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[0] &lt;= 0.467</text>\n",
       "<text text-anchor=\"middle\" x=\"1148\" y=\"-854.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.18</text>\n",
       "<text text-anchor=\"middle\" x=\"1148\" y=\"-839.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 10</text>\n",
       "<text text-anchor=\"middle\" x=\"1148\" y=\"-824.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [9, 1]</text>\n",
       "</g>\n",
       "<!-- 28&#45;&gt;29 -->\n",
       "<g id=\"edge29\" class=\"edge\">\n",
       "<title>28&#45;&gt;29</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1287.9703,-921.6646C1265.01,-910.073 1239.2237,-897.0547 1216.0991,-885.3801\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1217.548,-882.1909 1207.0437,-880.8085 1214.3932,-888.4397 1217.548,-882.1909\"/>\n",
       "</g>\n",
       "<!-- 32 -->\n",
       "<g id=\"node33\" class=\"node\">\n",
       "<title>32</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"1415.5,-885 1292.5,-885 1292.5,-817 1415.5,-817 1415.5,-885\"/>\n",
       "<text text-anchor=\"middle\" x=\"1354\" y=\"-869.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[15] &lt;= 0.5</text>\n",
       "<text text-anchor=\"middle\" x=\"1354\" y=\"-854.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.397</text>\n",
       "<text text-anchor=\"middle\" x=\"1354\" y=\"-839.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 22</text>\n",
       "<text text-anchor=\"middle\" x=\"1354\" y=\"-824.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [6, 16]</text>\n",
       "</g>\n",
       "<!-- 28&#45;&gt;32 -->\n",
       "<g id=\"edge32\" class=\"edge\">\n",
       "<title>28&#45;&gt;32</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1354,-920.9465C1354,-912.776 1354,-903.9318 1354,-895.3697\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1357.5001,-895.13 1354,-885.13 1350.5001,-895.13 1357.5001,-895.13\"/>\n",
       "</g>\n",
       "<!-- 30 -->\n",
       "<g id=\"node31\" class=\"node\">\n",
       "<title>30</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"1072,-773.5 958,-773.5 958,-720.5 1072,-720.5 1072,-773.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"1015\" y=\"-758.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"1015\" y=\"-743.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 1</text>\n",
       "<text text-anchor=\"middle\" x=\"1015\" y=\"-728.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 1]</text>\n",
       "</g>\n",
       "<!-- 29&#45;&gt;30 -->\n",
       "<g id=\"edge30\" class=\"edge\">\n",
       "<title>29&#45;&gt;30</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1104.4508,-816.9465C1089.2375,-805.0504 1072.1977,-791.726 1057.1043,-779.9237\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1058.9769,-776.9449 1048.9433,-773.5422 1054.6649,-782.4592 1058.9769,-776.9449\"/>\n",
       "</g>\n",
       "<!-- 31 -->\n",
       "<g id=\"node32\" class=\"node\">\n",
       "<title>31</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"1204,-773.5 1090,-773.5 1090,-720.5 1204,-720.5 1204,-773.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"1147\" y=\"-758.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"1147\" y=\"-743.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 9</text>\n",
       "<text text-anchor=\"middle\" x=\"1147\" y=\"-728.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [9, 0]</text>\n",
       "</g>\n",
       "<!-- 29&#45;&gt;31 -->\n",
       "<g id=\"edge31\" class=\"edge\">\n",
       "<title>29&#45;&gt;31</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1147.6726,-816.9465C1147.5698,-806.2621 1147.456,-794.4254 1147.3517,-783.5742\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1150.8513,-783.508 1147.2552,-773.5422 1143.8516,-783.5754 1150.8513,-783.508\"/>\n",
       "</g>\n",
       "<!-- 33 -->\n",
       "<g id=\"node34\" class=\"node\">\n",
       "<title>33</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"1345.5,-781 1222.5,-781 1222.5,-713 1345.5,-713 1345.5,-781\"/>\n",
       "<text text-anchor=\"middle\" x=\"1284\" y=\"-765.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[3] &lt;= 0.019</text>\n",
       "<text text-anchor=\"middle\" x=\"1284\" y=\"-750.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.124</text>\n",
       "<text text-anchor=\"middle\" x=\"1284\" y=\"-735.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 15</text>\n",
       "<text text-anchor=\"middle\" x=\"1284\" y=\"-720.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [1, 14]</text>\n",
       "</g>\n",
       "<!-- 32&#45;&gt;33 -->\n",
       "<g id=\"edge33\" class=\"edge\">\n",
       "<title>32&#45;&gt;33</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1331.0794,-816.9465C1325.2174,-808.2373 1318.8402,-798.7626 1312.727,-789.6801\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1315.4595,-787.4716 1306.9721,-781.13 1309.6524,-791.3802 1315.4595,-787.4716\"/>\n",
       "</g>\n",
       "<!-- 36 -->\n",
       "<g id=\"node37\" class=\"node\">\n",
       "<title>36</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"1478,-781 1364,-781 1364,-713 1478,-713 1478,-781\"/>\n",
       "<text text-anchor=\"middle\" x=\"1421\" y=\"-765.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[59] &lt;= 0.5</text>\n",
       "<text text-anchor=\"middle\" x=\"1421\" y=\"-750.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.408</text>\n",
       "<text text-anchor=\"middle\" x=\"1421\" y=\"-735.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 7</text>\n",
       "<text text-anchor=\"middle\" x=\"1421\" y=\"-720.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [5, 2]</text>\n",
       "</g>\n",
       "<!-- 32&#45;&gt;36 -->\n",
       "<g id=\"edge36\" class=\"edge\">\n",
       "<title>32&#45;&gt;36</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1375.9383,-816.9465C1381.549,-808.2373 1387.6529,-798.7626 1393.5042,-789.6801\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1396.5389,-791.4321 1399.0124,-781.13 1390.6543,-787.641 1396.5389,-791.4321\"/>\n",
       "</g>\n",
       "<!-- 34 -->\n",
       "<g id=\"node35\" class=\"node\">\n",
       "<title>34</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"1138,-669.5 1024,-669.5 1024,-616.5 1138,-616.5 1138,-669.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"1081\" y=\"-654.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"1081\" y=\"-639.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 1</text>\n",
       "<text text-anchor=\"middle\" x=\"1081\" y=\"-624.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [1, 0]</text>\n",
       "</g>\n",
       "<!-- 33&#45;&gt;34 -->\n",
       "<g id=\"edge34\" class=\"edge\">\n",
       "<title>33&#45;&gt;34</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1222.258,-715.508C1198.6976,-703.4763 1171.5962,-689.6191 1147,-677 1145.2368,-676.0954 1143.4473,-675.1768 1141.641,-674.2491\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1143.0082,-671.0167 1132.5142,-669.5585 1139.8085,-677.2426 1143.0082,-671.0167\"/>\n",
       "</g>\n",
       "<!-- 35 -->\n",
       "<g id=\"node36\" class=\"node\">\n",
       "<title>35</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"1279.5,-669.5 1156.5,-669.5 1156.5,-616.5 1279.5,-616.5 1279.5,-669.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"1218\" y=\"-654.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"1218\" y=\"-639.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 14</text>\n",
       "<text text-anchor=\"middle\" x=\"1218\" y=\"-624.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 14]</text>\n",
       "</g>\n",
       "<!-- 33&#45;&gt;35 -->\n",
       "<g id=\"edge35\" class=\"edge\">\n",
       "<title>33&#45;&gt;35</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1262.3891,-712.9465C1255.329,-701.8215 1247.4761,-689.4473 1240.3619,-678.237\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1243.1575,-676.1101 1234.8441,-669.5422 1237.2472,-679.8609 1243.1575,-676.1101\"/>\n",
       "</g>\n",
       "<!-- 37 -->\n",
       "<g id=\"node38\" class=\"node\">\n",
       "<title>37</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"1412,-669.5 1298,-669.5 1298,-616.5 1412,-616.5 1412,-669.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"1355\" y=\"-654.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"1355\" y=\"-639.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 5</text>\n",
       "<text text-anchor=\"middle\" x=\"1355\" y=\"-624.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [5, 0]</text>\n",
       "</g>\n",
       "<!-- 36&#45;&gt;37 -->\n",
       "<g id=\"edge37\" class=\"edge\">\n",
       "<title>36&#45;&gt;37</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1399.3891,-712.9465C1392.329,-701.8215 1384.4761,-689.4473 1377.3619,-678.237\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1380.1575,-676.1101 1371.8441,-669.5422 1374.2472,-679.8609 1380.1575,-676.1101\"/>\n",
       "</g>\n",
       "<!-- 38 -->\n",
       "<g id=\"node39\" class=\"node\">\n",
       "<title>38</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"1544,-669.5 1430,-669.5 1430,-616.5 1544,-616.5 1544,-669.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"1487\" y=\"-654.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"1487\" y=\"-639.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 2</text>\n",
       "<text text-anchor=\"middle\" x=\"1487\" y=\"-624.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 2]</text>\n",
       "</g>\n",
       "<!-- 36&#45;&gt;38 -->\n",
       "<g id=\"edge38\" class=\"edge\">\n",
       "<title>36&#45;&gt;38</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1442.6109,-712.9465C1449.671,-701.8215 1457.5239,-689.4473 1464.6381,-678.237\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1467.7528,-679.8609 1470.1559,-669.5422 1461.8425,-676.1101 1467.7528,-679.8609\"/>\n",
       "</g>\n",
       "<!-- 40 -->\n",
       "<g id=\"node41\" class=\"node\">\n",
       "<title>40</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"1753.5,-885 1630.5,-885 1630.5,-817 1753.5,-817 1753.5,-885\"/>\n",
       "<text text-anchor=\"middle\" x=\"1692\" y=\"-869.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[59] &lt;= 0.5</text>\n",
       "<text text-anchor=\"middle\" x=\"1692\" y=\"-854.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.1</text>\n",
       "<text text-anchor=\"middle\" x=\"1692\" y=\"-839.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 76</text>\n",
       "<text text-anchor=\"middle\" x=\"1692\" y=\"-824.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [72, 4]</text>\n",
       "</g>\n",
       "<!-- 39&#45;&gt;40 -->\n",
       "<g id=\"edge40\" class=\"edge\">\n",
       "<title>39&#45;&gt;40</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1692,-920.9465C1692,-912.776 1692,-903.9318 1692,-895.3697\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1695.5001,-895.13 1692,-885.13 1688.5001,-895.13 1695.5001,-895.13\"/>\n",
       "</g>\n",
       "<!-- 49 -->\n",
       "<g id=\"node50\" class=\"node\">\n",
       "<title>49</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"1886,-877.5 1772,-877.5 1772,-824.5 1886,-824.5 1886,-877.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"1829\" y=\"-862.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"1829\" y=\"-847.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 2</text>\n",
       "<text text-anchor=\"middle\" x=\"1829\" y=\"-832.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 2]</text>\n",
       "</g>\n",
       "<!-- 39&#45;&gt;49 -->\n",
       "<g id=\"edge49\" class=\"edge\">\n",
       "<title>39&#45;&gt;49</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1736.8589,-920.9465C1752.6748,-908.9403 1770.4073,-895.4791 1786.0607,-883.5962\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1788.1871,-886.3764 1794.0358,-877.5422 1783.9545,-880.8009 1788.1871,-886.3764\"/>\n",
       "</g>\n",
       "<!-- 41 -->\n",
       "<g id=\"node42\" class=\"node\">\n",
       "<title>41</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"1753.5,-781 1630.5,-781 1630.5,-713 1753.5,-713 1753.5,-781\"/>\n",
       "<text text-anchor=\"middle\" x=\"1692\" y=\"-765.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[54] &lt;= 0.5</text>\n",
       "<text text-anchor=\"middle\" x=\"1692\" y=\"-750.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.053</text>\n",
       "<text text-anchor=\"middle\" x=\"1692\" y=\"-735.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 73</text>\n",
       "<text text-anchor=\"middle\" x=\"1692\" y=\"-720.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [71, 2]</text>\n",
       "</g>\n",
       "<!-- 40&#45;&gt;41 -->\n",
       "<g id=\"edge41\" class=\"edge\">\n",
       "<title>40&#45;&gt;41</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1692,-816.9465C1692,-808.776 1692,-799.9318 1692,-791.3697\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1695.5001,-791.13 1692,-781.13 1688.5001,-791.13 1695.5001,-791.13\"/>\n",
       "</g>\n",
       "<!-- 46 -->\n",
       "<g id=\"node47\" class=\"node\">\n",
       "<title>46</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"1956,-781 1838,-781 1838,-713 1956,-713 1956,-781\"/>\n",
       "<text text-anchor=\"middle\" x=\"1897\" y=\"-765.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[4] &lt;= 0.448</text>\n",
       "<text text-anchor=\"middle\" x=\"1897\" y=\"-750.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.444</text>\n",
       "<text text-anchor=\"middle\" x=\"1897\" y=\"-735.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 3</text>\n",
       "<text text-anchor=\"middle\" x=\"1897\" y=\"-720.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [1, 2]</text>\n",
       "</g>\n",
       "<!-- 40&#45;&gt;46 -->\n",
       "<g id=\"edge46\" class=\"edge\">\n",
       "<title>40&#45;&gt;46</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1753.5068,-819.7966C1777.2458,-807.7533 1804.4972,-793.9282 1828.8303,-781.5837\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1830.6653,-784.5774 1837.9999,-776.9318 1827.4983,-778.3348 1830.6653,-784.5774\"/>\n",
       "</g>\n",
       "<!-- 42 -->\n",
       "<g id=\"node43\" class=\"node\">\n",
       "<title>42</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"1689.5,-677 1562.5,-677 1562.5,-609 1689.5,-609 1689.5,-677\"/>\n",
       "<text text-anchor=\"middle\" x=\"1626\" y=\"-661.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[19] &lt;= 0.369</text>\n",
       "<text text-anchor=\"middle\" x=\"1626\" y=\"-646.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.027</text>\n",
       "<text text-anchor=\"middle\" x=\"1626\" y=\"-631.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 72</text>\n",
       "<text text-anchor=\"middle\" x=\"1626\" y=\"-616.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [71, 1]</text>\n",
       "</g>\n",
       "<!-- 41&#45;&gt;42 -->\n",
       "<g id=\"edge42\" class=\"edge\">\n",
       "<title>41&#45;&gt;42</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1670.3891,-712.9465C1664.8622,-704.2373 1658.8494,-694.7626 1653.0854,-685.6801\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1655.9729,-683.6979 1647.6594,-677.13 1650.0626,-687.4487 1655.9729,-683.6979\"/>\n",
       "</g>\n",
       "<!-- 45 -->\n",
       "<g id=\"node46\" class=\"node\">\n",
       "<title>45</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"1822,-669.5 1708,-669.5 1708,-616.5 1822,-616.5 1822,-669.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"1765\" y=\"-654.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"1765\" y=\"-639.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 1</text>\n",
       "<text text-anchor=\"middle\" x=\"1765\" y=\"-624.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 1]</text>\n",
       "</g>\n",
       "<!-- 41&#45;&gt;45 -->\n",
       "<g id=\"edge45\" class=\"edge\">\n",
       "<title>41&#45;&gt;45</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1715.9029,-712.9465C1723.7892,-701.7113 1732.5697,-689.2021 1740.4998,-677.9043\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1743.4889,-679.7379 1746.3694,-669.5422 1737.7595,-675.7163 1743.4889,-679.7379\"/>\n",
       "</g>\n",
       "<!-- 43 -->\n",
       "<g id=\"node44\" class=\"node\">\n",
       "<title>43</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"1686.5,-565.5 1563.5,-565.5 1563.5,-512.5 1686.5,-512.5 1686.5,-565.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"1625\" y=\"-550.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"1625\" y=\"-535.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 71</text>\n",
       "<text text-anchor=\"middle\" x=\"1625\" y=\"-520.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [71, 0]</text>\n",
       "</g>\n",
       "<!-- 42&#45;&gt;43 -->\n",
       "<g id=\"edge43\" class=\"edge\">\n",
       "<title>42&#45;&gt;43</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1625.6726,-608.9465C1625.5698,-598.2621 1625.456,-586.4254 1625.3517,-575.5742\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1628.8513,-575.508 1625.2552,-565.5422 1621.8516,-575.5754 1628.8513,-575.508\"/>\n",
       "</g>\n",
       "<!-- 44 -->\n",
       "<g id=\"node45\" class=\"node\">\n",
       "<title>44</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"1819,-565.5 1705,-565.5 1705,-512.5 1819,-512.5 1819,-565.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"1762\" y=\"-550.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"1762\" y=\"-535.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 1</text>\n",
       "<text text-anchor=\"middle\" x=\"1762\" y=\"-520.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 1]</text>\n",
       "</g>\n",
       "<!-- 42&#45;&gt;44 -->\n",
       "<g id=\"edge44\" class=\"edge\">\n",
       "<title>42&#45;&gt;44</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1670.5315,-608.9465C1686.0879,-597.0504 1703.5121,-583.726 1718.9459,-571.9237\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1721.4735,-574.397 1727.291,-565.5422 1717.2213,-568.8364 1721.4735,-574.397\"/>\n",
       "</g>\n",
       "<!-- 47 -->\n",
       "<g id=\"node48\" class=\"node\">\n",
       "<title>47</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"1954,-669.5 1840,-669.5 1840,-616.5 1954,-616.5 1954,-669.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"1897\" y=\"-654.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"1897\" y=\"-639.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 2</text>\n",
       "<text text-anchor=\"middle\" x=\"1897\" y=\"-624.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 2]</text>\n",
       "</g>\n",
       "<!-- 46&#45;&gt;47 -->\n",
       "<g id=\"edge47\" class=\"edge\">\n",
       "<title>46&#45;&gt;47</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1897,-712.9465C1897,-702.2621 1897,-690.4254 1897,-679.5742\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1900.5001,-679.5421 1897,-669.5422 1893.5001,-679.5422 1900.5001,-679.5421\"/>\n",
       "</g>\n",
       "<!-- 48 -->\n",
       "<g id=\"node49\" class=\"node\">\n",
       "<title>48</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"2086,-669.5 1972,-669.5 1972,-616.5 2086,-616.5 2086,-669.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"2029\" y=\"-654.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"2029\" y=\"-639.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 1</text>\n",
       "<text text-anchor=\"middle\" x=\"2029\" y=\"-624.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [1, 0]</text>\n",
       "</g>\n",
       "<!-- 46&#45;&gt;48 -->\n",
       "<g id=\"edge48\" class=\"edge\">\n",
       "<title>46&#45;&gt;48</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1940.2217,-712.9465C1955.3206,-701.0504 1972.2324,-687.726 1987.2122,-675.9237\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1989.623,-678.4802 1995.3119,-669.5422 1985.2909,-672.9817 1989.623,-678.4802\"/>\n",
       "</g>\n",
       "<!-- 51 -->\n",
       "<g id=\"node52\" class=\"node\">\n",
       "<title>51</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"2472,-989 2340,-989 2340,-921 2472,-921 2472,-989\"/>\n",
       "<text text-anchor=\"middle\" x=\"2406\" y=\"-973.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[4] &lt;= 0.73</text>\n",
       "<text text-anchor=\"middle\" x=\"2406\" y=\"-958.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.433</text>\n",
       "<text text-anchor=\"middle\" x=\"2406\" y=\"-943.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 98</text>\n",
       "<text text-anchor=\"middle\" x=\"2406\" y=\"-928.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [31, 67]</text>\n",
       "</g>\n",
       "<!-- 50&#45;&gt;51 -->\n",
       "<g id=\"edge51\" class=\"edge\">\n",
       "<title>50&#45;&gt;51</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M2406,-1024.9465C2406,-1016.776 2406,-1007.9318 2406,-999.3697\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"2409.5001,-999.13 2406,-989.13 2402.5001,-999.13 2409.5001,-999.13\"/>\n",
       "</g>\n",
       "<!-- 90 -->\n",
       "<g id=\"node91\" class=\"node\">\n",
       "<title>90</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"2740.5,-989 2613.5,-989 2613.5,-921 2740.5,-921 2740.5,-989\"/>\n",
       "<text text-anchor=\"middle\" x=\"2677\" y=\"-973.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[13] &lt;= 0.502</text>\n",
       "<text text-anchor=\"middle\" x=\"2677\" y=\"-958.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.375</text>\n",
       "<text text-anchor=\"middle\" x=\"2677\" y=\"-943.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 36</text>\n",
       "<text text-anchor=\"middle\" x=\"2677\" y=\"-928.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [27, 9]</text>\n",
       "</g>\n",
       "<!-- 50&#45;&gt;90 -->\n",
       "<g id=\"edge90\" class=\"edge\">\n",
       "<title>50&#45;&gt;90</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M2472.2941,-1033.5587C2512.1265,-1018.2725 2562.8657,-998.8006 2603.8631,-983.0673\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"2605.1744,-986.313 2613.2565,-979.4624 2602.6664,-979.7777 2605.1744,-986.313\"/>\n",
       "</g>\n",
       "<!-- 52 -->\n",
       "<g id=\"node53\" class=\"node\">\n",
       "<title>52</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"2402,-885 2270,-885 2270,-817 2402,-817 2402,-885\"/>\n",
       "<text text-anchor=\"middle\" x=\"2336\" y=\"-869.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[3] &lt;= 0.699</text>\n",
       "<text text-anchor=\"middle\" x=\"2336\" y=\"-854.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.403</text>\n",
       "<text text-anchor=\"middle\" x=\"2336\" y=\"-839.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 93</text>\n",
       "<text text-anchor=\"middle\" x=\"2336\" y=\"-824.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [26, 67]</text>\n",
       "</g>\n",
       "<!-- 51&#45;&gt;52 -->\n",
       "<g id=\"edge52\" class=\"edge\">\n",
       "<title>51&#45;&gt;52</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M2383.0794,-920.9465C2377.2174,-912.2373 2370.8402,-902.7626 2364.727,-893.6801\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"2367.4595,-891.4716 2358.9721,-885.13 2361.6524,-895.3802 2367.4595,-891.4716\"/>\n",
       "</g>\n",
       "<!-- 89 -->\n",
       "<g id=\"node90\" class=\"node\">\n",
       "<title>89</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"2534,-877.5 2420,-877.5 2420,-824.5 2534,-824.5 2534,-877.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"2477\" y=\"-862.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"2477\" y=\"-847.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 5</text>\n",
       "<text text-anchor=\"middle\" x=\"2477\" y=\"-832.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [5, 0]</text>\n",
       "</g>\n",
       "<!-- 51&#45;&gt;89 -->\n",
       "<g id=\"edge89\" class=\"edge\">\n",
       "<title>51&#45;&gt;89</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M2429.2481,-920.9465C2436.9182,-909.7113 2445.4582,-897.2021 2453.1711,-885.9043\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"2456.1322,-887.7745 2458.8799,-877.5422 2450.3509,-883.8277 2456.1322,-887.7745\"/>\n",
       "</g>\n",
       "<!-- 53 -->\n",
       "<g id=\"node54\" class=\"node\">\n",
       "<title>53</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"2386,-781 2254,-781 2254,-713 2386,-713 2386,-781\"/>\n",
       "<text text-anchor=\"middle\" x=\"2320\" y=\"-765.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[3] &lt;= 0.272</text>\n",
       "<text text-anchor=\"middle\" x=\"2320\" y=\"-750.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.341</text>\n",
       "<text text-anchor=\"middle\" x=\"2320\" y=\"-735.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 78</text>\n",
       "<text text-anchor=\"middle\" x=\"2320\" y=\"-720.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [17, 61]</text>\n",
       "</g>\n",
       "<!-- 52&#45;&gt;53 -->\n",
       "<g id=\"edge53\" class=\"edge\">\n",
       "<title>52&#45;&gt;53</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M2330.761,-816.9465C2329.4902,-808.6863 2328.1134,-799.7374 2326.7827,-791.0875\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"2330.2307,-790.4815 2325.2508,-781.13 2323.3121,-791.5459 2330.2307,-790.4815\"/>\n",
       "</g>\n",
       "<!-- 82 -->\n",
       "<g id=\"node83\" class=\"node\">\n",
       "<title>82</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"2524,-781 2408,-781 2408,-713 2524,-713 2524,-781\"/>\n",
       "<text text-anchor=\"middle\" x=\"2466\" y=\"-765.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[70] &lt;= 0.5</text>\n",
       "<text text-anchor=\"middle\" x=\"2466\" y=\"-750.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.48</text>\n",
       "<text text-anchor=\"middle\" x=\"2466\" y=\"-735.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 15</text>\n",
       "<text text-anchor=\"middle\" x=\"2466\" y=\"-720.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [9, 6]</text>\n",
       "</g>\n",
       "<!-- 52&#45;&gt;82 -->\n",
       "<g id=\"edge82\" class=\"edge\">\n",
       "<title>52&#45;&gt;82</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M2378.5669,-816.9465C2390.3512,-807.519 2403.2568,-797.1946 2415.4481,-787.4415\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"2417.7152,-790.11 2423.3375,-781.13 2413.3423,-784.6439 2417.7152,-790.11\"/>\n",
       "</g>\n",
       "<!-- 54 -->\n",
       "<g id=\"node55\" class=\"node\">\n",
       "<title>54</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"2236,-677 2104,-677 2104,-609 2236,-609 2236,-677\"/>\n",
       "<text text-anchor=\"middle\" x=\"2170\" y=\"-661.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[13] &lt;= 0.31</text>\n",
       "<text text-anchor=\"middle\" x=\"2170\" y=\"-646.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.413</text>\n",
       "<text text-anchor=\"middle\" x=\"2170\" y=\"-631.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 55</text>\n",
       "<text text-anchor=\"middle\" x=\"2170\" y=\"-616.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [16, 39]</text>\n",
       "</g>\n",
       "<!-- 53&#45;&gt;54 -->\n",
       "<g id=\"edge54\" class=\"edge\">\n",
       "<title>53&#45;&gt;54</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M2270.8844,-712.9465C2257.0281,-703.3395 2241.8283,-692.8009 2227.5261,-682.8848\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"2229.4382,-679.9515 2219.226,-677.13 2225.4497,-685.7041 2229.4382,-679.9515\"/>\n",
       "</g>\n",
       "<!-- 77 -->\n",
       "<g id=\"node78\" class=\"node\">\n",
       "<title>77</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"2383.5,-677 2256.5,-677 2256.5,-609 2383.5,-609 2383.5,-677\"/>\n",
       "<text text-anchor=\"middle\" x=\"2320\" y=\"-661.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[23] &lt;= 0.347</text>\n",
       "<text text-anchor=\"middle\" x=\"2320\" y=\"-646.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.083</text>\n",
       "<text text-anchor=\"middle\" x=\"2320\" y=\"-631.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 23</text>\n",
       "<text text-anchor=\"middle\" x=\"2320\" y=\"-616.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [1, 22]</text>\n",
       "</g>\n",
       "<!-- 53&#45;&gt;77 -->\n",
       "<g id=\"edge77\" class=\"edge\">\n",
       "<title>53&#45;&gt;77</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M2320,-712.9465C2320,-704.776 2320,-695.9318 2320,-687.3697\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"2323.5001,-687.13 2320,-677.13 2316.5001,-687.13 2323.5001,-687.13\"/>\n",
       "</g>\n",
       "<!-- 55 -->\n",
       "<g id=\"node56\" class=\"node\">\n",
       "<title>55</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"1970,-573 1838,-573 1838,-505 1970,-505 1970,-573\"/>\n",
       "<text text-anchor=\"middle\" x=\"1904\" y=\"-557.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[1] &lt;= 0.148</text>\n",
       "<text text-anchor=\"middle\" x=\"1904\" y=\"-542.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.478</text>\n",
       "<text text-anchor=\"middle\" x=\"1904\" y=\"-527.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 38</text>\n",
       "<text text-anchor=\"middle\" x=\"1904\" y=\"-512.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [15, 23]</text>\n",
       "</g>\n",
       "<!-- 54&#45;&gt;55 -->\n",
       "<g id=\"edge55\" class=\"edge\">\n",
       "<title>54&#45;&gt;55</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M2103.9355,-612.5114C2100.9285,-611.2942 2097.9399,-610.1176 2095,-609 2047.3996,-590.9051 2031.2805,-591.9828 1979.7448,-573.1119\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1980.6569,-569.7162 1970.0645,-569.4886 1978.203,-576.272 1980.6569,-569.7162\"/>\n",
       "</g>\n",
       "<!-- 74 -->\n",
       "<g id=\"node75\" class=\"node\">\n",
       "<title>74</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"2111.5,-573 1988.5,-573 1988.5,-505 2111.5,-505 2111.5,-573\"/>\n",
       "<text text-anchor=\"middle\" x=\"2050\" y=\"-557.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[41] &lt;= 0.5</text>\n",
       "<text text-anchor=\"middle\" x=\"2050\" y=\"-542.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.111</text>\n",
       "<text text-anchor=\"middle\" x=\"2050\" y=\"-527.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 17</text>\n",
       "<text text-anchor=\"middle\" x=\"2050\" y=\"-512.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [1, 16]</text>\n",
       "</g>\n",
       "<!-- 54&#45;&gt;74 -->\n",
       "<g id=\"edge74\" class=\"edge\">\n",
       "<title>54&#45;&gt;74</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M2130.7075,-608.9465C2119.9333,-599.6088 2108.1436,-589.3911 2096.985,-579.7203\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"2099.2299,-577.0344 2089.3808,-573.13 2094.6454,-582.3243 2099.2299,-577.0344\"/>\n",
       "</g>\n",
       "<!-- 56 -->\n",
       "<g id=\"node57\" class=\"node\">\n",
       "<title>56</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"1764.5,-469 1641.5,-469 1641.5,-401 1764.5,-401 1764.5,-469\"/>\n",
       "<text text-anchor=\"middle\" x=\"1703\" y=\"-453.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[7] &lt;= 0.342</text>\n",
       "<text text-anchor=\"middle\" x=\"1703\" y=\"-438.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.291</text>\n",
       "<text text-anchor=\"middle\" x=\"1703\" y=\"-423.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 17</text>\n",
       "<text text-anchor=\"middle\" x=\"1703\" y=\"-408.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [3, 14]</text>\n",
       "</g>\n",
       "<!-- 55&#45;&gt;56 -->\n",
       "<g id=\"edge56\" class=\"edge\">\n",
       "<title>55&#45;&gt;56</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1838.1851,-504.9465C1817.4477,-494.2167 1794.4645,-482.3249 1773.4197,-471.4361\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1775.0214,-468.3241 1764.5314,-466.8371 1771.8045,-474.5412 1775.0214,-468.3241\"/>\n",
       "</g>\n",
       "<!-- 63 -->\n",
       "<g id=\"node64\" class=\"node\">\n",
       "<title>63</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"1969.5,-469 1842.5,-469 1842.5,-401 1969.5,-401 1969.5,-469\"/>\n",
       "<text text-anchor=\"middle\" x=\"1906\" y=\"-453.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[13] &lt;= 0.227</text>\n",
       "<text text-anchor=\"middle\" x=\"1906\" y=\"-438.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.49</text>\n",
       "<text text-anchor=\"middle\" x=\"1906\" y=\"-423.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 21</text>\n",
       "<text text-anchor=\"middle\" x=\"1906\" y=\"-408.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [12, 9]</text>\n",
       "</g>\n",
       "<!-- 55&#45;&gt;63 -->\n",
       "<g id=\"edge63\" class=\"edge\">\n",
       "<title>55&#45;&gt;63</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1904.6549,-504.9465C1904.812,-496.776 1904.9821,-487.9318 1905.1467,-479.3697\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1908.6506,-479.1955 1905.3437,-469.13 1901.6519,-479.0608 1908.6506,-479.1955\"/>\n",
       "</g>\n",
       "<!-- 57 -->\n",
       "<g id=\"node58\" class=\"node\">\n",
       "<title>57</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"1623,-357.5 1509,-357.5 1509,-304.5 1623,-304.5 1623,-357.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"1566\" y=\"-342.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"1566\" y=\"-327.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 1</text>\n",
       "<text text-anchor=\"middle\" x=\"1566\" y=\"-312.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [1, 0]</text>\n",
       "</g>\n",
       "<!-- 56&#45;&gt;57 -->\n",
       "<g id=\"edge57\" class=\"edge\">\n",
       "<title>56&#45;&gt;57</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1658.1411,-400.9465C1642.3252,-388.9403 1624.5927,-375.4791 1608.9393,-363.5962\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1611.0455,-360.8009 1600.9642,-357.5422 1606.8129,-366.3764 1611.0455,-360.8009\"/>\n",
       "</g>\n",
       "<!-- 58 -->\n",
       "<g id=\"node59\" class=\"node\">\n",
       "<title>58</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"1764.5,-365 1641.5,-365 1641.5,-297 1764.5,-297 1764.5,-365\"/>\n",
       "<text text-anchor=\"middle\" x=\"1703\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[16] &lt;= 0.5</text>\n",
       "<text text-anchor=\"middle\" x=\"1703\" y=\"-334.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.219</text>\n",
       "<text text-anchor=\"middle\" x=\"1703\" y=\"-319.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 16</text>\n",
       "<text text-anchor=\"middle\" x=\"1703\" y=\"-304.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [2, 14]</text>\n",
       "</g>\n",
       "<!-- 56&#45;&gt;58 -->\n",
       "<g id=\"edge58\" class=\"edge\">\n",
       "<title>56&#45;&gt;58</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1703,-400.9465C1703,-392.776 1703,-383.9318 1703,-375.3697\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1706.5001,-375.13 1703,-365.13 1699.5001,-375.13 1706.5001,-375.13\"/>\n",
       "</g>\n",
       "<!-- 59 -->\n",
       "<g id=\"node60\" class=\"node\">\n",
       "<title>59</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"1627.5,-261 1500.5,-261 1500.5,-193 1627.5,-193 1627.5,-261\"/>\n",
       "<text text-anchor=\"middle\" x=\"1564\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[19] &lt;= 0.031</text>\n",
       "<text text-anchor=\"middle\" x=\"1564\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.124</text>\n",
       "<text text-anchor=\"middle\" x=\"1564\" y=\"-215.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 15</text>\n",
       "<text text-anchor=\"middle\" x=\"1564\" y=\"-200.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [1, 14]</text>\n",
       "</g>\n",
       "<!-- 58&#45;&gt;59 -->\n",
       "<g id=\"edge59\" class=\"edge\">\n",
       "<title>58&#45;&gt;59</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1657.4862,-296.9465C1644.766,-287.4293 1630.824,-276.9978 1617.6794,-267.163\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1619.7198,-264.3184 1609.6161,-261.13 1615.5262,-269.9232 1619.7198,-264.3184\"/>\n",
       "</g>\n",
       "<!-- 62 -->\n",
       "<g id=\"node63\" class=\"node\">\n",
       "<title>62</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"1760,-253.5 1646,-253.5 1646,-200.5 1760,-200.5 1760,-253.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"1703\" y=\"-238.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"1703\" y=\"-223.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 1</text>\n",
       "<text text-anchor=\"middle\" x=\"1703\" y=\"-208.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [1, 0]</text>\n",
       "</g>\n",
       "<!-- 58&#45;&gt;62 -->\n",
       "<g id=\"edge62\" class=\"edge\">\n",
       "<title>58&#45;&gt;62</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1703,-296.9465C1703,-286.2621 1703,-274.4254 1703,-263.5742\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1706.5001,-263.5421 1703,-253.5422 1699.5001,-263.5422 1706.5001,-263.5421\"/>\n",
       "</g>\n",
       "<!-- 60 -->\n",
       "<g id=\"node61\" class=\"node\">\n",
       "<title>60</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"1553,-149.5 1439,-149.5 1439,-96.5 1553,-96.5 1553,-149.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"1496\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"1496\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 1</text>\n",
       "<text text-anchor=\"middle\" x=\"1496\" y=\"-104.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [1, 0]</text>\n",
       "</g>\n",
       "<!-- 59&#45;&gt;60 -->\n",
       "<g id=\"edge60\" class=\"edge\">\n",
       "<title>59&#45;&gt;60</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1541.7343,-192.9465C1534.4602,-181.8215 1526.3694,-169.4473 1519.0396,-158.237\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1521.7564,-155.9965 1513.3545,-149.5422 1515.8976,-159.8273 1521.7564,-155.9965\"/>\n",
       "</g>\n",
       "<!-- 61 -->\n",
       "<g id=\"node62\" class=\"node\">\n",
       "<title>61</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"1694.5,-149.5 1571.5,-149.5 1571.5,-96.5 1694.5,-96.5 1694.5,-149.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"1633\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"1633\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 14</text>\n",
       "<text text-anchor=\"middle\" x=\"1633\" y=\"-104.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 14]</text>\n",
       "</g>\n",
       "<!-- 59&#45;&gt;61 -->\n",
       "<g id=\"edge61\" class=\"edge\">\n",
       "<title>59&#45;&gt;61</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1586.5932,-192.9465C1594.0473,-181.7113 1602.3467,-169.2021 1609.8423,-157.9043\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1612.7782,-159.81 1615.3903,-149.5422 1606.9453,-155.94 1612.7782,-159.81\"/>\n",
       "</g>\n",
       "<!-- 64 -->\n",
       "<g id=\"node65\" class=\"node\">\n",
       "<title>64</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"1963,-365 1849,-365 1849,-297 1963,-297 1963,-365\"/>\n",
       "<text text-anchor=\"middle\" x=\"1906\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[11] &lt;= 0.5</text>\n",
       "<text text-anchor=\"middle\" x=\"1906\" y=\"-334.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.219</text>\n",
       "<text text-anchor=\"middle\" x=\"1906\" y=\"-319.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 8</text>\n",
       "<text text-anchor=\"middle\" x=\"1906\" y=\"-304.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [7, 1]</text>\n",
       "</g>\n",
       "<!-- 63&#45;&gt;64 -->\n",
       "<g id=\"edge64\" class=\"edge\">\n",
       "<title>63&#45;&gt;64</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1906,-400.9465C1906,-392.776 1906,-383.9318 1906,-375.3697\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1909.5001,-375.13 1906,-365.13 1902.5001,-375.13 1909.5001,-375.13\"/>\n",
       "</g>\n",
       "<!-- 67 -->\n",
       "<g id=\"node68\" class=\"node\">\n",
       "<title>67</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"2164,-365 2048,-365 2048,-297 2164,-297 2164,-365\"/>\n",
       "<text text-anchor=\"middle\" x=\"2106\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[70] &lt;= 0.5</text>\n",
       "<text text-anchor=\"middle\" x=\"2106\" y=\"-334.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.473</text>\n",
       "<text text-anchor=\"middle\" x=\"2106\" y=\"-319.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 13</text>\n",
       "<text text-anchor=\"middle\" x=\"2106\" y=\"-304.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [5, 8]</text>\n",
       "</g>\n",
       "<!-- 63&#45;&gt;67 -->\n",
       "<g id=\"edge67\" class=\"edge\">\n",
       "<title>63&#45;&gt;67</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1969.5561,-401.9509C1991.5721,-390.5025 2016.3184,-377.6344 2038.6306,-366.0321\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"2040.4434,-369.0344 2047.7008,-361.3156 2037.2139,-362.8239 2040.4434,-369.0344\"/>\n",
       "</g>\n",
       "<!-- 65 -->\n",
       "<g id=\"node66\" class=\"node\">\n",
       "<title>65</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"1892,-253.5 1778,-253.5 1778,-200.5 1892,-200.5 1892,-253.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"1835\" y=\"-238.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"1835\" y=\"-223.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 1</text>\n",
       "<text text-anchor=\"middle\" x=\"1835\" y=\"-208.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 1]</text>\n",
       "</g>\n",
       "<!-- 64&#45;&gt;65 -->\n",
       "<g id=\"edge65\" class=\"edge\">\n",
       "<title>64&#45;&gt;65</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1882.7519,-296.9465C1875.0818,-285.7113 1866.5418,-273.2021 1858.8289,-261.9043\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1861.6491,-259.8277 1853.1201,-253.5422 1855.8678,-263.7745 1861.6491,-259.8277\"/>\n",
       "</g>\n",
       "<!-- 66 -->\n",
       "<g id=\"node67\" class=\"node\">\n",
       "<title>66</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"2024,-253.5 1910,-253.5 1910,-200.5 2024,-200.5 2024,-253.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"1967\" y=\"-238.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"1967\" y=\"-223.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 7</text>\n",
       "<text text-anchor=\"middle\" x=\"1967\" y=\"-208.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [7, 0]</text>\n",
       "</g>\n",
       "<!-- 64&#45;&gt;66 -->\n",
       "<g id=\"edge66\" class=\"edge\">\n",
       "<title>64&#45;&gt;66</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M1925.9737,-296.9465C1932.499,-285.8215 1939.7569,-273.4473 1946.3322,-262.237\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"1949.3916,-263.9387 1951.432,-253.5422 1943.3536,-260.3971 1949.3916,-263.9387\"/>\n",
       "</g>\n",
       "<!-- 68 -->\n",
       "<g id=\"node69\" class=\"node\">\n",
       "<title>68</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"2169.5,-261 2042.5,-261 2042.5,-193 2169.5,-193 2169.5,-261\"/>\n",
       "<text text-anchor=\"middle\" x=\"2106\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[23] &lt;= 0.189</text>\n",
       "<text text-anchor=\"middle\" x=\"2106\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.32</text>\n",
       "<text text-anchor=\"middle\" x=\"2106\" y=\"-215.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 10</text>\n",
       "<text text-anchor=\"middle\" x=\"2106\" y=\"-200.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [2, 8]</text>\n",
       "</g>\n",
       "<!-- 67&#45;&gt;68 -->\n",
       "<g id=\"edge68\" class=\"edge\">\n",
       "<title>67&#45;&gt;68</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M2106,-296.9465C2106,-288.776 2106,-279.9318 2106,-271.3697\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"2109.5001,-271.13 2106,-261.13 2102.5001,-271.13 2109.5001,-271.13\"/>\n",
       "</g>\n",
       "<!-- 73 -->\n",
       "<g id=\"node74\" class=\"node\">\n",
       "<title>73</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"2302,-253.5 2188,-253.5 2188,-200.5 2302,-200.5 2302,-253.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"2245\" y=\"-238.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"2245\" y=\"-223.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 3</text>\n",
       "<text text-anchor=\"middle\" x=\"2245\" y=\"-208.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [3, 0]</text>\n",
       "</g>\n",
       "<!-- 67&#45;&gt;73 -->\n",
       "<g id=\"edge73\" class=\"edge\">\n",
       "<title>67&#45;&gt;73</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M2151.5138,-296.9465C2167.5606,-284.9403 2185.5519,-271.4791 2201.4339,-259.5962\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"2203.6152,-262.3354 2209.5254,-253.5422 2199.4217,-256.7306 2203.6152,-262.3354\"/>\n",
       "</g>\n",
       "<!-- 69 -->\n",
       "<g id=\"node70\" class=\"node\">\n",
       "<title>69</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"2093,-149.5 1979,-149.5 1979,-96.5 2093,-96.5 2093,-149.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"2036\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"2036\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 7</text>\n",
       "<text text-anchor=\"middle\" x=\"2036\" y=\"-104.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 7]</text>\n",
       "</g>\n",
       "<!-- 68&#45;&gt;69 -->\n",
       "<g id=\"edge69\" class=\"edge\">\n",
       "<title>68&#45;&gt;69</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M2083.0794,-192.9465C2075.5172,-181.7113 2067.0976,-169.2021 2059.4933,-157.9043\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"2062.3523,-155.8837 2053.8649,-149.5422 2056.5452,-159.7924 2062.3523,-155.8837\"/>\n",
       "</g>\n",
       "<!-- 70 -->\n",
       "<g id=\"node71\" class=\"node\">\n",
       "<title>70</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"2238.5,-157 2111.5,-157 2111.5,-89 2238.5,-89 2238.5,-157\"/>\n",
       "<text text-anchor=\"middle\" x=\"2175\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[13] &lt;= 0.269</text>\n",
       "<text text-anchor=\"middle\" x=\"2175\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.444</text>\n",
       "<text text-anchor=\"middle\" x=\"2175\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 3</text>\n",
       "<text text-anchor=\"middle\" x=\"2175\" y=\"-96.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [2, 1]</text>\n",
       "</g>\n",
       "<!-- 68&#45;&gt;70 -->\n",
       "<g id=\"edge70\" class=\"edge\">\n",
       "<title>68&#45;&gt;70</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M2128.5932,-192.9465C2134.3714,-184.2373 2140.6575,-174.7626 2146.6834,-165.6801\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"2149.744,-167.3978 2152.3561,-157.13 2143.911,-163.5278 2149.744,-167.3978\"/>\n",
       "</g>\n",
       "<!-- 71 -->\n",
       "<g id=\"node72\" class=\"node\">\n",
       "<title>71</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"2166,-53 2052,-53 2052,0 2166,0 2166,-53\"/>\n",
       "<text text-anchor=\"middle\" x=\"2109\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"2109\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 1</text>\n",
       "<text text-anchor=\"middle\" x=\"2109\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 1]</text>\n",
       "</g>\n",
       "<!-- 70&#45;&gt;71 -->\n",
       "<g id=\"edge71\" class=\"edge\">\n",
       "<title>70&#45;&gt;71</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M2151.7309,-88.9777C2145.656,-80.0954 2139.0979,-70.5067 2132.9865,-61.5711\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"2135.8286,-59.5267 2127.2943,-53.2485 2130.0507,-63.4785 2135.8286,-59.5267\"/>\n",
       "</g>\n",
       "<!-- 72 -->\n",
       "<g id=\"node73\" class=\"node\">\n",
       "<title>72</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"2298,-53 2184,-53 2184,0 2298,0 2298,-53\"/>\n",
       "<text text-anchor=\"middle\" x=\"2241\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"2241\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 2</text>\n",
       "<text text-anchor=\"middle\" x=\"2241\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [2, 0]</text>\n",
       "</g>\n",
       "<!-- 70&#45;&gt;72 -->\n",
       "<g id=\"edge72\" class=\"edge\">\n",
       "<title>70&#45;&gt;72</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M2198.2691,-88.9777C2204.344,-80.0954 2210.9021,-70.5067 2217.0135,-61.5711\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"2219.9493,-63.4785 2222.7057,-53.2485 2214.1714,-59.5267 2219.9493,-63.4785\"/>\n",
       "</g>\n",
       "<!-- 75 -->\n",
       "<g id=\"node76\" class=\"node\">\n",
       "<title>75</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"2105,-461.5 1991,-461.5 1991,-408.5 2105,-408.5 2105,-461.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"2048\" y=\"-446.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"2048\" y=\"-431.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 1</text>\n",
       "<text text-anchor=\"middle\" x=\"2048\" y=\"-416.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [1, 0]</text>\n",
       "</g>\n",
       "<!-- 74&#45;&gt;75 -->\n",
       "<g id=\"edge75\" class=\"edge\">\n",
       "<title>74&#45;&gt;75</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M2049.3451,-504.9465C2049.1397,-494.2621 2048.912,-482.4254 2048.7034,-471.5742\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"2052.2021,-471.473 2048.5104,-461.5422 2045.2034,-471.6076 2052.2021,-471.473\"/>\n",
       "</g>\n",
       "<!-- 76 -->\n",
       "<g id=\"node77\" class=\"node\">\n",
       "<title>76</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"2246.5,-461.5 2123.5,-461.5 2123.5,-408.5 2246.5,-408.5 2246.5,-461.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"2185\" y=\"-446.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"2185\" y=\"-431.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 16</text>\n",
       "<text text-anchor=\"middle\" x=\"2185\" y=\"-416.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 16]</text>\n",
       "</g>\n",
       "<!-- 74&#45;&gt;76 -->\n",
       "<g id=\"edge76\" class=\"edge\">\n",
       "<title>74&#45;&gt;76</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M2094.204,-504.9465C2109.6461,-493.0504 2126.9422,-479.726 2142.2625,-467.9237\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"2144.7603,-470.4176 2150.5462,-461.5422 2140.4883,-464.8723 2144.7603,-470.4176\"/>\n",
       "</g>\n",
       "<!-- 78 -->\n",
       "<g id=\"node79\" class=\"node\">\n",
       "<title>78</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"2252.5,-565.5 2129.5,-565.5 2129.5,-512.5 2252.5,-512.5 2252.5,-565.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"2191\" y=\"-550.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"2191\" y=\"-535.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 21</text>\n",
       "<text text-anchor=\"middle\" x=\"2191\" y=\"-520.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 21]</text>\n",
       "</g>\n",
       "<!-- 77&#45;&gt;78 -->\n",
       "<g id=\"edge78\" class=\"edge\">\n",
       "<title>77&#45;&gt;78</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M2277.7606,-608.9465C2263.0048,-597.0504 2246.4775,-583.726 2231.838,-571.9237\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"2233.9043,-569.0938 2223.9225,-565.5422 2229.5109,-574.5433 2233.9043,-569.0938\"/>\n",
       "</g>\n",
       "<!-- 79 -->\n",
       "<g id=\"node80\" class=\"node\">\n",
       "<title>79</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"2397.5,-573 2270.5,-573 2270.5,-505 2397.5,-505 2397.5,-573\"/>\n",
       "<text text-anchor=\"middle\" x=\"2334\" y=\"-557.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[19] &lt;= 0.022</text>\n",
       "<text text-anchor=\"middle\" x=\"2334\" y=\"-542.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.5</text>\n",
       "<text text-anchor=\"middle\" x=\"2334\" y=\"-527.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 2</text>\n",
       "<text text-anchor=\"middle\" x=\"2334\" y=\"-512.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [1, 1]</text>\n",
       "</g>\n",
       "<!-- 77&#45;&gt;79 -->\n",
       "<g id=\"edge79\" class=\"edge\">\n",
       "<title>77&#45;&gt;79</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M2324.5841,-608.9465C2325.6961,-600.6863 2326.9007,-591.7374 2328.0651,-583.0875\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"2331.5401,-583.5076 2329.4056,-573.13 2324.6027,-582.5736 2331.5401,-583.5076\"/>\n",
       "</g>\n",
       "<!-- 80 -->\n",
       "<g id=\"node81\" class=\"node\">\n",
       "<title>80</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"2386,-461.5 2272,-461.5 2272,-408.5 2386,-408.5 2386,-461.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"2329\" y=\"-446.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"2329\" y=\"-431.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 1</text>\n",
       "<text text-anchor=\"middle\" x=\"2329\" y=\"-416.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 1]</text>\n",
       "</g>\n",
       "<!-- 79&#45;&gt;80 -->\n",
       "<g id=\"edge80\" class=\"edge\">\n",
       "<title>79&#45;&gt;80</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M2332.3628,-504.9465C2331.8491,-494.2621 2331.2801,-482.4254 2330.7584,-471.5742\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"2334.2523,-471.3625 2330.2761,-461.5422 2327.2604,-471.6987 2334.2523,-471.3625\"/>\n",
       "</g>\n",
       "<!-- 81 -->\n",
       "<g id=\"node82\" class=\"node\">\n",
       "<title>81</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"2518,-461.5 2404,-461.5 2404,-408.5 2518,-408.5 2518,-461.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"2461\" y=\"-446.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"2461\" y=\"-431.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 1</text>\n",
       "<text text-anchor=\"middle\" x=\"2461\" y=\"-416.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [1, 0]</text>\n",
       "</g>\n",
       "<!-- 79&#45;&gt;81 -->\n",
       "<g id=\"edge81\" class=\"edge\">\n",
       "<title>79&#45;&gt;81</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M2375.5845,-504.9465C2390.1115,-493.0504 2406.3827,-479.726 2420.7951,-467.9237\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"2423.0686,-470.5858 2428.5879,-461.5422 2418.6335,-465.17 2423.0686,-470.5858\"/>\n",
       "</g>\n",
       "<!-- 83 -->\n",
       "<g id=\"node84\" class=\"node\">\n",
       "<title>83</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"2529.5,-677 2402.5,-677 2402.5,-609 2529.5,-609 2529.5,-677\"/>\n",
       "<text text-anchor=\"middle\" x=\"2466\" y=\"-661.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[13] &lt;= 0.241</text>\n",
       "<text text-anchor=\"middle\" x=\"2466\" y=\"-646.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.298</text>\n",
       "<text text-anchor=\"middle\" x=\"2466\" y=\"-631.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 11</text>\n",
       "<text text-anchor=\"middle\" x=\"2466\" y=\"-616.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [9, 2]</text>\n",
       "</g>\n",
       "<!-- 82&#45;&gt;83 -->\n",
       "<g id=\"edge83\" class=\"edge\">\n",
       "<title>82&#45;&gt;83</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M2466,-712.9465C2466,-704.776 2466,-695.9318 2466,-687.3697\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"2469.5001,-687.13 2466,-677.13 2462.5001,-687.13 2469.5001,-687.13\"/>\n",
       "</g>\n",
       "<!-- 88 -->\n",
       "<g id=\"node89\" class=\"node\">\n",
       "<title>88</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"2662,-669.5 2548,-669.5 2548,-616.5 2662,-616.5 2662,-669.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"2605\" y=\"-654.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"2605\" y=\"-639.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 4</text>\n",
       "<text text-anchor=\"middle\" x=\"2605\" y=\"-624.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 4]</text>\n",
       "</g>\n",
       "<!-- 82&#45;&gt;88 -->\n",
       "<g id=\"edge88\" class=\"edge\">\n",
       "<title>82&#45;&gt;88</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M2511.5138,-712.9465C2527.5606,-700.9403 2545.5519,-687.4791 2561.4339,-675.5962\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"2563.6152,-678.3354 2569.5254,-669.5422 2559.4217,-672.7306 2563.6152,-678.3354\"/>\n",
       "</g>\n",
       "<!-- 84 -->\n",
       "<g id=\"node85\" class=\"node\">\n",
       "<title>84</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"2530,-565.5 2416,-565.5 2416,-512.5 2530,-512.5 2530,-565.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"2473\" y=\"-550.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"2473\" y=\"-535.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 8</text>\n",
       "<text text-anchor=\"middle\" x=\"2473\" y=\"-520.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [8, 0]</text>\n",
       "</g>\n",
       "<!-- 83&#45;&gt;84 -->\n",
       "<g id=\"edge84\" class=\"edge\">\n",
       "<title>83&#45;&gt;84</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M2468.2921,-608.9465C2469.0112,-598.2621 2469.8079,-586.4254 2470.5383,-575.5742\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"2474.034,-575.7547 2471.2135,-565.5422 2467.0498,-575.2845 2474.034,-575.7547\"/>\n",
       "</g>\n",
       "<!-- 85 -->\n",
       "<g id=\"node86\" class=\"node\">\n",
       "<title>85</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"2662,-573 2548,-573 2548,-505 2662,-505 2662,-573\"/>\n",
       "<text text-anchor=\"middle\" x=\"2605\" y=\"-557.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[65] &lt;= 0.5</text>\n",
       "<text text-anchor=\"middle\" x=\"2605\" y=\"-542.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.444</text>\n",
       "<text text-anchor=\"middle\" x=\"2605\" y=\"-527.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 3</text>\n",
       "<text text-anchor=\"middle\" x=\"2605\" y=\"-512.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [1, 2]</text>\n",
       "</g>\n",
       "<!-- 83&#45;&gt;85 -->\n",
       "<g id=\"edge85\" class=\"edge\">\n",
       "<title>83&#45;&gt;85</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M2511.5138,-608.9465C2524.234,-599.4293 2538.176,-588.9978 2551.3206,-579.163\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"2553.4738,-581.9232 2559.3839,-573.13 2549.2802,-576.3184 2553.4738,-581.9232\"/>\n",
       "</g>\n",
       "<!-- 86 -->\n",
       "<g id=\"node87\" class=\"node\">\n",
       "<title>86</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"2657,-461.5 2543,-461.5 2543,-408.5 2657,-408.5 2657,-461.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"2600\" y=\"-446.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"2600\" y=\"-431.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 2</text>\n",
       "<text text-anchor=\"middle\" x=\"2600\" y=\"-416.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 2]</text>\n",
       "</g>\n",
       "<!-- 85&#45;&gt;86 -->\n",
       "<g id=\"edge86\" class=\"edge\">\n",
       "<title>85&#45;&gt;86</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M2603.3628,-504.9465C2602.8491,-494.2621 2602.2801,-482.4254 2601.7584,-471.5742\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"2605.2523,-471.3625 2601.2761,-461.5422 2598.2604,-471.6987 2605.2523,-471.3625\"/>\n",
       "</g>\n",
       "<!-- 87 -->\n",
       "<g id=\"node88\" class=\"node\">\n",
       "<title>87</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"2789,-461.5 2675,-461.5 2675,-408.5 2789,-408.5 2789,-461.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"2732\" y=\"-446.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"2732\" y=\"-431.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 1</text>\n",
       "<text text-anchor=\"middle\" x=\"2732\" y=\"-416.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [1, 0]</text>\n",
       "</g>\n",
       "<!-- 85&#45;&gt;87 -->\n",
       "<g id=\"edge87\" class=\"edge\">\n",
       "<title>85&#45;&gt;87</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M2646.5845,-504.9465C2661.1115,-493.0504 2677.3827,-479.726 2691.7951,-467.9237\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"2694.0686,-470.5858 2699.5879,-461.5422 2689.6335,-465.17 2694.0686,-470.5858\"/>\n",
       "</g>\n",
       "<!-- 91 -->\n",
       "<g id=\"node92\" class=\"node\">\n",
       "<title>91</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"2738.5,-877.5 2615.5,-877.5 2615.5,-824.5 2738.5,-824.5 2738.5,-877.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"2677\" y=\"-862.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"2677\" y=\"-847.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 19</text>\n",
       "<text text-anchor=\"middle\" x=\"2677\" y=\"-832.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [19, 0]</text>\n",
       "</g>\n",
       "<!-- 90&#45;&gt;91 -->\n",
       "<g id=\"edge91\" class=\"edge\">\n",
       "<title>90&#45;&gt;91</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M2677,-920.9465C2677,-910.2621 2677,-898.4254 2677,-887.5742\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"2680.5001,-887.5421 2677,-877.5422 2673.5001,-887.5422 2680.5001,-887.5421\"/>\n",
       "</g>\n",
       "<!-- 92 -->\n",
       "<g id=\"node93\" class=\"node\">\n",
       "<title>92</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"2914.5,-885 2787.5,-885 2787.5,-817 2914.5,-817 2914.5,-885\"/>\n",
       "<text text-anchor=\"middle\" x=\"2851\" y=\"-869.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[10] &lt;= 0.357</text>\n",
       "<text text-anchor=\"middle\" x=\"2851\" y=\"-854.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.498</text>\n",
       "<text text-anchor=\"middle\" x=\"2851\" y=\"-839.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 17</text>\n",
       "<text text-anchor=\"middle\" x=\"2851\" y=\"-824.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [8, 9]</text>\n",
       "</g>\n",
       "<!-- 90&#45;&gt;92 -->\n",
       "<g id=\"edge92\" class=\"edge\">\n",
       "<title>90&#45;&gt;92</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M2733.9741,-920.9465C2750.3478,-911.1599 2768.3389,-900.4066 2785.1989,-890.3294\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"2787.1099,-893.2648 2793.8979,-885.13 2783.5186,-887.2562 2787.1099,-893.2648\"/>\n",
       "</g>\n",
       "<!-- 93 -->\n",
       "<g id=\"node94\" class=\"node\">\n",
       "<title>93</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"2910,-781 2792,-781 2792,-713 2910,-713 2910,-781\"/>\n",
       "<text text-anchor=\"middle\" x=\"2851\" y=\"-765.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[4] &lt;= 0.468</text>\n",
       "<text text-anchor=\"middle\" x=\"2851\" y=\"-750.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.346</text>\n",
       "<text text-anchor=\"middle\" x=\"2851\" y=\"-735.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 9</text>\n",
       "<text text-anchor=\"middle\" x=\"2851\" y=\"-720.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [7, 2]</text>\n",
       "</g>\n",
       "<!-- 92&#45;&gt;93 -->\n",
       "<g id=\"edge93\" class=\"edge\">\n",
       "<title>92&#45;&gt;93</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M2851,-816.9465C2851,-808.776 2851,-799.9318 2851,-791.3697\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"2854.5001,-791.13 2851,-781.13 2847.5001,-791.13 2854.5001,-791.13\"/>\n",
       "</g>\n",
       "<!-- 98 -->\n",
       "<g id=\"node99\" class=\"node\">\n",
       "<title>98</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"3058,-781 2944,-781 2944,-713 3058,-713 3058,-781\"/>\n",
       "<text text-anchor=\"middle\" x=\"3001\" y=\"-765.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[45] &lt;= 0.5</text>\n",
       "<text text-anchor=\"middle\" x=\"3001\" y=\"-750.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.219</text>\n",
       "<text text-anchor=\"middle\" x=\"3001\" y=\"-735.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 8</text>\n",
       "<text text-anchor=\"middle\" x=\"3001\" y=\"-720.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [1, 7]</text>\n",
       "</g>\n",
       "<!-- 92&#45;&gt;98 -->\n",
       "<g id=\"edge98\" class=\"edge\">\n",
       "<title>92&#45;&gt;98</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M2900.1156,-816.9465C2913.9719,-807.3395 2929.1717,-796.8009 2943.4739,-786.8848\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"2945.5503,-789.7041 2951.774,-781.13 2941.5618,-783.9515 2945.5503,-789.7041\"/>\n",
       "</g>\n",
       "<!-- 94 -->\n",
       "<g id=\"node95\" class=\"node\">\n",
       "<title>94</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"2794,-677 2680,-677 2680,-609 2794,-609 2794,-677\"/>\n",
       "<text text-anchor=\"middle\" x=\"2737\" y=\"-661.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[48] &lt;= 0.5</text>\n",
       "<text text-anchor=\"middle\" x=\"2737\" y=\"-646.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.219</text>\n",
       "<text text-anchor=\"middle\" x=\"2737\" y=\"-631.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 8</text>\n",
       "<text text-anchor=\"middle\" x=\"2737\" y=\"-616.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [7, 1]</text>\n",
       "</g>\n",
       "<!-- 93&#45;&gt;94 -->\n",
       "<g id=\"edge94\" class=\"edge\">\n",
       "<title>93&#45;&gt;94</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M2813.6721,-712.9465C2803.535,-703.6986 2792.4517,-693.5876 2781.9417,-683.9994\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"2784.1583,-681.284 2774.4117,-677.13 2779.4405,-686.4553 2784.1583,-681.284\"/>\n",
       "</g>\n",
       "<!-- 97 -->\n",
       "<g id=\"node98\" class=\"node\">\n",
       "<title>97</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"2926,-669.5 2812,-669.5 2812,-616.5 2926,-616.5 2926,-669.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"2869\" y=\"-654.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"2869\" y=\"-639.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 1</text>\n",
       "<text text-anchor=\"middle\" x=\"2869\" y=\"-624.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 1]</text>\n",
       "</g>\n",
       "<!-- 93&#45;&gt;97 -->\n",
       "<g id=\"edge97\" class=\"edge\">\n",
       "<title>93&#45;&gt;97</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M2856.8939,-712.9465C2858.7431,-702.2621 2860.7918,-690.4254 2862.6698,-679.5742\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"2866.1494,-679.9926 2864.4062,-669.5422 2859.2519,-678.7987 2866.1494,-679.9926\"/>\n",
       "</g>\n",
       "<!-- 95 -->\n",
       "<g id=\"node96\" class=\"node\">\n",
       "<title>95</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"2794,-565.5 2680,-565.5 2680,-512.5 2794,-512.5 2794,-565.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"2737\" y=\"-550.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"2737\" y=\"-535.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 7</text>\n",
       "<text text-anchor=\"middle\" x=\"2737\" y=\"-520.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [7, 0]</text>\n",
       "</g>\n",
       "<!-- 94&#45;&gt;95 -->\n",
       "<g id=\"edge95\" class=\"edge\">\n",
       "<title>94&#45;&gt;95</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M2737,-608.9465C2737,-598.2621 2737,-586.4254 2737,-575.5742\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"2740.5001,-575.5421 2737,-565.5422 2733.5001,-575.5422 2740.5001,-575.5421\"/>\n",
       "</g>\n",
       "<!-- 96 -->\n",
       "<g id=\"node97\" class=\"node\">\n",
       "<title>96</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"2926,-565.5 2812,-565.5 2812,-512.5 2926,-512.5 2926,-565.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"2869\" y=\"-550.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"2869\" y=\"-535.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 1</text>\n",
       "<text text-anchor=\"middle\" x=\"2869\" y=\"-520.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 1]</text>\n",
       "</g>\n",
       "<!-- 94&#45;&gt;96 -->\n",
       "<g id=\"edge96\" class=\"edge\">\n",
       "<title>94&#45;&gt;96</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M2780.2217,-608.9465C2795.3206,-597.0504 2812.2324,-583.726 2827.2122,-571.9237\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"2829.623,-574.4802 2835.3119,-565.5422 2825.2909,-568.9817 2829.623,-574.4802\"/>\n",
       "</g>\n",
       "<!-- 99 -->\n",
       "<g id=\"node100\" class=\"node\">\n",
       "<title>99</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"3058,-669.5 2944,-669.5 2944,-616.5 3058,-616.5 3058,-669.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"3001\" y=\"-654.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"3001\" y=\"-639.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 6</text>\n",
       "<text text-anchor=\"middle\" x=\"3001\" y=\"-624.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 6]</text>\n",
       "</g>\n",
       "<!-- 98&#45;&gt;99 -->\n",
       "<g id=\"edge99\" class=\"edge\">\n",
       "<title>98&#45;&gt;99</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M3001,-712.9465C3001,-702.2621 3001,-690.4254 3001,-679.5742\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"3004.5001,-679.5421 3001,-669.5422 2997.5001,-679.5422 3004.5001,-679.5421\"/>\n",
       "</g>\n",
       "<!-- 100 -->\n",
       "<g id=\"node101\" class=\"node\">\n",
       "<title>100</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"3190,-677 3076,-677 3076,-609 3190,-609 3190,-677\"/>\n",
       "<text text-anchor=\"middle\" x=\"3133\" y=\"-661.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">x[68] &lt;= 0.5</text>\n",
       "<text text-anchor=\"middle\" x=\"3133\" y=\"-646.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.5</text>\n",
       "<text text-anchor=\"middle\" x=\"3133\" y=\"-631.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 2</text>\n",
       "<text text-anchor=\"middle\" x=\"3133\" y=\"-616.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [1, 1]</text>\n",
       "</g>\n",
       "<!-- 98&#45;&gt;100 -->\n",
       "<g id=\"edge100\" class=\"edge\">\n",
       "<title>98&#45;&gt;100</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M3044.2217,-712.9465C3056.1874,-703.519 3069.2915,-693.1946 3081.6704,-683.4415\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"3083.9923,-686.068 3089.6812,-677.13 3079.6602,-680.5695 3083.9923,-686.068\"/>\n",
       "</g>\n",
       "<!-- 101 -->\n",
       "<g id=\"node102\" class=\"node\">\n",
       "<title>101</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"3058,-565.5 2944,-565.5 2944,-512.5 3058,-512.5 3058,-565.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"3001\" y=\"-550.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"3001\" y=\"-535.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 1</text>\n",
       "<text text-anchor=\"middle\" x=\"3001\" y=\"-520.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [1, 0]</text>\n",
       "</g>\n",
       "<!-- 100&#45;&gt;101 -->\n",
       "<g id=\"edge101\" class=\"edge\">\n",
       "<title>100&#45;&gt;101</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M3089.7783,-608.9465C3074.6794,-597.0504 3057.7676,-583.726 3042.7878,-571.9237\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"3044.7091,-568.9817 3034.6881,-565.5422 3040.377,-574.4802 3044.7091,-568.9817\"/>\n",
       "</g>\n",
       "<!-- 102 -->\n",
       "<g id=\"node103\" class=\"node\">\n",
       "<title>102</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"3190,-565.5 3076,-565.5 3076,-512.5 3190,-512.5 3190,-565.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"3133\" y=\"-550.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"middle\" x=\"3133\" y=\"-535.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 1</text>\n",
       "<text text-anchor=\"middle\" x=\"3133\" y=\"-520.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 1]</text>\n",
       "</g>\n",
       "<!-- 100&#45;&gt;102 -->\n",
       "<g id=\"edge102\" class=\"edge\">\n",
       "<title>100&#45;&gt;102</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M3133,-608.9465C3133,-598.2621 3133,-586.4254 3133,-575.5742\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"3136.5001,-575.5421 3133,-565.5422 3129.5001,-575.5422 3136.5001,-575.5421\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.sources.Source at 0x148afe6fb490>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Source\n",
    "\n",
    "# Load the DOT file\n",
    "with open('tree.dot', 'r') as file:\n",
    "    dot_data = file.read()\n",
    "\n",
    "# Create a Source object from the DOT data\n",
    "src = Source(dot_data)\n",
    "\n",
    "# Display the graph\n",
    "src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d91af10c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tree.png.png'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_path = 'tree.png'\n",
    "src.render(filename=output_path, format='png', cleanup=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
